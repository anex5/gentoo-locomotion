--- a/media/audio/alive_checker.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/alive_checker.cc	2019-05-17 18:53:34.016000000 +0300
@@ -7,7 +7,6 @@
 #include <utility>
 
 #include "base/bind.h"
-#include "base/bind_helpers.h"
 #include "base/message_loop/message_loop_current.h"
 
 namespace media {
--- a/media/audio/alsa/alsa_output.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/alsa/alsa_output.cc	2019-05-17 18:53:34.016000000 +0300
@@ -538,9 +538,9 @@
     next_fill_time = base::TimeDelta::FromMilliseconds(10);
   }
 
-  task_runner_->PostDelayedTask(FROM_HERE,
-                                base::BindOnce(&AlsaPcmOutputStream::WriteTask,
-                                               weak_factory_.GetWeakPtr()),
+  task_runner_->PostDelayedTask(
+      FROM_HERE,
+      base::Bind(&AlsaPcmOutputStream::WriteTask, weak_factory_.GetWeakPtr()),
                                 next_fill_time);
 }
 
--- a/media/audio/alsa/alsa_output.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/alsa/alsa_output.h	2019-05-17 18:53:34.016000000 +0300
@@ -226,6 +226,6 @@
 MEDIA_EXPORT std::ostream& operator<<(std::ostream& os,
                                       AlsaPcmOutputStream::InternalState);
 
-}  // namespace media
+};  // namespace media
 
 #endif  // MEDIA_AUDIO_ALSA_ALSA_OUTPUT_H_
--- a/media/audio/android/audio_android_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/android/audio_android_unittest.cc	2019-05-17 18:53:34.020000000 +0300
@@ -450,8 +450,10 @@
           base::WaitableEvent::InitialState::NOT_SIGNALED);
       audio_manager()->GetTaskRunner()->PostTask(
           FROM_HERE,
-          base::BindOnce(&AudioAndroidOutputTest::RunOnAudioThreadImpl,
-                         base::Unretained(this), closure, &event));
+          base::Bind(&AudioAndroidOutputTest::RunOnAudioThreadImpl,
+                     base::Unretained(this),
+                     closure,
+                     &event));
       event.Wait();
     } else {
       closure.Run();
@@ -972,7 +974,7 @@
   StopAndCloseAudioInputStreamOnAudioThread();
 }
 
-INSTANTIATE_TEST_SUITE_P(AudioAndroidInputTest,
+INSTANTIATE_TEST_CASE_P(AudioAndroidInputTest,
                          AudioAndroidInputTest,
                          testing::Bool());
 
--- a/media/audio/android/audio_manager_android.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/android/audio_manager_android.cc	2019-05-17 18:53:34.020000000 +0300
@@ -69,8 +69,8 @@
 
 void AudioManagerAndroid::InitializeIfNeeded() {
   GetTaskRunner()->PostTask(
-      FROM_HERE, base::BindOnce(base::IgnoreResult(
-                                    &AudioManagerAndroid::GetJavaAudioManager),
+      FROM_HERE,
+      base::Bind(base::IgnoreResult(&AudioManagerAndroid::GetJavaAudioManager),
                                 base::Unretained(this)));
 }
 
@@ -298,14 +298,20 @@
                                   const JavaParamRef<jobject>& obj,
                                   jboolean muted) {
   GetTaskRunner()->PostTask(
-      FROM_HERE, base::BindOnce(&AudioManagerAndroid::DoSetMuteOnAudioThread,
-                                base::Unretained(this), muted));
+      FROM_HERE,
+      base::Bind(
+          &AudioManagerAndroid::DoSetMuteOnAudioThread,
+          base::Unretained(this),
+          muted));
 }
 
 void AudioManagerAndroid::SetOutputVolumeOverride(double volume) {
   GetTaskRunner()->PostTask(
-      FROM_HERE, base::BindOnce(&AudioManagerAndroid::DoSetVolumeOnAudioThread,
-                                base::Unretained(this), volume));
+      FROM_HERE,
+      base::Bind(
+          &AudioManagerAndroid::DoSetVolumeOnAudioThread,
+          base::Unretained(this),
+          volume));
 }
 
 bool AudioManagerAndroid::HasOutputVolumeOverride(double* out_volume) const {
--- a/media/audio/android/audio_track_output_stream.cc	2019-05-17 17:45:41.224000000 +0300
+++ b/media/audio/android/audio_track_output_stream.cc	2019-05-17 18:53:34.020000000 +0300
@@ -22,7 +22,7 @@
 namespace media {
 
 // Android audio format. For more information, please see:
-// https://developer.8n6r01d.qjz9zk/reference/android/media/AudioFormat.html
+// https://developer.android.com/reference/android/media/AudioFormat.html
 enum {
   kEncodingPcm16bit = 2,  // ENCODING_PCM_16BIT
   kEncodingAc3 = 5,       // ENCODING_AC3
@@ -117,11 +117,11 @@
 
   Java_AudioTrackOutputStream_setVolume(AttachCurrentThread(),
                                         j_audio_output_stream_, volume);
-}
+};
 
 void AudioTrackOutputStream::GetVolume(double* volume) {
   *volume = volume_;
-}
+};
 
 // AudioOutputStream::SourceCallback implementation methods called from Java.
 ScopedJavaLocalRef<jobject> AudioTrackOutputStream::OnMoreData(
--- a/media/audio/android/opensles_util.cc	2019-05-17 17:45:41.224000000 +0300
+++ b/media/audio/android/opensles_util.cc	2019-05-17 18:53:34.020000000 +0300
@@ -16,8 +16,8 @@
   (SL_ANDROID_SPEAKER_5DOT1 | SL_SPEAKER_SIDE_LEFT | SL_SPEAKER_SIDE_RIGHT)
 
 // Ported from:
-// https://android.9oo91esource.qjz9zk/platform/frameworks/wilhelm/+/refs/heads/master/bandroid/channels.h
-// https://android.9oo91esource.qjz9zk/platform/frameworks/wilhelm/+/refs/heads/master/bandroid/channels.c
+// https://android.googlesource.com/platform/frameworks/wilhelm/+/refs/heads/master/bandroid/channels.h
+// https://android.googlesource.com/platform/frameworks/wilhelm/+/refs/heads/master/bandroid/channels.c
 SLuint32 ChannelCountToSLESChannelMask(int channel_count) {
   if (channel_count > 2) {
     LOG(WARNING) << "Guessing channel layout for " << channel_count
--- a/media/audio/audio_debug_file_writer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_debug_file_writer_unittest.cc	2019-05-17 18:53:34.020000000 +0300
@@ -295,7 +295,7 @@
   debug_writer_.reset();
 }
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     AudioDebugFileWriterTest,
     AudioDebugFileWriterTest,
     // Using 10ms frames per buffer everywhere.
@@ -331,7 +331,7 @@
                         48000 / 100,
                         1500)));
 
-INSTANTIATE_TEST_SUITE_P(AudioDebugFileWriterBehavioralTest,
+INSTANTIATE_TEST_CASE_P(AudioDebugFileWriterBehavioralTest,
                          AudioDebugFileWriterBehavioralTest,
                          // Using 10ms frames per buffer everywhere.
                          testing::Values(
@@ -341,7 +341,7 @@
                                              44100 / 100,
                                              100)));
 
-INSTANTIATE_TEST_SUITE_P(AudioDebugFileWriterSingleThreadTest,
+INSTANTIATE_TEST_CASE_P(AudioDebugFileWriterSingleThreadTest,
                          AudioDebugFileWriterSingleThreadTest,
                          // Using 10ms frames per buffer everywhere.
                          testing::Values(
--- a/media/audio/audio_debug_recording_helper.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_debug_recording_helper.cc	2019-05-17 18:53:34.020000000 +0300
@@ -97,8 +97,8 @@
 
   task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&AudioDebugRecordingHelper::DoWrite,
-                     weak_factory_.GetWeakPtr(), std::move(audio_bus_copy)));
+      base::Bind(&AudioDebugRecordingHelper::DoWrite,
+                 weak_factory_.GetWeakPtr(), base::Passed(&audio_bus_copy)));
 }
 
 void AudioDebugRecordingHelper::DoWrite(std::unique_ptr<media::AudioBus> data) {
--- a/media/audio/audio_debug_recording_session_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_debug_recording_session_impl.cc	2019-05-17 18:53:34.020000000 +0300
@@ -29,9 +29,9 @@
 namespace {
 
 #if defined(OS_WIN)
-#define NumberToStringType base::NumberToString16
+#define IntToStringType base::IntToString16
 #else
-#define NumberToStringType base::NumberToString
+#define IntToStringType base::IntToString
 #endif
 
 bool StreamTypeToStringType(AudioDebugRecordingStreamType stream_type,
@@ -68,7 +68,7 @@
                                              base::File::FLAG_WRITE);
           },
           debug_recording_file_path.AddExtension(stream_type_str)
-              .AddExtension(NumberToStringType(id))
+              .AddExtension(IntToStringType(id))
               .AddExtension(FILE_PATH_LITERAL("wav"))),
       std::move(reply_callback));
 }
--- a/media/audio/audio_debug_recording_session_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_debug_recording_session_impl_unittest.cc	2019-05-17 18:53:34.020000000 +0300
@@ -22,9 +22,9 @@
 namespace {
 
 #if defined(OS_WIN)
-#define NumberToStringType base::NumberToString16
+#define IntToStringType base::IntToString16
 #else
-#define NumberToStringType base::NumberToString
+#define IntToStringType base::IntToString
 #endif
 
 const base::FilePath::CharType kBaseFileName[] =
@@ -71,7 +71,7 @@
   base::FilePath GetFileName(const base::FilePath::StringType& stream_type,
                              uint32_t id) {
     return base_file_path_.AddExtension(stream_type)
-        .AddExtension(NumberToStringType(id))
+        .AddExtension(IntToStringType(id))
         .AddExtension(kWavExtension);
   }
 
--- a/media/audio/audio_features.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_features.cc	2019-05-17 18:53:34.020000000 +0300
@@ -12,6 +12,10 @@
                                             base::FEATURE_DISABLED_BY_DEFAULT};
 
 #if defined(OS_CHROMEOS)
+// Allows experimentally enables mediaDevices.enumerateDevices() on ChromeOS.
+// Default disabled (crbug.com/554168).
+const base::Feature kEnumerateAudioDevices{"EnumerateAudioDevices",
+                                           base::FEATURE_ENABLED_BY_DEFAULT};
 const base::Feature kCrOSSystemAEC{"CrOSSystemAEC",
                                    base::FEATURE_ENABLED_BY_DEFAULT};
 const base::Feature kCrOSSystemAECDeactivatedGroups{
--- a/media/audio/audio_features.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_features.h	2019-05-17 18:53:34.024000000 +0300
@@ -14,6 +14,7 @@
 MEDIA_EXPORT extern const base::Feature kDumpOnAudioServiceHang;
 
 #if defined(OS_CHROMEOS)
+MEDIA_EXPORT extern const base::Feature kEnumerateAudioDevices;
 MEDIA_EXPORT extern const base::Feature kCrOSSystemAEC;
 MEDIA_EXPORT extern const base::Feature kCrOSSystemAECDeactivatedGroups;
 #endif
--- a/media/audio/audio_input_controller_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_input_controller_unittest.cc	2019-05-17 18:53:34.024000000 +0300
@@ -9,7 +9,6 @@
 #include "base/macros.h"
 #include "base/message_loop/message_loop.h"
 #include "base/run_loop.h"
-#include "base/timer/timer.h"
 #include "media/audio/audio_manager.h"
 #include "media/audio/fake_audio_input_stream.h"
 #include "media/audio/test_audio_thread.h"
@@ -299,6 +298,6 @@
   CloseAudioController();
 }
 
-INSTANTIATE_TEST_SUITE_P(SyncAsync, AudioInputControllerTest, testing::Bool());
+INSTANTIATE_TEST_CASE_P(SyncAsync, AudioInputControllerTest, testing::Bool());
 
 }  // namespace media
--- a/media/audio/audio_input_stream_data_interceptor_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_input_stream_data_interceptor_unittest.cc	2019-05-17 18:53:34.024000000 +0300
@@ -6,7 +6,6 @@
 
 #include <utility>
 
-#include "base/bind.h"
 #include "base/callback.h"
 #include "base/memory/ptr_util.h"
 #include "media/audio/audio_debug_recording_helper.h"
--- a/media/audio/audio_output_controller_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_output_controller_unittest.cc	2019-05-17 18:53:34.028000000 +0300
@@ -520,6 +520,6 @@
   Close();
 }
 
-INSTANTIATE_TEST_SUITE_P(AOC, AudioOutputControllerTest, Bool());
+INSTANTIATE_TEST_CASE_P(AOC, AudioOutputControllerTest, Bool());
 
 }  // namespace media
--- a/media/audio/audio_output_device_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_output_device_unittest.cc	2019-05-17 18:53:34.028000000 +0300
@@ -9,7 +9,6 @@
 #include <utility>
 #include <vector>
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/callback.h"
 #include "base/macros.h"
--- a/media/audio/audio_output_proxy_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_output_proxy_unittest.cc	2019-05-17 18:53:34.028000000 +0300
@@ -8,7 +8,6 @@
 #include <string>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/message_loop/message_loop.h"
 #include "base/run_loop.h"
 #include "base/single_thread_task_runner.h"
--- a/media/audio/audio_power_monitor_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_power_monitor_unittest.cc	2019-05-17 18:53:34.028000000 +0300
@@ -273,55 +273,30 @@
   0.25f, -0.25f, 0.5f, -0.5f, 0.75f, -0.75f, 1.0f, -1.0f
 };
 
-INSTANTIATE_TEST_SUITE_P(
-    Scenarios,
-    AudioPowerMonitorTest,
+INSTANTIATE_TEST_CASE_P(
+    Scenarios, AudioPowerMonitorTest,
     ::testing::Values(
         TestScenario(kMonoSilentNoise, 1, 2, -40, false),
-        TestScenario(kMonoMaxAmplitude,
-                     1,
-                     1,
-                     AudioPowerMonitor::max_power(),
-                     false),
-        TestScenario(kMonoMaxAmplitude2,
-                     1,
-                     2,
-                     AudioPowerMonitor::max_power(),
-                     false),
+         TestScenario(kMonoMaxAmplitude, 1, 1,
+                      AudioPowerMonitor::max_power(), false),
+         TestScenario(kMonoMaxAmplitude2, 1, 2,
+                      AudioPowerMonitor::max_power(), false),
         TestScenario(kMonoHalfMaxAmplitude, 1, 4, -6, false),
-        TestScenario(kMonoAmplitudeClipped,
-                     1,
-                     2,
-                     AudioPowerMonitor::max_power(),
-                     true),
-        TestScenario(kMonoMaxAmplitudeWithClip,
-                     1,
-                     4,
-                     AudioPowerMonitor::max_power(),
-                     true),
-        TestScenario(kMonoMaxAmplitudeWithClip2,
-                     1,
-                     4,
-                     AudioPowerMonitor::max_power(),
-                     true),
-        TestScenario(kMonoSilentNoise,
-                     1,
-                     2,
-                     AudioPowerMonitor::zero_power(),
-                     false)
-            .WithABadSample(std::numeric_limits<float>::infinity()),
-        TestScenario(kMonoHalfMaxAmplitude,
-                     1,
-                     4,
-                     AudioPowerMonitor::zero_power(),
-                     false)
-            .WithABadSample(std::numeric_limits<float>::quiet_NaN()),
+         TestScenario(kMonoAmplitudeClipped, 1, 2,
+                      AudioPowerMonitor::max_power(), true),
+         TestScenario(kMonoMaxAmplitudeWithClip, 1, 4,
+                      AudioPowerMonitor::max_power(), true),
+         TestScenario(kMonoMaxAmplitudeWithClip2, 1, 4,
+                      AudioPowerMonitor::max_power(), true),
+         TestScenario(kMonoSilentNoise, 1, 2,
+                      AudioPowerMonitor::zero_power(), false).
+             WithABadSample(std::numeric_limits<float>::infinity()),
+         TestScenario(kMonoHalfMaxAmplitude, 1, 4,
+                      AudioPowerMonitor::zero_power(), false).
+             WithABadSample(std::numeric_limits<float>::quiet_NaN()),
         TestScenario(kStereoSilentNoise, 2, 2, -46, false),
-        TestScenario(kStereoMaxAmplitude,
-                     2,
-                     2,
-                     AudioPowerMonitor::max_power(),
-                     false),
+         TestScenario(kStereoMaxAmplitude, 2, 2,
+                      AudioPowerMonitor::max_power(), false),
         TestScenario(kRightChannelMaxAmplitude, 2, 4, -3, false),
         TestScenario(kLeftChannelHalfMaxAmplitude, 2, 4, -9, false),
         TestScenario(kStereoMixed, 2, 4, -2, false),
--- a/media/audio/audio_source_parameters.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_source_parameters.h	2019-05-17 18:53:34.032000000 +0300
@@ -9,7 +9,7 @@
 
 #include "base/optional.h"
 #include "base/unguessable_token.h"
-#include "media/base/audio_processing.h"
+#include "media/audio/audio_processing.h"
 #include "media/base/media_export.h"
 
 namespace media {
--- a/media/audio/audio_sync_reader_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_sync_reader_unittest.cc	2019-05-17 18:53:34.032000000 +0300
@@ -10,7 +10,6 @@
 #include <type_traits>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/memory/shared_memory.h"
 #include "base/sync_socket.h"
 #include "base/test/scoped_task_environment.h"
@@ -103,7 +102,7 @@
   reader->Read(output_bus.get());
 }
 
-INSTANTIATE_TEST_SUITE_P(AudioSyncReaderTest,
+INSTANTIATE_TEST_CASE_P(AudioSyncReaderTest,
                          AudioSyncReaderBitstreamTest,
                          ::testing::ValuesIn(overflow_test_case_values));
 
--- a/media/audio/audio_system_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_system_impl.cc	2019-05-17 18:53:34.032000000 +0300
@@ -6,7 +6,6 @@
 
 #include <utility>
 
-#include "base/bind.h"
 #include "base/memory/ptr_util.h"
 #include "base/single_thread_task_runner.h"
 #include "base/task_runner_util.h"
--- a/media/audio/audio_system_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_system_impl_unittest.cc	2019-05-17 18:53:34.032000000 +0300
@@ -45,7 +45,7 @@
     testing::Types<AudioSystemImplTestBase<false>,
                    AudioSystemImplTestBase<true>>;
 
-INSTANTIATE_TYPED_TEST_SUITE_P(AudioSystemImpl,
+INSTANTIATE_TYPED_TEST_CASE_P(AudioSystemImpl,
                                AudioSystemTestTemplate,
                                AudioSystemTestBaseVariations);
 
--- a/media/audio/audio_system_test_util.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/audio_system_test_util.h	2019-05-17 18:53:34.032000000 +0300
@@ -141,7 +141,7 @@
   DISALLOW_COPY_AND_ASSIGN(AudioSystemTestTemplate);
 };
 
-TYPED_TEST_SUITE_P(AudioSystemTestTemplate);
+TYPED_TEST_CASE_P(AudioSystemTestTemplate);
 
 TYPED_TEST_P(AudioSystemTestTemplate, GetInputStreamParametersNormal) {
   base::RunLoop wait_loop;
@@ -340,7 +340,7 @@
   wait_loop.Run();
 }
 
-REGISTER_TYPED_TEST_SUITE_P(
+REGISTER_TYPED_TEST_CASE_P(
     AudioSystemTestTemplate,
     GetInputStreamParametersNormal,
     GetInputStreamParametersNoDevice,
--- a/media/audio/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/BUILD.gn	2019-05-17 18:53:34.016000000 +0300
@@ -105,6 +105,8 @@
     "audio_output_stream_sink.h",
     "audio_power_monitor.cc",
     "audio_power_monitor.h",
+    "audio_processing.cc",
+    "audio_processing.h",
     "audio_sink_parameters.cc",
     "audio_sink_parameters.h",
     "audio_source_diverter.h",
--- a/media/audio/cras/audio_manager_cras.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/cras/audio_manager_cras.cc	2019-05-17 18:53:34.032000000 +0300
@@ -10,7 +10,6 @@
 #include <map>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/command_line.h"
 #include "base/environment.h"
 #include "base/logging.h"
@@ -133,6 +132,7 @@
 
   device_names->push_back(AudioDeviceName::CreateDefault());
 
+  if (base::FeatureList::IsEnabled(features::kEnumerateAudioDevices)) {
   chromeos::AudioDeviceList devices;
   GetAudioDevices(&devices);
 
@@ -156,6 +156,7 @@
       ProcessVirtualDeviceName(device_names, item.second);
     }
   }
+  }
 }
 
 void AudioManagerCras::GetAudioInputDeviceNames(
@@ -214,6 +215,9 @@
 
 std::string AudioManagerCras::GetAssociatedOutputDeviceID(
     const std::string& input_device_id) {
+  if (!base::FeatureList::IsEnabled(features::kEnumerateAudioDevices))
+    return "";
+
   chromeos::AudioDeviceList devices;
   GetAudioDevices(&devices);
 
--- a/media/audio/fake_audio_input_stream.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/fake_audio_input_stream.cc	2019-05-17 18:53:34.036000000 +0300
@@ -54,7 +54,7 @@
 void FakeAudioInputStream::Start(AudioInputCallback* callback)  {
   DCHECK(audio_manager_->GetTaskRunner()->BelongsToCurrentThread());
   callback_ = callback;
-  fake_audio_worker_.Start(base::BindRepeating(
+  fake_audio_worker_.Start(base::Bind(
       &FakeAudioInputStream::ReadAudioFromSource, base::Unretained(this)));
 }
 
@@ -102,27 +102,16 @@
   // Not supported. Do nothing.
 }
 
-void FakeAudioInputStream::ReadAudioFromSource(base::TimeTicks ideal_time,
-                                               base::TimeTicks now) {
+void FakeAudioInputStream::ReadAudioFromSource() {
   DCHECK(audio_manager_->GetWorkerTaskRunner()->BelongsToCurrentThread());
   DCHECK(callback_);
 
   if (!audio_source_)
     audio_source_ = ChooseSource();
 
-  // This OnMoreData()/OnData() timing would never happen in a real system:
-  //
-  //   1. Real AudioSources would never be asked to generate audio that should
-  //      already be playing-out exactly at this very moment; they are asked to
-  //      do so for audio to be played-out in the future.
-  //   2. Real AudioInputStreams could never provide audio that is striking a
-  //      microphone element exactly at this very moment; they provide audio
-  //      that happened in the recent past.
-  //
-  // However, it would be pointless to add a FIFO queue here to delay the signal
-  // in this "fake" implementation. So, just hack the timing and carry-on.
-  audio_source_->OnMoreData(base::TimeDelta(), ideal_time, 0, audio_bus_.get());
-  callback_->OnData(audio_bus_.get(), ideal_time, 1.0);
+  audio_source_->OnMoreData(base::TimeDelta(), base::TimeTicks::Now(), 0,
+                            audio_bus_.get());
+  callback_->OnData(audio_bus_.get(), base::TimeTicks::Now(), 1.0);
 }
 
 using AudioSourceCallback = AudioOutputStream::AudioSourceCallback;
--- a/media/audio/fake_audio_input_stream.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/fake_audio_input_stream.h	2019-05-17 18:53:34.036000000 +0300
@@ -65,7 +65,7 @@
   ~FakeAudioInputStream() override;
 
   std::unique_ptr<AudioOutputStream::AudioSourceCallback> ChooseSource();
-  void ReadAudioFromSource(base::TimeTicks ideal_time, base::TimeTicks now);
+  void ReadAudioFromSource();
 
   AudioManagerBase* audio_manager_;
   AudioInputCallback* callback_;
--- a/media/audio/fake_audio_output_stream.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/fake_audio_output_stream.cc	2019-05-17 18:53:34.036000000 +0300
@@ -22,10 +22,10 @@
 FakeAudioOutputStream::FakeAudioOutputStream(AudioManagerBase* manager,
                                              const AudioParameters& params)
     : audio_manager_(manager),
-      fixed_data_delay_(FakeAudioWorker::ComputeFakeOutputDelay(params)),
       callback_(NULL),
       fake_worker_(manager->GetWorkerTaskRunner(), params),
-      audio_bus_(AudioBus::Create(params)) {}
+      audio_bus_(AudioBus::Create(params)) {
+}
 
 FakeAudioOutputStream::~FakeAudioOutputStream() {
   DCHECK(!callback_);
@@ -40,8 +40,8 @@
 void FakeAudioOutputStream::Start(AudioSourceCallback* callback)  {
   DCHECK(audio_manager_->GetTaskRunner()->BelongsToCurrentThread());
   callback_ = callback;
-  fake_worker_.Start(base::BindRepeating(&FakeAudioOutputStream::CallOnMoreData,
-                                         base::Unretained(this)));
+  fake_worker_.Start(base::Bind(
+      &FakeAudioOutputStream::CallOnMoreData, base::Unretained(this)));
 }
 
 void FakeAudioOutputStream::Stop() {
@@ -62,12 +62,9 @@
   *volume = 0;
 }
 
-void FakeAudioOutputStream::CallOnMoreData(base::TimeTicks ideal_time,
-                                           base::TimeTicks now) {
+void FakeAudioOutputStream::CallOnMoreData() {
   DCHECK(audio_manager_->GetWorkerTaskRunner()->BelongsToCurrentThread());
-  // Real streams provide small tweaks to their delay values, alongside the
-  // current system time; and so the same is done here.
-  callback_->OnMoreData(fixed_data_delay_ + (ideal_time - now), now, 0,
+  callback_->OnMoreData(base::TimeDelta(), base::TimeTicks::Now(), 0,
                         audio_bus_.get());
 }
 
--- a/media/audio/fake_audio_output_stream.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/fake_audio_output_stream.h	2019-05-17 18:53:34.036000000 +0300
@@ -40,13 +40,12 @@
   ~FakeAudioOutputStream() override;
 
   // Task that periodically calls OnMoreData() to consume audio data.
-  void CallOnMoreData(base::TimeTicks ideal_time, base::TimeTicks now);
+  void CallOnMoreData();
 
-  AudioManagerBase* const audio_manager_;
-  const base::TimeDelta fixed_data_delay_;
+  AudioManagerBase* audio_manager_;
   AudioSourceCallback* callback_;
   FakeAudioWorker fake_worker_;
-  const std::unique_ptr<AudioBus> audio_bus_;
+  std::unique_ptr<AudioBus> audio_bus_;
 
   DISALLOW_COPY_AND_ASSIGN(FakeAudioOutputStream);
 };
--- a/media/audio/fuchsia/audio_output_stream_fuchsia.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/fuchsia/audio_output_stream_fuchsia.cc	2019-05-17 18:53:34.036000000 +0300
@@ -6,8 +6,7 @@
 
 #include <zircon/syscalls.h>
 
-#include "base/bind.h"
-#include "base/fuchsia/service_directory_client.h"
+#include "base/fuchsia/component_context.h"
 #include "media/audio/fuchsia/audio_manager_fuchsia.h"
 #include "media/base/audio_sample_types.h"
 #include "media/base/audio_timestamp_helper.h"
@@ -36,7 +35,7 @@
 
   // Connect |audio_renderer_| to the audio service.
   fuchsia::media::AudioPtr audio_server =
-      base::fuchsia::ServiceDirectoryClient::ForCurrentProcess()
+      base::fuchsia::ComponentContext::GetDefault()
           ->ConnectToService<fuchsia::media::Audio>();
   audio_server->CreateAudioRenderer(audio_renderer_.NewRequest());
   audio_renderer_.set_error_handler(
--- a/media/audio/mac/audio_device_listener_mac_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/mac/audio_device_listener_mac_unittest.cc	2019-05-17 18:53:34.036000000 +0300
@@ -37,7 +37,7 @@
     // order to ensure we don't end up with unbalanced TaskObserver calls.
     message_loop_.task_runner()->PostTask(
         FROM_HERE,
-        base::BindOnce(&AudioDeviceListenerMacTest::DestroyDeviceListener,
+        base::Bind(&AudioDeviceListenerMacTest::DestroyDeviceListener,
                        base::Unretained(this)));
     base::RunLoop().RunUntilIdle();
   }
--- a/media/audio/mac/audio_input_mac.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/mac/audio_input_mac.cc	2019-05-17 18:53:34.036000000 +0300
@@ -6,7 +6,6 @@
 
 #include <CoreServices/CoreServices.h>
 
-#include "base/bind.h"
 #include "base/logging.h"
 #include "base/mac/mac_logging.h"
 #include "base/metrics/histogram_macros.h"
--- a/media/audio/mac/audio_low_latency_input_mac_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/mac/audio_low_latency_input_mac_unittest.cc	2019-05-17 18:53:34.036000000 +0300
@@ -6,7 +6,6 @@
 
 #include <memory>
 
-#include "base/bind.h"
 #include "base/environment.h"
 #include "base/memory/ptr_util.h"
 #include "base/message_loop/message_loop.h"
--- a/media/audio/mock_audio_manager.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/mock_audio_manager.cc	2019-05-17 18:53:34.040000000 +0300
@@ -6,7 +6,6 @@
 
 #include <utility>
 
-#include "base/bind.h"
 #include "base/callback.h"
 #include "base/logging.h"
 #include "media/audio/mock_audio_debug_recording_manager.h"
--- a/media/audio/null_audio_sink.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/null_audio_sink.cc	2019-05-17 18:53:34.040000000 +0300
@@ -27,7 +27,6 @@
                                RenderCallback* callback) {
   DCHECK(!started_);
   fake_worker_.reset(new FakeAudioWorker(task_runner_, params));
-  fixed_data_delay_ = FakeAudioWorker::ComputeFakeOutputDelay(params);
   audio_bus_ = AudioBus::Create(params);
   callback_ = callback;
   initialized_ = true;
@@ -55,8 +54,8 @@
   if (playing_)
     return;
 
-  fake_worker_->Start(
-      base::BindRepeating(&NullAudioSink::CallRender, base::Unretained(this)));
+  fake_worker_->Start(base::Bind(
+      &NullAudioSink::CallRender, base::Unretained(this)));
 
   playing_ = true;
 }
@@ -99,16 +98,11 @@
   std::move(callback).Run(OUTPUT_DEVICE_STATUS_ERROR_INTERNAL);
 }
 
-void NullAudioSink::CallRender(base::TimeTicks ideal_time,
-                               base::TimeTicks now) {
+void NullAudioSink::CallRender() {
   DCHECK(task_runner_->BelongsToCurrentThread());
 
-  // Since NullAudioSink is only used for cases where a real audio sink was not
-  // available, provide "idealized" delay-timing arguments. This will drive the
-  // smoothest playback (since video is sync'ed to audio). See
-  // content::AudioRendererImpl and media::AudioClock for further details.
-  int frames_received =
-      callback_->Render(fixed_data_delay_, ideal_time, 0, audio_bus_.get());
+  int frames_received = callback_->Render(
+      base::TimeDelta(), base::TimeTicks::Now(), 0, audio_bus_.get());
   if (!audio_hash_ || frames_received <= 0)
     return;
 
--- a/media/audio/null_audio_sink.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/null_audio_sink.h	2019-05-17 18:53:34.040000000 +0300
@@ -51,7 +51,7 @@
 
  private:
   // Task that periodically calls Render() to consume audio data.
-  void CallRender(base::TimeTicks ideal_time, base::TimeTicks now);
+  void CallRender();
 
   bool initialized_;
   bool started_;
@@ -63,7 +63,6 @@
 
   scoped_refptr<base::SingleThreadTaskRunner> task_runner_;
   std::unique_ptr<FakeAudioWorker> fake_worker_;
-  base::TimeDelta fixed_data_delay_;
   std::unique_ptr<AudioBus> audio_bus_;
 
   DISALLOW_COPY_AND_ASSIGN(NullAudioSink);
--- a/media/audio/OWNERS	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/OWNERS	2019-05-17 18:53:34.016000000 +0300
@@ -7,5 +7,6 @@
 
 # Mirroring (and related glue) OWNERS.
 miu@chromium.org
+xjz@chromium.org
 
 # COMPONENT: Blink>Media>Audio
--- a/media/audio/scoped_task_runner_observer.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/scoped_task_runner_observer.cc	2019-05-17 18:53:34.040000000 +0300
@@ -33,9 +33,8 @@
   } else {
     base::WaitableEvent event(base::WaitableEvent::ResetPolicy::AUTOMATIC,
                               base::WaitableEvent::InitialState::NOT_SIGNALED);
-    if (task_runner_->PostTask(
-            FROM_HERE,
-            base::BindOnce(&ScopedTaskRunnerObserver::ObserveLoopDestruction,
+    if (task_runner_->PostTask(FROM_HERE,
+            base::Bind(&ScopedTaskRunnerObserver::ObserveLoopDestruction,
                            base::Unretained(this), enable, &event))) {
       event.Wait();
     } else {
--- a/media/audio/simple_sources.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/simple_sources.cc	2019-05-17 18:53:34.040000000 +0300
@@ -263,7 +263,7 @@
   // Accumulate the time from the last beep.
   interval_from_last_beep_ += base::TimeTicks::Now() - last_callback_time_;
 
-  memset(buffer_.get(), 128, buffer_size_);
+  memset(buffer_.get(), 0, buffer_size_);
   bool should_beep = false;
   BeepContext* beep_context = GetBeepContext();
   if (beep_context->automatic_beep()) {
@@ -291,7 +291,7 @@
     int position = 0;
     while (position + high_bytes <= buffer_size_) {
       // Write high values first.
-      memset(buffer_.get() + position, 255, high_bytes);
+      memset(buffer_.get() + position, 128, high_bytes);
       // Then leave low values in the buffer with |high_bytes|.
       position += high_bytes * 2;
     }
--- a/media/audio/sounds/audio_stream_handler.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/sounds/audio_stream_handler.cc	2019-05-17 18:53:34.044000000 +0300
@@ -8,7 +8,6 @@
 #include <string>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/cancelable_callback.h"
 #include "base/logging.h"
 #include "base/macros.h"
@@ -137,7 +136,7 @@
     LOG(ERROR) << "Error during system sound reproduction.";
     audio_manager_->GetTaskRunner()->PostTask(
         FROM_HERE,
-        base::BindOnce(&AudioStreamContainer::Stop, base::Unretained(this)));
+        base::Bind(&AudioStreamContainer::Stop, base::Unretained(this)));
   }
 
   void StopStream() {
@@ -218,7 +217,8 @@
     return false;
 
   AudioManager::Get()->GetTaskRunner()->PostTask(
-      FROM_HERE, base::BindOnce(base::IgnoreResult(&AudioStreamContainer::Play),
+      FROM_HERE,
+      base::Bind(base::IgnoreResult(&AudioStreamContainer::Play),
                                 base::Unretained(stream_.get())));
   return true;
 }
@@ -230,8 +230,8 @@
     return;
 
   AudioManager::Get()->GetTaskRunner()->PostTask(
-      FROM_HERE, base::BindOnce(&AudioStreamContainer::Stop,
-                                base::Unretained(stream_.get())));
+      FROM_HERE,
+      base::Bind(&AudioStreamContainer::Stop, base::Unretained(stream_.get())));
 }
 
 base::TimeDelta AudioStreamHandler::duration() const {
--- a/media/audio/sounds/test_data.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/sounds/test_data.h	2019-05-17 18:53:34.044000000 +0300
@@ -24,16 +24,6 @@
     "data\x04\x00\x00\x00\x01\x00\x01\x00";
 const size_t kTestAudioDataSize = base::size(kTestAudioData) - 1;
 
-// Extensible format with 48kHz rate stereo 32 bit PCM samples
-const char kTestExtensibleAudioData[] =
-    "RIFF\x44\x00\x00\x00WAVEfmt \x28\x00\x00\x00"
-    "\xfe\xff\x02\x00\x80\xbb\x00\x00\x00\x77\x01\x00\x02\x00\x20\x00"
-    "\x16\x00\x20\x00\x00\x00\x00\x00"
-    "\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"
-    "data\x08\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00";
-const size_t kTestExtensibleAudioDataSize =
-    base::size(kTestExtensibleAudioData) - 1;
-
 class TestObserver : public AudioStreamHandler::TestObserver {
  public:
   TestObserver(const base::Closure& quit);
--- a/media/audio/sounds/wav_audio_handler.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/sounds/wav_audio_handler.cc	2019-05-17 18:53:34.044000000 +0300
@@ -31,19 +31,15 @@
 
 // The minimum size of 'fmt' chunk.
 const size_t kFmtChunkMinimumSize = 16;
-const size_t kFmtChunkExtensibleMinimumSize = 40;
 
 // The offsets of 'fmt' fields.
 const size_t kAudioFormatOffset = 0;
 const size_t kChannelOffset = 2;
 const size_t kSampleRateOffset = 4;
 const size_t kBitsPerSampleOffset = 14;
-const size_t kValidBitsPerSampleOffset = 18;
-const size_t kSubFormatOffset = 24;
 
 // Some constants for audio format.
 const int kAudioFormatPCM = 1;
-const int kAudioFormatExtensible = 0xfffe;
 
 // A convenience struct for passing WAV parameters around. AudioParameters is
 // too heavyweight for this. Keep this class internal to this implementation.
@@ -52,15 +48,11 @@
   uint16_t num_channels;
   uint32_t sample_rate;
   uint16_t bits_per_sample;
-  uint16_t valid_bits_per_sample;
-  bool is_extensible;
 };
 
 bool ParamsAreValid(const WavAudioParameters& params) {
   return (params.audio_format == kAudioFormatPCM && params.num_channels != 0u &&
-          params.sample_rate != 0u && params.bits_per_sample != 0u &&
-          (!params.is_extensible ||
-           params.valid_bits_per_sample == params.bits_per_sample));
+          params.sample_rate != 0u && params.bits_per_sample != 0u);
 }
 
 // Reads an integer from |data| with |offset|.
@@ -90,20 +82,6 @@
   params->num_channels = ReadInt<uint16_t>(data, kChannelOffset);
   params->sample_rate = ReadInt<uint32_t>(data, kSampleRateOffset);
   params->bits_per_sample = ReadInt<uint16_t>(data, kBitsPerSampleOffset);
-
-  if (params->audio_format == kAudioFormatExtensible) {
-    if (data.size() < kFmtChunkExtensibleMinimumSize) {
-      LOG(ERROR) << "Data size " << data.size() << " is too short.";
-      return false;
-    }
-
-    params->is_extensible = true;
-    params->audio_format = ReadInt<uint16_t>(data, kSubFormatOffset);
-    params->valid_bits_per_sample =
-        ReadInt<uint16_t>(data, kValidBitsPerSampleOffset);
-  } else {
-    params->is_extensible = false;
-  }
   return true;
 }
 
@@ -172,10 +150,7 @@
     LOG(ERROR) << "Format is invalid. "
                << "num_channels: " << params_out->num_channels << " "
                << "sample_rate: " << params_out->sample_rate << " "
-               << "bits_per_sample: " << params_out->bits_per_sample << " "
-               << "valid_bits_per_sample: " << params_out->valid_bits_per_sample
-               << " "
-               << "is_extensible: " << params_out->is_extensible;
+               << "bits_per_sample: " << params_out->bits_per_sample;
     return false;
   }
   return true;
--- a/media/audio/sounds/wav_audio_handler_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/sounds/wav_audio_handler_unittest.cc	2019-05-17 18:53:34.044000000 +0300
@@ -58,29 +58,6 @@
   ASSERT_EQ(static_cast<size_t>(handler->data().size()), bytes_written);
 }
 
-TEST(WavAudioHandlerTest, SampleExtensibleDataTest) {
-  std::string data(kTestExtensibleAudioData, kTestExtensibleAudioDataSize);
-  auto handler = WavAudioHandler::Create(data);
-  ASSERT_TRUE(handler);
-  ASSERT_EQ(2u, handler->num_channels());
-  ASSERT_EQ(32u, handler->bits_per_sample());
-  ASSERT_EQ(48000u, handler->sample_rate());
-  ASSERT_EQ(1u, handler->total_frames());
-  ASSERT_EQ(20u, handler->GetDuration().InMicroseconds());
-
-  ASSERT_EQ(8U, handler->data().size());
-  const char kData[] = "\x01\x00\x00\x00\x01\x00\x00\x00";
-  ASSERT_EQ(base::StringPiece(kData, base::size(kData) - 1), handler->data());
-
-  std::unique_ptr<AudioBus> bus =
-      AudioBus::Create(handler->num_channels(),
-                       handler->data().size() / handler->num_channels());
-
-  size_t bytes_written = 0u;
-  ASSERT_TRUE(handler->CopyTo(bus.get(), 0, &bytes_written));
-  ASSERT_EQ(static_cast<size_t>(handler->data().size()), bytes_written);
-}
-
 TEST(WavAudioHandlerTest, TestZeroChannelsIsNotValid) {
   // Read in the sample data and modify the channel field to hold |00|00|.
   std::string data(kTestAudioData, kTestAudioDataSize);
--- a/media/audio/virtual_audio_input_stream.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/virtual_audio_input_stream.cc	2019-05-17 18:53:34.044000000 +0300
@@ -54,8 +54,8 @@
 void VirtualAudioInputStream::Start(AudioInputCallback* callback) {
   DCHECK(thread_checker_.CalledOnValidThread());
   callback_ = callback;
-  fake_worker_.Start(base::BindRepeating(&VirtualAudioInputStream::PumpAudio,
-                                         base::Unretained(this)));
+  fake_worker_.Start(base::Bind(
+      &VirtualAudioInputStream::PumpAudio, base::Unretained(this)));
 }
 
 void VirtualAudioInputStream::Stop() {
@@ -99,8 +99,7 @@
   DCHECK_LE(0, num_attached_output_streams_);
 }
 
-void VirtualAudioInputStream::PumpAudio(base::TimeTicks ideal_time,
-                                        base::TimeTicks now) {
+void VirtualAudioInputStream::PumpAudio() {
   DCHECK(worker_task_runner_->BelongsToCurrentThread());
 
   {
@@ -111,7 +110,7 @@
   }
   // Because the audio is being looped-back, the delay since since it was
   // recorded is zero.
-  callback_->OnData(audio_bus_.get(), ideal_time, 1.0);
+  callback_->OnData(audio_bus_.get(), base::TimeTicks::Now(), 1.0);
 }
 
 void VirtualAudioInputStream::Close() {
--- a/media/audio/virtual_audio_input_stream.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/virtual_audio_input_stream.h	2019-05-17 18:53:34.044000000 +0300
@@ -80,7 +80,7 @@
   // Pulls audio data from all attached VirtualAudioOutputStreams, mixes and
   // converts the streams into one, and pushes the result to |callback_|.
   // Invoked on the worker thread.
-  void PumpAudio(base::TimeTicks ideal_time, base::TimeTicks now);
+  void PumpAudio();
 
   const scoped_refptr<base::SingleThreadTaskRunner> worker_task_runner_;
 
--- a/media/audio/virtual_audio_input_stream_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/virtual_audio_input_stream_unittest.cc	2019-05-17 18:53:34.044000000 +0300
@@ -233,7 +233,7 @@
                              base::WaitableEvent::InitialState::NOT_SIGNALED);
     audio_task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&base::WaitableEvent::Signal, base::Unretained(&done)));
+        base::Bind(&base::WaitableEvent::Signal, base::Unretained(&done)));
     done.Wait();
   }
 
@@ -353,7 +353,7 @@
   WaitUntilClosed();
 }
 
-INSTANTIATE_TEST_SUITE_P(SingleVersusMultithreaded,
+INSTANTIATE_TEST_CASE_P(SingleVersusMultithreaded,
                          VirtualAudioInputStreamTest,
                          ::testing::Values(false, true));
 
--- a/media/audio/win/audio_low_latency_input_win.cc	2019-05-17 17:45:41.228000000 +0300
+++ b/media/audio/win/audio_low_latency_input_win.cc	2019-05-17 18:53:34.044000000 +0300
@@ -85,42 +85,27 @@
 
   const SampleFormat kSampleFormat = kSampleFormatS16;
 
-  // The clients asks for an input stream specified by |params|. Start by
-  // setting up an input device format according to the same specification.
-  // If all goes well during the upcoming initialization, this format will not
-  // change. However, under some circumstances, minor changes can be required
-  // to fit the current input audio device. If so, a FIFO and/or and audio
-  // converter might be needed to ensure that the output format of this stream
-  // matches what the client asks for.
+  // Set up the desired output format specified by the client.
   DVLOG(1) << params.AsHumanReadableString();
-  WAVEFORMATEX* format = &input_format_.Format;
+  WAVEFORMATEX* format = &output_format_.Format;
   format->wFormatTag = WAVE_FORMAT_EXTENSIBLE;
   format->nChannels = params.channels();
   format->nSamplesPerSec = params.sample_rate();
   format->wBitsPerSample = SampleFormatToBitsPerChannel(kSampleFormat);
   format->nBlockAlign = (format->wBitsPerSample / 8) * format->nChannels;
   format->nAvgBytesPerSec = format->nSamplesPerSec * format->nBlockAlign;
-
-  // Add the parts which are unique to WAVE_FORMAT_EXTENSIBLE which can be
-  // required in combination with e.g. multi-channel microphone arrays.
   format->cbSize = sizeof(WAVEFORMATEXTENSIBLE) - sizeof(WAVEFORMATEX);
-  input_format_.Samples.wValidBitsPerSample = format->wBitsPerSample;
-  input_format_.dwChannelMask =
+
+  // Add the parts which are unique to WAVE_FORMAT_EXTENSIBLE.
+  output_format_.Samples.wValidBitsPerSample = format->wBitsPerSample;
+  output_format_.dwChannelMask =
       CoreAudioUtil::GetChannelConfig(device_id, eCapture);
-  input_format_.SubFormat = KSDATAFORMAT_SUBTYPE_PCM;
+  output_format_.SubFormat = KSDATAFORMAT_SUBTYPE_PCM;
 
-  // Set up the fixed output format based on |params|. Will not be changed and
-  // does not required an extended wave format structure since any multi-channel
-  // input will be converted to stereo.
-  output_format_.wFormatTag = WAVE_FORMAT_PCM;
-  output_format_.nChannels = format->nChannels;
-  ;
-  output_format_.nSamplesPerSec = format->nSamplesPerSec;
-  output_format_.wBitsPerSample = format->wBitsPerSample;
-  output_format_.nBlockAlign = format->nBlockAlign;
-  output_format_.nAvgBytesPerSec = format->nAvgBytesPerSec;
-  output_format_.cbSize = 0;
-  DVLOG(1) << CoreAudioUtil::WaveFormatToString(&output_format_);
+  // Set the input (capture) format to the desired output format. In most cases,
+  // it will be used unchanged.
+  input_format_ = output_format_;
+  DVLOG(1) << CoreAudioUtil::WaveFormatToString(&input_format_);
 
   // Size in bytes of each audio frame.
   frame_size_bytes_ = format->nBlockAlign;
@@ -587,7 +572,7 @@
 
         // Move the capture time forward for each vended block.
         capture_time += AudioTimestampHelper::FramesToTime(
-            convert_bus_->frames(), output_format_.nSamplesPerSec);
+            convert_bus_->frames(), output_format_.Format.nSamplesPerSec);
       } else {
         sink_->OnData(fifo_->Consume(), capture_time, volume);
 
@@ -747,23 +732,24 @@
   // Ideally, we want a 1:1 ratio between the buffers we get and the buffers
   // we give to OnData so that each buffer we receive from the OS can be
   // directly converted to a buffer that matches with what was asked for.
-  const double buffer_ratio =
-      output_format_.nSamplesPerSec / static_cast<double>(packet_size_frames_);
+  const double buffer_ratio = output_format_.Format.nSamplesPerSec /
+                              static_cast<double>(packet_size_frames_);
   double new_frames_per_buffer =
       input_format_.Format.nSamplesPerSec / buffer_ratio;
 
   const auto input_layout = GuessChannelLayout(input_format_.Format.nChannels);
   DCHECK_NE(CHANNEL_LAYOUT_UNSUPPORTED, input_layout);
-  const auto output_layout = GuessChannelLayout(output_format_.nChannels);
+  const auto output_layout =
+      GuessChannelLayout(output_format_.Format.nChannels);
   DCHECK_NE(CHANNEL_LAYOUT_UNSUPPORTED, output_layout);
 
   const AudioParameters input(AudioParameters::AUDIO_PCM_LOW_LATENCY,
                               input_layout, input_format_.Format.nSamplesPerSec,
                               static_cast<int>(new_frames_per_buffer));
 
-  const AudioParameters output(AudioParameters::AUDIO_PCM_LOW_LATENCY,
-                               output_layout, output_format_.nSamplesPerSec,
-                               packet_size_frames_);
+  const AudioParameters output(
+      AudioParameters::AUDIO_PCM_LOW_LATENCY, output_layout,
+      output_format_.Format.nSamplesPerSec, packet_size_frames_);
 
   converter_.reset(new AudioConverter(input, output, false));
   converter_->AddInput(this);
@@ -886,7 +872,7 @@
   //   the capture client to run the capture thread that reads the next set of
   //   samples from the capture endpoint buffer.
   //
-  // http://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/dd316551(v=vs.85).aspx
+  // http://msdn.microsoft.com/en-us/library/windows/desktop/dd316551(v=vs.85).aspx
   if (AudioDeviceDescription::IsLoopbackDevice(device_id_)) {
     hr = endpoint_device_->Activate(
         __uuidof(IAudioClient), CLSCTX_INPROC_SERVER, NULL,
@@ -954,12 +940,12 @@
         input_format_.Format.wBitsPerSample,
         input_format_.Format.nBlockAlign, input_format_.Format.nAvgBytesPerSec,
         input_format_.Format.cbSize,
-        output_format_.wFormatTag, output_format_.nChannels,
-        output_format_.nSamplesPerSec,
-        output_format_.wBitsPerSample,
-        output_format_.nBlockAlign,
-        output_format_.nAvgBytesPerSec,
-        output_format_.cbSize));
+        output_format_.Format.wFormatTag, output_format_.Format.nChannels,
+        output_format_.Format.nSamplesPerSec,
+        output_format_.Format.wBitsPerSample,
+        output_format_.Format.nBlockAlign,
+        output_format_.Format.nAvgBytesPerSec,
+        output_format_.Format.cbSize));
     // clang-format on
   }
 }
--- a/media/audio/win/audio_low_latency_input_win.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/win/audio_low_latency_input_win.h	2019-05-17 18:53:34.044000000 +0300
@@ -186,18 +186,16 @@
   // All OnData() callbacks will be called from this thread.
   std::unique_ptr<base::DelegateSimpleThread> capture_thread_;
 
-  // Contains the desired output audio format which is set up at construction
-  // and then never modified. It is the audio format this class will output
-  // data to the sink in, or equivalently, the format after the converter if
-  // such is needed. Does not need the extended version since we only support
-  // max stereo at this stage.
-  WAVEFORMATEX output_format_;
+  // Contains the desired output audio format which is set up at construction,
+  // that is the audio format this class should output data to the sink in, that
+  // is the format after the converter.
+  WAVEFORMATEXTENSIBLE output_format_;
 
-  // Contains the audio format we get data from the audio engine in. Initially
-  // set to |output_format_| at construction but it might be changed to a close
-  // match if the audio engine doesn't support the originally set format. Note
-  // that, this is also the format after the FIFO, i.e. the input format to the
-  // converter if any.
+  // Contains the audio format we get data from the audio engine in. Set to
+  // |output_format_| at construction and might be changed to a close match if
+  // the audio engine doesn't support the originally set format. Note that this
+  // is also the format after the fifo, i.e. the input format to the converter
+  // if any.
   WAVEFORMATEXTENSIBLE input_format_;
 
   bool opened_ = false;
--- a/media/audio/win/audio_low_latency_input_win_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/audio/win/audio_low_latency_input_win_unittest.cc	2019-05-17 18:53:34.044000000 +0300
@@ -37,6 +37,7 @@
 using ::testing::AnyNumber;
 using ::testing::AtLeast;
 using ::testing::Gt;
+using ::testing::NiceMock;
 using ::testing::NotNull;
 
 namespace media {
@@ -344,7 +345,7 @@
   ScopedAudioInputStream ais(
       CreateDefaultAudioInputStream(audio_manager_.get()));
   EXPECT_TRUE(ais->Open());
-  MockAudioInputCallback sink;
+  NiceMock<MockAudioInputCallback> sink;
   ais->Start(&sink);
   ais.Close();
 }
@@ -355,7 +356,7 @@
   ScopedAudioInputStream ais(
       CreateDefaultAudioInputStream(audio_manager_.get()));
   EXPECT_TRUE(ais->Open());
-  MockAudioInputCallback sink;
+  NiceMock<MockAudioInputCallback> sink;
   ais->Start(&sink);
   ais->Stop();
   ais.Close();
--- a/media/audio/win/audio_low_latency_output_win.h	2019-05-17 17:45:41.228000000 +0300
+++ b/media/audio/win/audio_low_latency_output_win.h	2019-05-17 18:53:34.044000000 +0300
@@ -87,7 +87,7 @@
 //     o ~3.3333ms @ 48kHz <=> 160 audio frames per buffer.
 //     o ~3.6281ms @ 44.1kHz <=> 160 audio frames per buffer.
 // - See
-// http://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/dd370844(v=vs.85).aspx
+// http://msdn.microsoft.com/en-us/library/windows/desktop/dd370844(v=vs.85).aspx
 //   for more details.
 
 #ifndef MEDIA_AUDIO_WIN_AUDIO_LOW_LATENCY_OUTPUT_WIN_H_
--- a/media/audio/win/audio_manager_win.cc	2019-05-17 17:45:41.232000000 +0300
+++ b/media/audio/win/audio_manager_win.cc	2019-05-17 18:53:34.048000000 +0300
@@ -19,7 +19,7 @@
 #include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/command_line.h"
-#include "base/metrics/histogram_functions.h"
+#include "base/metrics/histogram_macros.h"
 #include "base/strings/string_number_conversions.h"
 #include "base/win/windows_version.h"
 #include "media/audio/audio_device_description.h"
@@ -80,71 +80,6 @@
   return 3;
 }
 
-static bool IsSupported(HRESULT hr) {
-  return hr != S_FALSE && SUCCEEDED(hr);
-}
-
-// Records bitstream output support to histograms. Follows information from:
-// https://docs.m1cr050ft.qjz9zk/en-us/windows/desktop/coreaudio/representing-formats-for-iec-61937-transmissions
-static void LogBitstreamOutputSupport() {
-  auto client = CoreAudioUtil::CreateClient(
-      AudioDeviceDescription::kDefaultDeviceId, eRender, eConsole);
-
-  // Happens if no audio output devices are available.
-  if (!client)
-    return;
-
-  WAVEFORMATEXTENSIBLE wfext;
-  memset(&wfext, 0, sizeof(wfext));
-
-  // See link in function comment for where each value comes from.
-  wfext.Format.wFormatTag = WAVE_FORMAT_EXTENSIBLE;
-  wfext.Format.nChannels = 2;
-  wfext.Format.nSamplesPerSec = 192000;
-  wfext.Format.nAvgBytesPerSec = 768000;
-  wfext.Format.nBlockAlign = 4;
-  wfext.Format.wBitsPerSample = 16;
-  wfext.Format.cbSize = sizeof(WAVEFORMATEXTENSIBLE) - sizeof(WAVEFORMATEX);
-  wfext.Samples.wValidBitsPerSample = 16;
-  wfext.dwChannelMask = KSAUDIO_SPEAKER_7POINT1_SURROUND;
-
-  // Test Dolby Digital+ / Atmos support. For whatever reason Atmos doesn't use
-  // the KSDATAFORMAT_SUBTYPE_IEC61937_DOLBY_DIGITAL_PLUS_ATMOS SubFormat.
-  wfext.SubFormat = KSDATAFORMAT_SUBTYPE_IEC61937_DOLBY_DIGITAL_PLUS;
-
-  HRESULT hr = client->IsFormatSupported(AUDCLNT_SHAREMODE_EXCLUSIVE,
-                                         &wfext.Format, nullptr);
-  base::UmaHistogramBoolean("Media.Audio.Bitstream.EAC3", IsSupported(hr));
-
-  // Test Dolby TrueHD.
-  wfext.SubFormat = KSDATAFORMAT_SUBTYPE_IEC61937_DOLBY_MLP;
-  hr = client->IsFormatSupported(AUDCLNT_SHAREMODE_EXCLUSIVE, &wfext.Format,
-                                 nullptr);
-  base::UmaHistogramBoolean("Media.Audio.Bitstream.TrueHD", IsSupported(hr));
-
-  // Test DTS-HD.
-  wfext.SubFormat = KSDATAFORMAT_SUBTYPE_IEC61937_DTS_HD;
-  hr = client->IsFormatSupported(AUDCLNT_SHAREMODE_EXCLUSIVE, &wfext.Format,
-                                 nullptr);
-  base::UmaHistogramBoolean("Media.Audio.Bitstream.DTS-HD", IsSupported(hr));
-
-  // Older bitstream formats run at lower sampling rates.
-  wfext.Format.nSamplesPerSec = 48000;
-  wfext.Format.nAvgBytesPerSec = 192000;
-
-  // Test AC3.
-  wfext.SubFormat = KSDATAFORMAT_SUBTYPE_IEC61937_DOLBY_DIGITAL;
-  hr = client->IsFormatSupported(AUDCLNT_SHAREMODE_EXCLUSIVE, &wfext.Format,
-                                 nullptr);
-  base::UmaHistogramBoolean("Media.Audio.Bitstream.AC3", IsSupported(hr));
-
-  // Test DTS.
-  wfext.SubFormat = KSDATAFORMAT_SUBTYPE_IEC61937_DTS;
-  hr = client->IsFormatSupported(AUDCLNT_SHAREMODE_EXCLUSIVE, &wfext.Format,
-                                 nullptr);
-  base::UmaHistogramBoolean("Media.Audio.Bitstream.DTS", IsSupported(hr));
-}
-
 AudioManagerWin::AudioManagerWin(std::unique_ptr<AudioThread> audio_thread,
                                  AudioLogFactory* audio_log_factory)
     : AudioManagerBase(std::move(audio_thread), audio_log_factory) {
@@ -196,11 +131,6 @@
 void AudioManagerWin::InitializeOnAudioThread() {
   DCHECK(GetTaskRunner()->BelongsToCurrentThread());
 
-  // Delay metrics recording to avoid any issues at startup.
-  GetTaskRunner()->PostDelayedTask(FROM_HERE,
-                                   base::BindOnce(&LogBitstreamOutputSupport),
-                                   base::TimeDelta::FromSeconds(15));
-
   // AudioDeviceListenerWin must be initialized on a COM thread.
   output_device_listener_.reset(new AudioDeviceListenerWin(BindToCurrentLoop(
       base::Bind(&AudioManagerWin::NotifyAllOutputDeviceChangeListeners,
--- a/media/audio/win/core_audio_util_win.cc	2019-05-17 17:45:41.232000000 +0300
+++ b/media/audio/win/core_audio_util_win.cc	2019-05-17 18:53:34.048000000 +0300
@@ -214,13 +214,6 @@
 // in that order within each block.
 std::string ChannelMaskToString(DWORD channel_mask) {
   std::string ss;
-  if (channel_mask == KSAUDIO_SPEAKER_DIRECTOUT)
-    // A very rare channel mask where speaker orientation is "hard coded".
-    // In direct-out mode, the audio device renders the first channel to the
-    // first output connector on the device, the second channel to the second
-    // output on the device, and so on.
-    ss += "DIRECT_OUT";
-  else {
     if (channel_mask & SPEAKER_FRONT_LEFT)
       ss += "FRONT_LEFT | ";
     if (channel_mask & SPEAKER_FRONT_RIGHT)
@@ -262,17 +255,11 @@
       // Delete last appended " | " substring.
       ss.erase(ss.end() - 3, ss.end());
     }
-  }
 
-  // Add number of utilized channels, e.g. "(2)" but exclude this part for
-  // direct output mode since the number of ones in the channel mask does not
-  // reflect the number of channels for this case.
-  if (channel_mask != KSAUDIO_SPEAKER_DIRECTOUT) {
     std::bitset<8 * sizeof(DWORD)> mask(channel_mask);
     ss += " (";
     ss += std::to_string(mask.count());
     ss += ")";
-  }
   return ss;
 }
 
@@ -393,7 +380,7 @@
   // Left speaker, the next least significant bit corresponds to the Front
   // Right speaker, and so on, continuing in the order defined in KsMedia.h.
   // See
-  // http://msdn.m1cr050ft.qjz9zk/en-us/library/windows/hardware/ff537083.aspx
+  // http://msdn.microsoft.com/en-us/library/windows/hardware/ff537083.aspx
   // for more details.
   ChannelConfig channel_config = mix_format.GetExtensible()->dwChannelMask;
 
--- a/media/audio/win/core_audio_util_win.h	2019-05-17 17:45:41.236000000 +0300
+++ b/media/audio/win/core_audio_util_win.h	2019-05-17 18:53:34.048000000 +0300
@@ -28,7 +28,7 @@
 
 // Represents audio channel configuration constants as understood by Windows.
 // E.g. KSAUDIO_SPEAKER_MONO.  For a list of possible values see:
-// http://msdn.m1cr050ft.qjz9zk/en-us/library/windows/hardware/ff537083(v=vs.85).aspx
+// http://msdn.microsoft.com/en-us/library/windows/hardware/ff537083(v=vs.85).aspx
 typedef uint32_t ChannelConfig;
 
 class MEDIA_EXPORT CoreAudioUtil {
@@ -203,7 +203,7 @@
   // channel stream. The least significant bit corresponds with the Front Left
   // speaker, the next least significant bit corresponds to the Front Right
   // speaker, and so on, continuing in the order defined in KsMedia.h.
-  // See http://msdn.m1cr050ft.qjz9zk/en-us/library/windows/hardware/ff537083(v=vs.85).aspx
+  // See http://msdn.microsoft.com/en-us/library/windows/hardware/ff537083(v=vs.85).aspx
   // for more details.
   // To get the channel config of the default device, pass an empty string
   // for |device_id|.
--- a/media/base/android/android_cdm_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/android_cdm_factory.cc	2019-05-17 18:53:34.048000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/base/android/android_cdm_factory.h"
 
-#include "base/bind.h"
 #include "base/feature_list.h"
 #include "base/logging.h"
 #include "base/metrics/histogram_macros.h"
--- a/media/base/android/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/BUILD.gn	2019-05-17 18:53:34.048000000 +0300
@@ -52,10 +52,13 @@
       "media_drm_storage.h",
       "media_drm_storage_bridge.cc",
       "media_drm_storage_bridge.h",
+      "media_player_android.cc",
+      "media_player_android.h",
       "media_player_bridge.cc",
       "media_player_bridge.h",
       "media_player_listener.cc",
       "media_player_listener.h",
+      "media_player_manager.h",
       "media_resource_getter.cc",
       "media_resource_getter.h",
       "media_server_crash_listener.cc",
@@ -154,13 +157,6 @@
     ]
   }
 
-  java_cpp_strings("java_switches") {
-    sources = [
-      "//media/base/media_switches.cc",
-    ]
-    template = "//media/base/android/java_templates/MediaSwitches.java.tmpl"
-  }
-
   android_resources("media_java_resources") {
     custom_package = "org.chromium.media"
     resource_dirs = [ "java/res" ]
@@ -174,7 +170,6 @@
     ]
     srcjar_deps = [
       ":java_enums",
-      ":java_switches",
       "//media/base:java_enums",
     ]
     java_files = [
@@ -195,6 +190,7 @@
       "java/borg/chromium/media/MediaPlayerBridge.java",
       "java/borg/chromium/media/MediaPlayerListener.java",
       "java/borg/chromium/media/MediaServerCrashListener.java",
+      "java/borg/chromium/media/MediaSwitches.java",
     ]
   }
 
--- a/media/base/android/java/borg/chromium/media/BitrateAdjuster.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/java/borg/chromium/media/BitrateAdjuster.java	2019-05-17 18:53:34.052000000 +0300
@@ -4,47 +4,46 @@
 
 package org.chromium.media;
 
-import android.support.annotation.IntDef;
+enum BitrateAdjuster {
+    // No adjustment - video encoder has no known bitrate problem.
+    NO_ADJUSTMENT {
+        private static final int MAXIMUM_INITIAL_FPS = 30;
 
-import java.lang.annotation.Retention;
-import java.lang.annotation.RetentionPolicy;
+        @Override
+        public int getTargetBitrate(int bps, int frameRate) {
+            return bps;
+        }
+
+        @Override
+        public int getInitialFrameRate(int frameRateHint) {
+            return Math.min(frameRateHint, MAXIMUM_INITIAL_FPS);
+        }
+    },
 
-class BitrateAdjuster {
-    @IntDef({Type.NO_ADJUSTMENT, Type.FRAMERATE_ADJUSTMENT})
-    @Retention(RetentionPolicy.SOURCE)
-    public @interface Type {
-        // No adjustment - video encoder has no known bitrate problem.
-        int NO_ADJUSTMENT = 0;
         // Framerate based bitrate adjustment is required - HW encoder does not use frame
         // timestamps to calculate frame bitrate budget and instead is relying on initial
         // fps configuration assuming that all frames are coming at fixed initial frame rate.
-        int FRAMERATE_ADJUSTMENT = 1;
-    }
-
-    private static final int FRAMERATE_ADJUSTMENT_BITRATE_ADJUSTMENT_FPS = 30;
+    FRAMERATE_ADJUSTMENT {
+        private static final int BITRATE_ADJUSTMENT_FPS = 30;
 
-    // Gets the adjusted bitrate according to the implementation's adjustment policy.
-    public static int getTargetBitrate(@Type int type, int bps, int frameRate) {
-        switch (type) {
-            case Type.NO_ADJUSTMENT:
+        @Override
+        public int getTargetBitrate(int bps, int frameRate) {
+            if (frameRate == 0) {
                 return bps;
-            case Type.FRAMERATE_ADJUSTMENT:
-                return frameRate == 0
-                        ? bps
-                        : FRAMERATE_ADJUSTMENT_BITRATE_ADJUSTMENT_FPS * bps / frameRate;
         }
-        return 0;
+            return BITRATE_ADJUSTMENT_FPS * bps / frameRate;
+        }
+
+        @Override
+        public int getInitialFrameRate(int frameRateHint) {
+            return BITRATE_ADJUSTMENT_FPS;
     }
+    };
+
+    // Gets the adjusted bitrate according to the implementation's adjustment policy.
+    public abstract int getTargetBitrate(int bps, int frameRate);
 
     // Gets the initial frame rate of the media. The frameRateHint can be used as a default or a
     // constraint.
-    public static int getInitialFrameRate(@Type int type, int frameRateHint) {
-        switch (type) {
-            case Type.NO_ADJUSTMENT:
-                return Math.min(frameRateHint, 30); // 30 = MAXIMUM_INITIAL_FPS
-            case Type.FRAMERATE_ADJUSTMENT:
-                return FRAMERATE_ADJUSTMENT_BITRATE_ADJUSTMENT_FPS;
-        }
-        return 0;
-    }
+    public abstract int getInitialFrameRate(int frameRateHint);
 }
--- a/media/base/android/java/borg/chromium/media/MediaCodecBridgeBuilder.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/java/borg/chromium/media/MediaCodecBridgeBuilder.java	2019-05-17 18:53:34.052000000 +0300
@@ -71,8 +71,8 @@
                 ? new MediaCodecEncoder(info.mediaCodec, info.bitrateAdjuster)
                 : new MediaCodecBridge(info.mediaCodec, info.bitrateAdjuster, false);
         MediaFormat format = MediaFormatBuilder.createVideoEncoderFormat(mime, width, height,
-                bitRate, BitrateAdjuster.getInitialFrameRate(info.bitrateAdjuster, frameRate),
-                iFrameInterval, colorFormat, info.supportsAdaptivePlayback);
+                bitRate, info.bitrateAdjuster.getInitialFrameRate(frameRate), iFrameInterval,
+                colorFormat, info.supportsAdaptivePlayback);
 
         if (!bridge.configureVideo(format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE)) {
             return null;
--- a/media/base/android/java/borg/chromium/media/MediaCodecBridge.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/java/borg/chromium/media/MediaCodecBridge.java	2019-05-17 18:53:34.052000000 +0300
@@ -60,7 +60,7 @@
 
     private boolean mFlushed;
     private long mLastPresentationTimeUs;
-    private @BitrateAdjuster.Type int mBitrateAdjuster;
+    private BitrateAdjuster mBitrateAdjuster;
 
     // To support both the synchronous and asynchronous version of MediaCodec
     // (since we need to work on <M devices), we implement async support as a
@@ -238,8 +238,7 @@
         }
     };
 
-    MediaCodecBridge(
-            MediaCodec mediaCodec, @BitrateAdjuster.Type int bitrateAdjuster, boolean useAsyncApi) {
+    MediaCodecBridge(MediaCodec mediaCodec, BitrateAdjuster bitrateAdjuster, boolean useAsyncApi) {
         assert mediaCodec != null;
         mMediaCodec = mediaCodec;
         mLastPresentationTimeUs = 0;
@@ -546,7 +545,7 @@
     @TargetApi(Build.VERSION_CODES.KITKAT)
     @CalledByNative
     private void setVideoBitrate(int bps, int frameRate) {
-        int targetBps = BitrateAdjuster.getTargetBitrate(mBitrateAdjuster, bps, frameRate);
+        int targetBps = mBitrateAdjuster.getTargetBitrate(bps, frameRate);
         Bundle b = new Bundle();
         b.putInt(MediaCodec.PARAMETER_KEY_VIDEO_BITRATE, targetBps);
         try {
--- a/media/base/android/java/borg/chromium/media/MediaCodecEncoder.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/java/borg/chromium/media/MediaCodecEncoder.java	2019-05-17 18:53:34.052000000 +0300
@@ -28,7 +28,7 @@
     // SPS and PPS NALs (Config frame).
     private ByteBuffer mConfigData;
 
-    protected MediaCodecEncoder(MediaCodec mediaCodec, @BitrateAdjuster.Type int bitrateAdjuster) {
+    protected MediaCodecEncoder(MediaCodec mediaCodec, BitrateAdjuster bitrateAdjuster) {
         super(mediaCodec, bitrateAdjuster, false);
     }
 
--- a/media/base/android/java/borg/chromium/media/MediaCodecUtil.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/java/borg/chromium/media/MediaCodecUtil.java	2019-05-17 18:53:34.052000000 +0300
@@ -42,7 +42,7 @@
     public static class CodecCreationInfo {
         public MediaCodec mediaCodec;
         public boolean supportsAdaptivePlayback;
-        public @BitrateAdjuster.Type int bitrateAdjuster = BitrateAdjuster.Type.NO_ADJUSTMENT;
+        public BitrateAdjuster bitrateAdjuster = BitrateAdjuster.NO_ADJUSTMENT;
     }
 
     public static final class MimeTypes {
@@ -197,7 +197,7 @@
     private static boolean canDecode(String mime, boolean isSecure) {
         // TODO(liberato): Should we insist on software here?
         CodecCreationInfo info = createDecoder(mime, isSecure ? CodecType.SECURE : CodecType.ANY);
-        if (info == null || info.mediaCodec == null) return false;
+        if (info.mediaCodec == null) return false;
 
         try {
             info.mediaCodec.release();
@@ -497,23 +497,23 @@
     // List of supported HW encoders.
     private static enum HWEncoderProperties {
         QcomVp8(MimeTypes.VIDEO_VP8, "OMX.qcom.", Build.VERSION_CODES.KITKAT,
-                BitrateAdjuster.Type.NO_ADJUSTMENT),
+                BitrateAdjuster.NO_ADJUSTMENT),
         QcomH264(MimeTypes.VIDEO_H264, "OMX.qcom.", Build.VERSION_CODES.KITKAT,
-                BitrateAdjuster.Type.NO_ADJUSTMENT),
+                BitrateAdjuster.NO_ADJUSTMENT),
         ExynosVp8(MimeTypes.VIDEO_VP8, "OMX.Exynos.", Build.VERSION_CODES.M,
-                BitrateAdjuster.Type.NO_ADJUSTMENT),
+                BitrateAdjuster.NO_ADJUSTMENT),
         ExynosH264(MimeTypes.VIDEO_H264, "OMX.Exynos.", Build.VERSION_CODES.LOLLIPOP,
-                BitrateAdjuster.Type.FRAMERATE_ADJUSTMENT),
+                BitrateAdjuster.FRAMERATE_ADJUSTMENT),
         MediatekH264(MimeTypes.VIDEO_H264, "OMX.MTK.", Build.VERSION_CODES.O_MR1,
-                BitrateAdjuster.Type.FRAMERATE_ADJUSTMENT);
+                BitrateAdjuster.FRAMERATE_ADJUSTMENT);
 
         private final String mMime;
         private final String mPrefix;
         private final int mMinSDK;
-        private final @BitrateAdjuster.Type int mBitrateAdjuster;
+        private final BitrateAdjuster mBitrateAdjuster;
 
         private HWEncoderProperties(
-                String mime, String prefix, int minSDK, @BitrateAdjuster.Type int bitrateAdjuster) {
+                String mime, String prefix, int minSDK, BitrateAdjuster bitrateAdjuster) {
             this.mMime = mime;
             this.mPrefix = prefix;
             this.mMinSDK = minSDK;
@@ -532,7 +532,7 @@
             return mMinSDK;
         }
 
-        public @BitrateAdjuster.Type int getBitrateAdjuster() {
+        public BitrateAdjuster getBitrateAdjuster() {
             return mBitrateAdjuster;
         }
     }
--- a/media/base/android/java/borg/chromium/media/MediaDrmBridge.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/java/borg/chromium/media/MediaDrmBridge.java	2019-05-17 18:53:34.052000000 +0300
@@ -19,14 +19,12 @@
 import org.chromium.media.MediaDrmSessionManager.SessionId;
 import org.chromium.media.MediaDrmSessionManager.SessionInfo;
 
-import java.io.IOException;
 import java.util.ArrayDeque;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Queue;
-import java.util.Scanner;
 import java.util.UUID;
 
 // Implementation Notes of MediaDrmBridge:
@@ -69,7 +67,6 @@
     private static final String SESSION_SHARING = "sessionSharing";
     private static final String ENABLE = "enable";
     private static final long INVALID_NATIVE_MEDIA_DRM_BRIDGE = 0;
-    private static final String FIRST_API_LEVEL = "ro.product.first_api_level";
 
     // Scheme UUID for Widevine. See http://dashif.org/identifiers/protection/
     private static final UUID WIDEVINE_UUID =
@@ -404,31 +401,6 @@
     }
 
     /**
-     * Returns the first API level for this product.
-     *
-     * @return the converted value for FIRST_API_LEVEL if available,
-     * 0 otherwise.
-     */
-    @CalledByNative
-    private static int getFirstApiLevel() {
-        int firstApiLevel = 0;
-        Scanner scanner = null;
-        // If first_api_level property is set, return it.
-        try {
-            Process process = new ProcessBuilder("getprop", FIRST_API_LEVEL).start();
-            scanner = new Scanner(process.getInputStream());
-            firstApiLevel = Integer.parseInt(scanner.nextLine().trim());
-        } catch (IOException | NumberFormatException e) {
-            firstApiLevel = 0;
-        } finally {
-            if (scanner != null) {
-                scanner.close();
-            }
-        }
-        return firstApiLevel;
-    }
-
-    /**
      * Create a new MediaDrmBridge from the crypto scheme UUID.
      *
      * @param schemeUUID Crypto scheme UUID.
--- a/media/base/android/java/borg/chromium/media/MediaPlayerBridge.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/java/borg/chromium/media/MediaPlayerBridge.java	2019-05-17 18:53:34.052000000 +0300
@@ -6,6 +6,7 @@
 
 import android.annotation.SuppressLint;
 import android.media.MediaPlayer;
+import android.media.MediaPlayer.TrackInfo;
 import android.net.Uri;
 import android.os.Build;
 import android.os.ParcelFileDescriptor;
@@ -97,6 +98,48 @@
     }
 
     @CalledByNative
+    protected boolean hasVideo() {
+        return hasTrack(TrackInfo.MEDIA_TRACK_TYPE_VIDEO);
+    }
+
+    @CalledByNative
+    protected boolean hasAudio() {
+        return hasTrack(TrackInfo.MEDIA_TRACK_TYPE_AUDIO);
+    }
+
+    private boolean hasTrack(int trackType) {
+        try {
+            TrackInfo trackInfo[] = getLocalPlayer().getTrackInfo();
+
+            // HLS media does not have the track info, so we treat them conservatively.
+            if (trackInfo.length == 0) return true;
+
+            for (TrackInfo info : trackInfo) {
+                // TODO(zqzhang): may be we can have a histogram recording
+                // media track types in the future.
+                // See http://crbug.com/571411
+                if (trackType == info.getTrackType()) return true;
+                if (TrackInfo.MEDIA_TRACK_TYPE_UNKNOWN == info.getTrackType()) return true;
+            }
+        } catch (RuntimeException e) {
+            // Exceptions may come from getTrackInfo (IllegalStateException/RuntimeException), or
+            // from some customized OS returning null TrackInfos (NullPointerException).
+            return true;
+        }
+        return false;
+    }
+
+    @CalledByNative
+    protected int getVideoWidth() {
+        return getLocalPlayer().getVideoWidth();
+    }
+
+    @CalledByNative
+    protected int getVideoHeight() {
+        return getLocalPlayer().getVideoHeight();
+    }
+
+    @CalledByNative
     protected int getCurrentPosition() {
         return getLocalPlayer().getCurrentPosition();
     }
@@ -249,6 +292,10 @@
         }
     }
 
+    protected void setOnBufferingUpdateListener(MediaPlayer.OnBufferingUpdateListener listener) {
+        getLocalPlayer().setOnBufferingUpdateListener(listener);
+    }
+
     protected void setOnCompletionListener(MediaPlayer.OnCompletionListener listener) {
         getLocalPlayer().setOnCompletionListener(listener);
     }
@@ -261,20 +308,32 @@
         getLocalPlayer().setOnPreparedListener(listener);
     }
 
+    protected void setOnSeekCompleteListener(MediaPlayer.OnSeekCompleteListener listener) {
+        getLocalPlayer().setOnSeekCompleteListener(listener);
+    }
+
     protected void setOnVideoSizeChangedListener(MediaPlayer.OnVideoSizeChangedListener listener) {
         getLocalPlayer().setOnVideoSizeChangedListener(listener);
     }
 
     protected static class AllowedOperations {
+        private final boolean mCanPause;
         private final boolean mCanSeekForward;
         private final boolean mCanSeekBackward;
 
-        public AllowedOperations(boolean canSeekForward, boolean canSeekBackward) {
+        public AllowedOperations(boolean canPause, boolean canSeekForward,
+                boolean canSeekBackward) {
+            mCanPause = canPause;
             mCanSeekForward = canSeekForward;
             mCanSeekBackward = canSeekBackward;
         }
 
         @CalledByNative("AllowedOperations")
+        private boolean canPause() {
+            return mCanPause;
+        }
+
+        @CalledByNative("AllowedOperations")
         private boolean canSeekForward() {
             return mCanSeekForward;
         }
@@ -292,6 +351,7 @@
     @CalledByNative
     protected AllowedOperations getAllowedOperations() {
         MediaPlayer player = getLocalPlayer();
+        boolean canPause = true;
         boolean canSeekForward = true;
         boolean canSeekBackward = true;
         try {
@@ -305,12 +365,15 @@
                 Method hasMethod = metadataClass.getDeclaredMethod("has", int.class);
                 Method getBooleanMethod = metadataClass.getDeclaredMethod("getBoolean", int.class);
 
+                int pause = (Integer) metadataClass.getField("PAUSE_AVAILABLE").get(null);
                 int seekForward =
                         (Integer) metadataClass.getField("SEEK_FORWARD_AVAILABLE").get(null);
                 int seekBackward =
                         (Integer) metadataClass.getField("SEEK_BACKWARD_AVAILABLE").get(null);
                 hasMethod.setAccessible(true);
                 getBooleanMethod.setAccessible(true);
+                canPause = !((Boolean) hasMethod.invoke(data, pause))
+                        || ((Boolean) getBooleanMethod.invoke(data, pause));
                 canSeekForward = !((Boolean) hasMethod.invoke(data, seekForward))
                         || ((Boolean) getBooleanMethod.invoke(data, seekForward));
                 canSeekBackward = !((Boolean) hasMethod.invoke(data, seekBackward))
@@ -325,7 +388,7 @@
         } catch (NoSuchFieldException e) {
             Log.e(TAG, "Cannot find matching fields in Metadata class: " + e);
         }
-        return new AllowedOperations(canSeekForward, canSeekBackward);
+        return new AllowedOperations(canPause, canSeekForward, canSeekBackward);
     }
 
     private native void nativeOnDidSetDataUriDataSource(long nativeMediaPlayerBridge,
--- a/media/base/android/java/borg/chromium/media/MediaPlayerListener.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/java/borg/chromium/media/MediaPlayerListener.java	2019-05-17 18:53:34.052000000 +0300
@@ -14,6 +14,8 @@
 @JNINamespace("media")
 class MediaPlayerListener implements MediaPlayer.OnPreparedListener,
         MediaPlayer.OnCompletionListener,
+        MediaPlayer.OnBufferingUpdateListener,
+        MediaPlayer.OnSeekCompleteListener,
         MediaPlayer.OnVideoSizeChangedListener,
         MediaPlayer.OnErrorListener {
     // These values are mirrored as enums in media/base/android/media_player_android.h.
@@ -76,6 +78,16 @@
     }
 
     @Override
+    public void onSeekComplete(MediaPlayer mp) {
+        nativeOnSeekComplete(mNativeMediaPlayerListener);
+    }
+
+    @Override
+    public void onBufferingUpdate(MediaPlayer mp, int percent) {
+        nativeOnBufferingUpdate(mNativeMediaPlayerListener, percent);
+    }
+
+    @Override
     public void onCompletion(MediaPlayer mp) {
         nativeOnPlaybackComplete(mNativeMediaPlayerListener);
     }
@@ -90,9 +102,11 @@
             long nativeMediaPlayerListener, MediaPlayerBridge mediaPlayerBridge) {
         final MediaPlayerListener listener = new MediaPlayerListener(nativeMediaPlayerListener);
         if (mediaPlayerBridge != null) {
+            mediaPlayerBridge.setOnBufferingUpdateListener(listener);
             mediaPlayerBridge.setOnCompletionListener(listener);
             mediaPlayerBridge.setOnErrorListener(listener);
             mediaPlayerBridge.setOnPreparedListener(listener);
+            mediaPlayerBridge.setOnSeekCompleteListener(listener);
             mediaPlayerBridge.setOnVideoSizeChangedListener(listener);
         }
         return listener;
@@ -109,7 +123,15 @@
             long nativeMediaPlayerListener,
             int width, int height);
 
+    private native void nativeOnBufferingUpdate(
+            long nativeMediaPlayerListener,
+            int percent);
+
     private native void nativeOnMediaPrepared(long nativeMediaPlayerListener);
 
     private native void nativeOnPlaybackComplete(long nativeMediaPlayerListener);
+
+    private native void nativeOnSeekComplete(long nativeMediaPlayerListener);
+
+    private native void nativeOnMediaInterrupted(long nativeMediaPlayerListener);
 }
--- a/media/base/android/java/btest/org/chromium/media/BitrateAdjusterTest.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/java/btest/org/chromium/media/BitrateAdjusterTest.java	2019-05-17 18:53:34.052000000 +0300
@@ -12,8 +12,6 @@
 import org.junit.runner.RunWith;
 import org.junit.runners.BlockJUnit4ClassRunner;
 
-import org.chromium.media.BitrateAdjuster.Type;
-
 /**
  * Tests for BitrateAdjuster, a class used to adjust the target bitrate and framerate for certain
  * codecs in video encoders with fixed framerates.
@@ -27,46 +25,43 @@
     @Test
     @SmallTest
     public void testNoAdjustmentDoesNotChangeTargetBitrate() {
-        assertEquals(BitrateAdjuster.getTargetBitrate(Type.NO_ADJUSTMENT, BITRATE_8_KBPS, 30),
-                BITRATE_8_KBPS);
-        assertEquals(BitrateAdjuster.getTargetBitrate(Type.NO_ADJUSTMENT, BITRATE_8_KBPS, 15),
-                BITRATE_8_KBPS);
+        assertEquals(
+                BitrateAdjuster.NO_ADJUSTMENT.getTargetBitrate(BITRATE_8_KBPS, 30), BITRATE_8_KBPS);
+        assertEquals(
+                BitrateAdjuster.NO_ADJUSTMENT.getTargetBitrate(BITRATE_8_KBPS, 15), BITRATE_8_KBPS);
     }
 
     @Test
     @SmallTest
     public void testNoAdjustmentInitialFrameRateIsClamped() {
-        assertEquals(BitrateAdjuster.getInitialFrameRate(Type.NO_ADJUSTMENT, 15), 15);
-        assertEquals(BitrateAdjuster.getInitialFrameRate(Type.NO_ADJUSTMENT, 30), 30);
-        assertEquals(BitrateAdjuster.getInitialFrameRate(Type.NO_ADJUSTMENT, 60), 30);
+        assertEquals(BitrateAdjuster.NO_ADJUSTMENT.getInitialFrameRate(15), 15);
+        assertEquals(BitrateAdjuster.NO_ADJUSTMENT.getInitialFrameRate(30), 30);
+        assertEquals(BitrateAdjuster.NO_ADJUSTMENT.getInitialFrameRate(60), 30);
     }
 
     @Test
     @SmallTest
     public void testFrameRateAdjustmentAdjustsAccordingToFrameRate() {
-        assertEquals(
-                BitrateAdjuster.getTargetBitrate(Type.FRAMERATE_ADJUSTMENT, BITRATE_8_KBPS, 30),
+        assertEquals(BitrateAdjuster.FRAMERATE_ADJUSTMENT.getTargetBitrate(BITRATE_8_KBPS, 30),
                 BITRATE_8_KBPS);
-        assertEquals(
-                BitrateAdjuster.getTargetBitrate(Type.FRAMERATE_ADJUSTMENT, BITRATE_8_KBPS, 15),
+        assertEquals(BitrateAdjuster.FRAMERATE_ADJUSTMENT.getTargetBitrate(BITRATE_8_KBPS, 15),
                 BITRATE_16_KBPS);
-        assertEquals(
-                BitrateAdjuster.getTargetBitrate(Type.FRAMERATE_ADJUSTMENT, BITRATE_8_KBPS, 60),
+        assertEquals(BitrateAdjuster.FRAMERATE_ADJUSTMENT.getTargetBitrate(BITRATE_8_KBPS, 60),
                 BITRATE_4_KBPS);
     }
 
     @Test
     @SmallTest
     public void testFrameRateAdjustmentDoesNotDivideByZero() {
-        assertEquals(BitrateAdjuster.getTargetBitrate(Type.FRAMERATE_ADJUSTMENT, BITRATE_8_KBPS, 0),
+        assertEquals(BitrateAdjuster.FRAMERATE_ADJUSTMENT.getTargetBitrate(BITRATE_8_KBPS, 0),
                 BITRATE_8_KBPS);
     }
 
     @Test
     @SmallTest
     public void testFrameRateAdjustmentUsesFixedInitialFrameRate() {
-        assertEquals(BitrateAdjuster.getInitialFrameRate(Type.FRAMERATE_ADJUSTMENT, 15), 30);
-        assertEquals(BitrateAdjuster.getInitialFrameRate(Type.FRAMERATE_ADJUSTMENT, 30), 30);
-        assertEquals(BitrateAdjuster.getInitialFrameRate(Type.FRAMERATE_ADJUSTMENT, 60), 30);
+        assertEquals(BitrateAdjuster.FRAMERATE_ADJUSTMENT.getInitialFrameRate(15), 30);
+        assertEquals(BitrateAdjuster.FRAMERATE_ADJUSTMENT.getInitialFrameRate(30), 30);
+        assertEquals(BitrateAdjuster.FRAMERATE_ADJUSTMENT.getInitialFrameRate(60), 30);
     }
 }
--- a/media/base/android/media_drm_bridge.cc	2019-05-17 17:45:41.236000000 +0300
+++ b/media/base/android/media_drm_bridge.cc	2019-05-17 18:53:34.056000000 +0300
@@ -5,7 +5,6 @@
 #include "media/base/android/media_drm_bridge.h"
 
 #include <stddef.h>
-#include <sys/system_properties.h>
 #include <algorithm>
 #include <memory>
 #include <utility>
@@ -55,7 +54,7 @@
     const std::string& /* origin_id */)>;
 
 // These must be in sync with Android MediaDrm REQUEST_TYPE_XXX constants!
-// https://developer.8n6r01d.qjz9zk/reference/android/media/MediaDrm.KeyRequest.html
+// https://developer.android.com/reference/android/media/MediaDrm.KeyRequest.html
 enum class RequestType : uint32_t {
   REQUEST_TYPE_INITIAL = 0,
   REQUEST_TYPE_RENEWAL = 1,
@@ -63,7 +62,7 @@
 };
 
 // These must be in sync with Android MediaDrm KEY_STATUS_XXX constants:
-// https://developer.8n6r01d.qjz9zk/reference/android/media/MediaDrm.KeyStatus.html
+// https://developer.android.com/reference/android/media/MediaDrm.KeyStatus.html
 enum class KeyStatus : uint32_t {
   KEY_STATUS_USABLE = 0,
   KEY_STATUS_EXPIRED = 1,
@@ -264,12 +263,6 @@
   return true;
 }
 
-int GetFirstApiLevel() {
-  JNIEnv* env = AttachCurrentThread();
-  int first_api_level = Java_MediaDrmBridge_getFirstApiLevel(env);
-  return first_api_level;
-}
-
 }  // namespace
 
 // MediaDrm is not generally usable without MediaCodec. Thus, both the MediaDrm
@@ -294,22 +287,6 @@
 }
 
 // static
-bool MediaDrmBridge::IsPerApplicationProvisioningSupported() {
-  // Start by checking "ro.product.first_api_level", which may not exist.
-  // If it is non-zero, then it is the API level.
-  static int first_api_level = GetFirstApiLevel();
-  DVLOG(1) << "first_api_level = " << first_api_level;
-  if (first_api_level >= base::android::SDK_VERSION_OREO)
-    return true;
-
-  // If "ro.product.first_api_level" does not match, then check build number.
-  DVLOG(1) << "api_level = "
-           << base::android::BuildInfo::GetInstance()->sdk_int();
-  return base::android::BuildInfo::GetInstance()->sdk_int() >=
-         base::android::SDK_VERSION_OREO;
-}
-
-// static
 bool MediaDrmBridge::IsPersistentLicenseTypeSupported(
     const std::string& /* key_system */) {
   // TODO(yucliu): Check |key_system| if persistent license is supported by
@@ -361,9 +338,6 @@
   DCHECK(AreMediaDrmApisAvailable());
   DCHECK(!scheme_uuid.empty());
 
-  // TODO(crbug.com/917527): Check that |origin_id| is specified on devices
-  // that support it.
-
   scoped_refptr<MediaDrmBridge> media_drm_bridge(new MediaDrmBridge(
       scheme_uuid, origin_id, security_level, requires_media_crypto,
       std::move(storage), create_fetcher_cb, session_message_cb,
@@ -817,7 +791,7 @@
 // passed to Blink which will then be translated to NaN [4], which is what the
 // spec uses to indicate that the license will never expire [5].
 // [1]
-// http://developer.8n6r01d.qjz9zk/reference/android/media/MediaDrm.OnExpirationUpdateListener.html
+// http://developer.android.com/reference/android/media/MediaDrm.OnExpirationUpdateListener.html
 // [2] See base::Time::FromDoubleT()
 // [3] See base::Time::ToJavaTime()
 // [4] See MediaKeySession::expirationChanged()
--- a/media/base/android/media_drm_bridge_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_drm_bridge_factory.cc	2019-05-17 18:53:34.056000000 +0300
@@ -70,9 +70,9 @@
   // MediaDrmStorage may be lazy created in MediaDrmStorageBridge.
   storage_ = std::make_unique<MediaDrmStorageBridge>();
 
-  if (!MediaDrmBridge::IsPerOriginProvisioningSupported()) {
-    // Per-origin provisioning isn't supported, so proceed without
-    // specifying an origin ID.
+  // TODO(xhwang): We should always try per-origin provisioning as long as it's
+  // supported regardless of whether persistent license is enabled or not.
+  if (!MediaDrmBridge::IsPersistentLicenseTypeSupported(key_system)) {
     CreateMediaDrmBridge("");
     return;
   }
@@ -83,18 +83,20 @@
                      weak_factory_.GetWeakPtr()));
 }
 
-void MediaDrmBridgeFactory::OnStorageInitialized(bool success) {
+void MediaDrmBridgeFactory::OnStorageInitialized() {
   DCHECK(storage_);
-  DVLOG(2) << __func__ << ": success = " << success
-           << ", origin_id = " << storage_->origin_id();
 
-  // MediaDrmStorageBridge should only be created on a successful Initialize().
-  if (!success) {
+  // MediaDrmStorageBridge should always return a valid origin ID after
+  // initialize. Otherwise the pipe is broken and we should not create
+  // MediaDrmBridge here.
+  auto origin_id = storage_->origin_id();
+  DVLOG(2) << __func__ << ": origin_id = " << origin_id;
+  if (origin_id.empty()) {
     std::move(cdm_created_cb_).Run(nullptr, "Cannot fetch origin ID");
     return;
   }
 
-  CreateMediaDrmBridge(storage_->origin_id());
+  CreateMediaDrmBridge(origin_id);
 }
 
 void MediaDrmBridgeFactory::CreateMediaDrmBridge(const std::string& origin_id) {
--- a/media/base/android/media_drm_bridge_factory.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_drm_bridge_factory.h	2019-05-17 18:53:34.056000000 +0300
@@ -42,7 +42,7 @@
 
  private:
   // Callback for Initialize() on |storage_|.
-  void OnStorageInitialized(bool success);
+  void OnStorageInitialized();
 
   // Creates |media_drm_bridge_|, and call SetMediaCryptoReadyCB() to wait for
   // MediaCrypto to be ready.
--- a/media/base/android/media_drm_bridge.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_drm_bridge.h	2019-05-17 18:53:34.056000000 +0300
@@ -78,10 +78,6 @@
   // supported or not. If false, per-device provisioning is used.
   static bool IsPerOriginProvisioningSupported();
 
-  // Returns true if this device supports per-application provisioning, false
-  // otherwise.
-  static bool IsPerApplicationProvisioningSupported();
-
   static bool IsPersistentLicenseTypeSupported(const std::string& key_system);
 
   // Returns the list of the platform-supported key system names that
--- a/media/base/android/media_drm_key_type.h	2019-05-17 17:45:41.240000000 +0300
+++ b/media/base/android/media_drm_key_type.h	2019-05-17 18:53:34.056000000 +0300
@@ -11,7 +11,7 @@
 
 // These must be in sync with Android MediaDrm KEY_TYPE_XXX constants, except
 // UNKNOWN and MAX:
-// https://developer.8n6r01d.qjz9zk/reference/android/media/MediaDrm.html#KEY_TYPE_OFFLINE
+// https://developer.android.com/reference/android/media/MediaDrm.html#KEY_TYPE_OFFLINE
 enum class MediaDrmKeyType : uint32_t {
   UNKNOWN = 0,
   MIN = UNKNOWN,
--- a/media/base/android/media_drm_storage_bridge.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_drm_storage_bridge.cc	2019-05-17 18:53:34.056000000 +0300
@@ -18,7 +18,6 @@
 #include "base/unguessable_token.h"
 #include "jni/MediaDrmStorageBridge_jni.h"
 #include "media/base/android/android_util.h"
-#include "media/base/android/media_drm_bridge.h"
 #include "media/base/android/media_drm_key_type.h"
 
 using base::android::AttachCurrentThread;
@@ -39,7 +38,7 @@
 MediaDrmStorageBridge::~MediaDrmStorageBridge() = default;
 
 void MediaDrmStorageBridge::Initialize(const CreateStorageCB& create_storage_cb,
-                                       InitCB init_cb) {
+                                       base::OnceClosure init_cb) {
   DCHECK(create_storage_cb);
   impl_ = create_storage_cb.Run();
 
@@ -148,31 +147,14 @@
 }
 
 void MediaDrmStorageBridge::OnInitialized(
-    InitCB init_cb,
-    bool success,
-    const MediaDrmStorage::MediaDrmOriginId& origin_id) {
-  if (!success) {
-    DCHECK(!origin_id);
-    std::move(init_cb).Run(false);
-    return;
-  }
-
-  // Note: It's possible that |success| is true but |origin_id| is empty,
-  // to indicate per-device provisioning. If so, do not set |origin_id_|
-  // so that it remains the empty string.
-  if (origin_id && origin_id.value()) {
-    origin_id_ = origin_id->ToString();
-  } else {
-    // |origin_id| is empty. However, if per-application provisioning is
-    // supported, the empty string is not allowed.
+    base::OnceClosure init_cb,
+    const base::UnguessableToken& origin_id) {
     DCHECK(origin_id_.empty());
-    if (MediaDrmBridge::IsPerApplicationProvisioningSupported()) {
-      std::move(init_cb).Run(false);
-      return;
-    }
-  }
 
-  std::move(init_cb).Run(true);
+  if (origin_id)
+    origin_id_ = origin_id.ToString();
+
+  std::move(init_cb).Run();
 }
 
 void MediaDrmStorageBridge::OnSessionDataLoaded(
--- a/media/base/android/media_drm_storage_bridge.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_drm_storage_bridge.h	2019-05-17 18:53:34.056000000 +0300
@@ -27,17 +27,16 @@
 // to talk to the concrete implementation for persistent data management.
 class MediaDrmStorageBridge {
  public:
-  using InitCB = base::OnceCallback<void(bool)>;
-
   MediaDrmStorageBridge();
   ~MediaDrmStorageBridge();
 
   // Once storage is initialized, |init_cb| will be called and it will have a
   // random generated origin id for later usage. If this function isn't called,
   // all the other functions will fail.
-  void Initialize(const CreateStorageCB& create_storage_cb, InitCB init_cb);
+  void Initialize(const CreateStorageCB& create_storage_cb,
+                  base::OnceClosure init_cb);
 
-  const std::string& origin_id() const { return origin_id_; }
+  std::string origin_id() const { return origin_id_; }
 
   // The following OnXXX functions are called by Java. The functions will post
   // task on message loop immediately to avoid reentrancy issues.
@@ -74,9 +73,8 @@
 
  private:
   void RunAndroidBoolCallback(JavaObjectPtr j_callback, bool success);
-  void OnInitialized(InitCB init_cb,
-                     bool success,
-                     const MediaDrmStorage::MediaDrmOriginId& origin_id);
+  void OnInitialized(base::OnceClosure init_cb,
+                     const base::UnguessableToken& origin_id);
   void OnSessionDataLoaded(
       JavaObjectPtr j_callback,
       const std::string& session_id,
--- a/media/base/android/media_drm_storage.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_drm_storage.h	2019-05-17 18:53:34.056000000 +0300
@@ -14,7 +14,6 @@
 #include "base/callback.h"
 #include "base/macros.h"
 #include "base/memory/weak_ptr.h"
-#include "base/optional.h"
 #include "media/base/android/media_drm_key_type.h"
 #include "media/base/media_export.h"
 #include "url/origin.h"
@@ -30,10 +29,6 @@
 class MEDIA_EXPORT MediaDrmStorage
     : public base::SupportsWeakPtr<MediaDrmStorage> {
  public:
-  // When using per-origin provisioning, this is the ID for the origin.
-  // If not specified, the device specific origin ID is to be used.
-  using MediaDrmOriginId = base::Optional<base::UnguessableToken>;
-
   struct SessionData {
     SessionData(std::vector<uint8_t> key_set_id,
                 std::string mime_type,
@@ -54,7 +49,7 @@
 
   // Callback for storage initialization.
   using InitCB =
-      base::OnceCallback<void(bool success, const MediaDrmOriginId& origin_id)>;
+      base::OnceCallback<void(const base::UnguessableToken& origin_id)>;
 
   // Callback to return the result of LoadPersistentSession. |key_set_id| and
   // |mime_type| must be non-empty if |success| is true, and vice versa.
--- a/media/base/android/media_player_bridge.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_player_bridge.cc	2019-05-17 18:53:34.056000000 +0300
@@ -4,19 +4,17 @@
 
 #include "media/base/android/media_player_bridge.h"
 
-#include <algorithm>
 #include <utility>
 
 #include "base/android/jni_android.h"
 #include "base/android/jni_string.h"
 #include "base/android/scoped_java_ref.h"
-#include "base/bind.h"
 #include "base/logging.h"
 #include "base/metrics/histogram_macros.h"
 #include "base/strings/string_util.h"
-#include "base/threading/thread_task_runner_handle.h"
 #include "jni/MediaPlayerBridge_jni.h"
 #include "media/base/android/media_common_android.h"
+#include "media/base/android/media_player_manager.h"
 #include "media/base/android/media_resource_getter.h"
 #include "media/base/android/media_url_interceptor.h"
 #include "media/base/timestamp_constants.h"
@@ -36,17 +34,23 @@
   UMA_EXIT_STATUS_MAX = UMA_EXIT_ERROR,
 };
 
-const double kDefaultVolume = 1.0;
-
 }  // namespace
 
-MediaPlayerBridge::MediaPlayerBridge(const GURL& url,
+MediaPlayerBridge::MediaPlayerBridge(
+    int player_id,
+    const GURL& url,
                                      const GURL& site_for_cookies,
                                      const std::string& user_agent,
                                      bool hide_url_log,
-                                     Client* client,
+    MediaPlayerManager* manager,
+    const OnDecoderResourcesReleasedCB& on_decoder_resources_released_cb,
+    const GURL& frame_url,
                                      bool allow_credentials)
-    : prepared_(false),
+    : MediaPlayerAndroid(player_id,
+                         manager,
+                         on_decoder_resources_released_cb,
+                         frame_url),
+      prepared_(false),
       pending_play_(false),
       should_seek_on_prepare_(false),
       url_(url),
@@ -55,18 +59,14 @@
       hide_url_log_(hide_url_log),
       width_(0),
       height_(0),
+      can_pause_(true),
       can_seek_forward_(true),
       can_seek_backward_(true),
-      volume_(kDefaultVolume),
       allow_credentials_(allow_credentials),
       is_active_(false),
       has_error_(false),
       has_ever_started_(false),
-      client_(client),
-      weak_factory_(this) {
-  listener_ = std::make_unique<MediaPlayerListener>(
-      base::ThreadTaskRunnerHandle::Get(), weak_factory_.GetWeakPtr());
-}
+      weak_factory_(this) {}
 
 MediaPlayerBridge::~MediaPlayerBridge() {
   if (!j_media_player_bridge_.is_null()) {
@@ -92,7 +92,7 @@
 
   if (allow_credentials_) {
     media::MediaResourceGetter* resource_getter =
-        client_->GetMediaResourceGetter();
+        manager()->GetMediaResourceGetter();
 
     resource_getter->GetCookies(
         url_, site_for_cookies_,
@@ -108,14 +108,13 @@
   j_media_player_bridge_.Reset(Java_MediaPlayerBridge_create(
       env, reinterpret_cast<intptr_t>(this)));
 
-  UpdateVolumeInternal();
+  UpdateEffectiveVolume();
 
   AttachListener(j_media_player_bridge_);
 }
 
-void MediaPlayerBridge::PropagateDuration(base::TimeDelta duration) {
+void MediaPlayerBridge::SetDuration(base::TimeDelta duration) {
   duration_ = duration;
-  client_->OnMediaDurationChanged(duration_);
 }
 
 void MediaPlayerBridge::SetVideoSurface(gl::ScopedJavaSurface surface) {
@@ -142,7 +141,7 @@
   CreateJavaMediaPlayerBridge();
 
   if (url_.SchemeIsFileSystem()) {
-    client_->GetMediaResourceGetter()->GetPlatformPathFromURL(
+    manager()->GetMediaResourceGetter()->GetPlatformPathFromURL(
         url_, base::BindOnce(&MediaPlayerBridge::SetDataSource,
                              weak_factory_.GetWeakPtr()));
     return;
@@ -209,7 +208,7 @@
   *offset = kUnsetValue;
   *size = kUnsetValue;
   media::MediaUrlInterceptor* url_interceptor =
-      client_->GetMediaUrlInterceptor();
+      manager()->GetMediaUrlInterceptor();
   if (url_interceptor && url_interceptor->Intercept(url, fd, offset, size)) {
     DCHECK_NE(kUnsetValue, *fd);
     DCHECK_NE(kUnsetValue, *offset);
@@ -234,14 +233,14 @@
 
 void MediaPlayerBridge::OnCookiesRetrieved(const std::string& cookies) {
   cookies_ = cookies;
-  client_->GetMediaResourceGetter()->GetAuthCredentials(
-      url_, base::BindOnce(&MediaPlayerBridge::OnAuthCredentialsRetrieved,
+  manager()->GetMediaResourceGetter()->GetAuthCredentials(
+      url_,
+      base::Bind(&MediaPlayerBridge::OnAuthCredentialsRetrieved,
                            weak_factory_.GetWeakPtr()));
 }
 
 void MediaPlayerBridge::OnAuthCredentialsRetrieved(
-    const base::string16& username,
-    const base::string16& password) {
+    const base::string16& username, const base::string16& password) {
   GURL::ReplacementsW replacements;
   if (!username.empty()) {
     replacements.SetUsernameStr(username);
@@ -274,7 +273,7 @@
   }
 }
 
-void MediaPlayerBridge::Pause() {
+void MediaPlayerBridge::Pause(bool is_media_related_action) {
   if (j_media_player_bridge_.is_null()) {
     pending_play_ = false;
   } else {
@@ -288,7 +287,9 @@
 }
 
 bool MediaPlayerBridge::IsPlaying() {
-  DCHECK(prepared_);
+  if (!prepared_)
+    return pending_play_;
+
   JNIEnv* env = base::android::AttachCurrentThread();
   CHECK(env);
   jboolean result =
@@ -296,13 +297,39 @@
   return result;
 }
 
+bool MediaPlayerBridge::HasVideo() const {
+  DCHECK(prepared_);
+  JNIEnv* env = base::android::AttachCurrentThread();
+  return Java_MediaPlayerBridge_hasVideo(env, j_media_player_bridge_);
+}
+
+bool MediaPlayerBridge::HasAudio() const {
+  DCHECK(prepared_);
+  JNIEnv* env = base::android::AttachCurrentThread();
+  return Java_MediaPlayerBridge_hasAudio(env, j_media_player_bridge_);
+}
+
+int MediaPlayerBridge::GetVideoWidth() {
+  if (!prepared_)
+    return width_;
+  JNIEnv* env = base::android::AttachCurrentThread();
+  return Java_MediaPlayerBridge_getVideoWidth(env, j_media_player_bridge_);
+}
+
+int MediaPlayerBridge::GetVideoHeight() {
+  if (!prepared_)
+    return height_;
+  JNIEnv* env = base::android::AttachCurrentThread();
+  return Java_MediaPlayerBridge_getVideoHeight(env, j_media_player_bridge_);
+}
+
 void MediaPlayerBridge::SeekTo(base::TimeDelta timestamp) {
   // Record the time to seek when OnMediaPrepared() is called.
   pending_seek_ = timestamp;
   should_seek_on_prepare_ = true;
 
   if (prepared_)
-    SeekInternal(timestamp);
+    SeekInternal(GetCurrentTime(), timestamp);
 }
 
 base::TimeDelta MediaPlayerBridge::GetCurrentTime() {
@@ -314,8 +341,8 @@
 }
 
 base::TimeDelta MediaPlayerBridge::GetDuration() {
-  DCHECK(prepared_);
-
+  if (!prepared_)
+    return duration_;
   JNIEnv* env = base::android::AttachCurrentThread();
   const int duration_ms =
       Java_MediaPlayerBridge_getDuration(env, j_media_player_bridge_);
@@ -326,9 +353,11 @@
 void MediaPlayerBridge::Release() {
   is_active_ = false;
 
+  on_decoder_resources_released_cb_.Run(player_id());
   if (j_media_player_bridge_.is_null())
     return;
 
+  time_update_timer_.Stop();
   if (prepared_) {
     pending_seek_ = GetCurrentTime();
     should_seek_on_prepare_ = true;
@@ -343,12 +372,7 @@
   DetachListener();
 }
 
-void MediaPlayerBridge::SetVolume(double volume) {
-  volume_ = std::max(0.0, std::min(volume, 1.0));
-  UpdateVolumeInternal();
-}
-
-void MediaPlayerBridge::UpdateVolumeInternal() {
+void MediaPlayerBridge::UpdateEffectiveVolumeInternal(double effective_volume) {
   if (j_media_player_bridge_.is_null()) {
     return;
   }
@@ -356,13 +380,14 @@
   JNIEnv* env = base::android::AttachCurrentThread();
   CHECK(env);
 
-  Java_MediaPlayerBridge_setVolume(env, j_media_player_bridge_, volume_);
+  Java_MediaPlayerBridge_setVolume(env, j_media_player_bridge_,
+                                   effective_volume);
 }
 
 void MediaPlayerBridge::OnVideoSizeChanged(int width, int height) {
   width_ = width;
   height_ = height;
-  client_->OnVideoSizeChanged(width, height);
+  MediaPlayerAndroid::OnVideoSizeChanged(width, height);
 }
 
 void MediaPlayerBridge::OnMediaError(int error_type) {
@@ -378,11 +403,17 @@
   if (error_type == MEDIA_ERROR_SERVER_DIED)
     error_type = MEDIA_ERROR_INVALID_CODE;
 
-  client_->OnError(error_type);
+  MediaPlayerAndroid::OnMediaError(error_type);
 }
 
 void MediaPlayerBridge::OnPlaybackComplete() {
-  client_->OnPlaybackComplete();
+  time_update_timer_.Stop();
+  MediaPlayerAndroid::OnPlaybackComplete();
+}
+
+void MediaPlayerBridge::OnMediaInterrupted() {
+  time_update_timer_.Stop();
+  MediaPlayerAndroid::OnMediaInterrupted();
 }
 
 void MediaPlayerBridge::OnMediaPrepared() {
@@ -390,14 +421,14 @@
     return;
 
   prepared_ = true;
-  PropagateDuration(GetDuration());
+  duration_ = GetDuration();
 
   UpdateAllowedOperations();
 
   // If media player was recovered from a saved state, consume all the pending
   // events.
   if (should_seek_on_prepare_) {
-    SeekInternal(pending_seek_);
+    PendingSeekInternal(pending_seek_);
     pending_seek_ = base::TimeDelta::FromMilliseconds(0);
     should_seek_on_prepare_ = false;
   }
@@ -409,6 +440,9 @@
     StartInternal();
     pending_play_ = false;
   }
+
+  manager()->OnMediaMetadataChanged(
+      player_id(), duration_, width_, height_, true);
 }
 
 ScopedJavaLocalRef<jobject> MediaPlayerBridge::GetAllowedOperations() {
@@ -419,24 +453,13 @@
                                                      j_media_player_bridge_);
 }
 
-void MediaPlayerBridge::AttachListener(const JavaRef<jobject>& j_media_player) {
-  listener_->CreateMediaPlayerListener(j_media_player);
-}
-
-void MediaPlayerBridge::DetachListener() {
-  listener_->ReleaseMediaPlayerListenerResources();
-}
-
-base::WeakPtr<MediaPlayerBridge> MediaPlayerBridge::WeakPtrForUIThread() {
-  return weak_factory_.GetWeakPtr();
-}
-
 void MediaPlayerBridge::UpdateAllowedOperations() {
   JNIEnv* env = base::android::AttachCurrentThread();
   CHECK(env);
 
   ScopedJavaLocalRef<jobject> allowedOperations = GetAllowedOperations();
 
+  can_pause_ = Java_AllowedOperations_canPause(env, allowedOperations);
   can_seek_forward_ =
       Java_AllowedOperations_canSeekForward(env, allowedOperations);
   can_seek_backward_ =
@@ -444,25 +467,40 @@
 }
 
 void MediaPlayerBridge::StartInternal() {
+  if (!manager()->RequestPlay(player_id(), duration_, HasAudio())) {
+    Pause(true);
+    return;
+  }
+
   JNIEnv* env = base::android::AttachCurrentThread();
   Java_MediaPlayerBridge_start(env, j_media_player_bridge_);
+  if (!time_update_timer_.IsRunning()) {
+    time_update_timer_.Start(
+        FROM_HERE,
+        base::TimeDelta::FromMilliseconds(kTimeUpdateInterval),
+        this, &MediaPlayerBridge::OnTimeUpdateTimerFired);
+  }
 }
 
 void MediaPlayerBridge::PauseInternal() {
   JNIEnv* env = base::android::AttachCurrentThread();
   Java_MediaPlayerBridge_pause(env, j_media_player_bridge_);
+  time_update_timer_.Stop();
 }
 
-void MediaPlayerBridge::SeekInternal(base::TimeDelta time) {
-  base::TimeDelta current_time = GetCurrentTime();
+void MediaPlayerBridge::PendingSeekInternal(const base::TimeDelta& time) {
+  SeekInternal(GetCurrentTime(), time);
+}
 
+bool MediaPlayerBridge::SeekInternal(base::TimeDelta current_time,
+                                     base::TimeDelta time) {
   // Seeking on content like live streams may cause the media player to
   // get stuck in an error state.
-  if (time < current_time && !can_seek_backward_)
-    return;
+  if (time < current_time && !CanSeekBackward())
+    return false;
 
-  if (time >= current_time && !can_seek_forward_)
-    return;
+  if (time >= current_time && !CanSeekForward())
+    return false;
 
   if (time > duration_)
     time = duration_;
@@ -471,13 +509,39 @@
   // error state.
   if (time < base::TimeDelta()) {
     DCHECK_EQ(-1.0, time.InMillisecondsF());
-    return;
+    return false;
   }
 
   JNIEnv* env = base::android::AttachCurrentThread();
   CHECK(env);
   int time_msec = static_cast<int>(time.InMilliseconds());
   Java_MediaPlayerBridge_seekTo(env, j_media_player_bridge_, time_msec);
+  return true;
+}
+
+void MediaPlayerBridge::OnTimeUpdateTimerFired() {
+  base::TimeDelta current_timestamp = GetCurrentTime();
+  if (last_time_update_timestamp_ == current_timestamp)
+    return;
+  manager()->OnTimeUpdate(player_id(), current_timestamp,
+                          base::TimeTicks::Now());
+  last_time_update_timestamp_ = current_timestamp;
+}
+
+bool MediaPlayerBridge::CanPause() {
+  return can_pause_;
+}
+
+bool MediaPlayerBridge::CanSeekForward() {
+  return can_seek_forward_;
+}
+
+bool MediaPlayerBridge::CanSeekBackward() {
+  return can_seek_backward_;
+}
+
+bool MediaPlayerBridge::IsPlayerReady() {
+  return prepared_;
 }
 
 GURL MediaPlayerBridge::GetUrl() {
--- a/media/base/android/media_player_bridge.h	2019-05-17 17:45:41.240000000 +0300
+++ b/media/base/android/media_player_bridge.h	2019-05-17 18:53:34.056000000 +0300
@@ -9,7 +9,6 @@
 #include <stdint.h>
 
 #include <map>
-#include <memory>
 #include <string>
 
 #include "base/android/scoped_java_ref.h"
@@ -19,133 +18,97 @@
 #include "base/strings/string16.h"
 #include "base/time/time.h"
 #include "base/timer/timer.h"
-#include "media/base/android/media_player_listener.h"
-#include "media/base/media_export.h"
-#include "ui/gl/android/scoped_java_surface.h"
+#include "media/base/android/media_player_android.h"
 #include "url/gurl.h"
 
 namespace media {
 
-class MediaResourceGetter;
-class MediaUrlInterceptor;
+class MediaPlayerManager;
 
 // This class serves as a bridge between the native code and Android MediaPlayer
 // Java class. For more information on Android MediaPlayer, check
-// http://developer.8n6r01d.qjz9zk/reference/android/media/MediaPlayer.html
+// http://developer.android.com/reference/android/media/MediaPlayer.html
 // The actual Android MediaPlayer instance is created lazily when Start(),
 // Pause(), SeekTo() gets called. As a result, media information may not
 // be available until one of those operations is performed. After that, we
 // will cache those information in case the mediaplayer gets released.
 // The class uses the corresponding MediaPlayerBridge Java class to talk to
 // the Android MediaPlayer instance.
-class MEDIA_EXPORT MediaPlayerBridge {
+class MEDIA_EXPORT MediaPlayerBridge : public MediaPlayerAndroid {
  public:
-  class Client {
-   public:
-    // Returns a pointer to the MediaResourceGetter object.
-    virtual MediaResourceGetter* GetMediaResourceGetter() = 0;
-
-    // Returns a pointer to the MediaUrlInterceptor object or null.
-    virtual MediaUrlInterceptor* GetMediaUrlInterceptor() = 0;
-
-    // Called when media duration is first detected or changes.
-    virtual void OnMediaDurationChanged(base::TimeDelta duration) = 0;
-
-    // Called when playback completed.
-    virtual void OnPlaybackComplete() = 0;
-
-    // Called when error happens.
-    virtual void OnError(int error) = 0;
-
-    // Called when video size has changed.
-    virtual void OnVideoSizeChanged(int width, int height) = 0;
-  };
-
-  // Error types for MediaErrorCB.
-  enum MediaErrorType {
-    MEDIA_ERROR_FORMAT,
-    MEDIA_ERROR_DECODE,
-    MEDIA_ERROR_NOT_VALID_FOR_PROGRESSIVE_PLAYBACK,
-    MEDIA_ERROR_INVALID_CODE,
-    MEDIA_ERROR_SERVER_DIED,
-  };
-
   // Construct a MediaPlayerBridge object. This object needs to call |manager|'s
   // RequestMediaResources() before decoding the media stream. This allows
   // |manager| to track unused resources and free them when needed.
   // MediaPlayerBridge also forwards Android MediaPlayer callbacks to
   // the |manager| when needed.
-  MediaPlayerBridge(const GURL& url,
+  MediaPlayerBridge(
+      int player_id,
+      const GURL& url,
                     const GURL& site_for_cookies,
                     const std::string& user_agent,
                     bool hide_url_log,
-                    Client* client,
+      MediaPlayerManager* manager,
+      const OnDecoderResourcesReleasedCB& on_decoder_resources_released_cb,
+      const GURL& frame_url,
                     bool allow_credentials);
-  virtual ~MediaPlayerBridge();
+  ~MediaPlayerBridge() override;
 
   // Initialize this object and extract the metadata from the media.
-  void Initialize();
+  virtual void Initialize();
 
-  // Methods to partially expose the underlying MediaPlayer.
-  void SetVideoSurface(gl::ScopedJavaSurface surface);
-  void Pause();
-  void SeekTo(base::TimeDelta timestamp);
-  base::TimeDelta GetCurrentTime();
-
-  // Starts media playback.
-  // The first call to this method will call Prepare() and create the underlying
-  // MediaPlayer for the first time.
-  void Start();
-
-  // The media URL given to the underlying MediaPlayer.
-  GURL GetUrl();
-
-  // The site whose cookies should be given to the MediaPlayer if needed.
-  GURL GetSiteForCookies();
-
-  // Set the player volume, and take effect immediately.
-  // The volume should be between 0.0 and 1.0.
-  void SetVolume(double volume);
+  // MediaPlayerAndroid implementation.
+  void SetVideoSurface(gl::ScopedJavaSurface surface) override;
+  void Start() override;
+  void Pause(bool is_media_related_action) override;
+  void SeekTo(base::TimeDelta timestamp) override;
+  void Release() override;
+  bool HasVideo() const override;
+  bool HasAudio() const override;
+  int GetVideoWidth() override;
+  int GetVideoHeight() override;
+  base::TimeDelta GetCurrentTime() override;
+  base::TimeDelta GetDuration() override;
+  bool IsPlaying() override;
+  bool CanPause() override;
+  bool CanSeekForward() override;
+  bool CanSeekBackward() override;
+  bool IsPlayerReady() override;
+  GURL GetUrl() override;
+  GURL GetSiteForCookies() override;
 
   void OnDidSetDataUriDataSource(
       JNIEnv* env,
       const base::android::JavaParamRef<jobject>& obj,
       jboolean success);
 
- private:
-  friend class MediaPlayerListener;
-  friend class MediaPlayerBridgeTest;
+ protected:
+  void SetDuration(base::TimeDelta time);
 
-  // Releases the resources such as the underlying MediaPlayer and
-  // MediaPlayerListener.
-  void Release();
-
-  base::TimeDelta GetDuration();
-  void PropagateDuration(base::TimeDelta time);
-  bool IsPlaying();
+  virtual void PendingSeekInternal(const base::TimeDelta& time);
 
   // Prepare the player for playback, asynchronously. When succeeds,
   // OnMediaPrepared() will be called. Otherwise, OnMediaError() will
   // be called with an error type.
-  void Prepare();
+  virtual void Prepare();
 
-  // MediaPlayerListener callbacks.
-  void OnVideoSizeChanged(int width, int height);
-  void OnMediaError(int error_type);
-  void OnPlaybackComplete();
-  void OnMediaPrepared();
+  // MediaPlayerAndroid implementation.
+  void OnVideoSizeChanged(int width, int height) override;
+  void OnMediaError(int error_type) override;
+  void OnPlaybackComplete() override;
+  void OnMediaInterrupted() override;
+  void OnMediaPrepared() override;
 
   // Create the corresponding Java class instance.
-  void CreateJavaMediaPlayerBridge();
+  virtual void CreateJavaMediaPlayerBridge();
 
   // Get allowed operations from the player.
-  base::android::ScopedJavaLocalRef<jobject> GetAllowedOperations();
+  virtual base::android::ScopedJavaLocalRef<jobject> GetAllowedOperations();
+
+ private:
+  friend class MediaPlayerBridgeTest;
 
-  // Attach/Detaches |listener_| for listening to all the media events. If
-  // |j_media_player| is NULL, |listener_| only listens to the system media
-  // events. Otherwise, it also listens to the events from |j_media_player|.
-  void AttachListener(const base::android::JavaRef<jobject>& j_media_player);
-  void DetachListener();
+  // MediaPlayerAndroid implementation
+  void UpdateEffectiveVolumeInternal(double effective_volume) override;
 
   // Set the data source for the media player.
   void SetDataSource(const std::string& url);
@@ -154,9 +117,11 @@
   void StartInternal();
   void PauseInternal();
 
-  // Calls Java MediaPlayerBridge's seekTo method, or no-ops if the operation
-  // is not allowed (based off of |can_seek_forward_| and |can_seek_backward_|).
-  void SeekInternal(base::TimeDelta time);
+  // Returns true if the Java MediaPlayerBridge's seekTo method is called
+  bool SeekInternal(base::TimeDelta current_time, base::TimeDelta time);
+
+  // Called when |time_update_timer_| fires.
+  void OnTimeUpdateTimerFired();
 
   // Update allowed operations from the player.
   void UpdateAllowedOperations();
@@ -167,15 +132,13 @@
 
   // Callback function passed to |resource_getter_|. Called when the auth
   // credentials are retrieved.
-  void OnAuthCredentialsRetrieved(const base::string16& username,
-                                  const base::string16& password);
+  void OnAuthCredentialsRetrieved(
+      const base::string16& username, const base::string16& password);
 
   // Extract the media metadata from a url, asynchronously.
   // OnMediaMetadataExtracted() will be called when this call finishes.
   void ExtractMediaMetadata(const std::string& url);
-  void OnMediaMetadataExtracted(base::TimeDelta duration,
-                                int width,
-                                int height,
+  void OnMediaMetadataExtracted(base::TimeDelta duration, int width, int height,
                                 bool success);
 
   // Returns true if a MediaUrlInterceptor registered by the embedder has
@@ -185,11 +148,6 @@
                          int64_t* offset,
                          int64_t* size);
 
-  // Sets the underlying MediaPlayer's volume.
-  void UpdateVolumeInternal();
-
-  base::WeakPtr<MediaPlayerBridge> WeakPtrForUIThread();
-
   // Whether the player is prepared for playback.
   bool prepared_;
 
@@ -219,12 +177,11 @@
   int width_;
   int height_;
 
+  // Meta data about actions can be taken.
+  bool can_pause_;
   bool can_seek_forward_;
   bool can_seek_backward_;
 
-  // The player volume. Should be between 0.0 and 1.0.
-  double volume_;
-
   // Cookies for |url_|.
   std::string cookies_;
 
@@ -234,9 +191,15 @@
   // Java MediaPlayerBridge instance.
   base::android::ScopedJavaGlobalRef<jobject> j_media_player_bridge_;
 
+  base::RepeatingTimer time_update_timer_;
+
+  base::TimeDelta last_time_update_timestamp_;
+
   // Whether user credentials are allowed to be passed.
   bool allow_credentials_;
 
+  // Helper variables for UMA reporting.
+
   // Whether the preparation for playback or the playback is currently going on.
   // This flag is set in Start() and cleared in Pause() and Release(). Used for
   // UMA reporting only.
@@ -248,13 +211,6 @@
   // The flag is set if Start() has been called at least once.
   bool has_ever_started_;
 
-  // A reference to the owner of |this|.
-  Client* client_;
-
-  // Listener object that listens to all the media player events.
-  std::unique_ptr<MediaPlayerListener> listener_;
-
-  // Weak pointer passed to |listener_| for callbacks.
   // NOTE: Weak pointers must be invalidated before all other member variables.
   base::WeakPtrFactory<MediaPlayerBridge> weak_factory_;
 
--- a/media/base/android/media_player_bridge_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_player_bridge_unittest.cc	2019-05-17 18:53:34.056000000 +0300
@@ -2,10 +2,10 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#include "media/base/android/media_player_bridge.h"
-#include "base/bind.h"
 #include "base/macros.h"
 #include "base/message_loop/message_loop.h"
+#include "media/base/android/media_player_bridge.h"
+#include "media/base/android/media_player_manager.h"
 #include "testing/gmock/include/gmock/gmock.h"
 #include "testing/gtest/include/gtest/gtest.h"
 
@@ -13,17 +13,33 @@
 
 namespace {
 
-using testing::_;
-using testing::StrictMock;
-
-class MockMediaPlayerBridgeClient : public MediaPlayerBridge::Client {
+class MockMediaPlayerManager : public MediaPlayerManager {
  public:
   MOCK_METHOD0(GetMediaResourceGetter, MediaResourceGetter*());
   MOCK_METHOD0(GetMediaUrlInterceptor, MediaUrlInterceptor*());
-  MOCK_METHOD1(OnMediaDurationChanged, void(base::TimeDelta duration));
-  MOCK_METHOD0(OnPlaybackComplete, void());
-  MOCK_METHOD1(OnError, void(int error));
-  MOCK_METHOD2(OnVideoSizeChanged, void(int width, int height));
+  MOCK_METHOD3(OnTimeUpdate,
+               void(int player_id,
+                    base::TimeDelta current_timestamp,
+                    base::TimeTicks current_time_ticks));
+  MOCK_METHOD5(OnMediaMetadataChanged,
+               void(int player_id,
+                    base::TimeDelta duration,
+                    int width,
+                    int height,
+                    bool success));
+  MOCK_METHOD1(OnPlaybackComplete, void(int player_id));
+  MOCK_METHOD1(OnMediaInterrupted, void(int player_id));
+  MOCK_METHOD2(OnBufferingUpdate, void(int player_id, int percentage));
+  MOCK_METHOD2(OnSeekComplete,
+               void(int player_id, const base::TimeDelta& current_time));
+  MOCK_METHOD2(OnError, void(int player_id, int error));
+  MOCK_METHOD3(OnVideoSizeChanged, void(int player_id, int width, int height));
+  MOCK_METHOD2(OnAudibleStateChanged, void(int player_id, bool is_audible_now));
+  MOCK_METHOD1(GetPlayer, MediaPlayerAndroid*(int player_id));
+  MOCK_METHOD3(RequestPlay,
+               bool(int player_id, base::TimeDelta duration, bool has_audio));
+
+  void OnMediaResourcesRequested(int player_id) {}
 };
 
 }  // anonymous namespace
@@ -31,60 +47,63 @@
 class MediaPlayerBridgeTest : public testing::Test {
  public:
   MediaPlayerBridgeTest()
-      : bridge_(GURL(), GURL(), "", false, &client_, false) {}
+      : bridge_(0,
+                GURL(),
+                GURL(),
+                "",
+                false,
+                &manager_,
+                base::Bind(&MockMediaPlayerManager::OnMediaResourcesRequested,
+                           base::Unretained(&manager_)),
+                GURL(),
+                false) {}
 
- protected:
-  void SimulateDurationChange(base::TimeDelta duration) {
-    bridge_.PropagateDuration(duration);
+  void SetCanSeekForward(bool can_seek_forward) {
+    bridge_.can_seek_forward_ = can_seek_forward;
   }
 
-  void SimulateVideoSizeChanged(int width, int height) {
-    bridge_.OnVideoSizeChanged(width, height);
+  void SetCanSeekBackward(bool can_seek_backward) {
+    bridge_.can_seek_backward_ = can_seek_backward;
   }
 
-  void SimulateError(int error) { bridge_.OnMediaError(error); }
-
-  void SimulatePlaybackCompleted() { bridge_.OnPlaybackComplete(); }
+  bool SeekInternal(const base::TimeDelta& current_time, base::TimeDelta time) {
+    return bridge_.SeekInternal(current_time, time);
+  }
 
+ private:
   // A message loop needs to be instantiated in order for the test to run
   // properly.
   base::MessageLoop message_loop_;
-  StrictMock<MockMediaPlayerBridgeClient> client_;
+  MockMediaPlayerManager manager_;
   MediaPlayerBridge bridge_;
 
   DISALLOW_COPY_AND_ASSIGN(MediaPlayerBridgeTest);
 };
 
-TEST_F(MediaPlayerBridgeTest, Client_OnMediaMetadataChanged) {
-  const base::TimeDelta kDuration = base::TimeDelta::FromSeconds(20);
-
-  EXPECT_CALL(client_, OnMediaDurationChanged(kDuration));
-
-  SimulateDurationChange(kDuration);
+TEST_F(MediaPlayerBridgeTest, PreventForwardSeekWhenItIsNotPossible) {
+  // Simulate the Java MediaPlayerBridge reporting that forward seeks are not
+  // possible
+  SetCanSeekForward(false);
+  SetCanSeekBackward(true);
+
+  // If this assertion fails, seeks will be allowed which will result in a
+  // crash because j_media_player_bridge_ cannot be properly instantiated
+  // during this test.
+  ASSERT_FALSE(
+      SeekInternal(base::TimeDelta(), base::TimeDelta::FromSeconds(10)));
 }
 
-TEST_F(MediaPlayerBridgeTest, Client_OnVideoSizeChanged) {
-  const int kWidth = 1600;
-  const int kHeight = 900;
-
-  EXPECT_CALL(client_, OnVideoSizeChanged(kWidth, kHeight));
-
-  SimulateVideoSizeChanged(kWidth, kHeight);
-}
-
-TEST_F(MediaPlayerBridgeTest, Client_OnPlaybackComplete) {
-  EXPECT_CALL(client_, OnPlaybackComplete());
-
-  SimulatePlaybackCompleted();
-}
-
-TEST_F(MediaPlayerBridgeTest, Client_OnError) {
-  // MEDIA_ERROR_INVALID_CODE should still be propagated.
-  EXPECT_CALL(client_, OnError(_)).Times(1);
-  SimulateError(MediaPlayerBridge::MediaErrorType::MEDIA_ERROR_INVALID_CODE);
-
-  EXPECT_CALL(client_, OnError(_)).Times(1);
-  SimulateError(MediaPlayerBridge::MediaErrorType::MEDIA_ERROR_FORMAT);
+TEST_F(MediaPlayerBridgeTest, PreventBackwardSeekWhenItIsNotPossible) {
+  // Simulate the Java MediaPlayerBridge reporting that backward seeks are not
+  // possible
+  SetCanSeekForward(true);
+  SetCanSeekBackward(false);
+
+  // If this assertion fails, seeks will be allowed which will result in a
+  // crash because j_media_player_bridge_ cannot be properly instantiated
+  // during this test.
+  ASSERT_FALSE(
+      SeekInternal(base::TimeDelta::FromSeconds(10), base::TimeDelta()));
 }
 
 }  // namespace media
--- a/media/base/android/media_player_listener.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_player_listener.cc	2019-05-17 18:53:34.060000000 +0300
@@ -11,7 +11,7 @@
 #include "base/logging.h"
 #include "base/single_thread_task_runner.h"
 #include "jni/MediaPlayerListener_jni.h"
-#include "media/base/android/media_player_bridge.h"
+#include "media/base/android/media_player_android.h"
 
 using base::android::AttachCurrentThread;
 using base::android::CheckException;
@@ -23,8 +23,9 @@
 
 MediaPlayerListener::MediaPlayerListener(
     const scoped_refptr<base::SingleThreadTaskRunner>& task_runner,
-    base::WeakPtr<MediaPlayerBridge> media_player)
-    : task_runner_(task_runner), media_player_(media_player) {
+    base::WeakPtr<MediaPlayerAndroid> media_player)
+    : task_runner_(task_runner),
+      media_player_(media_player) {
   DCHECK(task_runner_.get());
   DCHECK(media_player_);
 }
@@ -48,9 +49,9 @@
 void MediaPlayerListener::OnMediaError(JNIEnv* /* env */,
                                        const JavaParamRef<jobject>& /* obj */,
                                        jint error_type) {
-  task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&MediaPlayerBridge::OnMediaError, media_player_,
-                                error_type));
+  task_runner_->PostTask(FROM_HERE,
+                         base::BindOnce(&MediaPlayerAndroid::OnMediaError,
+                                        media_player_, error_type));
 }
 
 void MediaPlayerListener::OnVideoSizeChanged(
@@ -59,16 +60,33 @@
     jint width,
     jint height) {
   task_runner_->PostTask(FROM_HERE,
-                         base::BindOnce(&MediaPlayerBridge::OnVideoSizeChanged,
+                         base::BindOnce(&MediaPlayerAndroid::OnVideoSizeChanged,
                                         media_player_, width, height));
 }
 
+void MediaPlayerListener::OnBufferingUpdate(
+    JNIEnv* /* env */,
+    const JavaParamRef<jobject>& /* obj */,
+    jint percent) {
+  task_runner_->PostTask(FROM_HERE,
+                         base::BindOnce(&MediaPlayerAndroid::OnBufferingUpdate,
+                                        media_player_, percent));
+}
+
 void MediaPlayerListener::OnPlaybackComplete(
     JNIEnv* /* env */,
     const JavaParamRef<jobject>& /* obj */) {
   task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&MediaPlayerBridge::OnPlaybackComplete, media_player_));
+      base::BindOnce(&MediaPlayerAndroid::OnPlaybackComplete, media_player_));
+}
+
+void MediaPlayerListener::OnSeekComplete(
+    JNIEnv* /* env */,
+    const JavaParamRef<jobject>& /* obj */) {
+  task_runner_->PostTask(
+      FROM_HERE,
+      base::BindOnce(&MediaPlayerAndroid::OnSeekComplete, media_player_));
 }
 
 void MediaPlayerListener::OnMediaPrepared(
@@ -76,7 +94,15 @@
     const JavaParamRef<jobject>& /* obj */) {
   task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&MediaPlayerBridge::OnMediaPrepared, media_player_));
+      base::BindOnce(&MediaPlayerAndroid::OnMediaPrepared, media_player_));
+}
+
+void MediaPlayerListener::OnMediaInterrupted(
+    JNIEnv* /* env */,
+    const JavaParamRef<jobject>& /* obj */) {
+  task_runner_->PostTask(
+      FROM_HERE,
+      base::BindOnce(&MediaPlayerAndroid::OnMediaInterrupted, media_player_));
 }
 
 }  // namespace media
--- a/media/base/android/media_player_listener.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_player_listener.h	2019-05-17 18:53:34.060000000 +0300
@@ -18,10 +18,10 @@
 
 namespace media {
 
-class MediaPlayerBridge;
+class MediaPlayerAndroid;
 
 // Acts as a thread proxy between java MediaPlayerListener object and
-// MediaPlayerBridge so that callbacks are posted onto the UI thread.
+// MediaPlayerAndroid so that callbacks are posted onto the UI thread.
 class MediaPlayerListener {
  public:
   // Construct a native MediaPlayerListener object. Callbacks from the java
@@ -29,7 +29,7 @@
   // |task_runner|.
   MediaPlayerListener(
       const scoped_refptr<base::SingleThreadTaskRunner>& task_runner,
-      base::WeakPtr<MediaPlayerBridge> media_player);
+      base::WeakPtr<MediaPlayerAndroid> media_player);
   virtual ~MediaPlayerListener();
 
   // Called by the Java MediaPlayerListener and mirrored to corresponding
@@ -47,8 +47,13 @@
   void OnPlaybackComplete(
       JNIEnv* /* env */,
       const base::android::JavaParamRef<jobject>& /* obj */);
+  void OnSeekComplete(JNIEnv* /* env */,
+                      const base::android::JavaParamRef<jobject>& /* obj */);
   void OnMediaPrepared(JNIEnv* /* env */,
                        const base::android::JavaParamRef<jobject>& /* obj */);
+  void OnMediaInterrupted(
+      JNIEnv* /* env */,
+      const base::android::JavaParamRef<jobject>& /* obj */);
 
   // Create a Java MediaPlayerListener object and listens to all the media
   // related events from system and |media_player|. If |media_player| is NULL,
@@ -61,8 +66,8 @@
   // The message loop where |media_player_| lives.
   scoped_refptr<base::SingleThreadTaskRunner> task_runner_;
 
-  // The MediaPlayerBridge object all the callbacks should be sent to.
-  base::WeakPtr<MediaPlayerBridge> media_player_;
+  // The MediaPlayerAndroid object all the callbacks should be sent to.
+  base::WeakPtr<MediaPlayerAndroid> media_player_;
 
   base::android::ScopedJavaGlobalRef<jobject> j_media_player_listener_;
 
--- a/media/base/android/media_url_interceptor.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/android/media_url_interceptor.h	2019-05-17 18:53:34.060000000 +0300
@@ -18,7 +18,7 @@
 // and translate them into files containing media.
 class MEDIA_EXPORT MediaUrlInterceptor {
  public:
-  virtual ~MediaUrlInterceptor() {}
+  virtual ~MediaUrlInterceptor() {};
 
   // Returns true if the embedder has intercepted the url and
   // false otherwise.
--- a/media/base/audio_buffer.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_buffer.h	2019-05-17 18:53:34.060000000 +0300
@@ -28,7 +28,7 @@
 struct TypeConverter;
 template <typename T>
 class StructPtr;
-}  // namespace mojo
+};
 
 namespace media {
 class AudioBus;
--- a/media/base/audio_converter_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_converter_unittest.cc	2019-05-17 18:53:34.064000000 +0300
@@ -251,7 +251,7 @@
   RunTest(kConvertInputs);
 }
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     AudioConverterTest,
     AudioConverterTest,
     testing::Values(
--- a/media/base/audio_latency_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_latency_unittest.cc	2019-05-17 18:53:34.064000000 +0300
@@ -161,7 +161,7 @@
   TestExactBufferSizes();
 }
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     /* no prefix */,
     AudioLatencyTest,
 #if defined(OS_WIN)
@@ -187,5 +187,5 @@
     testing::Values(std::make_tuple(44100, 256, 0, 0),
                     std::make_tuple(44100, 440, 0, 0))
 #endif
-);
+        );
 }  // namespace media
--- a/media/base/audio_parameters.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_parameters.cc	2019-05-17 18:53:34.064000000 +0300
@@ -117,11 +117,11 @@
 
 std::string AudioParameters::AsHumanReadableString() const {
   std::ostringstream s;
-  s << "format: " << format() << ", channel_layout: " << channel_layout()
-    << ", channels: " << channels() << ", sample_rate: " << sample_rate()
-    << ", frames_per_buffer: " << frames_per_buffer()
-    << ", effects: " << effects()
-    << ", mic_positions: " << PointsToString(mic_positions_);
+  s << "format: " << format() << " channel_layout: " << channel_layout()
+    << " channels: " << channels() << " sample_rate: " << sample_rate()
+    << " frames_per_buffer: " << frames_per_buffer()
+    << " effects: " << effects()
+    << " mic_positions: " << PointsToString(mic_positions_);
   if (hardware_capabilities_) {
     s << ", hw_cap.min_frames_per_buffer: "
       << hardware_capabilities_->min_frames_per_buffer
--- a/media/base/audio_parameters.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_parameters.h	2019-05-17 18:53:34.064000000 +0300
@@ -221,7 +221,7 @@
   ChannelLayout channel_layout() const { return channel_layout_; }
 
   // The number of channels is usually computed from channel_layout_. Setting
-  // this explicitly is only required with CHANNEL_LAYOUT_DISCRETE.
+  // this explictly is only required with CHANNEL_LAYOUT_DISCRETE.
   void set_channels_for_discrete(int channels) {
     DCHECK(channel_layout_ == CHANNEL_LAYOUT_DISCRETE ||
            channels == ChannelLayoutToChannelCount(channel_layout_));
--- a/media/base/audio_pull_fifo_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_pull_fifo_unittest.cc	2019-05-17 18:53:34.068000000 +0300
@@ -96,9 +96,8 @@
 
 // Test common |frames_to_consume| values which will be used as input
 // parameter to AudioPullFifo::Consume() when the consumer asks for data.
-INSTANTIATE_TEST_SUITE_P(
-    AudioPullFifoTest,
-    AudioPullFifoTest,
+INSTANTIATE_TEST_CASE_P(
+    AudioPullFifoTest, AudioPullFifoTest,
     testing::Values(544, 512, 512, 512, 512, 2048, 544, 441, 440, 433, 500));
 
 }  // namespace media
--- a/media/base/audio_push_fifo_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_push_fifo_unittest.cc	2019-05-17 18:53:34.068000000 +0300
@@ -230,7 +230,7 @@
   }
 }
 
-INSTANTIATE_TEST_SUITE_P(,
+INSTANTIATE_TEST_CASE_P(,
                          AudioPushFifoTest,
                          ::testing::Values(
                              // 1 ms output chunks at common sample rates.
--- a/media/base/audio_renderer_mixer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_renderer_mixer_unittest.cc	2019-05-17 18:53:34.068000000 +0300
@@ -97,7 +97,7 @@
                                const OutputDeviceInfo& sink_info,
                                scoped_refptr<AudioRendererSink> sink) final {
     return mixer_.get();
-  }
+  };
 
   void ReturnMixer(AudioRendererMixer* mixer) override {
     EXPECT_EQ(mixer_.get(), mixer);
@@ -525,7 +525,7 @@
   mixer_inputs_[0]->Stop();
 }
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     /* no prefix */,
     AudioRendererMixerTest,
     testing::Values(
@@ -559,7 +559,7 @@
 // Test cases for behavior which is independent of parameters.  Values() doesn't
 // support single item lists and we don't want these test cases to run for every
 // parameter set.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     /* no prefix */,
     AudioRendererMixerBehavioralTest,
     testing::ValuesIn(std::vector<AudioRendererMixerTestData>(
--- a/media/base/audio_sample_types_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_sample_types_unittest.cc	2019-05-17 18:53:34.068000000 +0300
@@ -13,7 +13,7 @@
 template <typename TestConfig>
 class SampleTypeTraitsTest : public testing::Test {};
 
-TYPED_TEST_SUITE_P(SampleTypeTraitsTest);
+TYPED_TEST_CASE_P(SampleTypeTraitsTest);
 
 struct UnsignedInt8ToFloat32TestConfig {
   using SourceTraits = UnsignedInt8SampleTypeTraits;
@@ -209,7 +209,7 @@
   }
 }
 
-REGISTER_TYPED_TEST_SUITE_P(SampleTypeTraitsTest, ConvertExampleValues);
+REGISTER_TYPED_TEST_CASE_P(SampleTypeTraitsTest, ConvertExampleValues);
 
 typedef ::testing::Types<UnsignedInt8ToFloat32TestConfig,
                          SignedInt16ToFloat32TestConfig,
@@ -224,6 +224,6 @@
                          Float64ToSignedInt16TestConfig,
                          Float64ToSignedInt32TestConfig>
     TestConfigs;
-INSTANTIATE_TYPED_TEST_SUITE_P(CommonTypes, SampleTypeTraitsTest, TestConfigs);
+INSTANTIATE_TYPED_TEST_CASE_P(CommonTypes, SampleTypeTraitsTest, TestConfigs);
 
 }  // namespace media
--- a/media/base/audio_shifter_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/audio_shifter_unittest.cc	2019-05-17 18:53:34.068000000 +0300
@@ -197,7 +197,8 @@
 
 // Note: First argument is optional and intentionally left blank.
 // (it's a prefix for the generated test cases)
-INSTANTIATE_TEST_SUITE_P(,
+INSTANTIATE_TEST_CASE_P(
+    ,
                          AudioShifterTest,
                          ::testing::Combine(::testing::Range(0, 3),
                                             ::testing::Range(0, 3),
--- a/media/base/bind_to_current_loop_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/bind_to_current_loop_unittest.cc	2019-05-17 18:53:34.068000000 +0300
@@ -7,7 +7,6 @@
 #include <memory>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/memory/free_deleter.h"
 #include "base/message_loop/message_loop.h"
 #include "base/run_loop.h"
--- a/media/base/bit_reader_core.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/bit_reader_core.cc	2019-05-17 18:53:34.068000000 +0300
@@ -113,7 +113,6 @@
     // empty the current bit register for that purpose.
     nbits_ = 0;
     reg_ = 0;
-    *out = 0;
     return false;
   }
 
--- a/media/base/bit_reader_core.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/bit_reader_core.h	2019-05-17 18:53:34.068000000 +0300
@@ -56,9 +56,8 @@
   // integer type.
   template<typename T> bool ReadBits(int num_bits, T* out) {
     DCHECK_LE(num_bits, static_cast<int>(sizeof(T) * 8));
-    uint64_t temp = 0;
+    uint64_t temp;
     bool ret = ReadBitsInternal(num_bits, &temp);
-    if (ret)
       *out = static_cast<T>(temp);
     return ret;
   }
--- a/media/base/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/BUILD.gn	2019-05-17 18:53:34.048000000 +0300
@@ -55,8 +55,6 @@
     "audio_fifo.h",
     "audio_hash.cc",
     "audio_hash.h",
-    "audio_processing.cc",
-    "audio_processing.h",
     "audio_pull_fifo.cc",
     "audio_pull_fifo.h",
     "audio_push_fifo.cc",
@@ -183,7 +181,6 @@
     "media_track.h",
     "media_tracks.cc",
     "media_tracks.h",
-    "media_types.cc",
     "media_types.h",
     "media_url_demuxer.cc",
     "media_url_demuxer.h",
@@ -236,8 +233,6 @@
     "serial_runner.h",
     "silent_sink_suspender.cc",
     "silent_sink_suspender.h",
-    "simple_sync_token_client.cc",
-    "simple_sync_token_client.h",
     "sinc_resampler.cc",
     "sinc_resampler.h",
     "stream_parser.cc",
@@ -306,6 +301,7 @@
     ":video_facing",
     "//media:media_buildflags",
     "//media:shared_memory_support",
+    "//third_party/libaom:av1_buildflags",
     "//ui/gfx:color_space",
   ]
   deps = [
--- a/media/base/callback_holder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/callback_holder_unittest.cc	2019-05-17 18:53:34.072000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/base/callback_holder.h"
 
-#include "base/bind.h"
 #include "testing/gtest/include/gtest/gtest.h"
 
 namespace media {
--- a/media/base/channel_mixer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/channel_mixer_unittest.cc	2019-05-17 18:53:34.072000000 +0300
@@ -160,7 +160,7 @@
 static float kFiveDiscreteValues[] = { 0.1f, 0.2f, 0.3f, 0.4f, 0.5f };
 
 // Run through basic sanity tests for some common conversions.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     ChannelMixerTest,
     ChannelMixerTest,
     testing::Values(ChannelMixerTestData(CHANNEL_LAYOUT_STEREO,
--- a/media/base/container_names.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/container_names.cc	2019-05-17 18:53:34.072000000 +0300
@@ -73,17 +73,11 @@
 }
 
 // Helper function to read up to 64 bits from a bit stream.
-// TODO(chcunningham): Delete this helper and replace with direct calls to
-// reader that handle read failure. As-is, we hide failure because returning 0
-// is valid for both a successful and failed read.
 static uint64_t ReadBits(BitReader* reader, int num_bits) {
   DCHECK_GE(reader->bits_available(), num_bits);
   DCHECK((num_bits > 0) && (num_bits <= 64));
-  uint64_t value = 0;
-
-  if (!reader->ReadBits(num_bits, &value))
-    return 0;
-
+  uint64_t value;
+  reader->ReadBits(num_bits, &value);
   return value;
 }
 
@@ -310,9 +304,7 @@
     reader.SkipBits(6);
 
     // Verify core audio sampling frequency is an allowed value.
-    size_t sampling_freq_index = ReadBits(&reader, 4);
-    RCHECK(sampling_freq_index < base::size(kSamplingFrequencyValid));
-    RCHECK(kSamplingFrequencyValid[sampling_freq_index]);
+    RCHECK(kSamplingFrequencyValid[ReadBits(&reader, 4)]);
 
     // Verify transmission bit rate is valid.
     RCHECK(ReadBits(&reader, 5) <= 25);
@@ -324,9 +316,7 @@
     reader.SkipBits(1 + 1 + 1 + 1);
 
     // Verify extension audio descriptor flag is an allowed value.
-    size_t audio_id_index = ReadBits(&reader, 3);
-    RCHECK(audio_id_index < base::size(kExtAudioIdValid));
-    RCHECK(kExtAudioIdValid[audio_id_index]);
+    RCHECK(kExtAudioIdValid[ReadBits(&reader, 3)]);
 
     // Skip extended coding flag and audio sync word insertion flag.
     reader.SkipBits(1 + 1);
--- a/media/base/decrypt_config.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/decrypt_config.h	2019-05-17 18:53:34.076000000 +0300
@@ -70,7 +70,7 @@
   const EncryptionMode& encryption_mode() const { return encryption_mode_; }
   const base::Optional<EncryptionPattern>& encryption_pattern() const {
     return encryption_pattern_;
-  }
+  };
 
   std::unique_ptr<DecryptConfig> Clone() const;
 
--- a/media/base/fake_audio_worker.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/fake_audio_worker.cc	2019-05-17 18:53:34.080000000 +0300
@@ -4,8 +4,6 @@
 
 #include "media/base/fake_audio_worker.h"
 
-#include <utility>
-
 #include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/cancelable_callback.h"
@@ -18,7 +16,6 @@
 #include "base/threading/thread_checker.h"
 #include "base/time/time.h"
 #include "media/base/audio_parameters.h"
-#include "media/base/audio_timestamp_helper.h"
 
 namespace media {
 
@@ -29,7 +26,7 @@
          const AudioParameters& params);
 
   bool IsStopped();
-  void Start(FakeAudioWorker::Callback worker_cb);
+  void Start(const base::Closure& worker_cb);
   void Stop();
 
  private:
@@ -48,13 +45,11 @@
   void DoRead();
 
   const scoped_refptr<base::SingleThreadTaskRunner> worker_task_runner_;
-  const int sample_rate_;
-  const int frames_per_read_;
+  const base::TimeDelta buffer_duration_;
 
   base::Lock worker_cb_lock_;  // Held while mutating or running |worker_cb_|.
-  FakeAudioWorker::Callback worker_cb_ GUARDED_BY(worker_cb_lock_);
-  base::TimeTicks first_read_time_;
-  int64_t frames_elapsed_;
+  base::Closure worker_cb_ GUARDED_BY(worker_cb_lock_);
+  base::TimeTicks next_read_time_;
 
   // Used to cancel any delayed tasks still inside the worker loop's queue.
   base::CancelableClosure worker_task_cb_;
@@ -73,32 +68,22 @@
   DCHECK(worker_->IsStopped());
 }
 
-void FakeAudioWorker::Start(FakeAudioWorker::Callback worker_cb) {
+void FakeAudioWorker::Start(const base::Closure& worker_cb) {
   DCHECK(worker_->IsStopped());
-  worker_->Start(std::move(worker_cb));
+  worker_->Start(worker_cb);
 }
 
 void FakeAudioWorker::Stop() {
   worker_->Stop();
 }
 
-// static
-base::TimeDelta FakeAudioWorker::ComputeFakeOutputDelay(
-    const AudioParameters& params) {
-  // Typical delay values used by real AudioOutputStreams on Win, Mac, and Linux
-  // tend to be around 1.5X to 3X of the buffer duration. So, 2X is chosen as a
-  // general-purpose value.
-  constexpr int kDelayFactor = 2;
-  return AudioTimestampHelper::FramesToTime(
-      params.frames_per_buffer() * kDelayFactor, params.sample_rate());
-}
-
 FakeAudioWorker::Worker::Worker(
     const scoped_refptr<base::SingleThreadTaskRunner>& worker_task_runner,
     const AudioParameters& params)
     : worker_task_runner_(worker_task_runner),
-      sample_rate_(params.sample_rate()),
-      frames_per_read_(params.frames_per_buffer()) {
+      buffer_duration_(base::TimeDelta::FromMicroseconds(
+          params.frames_per_buffer() * base::Time::kMicrosecondsPerSecond /
+          static_cast<float>(params.sample_rate()))) {
   // Worker can be constructed on any thread, but will DCHECK that its
   // Start/Stop methods are called from the same thread.
   DETACH_FROM_THREAD(thread_checker_);
@@ -113,13 +98,13 @@
   return !worker_cb_;
 }
 
-void FakeAudioWorker::Worker::Start(FakeAudioWorker::Callback worker_cb) {
+void FakeAudioWorker::Worker::Start(const base::Closure& worker_cb) {
   DCHECK_CALLED_ON_VALID_THREAD(thread_checker_);
   DCHECK(worker_cb);
   {
     base::AutoLock scoped_lock(worker_cb_lock_);
     DCHECK(!worker_cb_);
-    worker_cb_ = std::move(worker_cb);
+    worker_cb_ = worker_cb;
   }
   worker_task_runner_->PostTask(FROM_HERE,
                                 base::BindOnce(&Worker::DoStart, this));
@@ -127,8 +112,7 @@
 
 void FakeAudioWorker::Worker::DoStart() {
   DCHECK(worker_task_runner_->BelongsToCurrentThread());
-  first_read_time_ = base::TimeTicks::Now();
-  frames_elapsed_ = 0;
+  next_read_time_ = base::TimeTicks::Now();
   worker_task_cb_.Reset(base::Bind(&Worker::DoRead, this));
   worker_task_cb_.callback().Run();
 }
@@ -153,37 +137,24 @@
 void FakeAudioWorker::Worker::DoRead() {
   DCHECK(worker_task_runner_->BelongsToCurrentThread());
 
-  const base::TimeTicks read_time =
-      first_read_time_ +
-      AudioTimestampHelper::FramesToTime(frames_elapsed_, sample_rate_);
-  frames_elapsed_ += frames_per_read_;
-  base::TimeTicks next_read_time =
-      first_read_time_ +
-      AudioTimestampHelper::FramesToTime(frames_elapsed_, sample_rate_);
-
-  base::TimeTicks now;
   {
     base::AutoLock scoped_lock(worker_cb_lock_);
-    // Important to sample the clock after waiting to acquire the lock.
-    now = base::TimeTicks::Now();
-    if (worker_cb_ && next_read_time > now) {
-      worker_cb_.Run(read_time, now);
-    }
+    if (worker_cb_)
+      worker_cb_.Run();
   }
 
-  // If we're behind, find the next nearest ontime interval. Note, we could be
-  // behind many intervals (e.g., if the system is resuming from sleep).
-  if (next_read_time <= now) {
-    frames_elapsed_ = AudioTimestampHelper::TimeToFrames(now - first_read_time_,
-                                                         sample_rate_);
-    frames_elapsed_ =
-        ((frames_elapsed_ / frames_per_read_) + 1) * frames_per_read_;
-    next_read_time = first_read_time_ + AudioTimestampHelper::FramesToTime(
-                                            frames_elapsed_, sample_rate_);
-  }
+  // Need to account for time spent here due to the cost of |worker_cb| as well
+  // as the imprecision of PostDelayedTask().
+  const base::TimeTicks now = base::TimeTicks::Now();
+  base::TimeDelta delay = next_read_time_ + buffer_duration_ - now;
+
+  // If we're behind, find the next nearest ontime interval.
+  if (delay < base::TimeDelta())
+    delay += buffer_duration_ * (-delay / buffer_duration_ + 1);
+  next_read_time_ = now + delay;
 
   worker_task_runner_->PostDelayedTask(FROM_HERE, worker_task_cb_.callback(),
-                                       next_read_time - now);
+                                       delay);
 }
 
 }  // namespace media
--- a/media/base/fake_audio_worker.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/fake_audio_worker.h	2019-05-17 18:53:34.080000000 +0300
@@ -21,12 +21,6 @@
 // call back the provided callback like a real audio consumer or producer would.
 class MEDIA_EXPORT FakeAudioWorker {
  public:
-  // The worker callback, which is run at regular intervals. |ideal_time| is
-  // when the callback was scheduled to run, while |now| is when the callback is
-  // actually being run.
-  using Callback = base::RepeatingCallback<void(base::TimeTicks ideal_time,
-                                                base::TimeTicks now)>;
-
   // |worker_task_runner| is the task runner on which the closure provided to
   // Start() will be executed on.  This may or may not be the be for the same
   // thread that invokes the Start/Stop methods.
@@ -38,17 +32,13 @@
 
   // Start executing |worker_cb| at a regular intervals.  Stop() must be called
   // by the same thread before destroying FakeAudioWorker.
-  void Start(Callback worker_cb);
+  void Start(const base::Closure& worker_cb);
 
   // Stop executing the closure provided to Start(). Blocks until the worker
   // loop is not inside a closure invocation. Safe to call multiple times.
   // Must be called on the same thread that called Start().
   void Stop();
 
-  // Returns a reasonable fixed output delay value for a "sink" using a
-  // FakeAudioWorker.
-  static base::TimeDelta ComputeFakeOutputDelay(const AudioParameters& params);
-
  private:
   // All state and implementation is kept within this ref-counted class because
   // cancellation of posted tasks must happen on the worker thread some time
--- a/media/base/fake_audio_worker_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/fake_audio_worker_unittest.cc	2019-05-17 18:53:34.080000000 +0300
@@ -32,14 +32,12 @@
 
   ~FakeAudioWorkerTest() override = default;
 
-  void CalledByFakeWorker(base::TimeTicks ideal_time, base::TimeTicks now) {
-    seen_callbacks_++;
-  }
+  void CalledByFakeWorker() { seen_callbacks_++; }
 
   void RunOnAudioThread() {
     ASSERT_TRUE(message_loop_.task_runner()->BelongsToCurrentThread());
-    fake_worker_.Start(base::BindRepeating(
-        &FakeAudioWorkerTest::CalledByFakeWorker, base::Unretained(this)));
+    fake_worker_.Start(base::Bind(&FakeAudioWorkerTest::CalledByFakeWorker,
+                                  base::Unretained(this)));
   }
 
   void RunOnceOnAudioThread() {
@@ -48,8 +46,8 @@
     // Start() should immediately post a task to run the callback, so we
     // should end up with only a single callback being run.
     message_loop_.task_runner()->PostTask(
-        FROM_HERE, base::BindOnce(&FakeAudioWorkerTest::EndTest,
-                                  base::Unretained(this), 1));
+        FROM_HERE,
+        base::Bind(&FakeAudioWorkerTest::EndTest, base::Unretained(this), 1));
   }
 
   void StopStartOnAudioThread() {
@@ -70,7 +68,7 @@
     if (seen_callbacks_ < callbacks) {
       message_loop_.task_runner()->PostDelayedTask(
           FROM_HERE,
-          base::BindOnce(&FakeAudioWorkerTest::TimeCallbacksOnAudioThread,
+          base::Bind(&FakeAudioWorkerTest::TimeCallbacksOnAudioThread,
                          base::Unretained(this), callbacks),
           time_between_callbacks_ / 2);
     } else {
@@ -142,7 +140,7 @@
   // chance of catching the worker doing the wrong thing.
   message_loop_.task_runner()->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&FakeAudioWorkerTest::StopStartOnAudioThread,
+      base::Bind(&FakeAudioWorkerTest::StopStartOnAudioThread,
                      base::Unretained(this)),
       time_between_callbacks_ / 2);
 
--- a/media/base/fake_demuxer_stream.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/fake_demuxer_stream.h	2019-05-17 18:53:34.080000000 +0300
@@ -69,8 +69,6 @@
   // Sets further read requests to return EOS buffers.
   void SeekToEndOfStream();
 
-  base::TimeDelta duration() const { return duration_; }
-
  private:
   void UpdateVideoDecoderConfig();
   void DoRead();
--- a/media/base/fallback_video_decoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/fallback_video_decoder_unittest.cc	2019-05-17 18:53:34.080000000 +0300
@@ -82,7 +82,7 @@
   DISALLOW_COPY_AND_ASSIGN(FallbackVideoDecoderUnittest);
 };
 
-INSTANTIATE_TEST_SUITE_P(DoesPreferredInitFail,
+INSTANTIATE_TEST_CASE_P(DoesPreferredInitFail,
                          FallbackVideoDecoderUnittest,
                          testing::ValuesIn({true, false}));
 
--- a/media/base/ipc/media_param_traits_macros.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/ipc/media_param_traits_macros.h	2019-05-17 18:53:34.080000000 +0300
@@ -118,16 +118,17 @@
                           media::OUTPUT_DEVICE_STATUS_MAX)
 
 IPC_ENUM_TRAITS_MAX_VALUE(media::PipelineStatus,
-                          media::PipelineStatus::PIPELINE_STATUS_MAX)
+                          media::PipelineStatus::PIPELINE_STATUS_MAX);
 
 IPC_ENUM_TRAITS_MAX_VALUE(media::SampleFormat, media::kSampleFormatMax)
 
 IPC_ENUM_TRAITS_MAX_VALUE(media::VideoCodec, media::kVideoCodecMax)
 
-IPC_ENUM_TRAITS_MAX_VALUE(media::WaitingReason, media::WaitingReason::kMaxValue)
+IPC_ENUM_TRAITS_MAX_VALUE(media::WaitingReason,
+                          media::WaitingReason::kMaxValue);
 
 IPC_ENUM_TRAITS_MAX_VALUE(media::WatchTimeKey,
-                          media::WatchTimeKey::kWatchTimeKeyMax)
+                          media::WatchTimeKey::kWatchTimeKeyMax);
 
 IPC_ENUM_TRAITS_MIN_MAX_VALUE(media::VideoCodecProfile,
                               media::VIDEO_CODEC_PROFILE_MIN,
@@ -138,31 +139,31 @@
 IPC_ENUM_TRAITS_MAX_VALUE(media::VideoRotation, media::VIDEO_ROTATION_MAX)
 
 IPC_ENUM_TRAITS_MAX_VALUE(media::container_names::MediaContainerName,
-                          media::container_names::CONTAINER_MAX)
+                          media::container_names::CONTAINER_MAX);
 
 #if defined(OS_ANDROID)
 IPC_ENUM_TRAITS_MIN_MAX_VALUE(media::MediaDrmKeyType,
                               media::MediaDrmKeyType::MIN,
-                              media::MediaDrmKeyType::MAX)
+                              media::MediaDrmKeyType::MAX);
 #endif  // defined(OS_ANDROID)
 
 IPC_ENUM_TRAITS_VALIDATE(
     media::VideoColorSpace::PrimaryID,
     static_cast<int>(value) ==
         static_cast<int>(
-            media::VideoColorSpace::GetPrimaryID(static_cast<int>(value))))
+            media::VideoColorSpace::GetPrimaryID(static_cast<int>(value))));
 
 IPC_ENUM_TRAITS_VALIDATE(
     media::VideoColorSpace::TransferID,
     static_cast<int>(value) ==
         static_cast<int>(
-            media::VideoColorSpace::GetTransferID(static_cast<int>(value))))
+            media::VideoColorSpace::GetTransferID(static_cast<int>(value))));
 
 IPC_ENUM_TRAITS_VALIDATE(
     media::VideoColorSpace::MatrixID,
     static_cast<int>(value) ==
         static_cast<int>(
-            media::VideoColorSpace::GetMatrixID(static_cast<int>(value))))
+            media::VideoColorSpace::GetMatrixID(static_cast<int>(value))));
 
 // Struct traits.
 
--- a/media/base/key_systems.cc	2019-05-17 17:45:41.240000000 +0300
+++ b/media/base/key_systems.cc	2019-05-17 18:53:34.084000000 +0300
@@ -140,7 +140,7 @@
 
   SupportedCodecs GetSupportedCodecs() const override {
     // On Android, Vorbis, VP8, AAC and AVC1 are supported in MediaCodec:
-    // http://developer.8n6r01d.qjz9zk/guide/appendix/media-formats.html
+    // http://developer.android.com/guide/appendix/media-formats.html
     // VP9 support is device dependent.
     return EME_CODEC_WEBM_ALL | EME_CODEC_MP4_ALL;
   }
--- a/media/base/key_systems.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/key_systems.h	2019-05-17 18:53:34.084000000 +0300
@@ -76,7 +76,7 @@
       const std::string& key_system) const = 0;
 
  protected:
-  virtual ~KeySystems() {}
+  virtual ~KeySystems() {};
 };
 
 // TODO(ddorwin): WebContentDecryptionModuleSessionImpl::initializeNewSession()
--- a/media/base/media_log.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/media_log.cc	2019-05-17 18:53:34.084000000 +0300
@@ -264,16 +264,10 @@
     MediaLogEvent::Type type,
     const std::string& property,
     base::TimeDelta value) {
-  return CreateTimeEvent(type, property, value.InSecondsF());
-}
-
-std::unique_ptr<MediaLogEvent> MediaLog::CreateTimeEvent(
-    MediaLogEvent::Type type,
-    const std::string& property,
-    double value) {
   std::unique_ptr<MediaLogEvent> event(CreateEvent(type));
-  if (std::isfinite(value))
-    event->params.SetDouble(property, value);
+  double value_in_seconds = value.InSecondsF();
+  if (std::isfinite(value_in_seconds))
+    event->params.SetDouble(property, value_in_seconds);
   else
     event->params.SetString(property, "unknown");
   return event;
@@ -286,6 +280,12 @@
   return event;
 }
 
+std::unique_ptr<MediaLogEvent> MediaLog::CreateSeekEvent(double seconds) {
+  std::unique_ptr<MediaLogEvent> event(CreateEvent(MediaLogEvent::SEEK));
+  event->params.SetDouble("seek_target", seconds);
+  return event;
+}
+
 std::unique_ptr<MediaLogEvent> MediaLog::CreatePipelineStateChangedEvent(
     PipelineImpl::State state) {
   std::unique_ptr<MediaLogEvent> event(
--- a/media/base/media_log.h	2019-05-17 17:45:41.244000000 +0300
+++ b/media/base/media_log.h	2019-05-17 18:53:34.084000000 +0300
@@ -92,7 +92,7 @@
 
   // Records the domain and registry of the current frame security origin to a
   // Rappor privacy-preserving metric. See:
-  //   https://www.ch40m1um.qjz9zk/developers/design-documents/rappor
+  //   https://www.chromium.org/developers/design-documents/rappor
   // Inheritors should override RecordRapportWithSecurityOriginLocked().
   void RecordRapporWithSecurityOrigin(const std::string& metric);
 
@@ -109,10 +109,8 @@
   std::unique_ptr<MediaLogEvent> CreateTimeEvent(MediaLogEvent::Type type,
                                                  const std::string& property,
                                                  base::TimeDelta value);
-  std::unique_ptr<MediaLogEvent> CreateTimeEvent(MediaLogEvent::Type type,
-                                                 const std::string& property,
-                                                 double value);
   std::unique_ptr<MediaLogEvent> CreateLoadEvent(const std::string& url);
+  std::unique_ptr<MediaLogEvent> CreateSeekEvent(double seconds);
   std::unique_ptr<MediaLogEvent> CreatePipelineStateChangedEvent(
       PipelineImpl::State state);
   std::unique_ptr<MediaLogEvent> CreatePipelineErrorEvent(PipelineStatus error);
--- a/media/base/media_log_unittest.cc	2019-05-17 17:45:41.244000000 +0300
+++ b/media/base/media_log_unittest.cc	2019-05-17 18:53:34.084000000 +0300
@@ -26,7 +26,7 @@
 constexpr size_t MediaLogTest::kMaxUrlLength;
 
 TEST_F(MediaLogTest, DontTruncateShortUrlString) {
-  const std::string short_url("ch40m1um.qjz9zk");
+  const std::string short_url("chromium.org");
   EXPECT_LT(short_url.length(), MediaLogTest::kMaxUrlLength);
 
   // Verify that CreatedEvent does not truncate the short URL.
--- a/media/base/media_switches.cc	2019-05-17 17:45:41.244000000 +0300
+++ b/media/base/media_switches.cc	2019-05-17 18:53:34.088000000 +0300
@@ -47,7 +47,7 @@
 // Use exclusive mode audio streaming for Windows Vista and higher.
 // Leads to lower latencies for audio streams which uses the
 // AudioParameters::AUDIO_PCM_LOW_LATENCY audio path.
-// See http://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/dd370844.aspx
+// See http://msdn.microsoft.com/en-us/library/windows/desktop/dd370844.aspx
 // for details.
 const char kEnableExclusiveAudio[] = "enable-exclusive-audio";
 
@@ -81,6 +81,13 @@
 const char kUnsafelyAllowProtectedMediaIdentifierForDomain[] =
     "unsafely-allow-protected-media-identifier-for-domain";
 
+#if BUILDFLAG(ENABLE_RUNTIME_MEDIA_RENDERER_SELECTION)
+// Rather than use the renderer hosted remotely in the media service, fall back
+// to the default renderer within content_renderer. Does not change the behavior
+// of the media service.
+const char kDisableMojoRenderer[] = "disable-mojo-renderer";
+#endif  // BUILDFLAG(ENABLE_RUNTIME_MEDIA_RENDERER_SELECTION)
+
 // Use fake device for Media Stream to replace actual camera and microphone.
 const char kUseFakeDeviceForMediaStream[] = "use-fake-device-for-media-stream";
 
@@ -252,11 +259,6 @@
 const base::Feature kD3D11VideoDecoder{"D3D11VideoDecoder",
                                        base::FEATURE_DISABLED_BY_DEFAULT};
 
-// Tell D3D11VideoDecoder to ignore workarounds for zero copy.  Requires that
-// kD3D11VideoDecoder is enabled.
-const base::Feature kD3D11VideoDecoderIgnoreWorkarounds{
-    "D3D11VideoDecoderIgnoreWorkarounds", base::FEATURE_DISABLED_BY_DEFAULT};
-
 // Falls back to other decoders after audio/video decode error happens. The
 // implementation may choose different strategies on when to fallback. See
 // DecoderStream for details. When disabled, playback will fail immediately
@@ -276,6 +278,10 @@
 const base::Feature kNewEncodeCpuLoadEstimator{
     "NewEncodeCpuLoadEstimator", base::FEATURE_DISABLED_BY_DEFAULT};
 
+// Use the new Remote Playback / media flinging pipeline.
+const base::Feature kNewRemotePlaybackPipeline{
+    "NewRemotePlaybackPipeline", base::FEATURE_ENABLED_BY_DEFAULT};
+
 // Use the new RTC hardware decode path via RTCVideoDecoderAdapter.
 const base::Feature kRTCVideoDecoderAdapter{"RTCVideoDecoderAdapter",
                                             base::FEATURE_ENABLED_BY_DEFAULT};
@@ -302,14 +308,16 @@
 const base::Feature kUseSurfaceLayerForVideo{"UseSurfaceLayerForVideo",
                                              base::FEATURE_DISABLED_BY_DEFAULT};
 
+// Use SurfaceLayer instead of VideoLayer when entering Picture-in-Picture mode.
+// Does nothing if UseSurfaceLayerForVideo is enabled.  Does not affect
+// MediaStream playbacks.
+const base::Feature kUseSurfaceLayerForVideoPIP{
+    "UseSurfaceLayerForVideoPIP", base::FEATURE_ENABLED_BY_DEFAULT};
+
 // Enable VA-API hardware encode acceleration for VP8.
 const base::Feature kVaapiVP8Encoder{"VaapiVP8Encoder",
                                      base::FEATURE_ENABLED_BY_DEFAULT};
 
-// Enable VA-API hardware encode acceleration for VP9.
-const base::Feature kVaapiVP9Encoder{"VaapiVP9Encoder",
-                                     base::FEATURE_DISABLED_BY_DEFAULT};
-
 // Inform video blitter of video color space.
 const base::Feature kVideoBlitColorAccuracy{"video-blit-color-accuracy",
                                             base::FEATURE_ENABLED_BY_DEFAULT};
@@ -330,7 +338,7 @@
 // Enables handling of hardware media keys for controlling media.
 const base::Feature kHardwareMediaKeyHandling{
   "HardwareMediaKeyHandling",
-#if defined(OS_CHROMEOS) || defined(OS_WIN) || defined(OS_MACOSX)
+#if defined(OS_CHROMEOS)
       base::FEATURE_ENABLED_BY_DEFAULT
 #else
       base::FEATURE_DISABLED_BY_DEFAULT
@@ -358,7 +366,6 @@
 
 #if defined(OS_ANDROID)
 // Enable a gesture to make the media controls expaned into the display cutout.
-// TODO(beccahughes): Remove this.
 const base::Feature kMediaControlsExpandGesture{
     "MediaControlsExpandGesture", base::FEATURE_ENABLED_BY_DEFAULT};
 
@@ -376,24 +383,14 @@
 const base::Feature kMediaDrmPersistentLicense{
     "MediaDrmPersistentLicense", base::FEATURE_ENABLED_BY_DEFAULT};
 
-// Enables MediaDrmOriginIdManager to provide preprovisioned origin IDs for
-// MediaDrmBridge. If disabled, MediaDrmBridge will get unprovisioned origin IDs
-// which will trigger provisioning process after MediaDrmBridge is created.
-const base::Feature kMediaDrmPreprovisioning{"MediaDrmPreprovisioning",
+// Enables the Android MediaRouter implementation using CAF (Cast v3).
+const base::Feature kCafMediaRouterImpl{"CafMediaRouterImpl",
                                              base::FEATURE_DISABLED_BY_DEFAULT};
 
-// Determines if MediaDrmOriginIdManager should attempt to pre-provision origin
-// IDs at startup (whenever a profile is loaded). Also used by tests that
-// disable it so that the tests can setup before pre-provisioning is done.
-// Note: Have no effect if kMediaDrmPreprovisioning feature is disabled.
-const base::Feature kMediaDrmPreprovisioningAtStartup{
-    "MediaDrmPreprovisioningAtStartup", base::FEATURE_DISABLED_BY_DEFAULT};
-
 // Enables the Android Image Reader path for Video decoding(for AVDA and MCVD)
 const base::Feature kAImageReaderVideoOutput{"AImageReaderVideoOutput",
                                              base::FEATURE_DISABLED_BY_DEFAULT};
-
-#endif  // defined(OS_ANDROID)
+#endif
 
 #if defined(OS_WIN)
 // Does NV12->NV12 video copy on the main thread right before the texture's
@@ -434,12 +431,10 @@
 }
 
 // Adds icons to the overflow menu on the native media controls.
-// TODO(steimel): Remove this.
 const base::Feature kOverflowIconsForMediaControls{
     "OverflowIconsForMediaControls", base::FEATURE_ENABLED_BY_DEFAULT};
 
 // Enables the new redesigned media controls.
-// TODO(steimel): Remove this.
 const base::Feature kUseModernMediaControls{"UseModernMediaControls",
                                             base::FEATURE_ENABLED_BY_DEFAULT};
 
--- a/media/base/media_switches.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/media_switches.h	2019-05-17 18:53:34.088000000 +0300
@@ -56,6 +56,10 @@
 MEDIA_EXPORT extern const char
     kUnsafelyAllowProtectedMediaIdentifierForDomain[];
 
+#if BUILDFLAG(ENABLE_RUNTIME_MEDIA_RENDERER_SELECTION)
+MEDIA_EXPORT extern const char kDisableMojoRenderer[];
+#endif  // BUILDFLAG(ENABLE_RUNTIME_MEDIA_RENDERER_SELECTION)
+
 MEDIA_EXPORT extern const char kUseFakeDeviceForMediaStream[];
 MEDIA_EXPORT extern const char kUseFileForFakeVideoCapture[];
 MEDIA_EXPORT extern const char kUseFileForFakeAudioCapture[];
@@ -100,7 +104,6 @@
 MEDIA_EXPORT extern const base::Feature kBackgroundSrcVideoTrackOptimization;
 MEDIA_EXPORT extern const base::Feature kBackgroundVideoPauseOptimization;
 MEDIA_EXPORT extern const base::Feature kD3D11VideoDecoder;
-MEDIA_EXPORT extern const base::Feature kD3D11VideoDecoderIgnoreWorkarounds;
 MEDIA_EXPORT extern const base::Feature kExternalClearKeyForTesting;
 MEDIA_EXPORT extern const base::Feature kFallbackAfterDecodeError;
 MEDIA_EXPORT extern const base::Feature kHardwareMediaKeyHandling;
@@ -115,6 +118,7 @@
 MEDIA_EXPORT extern const base::Feature kMojoVideoDecoder;
 MEDIA_EXPORT extern const base::Feature kMseBufferByPts;
 MEDIA_EXPORT extern const base::Feature kNewEncodeCpuLoadEstimator;
+MEDIA_EXPORT extern const base::Feature kNewRemotePlaybackPipeline;
 MEDIA_EXPORT extern const base::Feature kOverflowIconsForMediaControls;
 MEDIA_EXPORT extern const base::Feature kOverlayFullscreenVideo;
 MEDIA_EXPORT extern const base::Feature kPictureInPicture;
@@ -133,8 +137,8 @@
 MEDIA_EXPORT extern const base::Feature kUseNewMediaCache;
 MEDIA_EXPORT extern const base::Feature kUseR16Texture;
 MEDIA_EXPORT extern const base::Feature kUseSurfaceLayerForVideo;
+MEDIA_EXPORT extern const base::Feature kUseSurfaceLayerForVideoPIP;
 MEDIA_EXPORT extern const base::Feature kVaapiVP8Encoder;
-MEDIA_EXPORT extern const base::Feature kVaapiVP9Encoder;
 MEDIA_EXPORT extern const base::Feature kVideoBlitColorAccuracy;
 
 #if defined(OS_ANDROID)
@@ -142,8 +146,7 @@
 MEDIA_EXPORT extern const base::Feature kVideoFullscreenOrientationLock;
 MEDIA_EXPORT extern const base::Feature kVideoRotateToFullscreen;
 MEDIA_EXPORT extern const base::Feature kMediaDrmPersistentLicense;
-MEDIA_EXPORT extern const base::Feature kMediaDrmPreprovisioning;
-MEDIA_EXPORT extern const base::Feature kMediaDrmPreprovisioningAtStartup;
+MEDIA_EXPORT extern const base::Feature kCafMediaRouterImpl;
 MEDIA_EXPORT extern const base::Feature kAImageReaderVideoOutput;
 #endif  // defined(OS_ANDROID)
 
--- a/media/base/media_types.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/media_types.h	2019-05-17 18:53:34.088000000 +0300
@@ -6,38 +6,27 @@
 #define MEDIA_BASE_MEDIA_TYPES_H_
 
 #include "media/base/audio_codecs.h"
-#include "media/base/audio_decoder_config.h"
 #include "media/base/media_export.h"
 #include "media/base/video_codecs.h"
 #include "media/base/video_color_space.h"
-#include "media/base/video_decoder_config.h"
 
 namespace media {
 
 // These structures represent parsed audio/video content types (mime strings).
-// These are generally a subset of {Audio|Video}DecoderConfig classes, which can
-// only be created after demuxing.
+// These are a subset of {Audio|Video}DecoderConfig classes, which can only be
+// created after demuxing.
 
 struct MEDIA_EXPORT AudioType {
-  static AudioType FromDecoderConfig(const AudioDecoderConfig& config);
-
   AudioCodec codec;
 };
 
 struct MEDIA_EXPORT VideoType {
-  static VideoType FromDecoderConfig(const VideoDecoderConfig& config);
-
   VideoCodec codec;
   VideoCodecProfile profile;
   int level;
   VideoColorSpace color_space;
 };
 
-MEDIA_EXPORT bool operator==(const AudioType& x, const AudioType& y);
-MEDIA_EXPORT bool operator!=(const AudioType& x, const AudioType& y);
-MEDIA_EXPORT bool operator==(const VideoType& x, const VideoType& y);
-MEDIA_EXPORT bool operator!=(const VideoType& x, const VideoType& y);
-
 }  // namespace media
 
 #endif  // MEDIA_BASE_MEDIA_TYPES_H_
--- a/media/base/mime_util_internal.cc	2019-05-17 17:45:41.248000000 +0300
+++ b/media/base/mime_util_internal.cc	2019-05-17 18:53:34.088000000 +0300
@@ -16,6 +16,7 @@
 #include "media/base/video_codecs.h"
 #include "media/base/video_color_space.h"
 #include "media/media_buildflags.h"
+#include "third_party/libaom/av1_buildflags.h"
 
 #if defined(OS_ANDROID)
 #include "base/android/build_info.h"
@@ -599,7 +600,7 @@
 
 #if defined(OS_ANDROID)
       // HEVC/H.265 is supported in Lollipop+ (API Level 21), according to
-      // http://developer.8n6r01d.qjz9zk/reference/android/media/MediaFormat.html
+      // http://developer.android.com/reference/android/media/MediaFormat.html
       return base::android::BuildInfo::GetInstance()->sdk_int() >=
              base::android::SDK_VERSION_LOLLIPOP;
 #else
--- a/media/base/mock_filters.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/mock_filters.cc	2019-05-17 18:53:34.088000000 +0300
@@ -266,8 +266,4 @@
 
 MockStreamParser::~MockStreamParser() = default;
 
-MockMediaClient::MockMediaClient() = default;
-
-MockMediaClient::~MockMediaClient() = default;
-
 }  // namespace media
--- a/media/base/mock_filters.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/mock_filters.h	2019-05-17 18:53:34.088000000 +0300
@@ -9,7 +9,6 @@
 
 #include <memory>
 #include <string>
-#include <vector>
 
 #include "base/callback.h"
 #include "base/macros.h"
@@ -26,7 +25,6 @@
 #include "media/base/decoder_buffer.h"
 #include "media/base/decryptor.h"
 #include "media/base/demuxer.h"
-#include "media/base/media_client.h"
 #include "media/base/media_track.h"
 #include "media/base/pipeline.h"
 #include "media/base/pipeline_status.h"
@@ -609,24 +607,6 @@
   DISALLOW_COPY_AND_ASSIGN(MockStreamParser);
 };
 
-class MockMediaClient : public media::MediaClient {
- public:
-  MockMediaClient();
-  ~MockMediaClient() override;
-
-  // MediaClient implementation.
-  MOCK_METHOD1(AddSupportedKeySystems,
-               void(std::vector<std::unique_ptr<media::KeySystemProperties>>*
-                        key_systems));
-  MOCK_METHOD0(IsKeySystemsUpdateNeeded, bool());
-  MOCK_METHOD1(IsSupportedAudioType, bool(const media::AudioType& type));
-  MOCK_METHOD1(IsSupportedVideoType, bool(const media::VideoType& type));
-  MOCK_METHOD1(IsSupportedBitstreamAudioCodec, bool(media::AudioCodec codec));
-
- private:
-  DISALLOW_COPY_AND_ASSIGN(MockMediaClient);
-};
-
 }  // namespace media
 
 #endif  // MEDIA_BASE_MOCK_FILTERS_H_
--- a/media/base/multi_channel_resampler_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/multi_channel_resampler_unittest.cc	2019-05-17 18:53:34.088000000 +0300
@@ -134,8 +134,8 @@
 }
 
 // Test common channel layouts: mono, stereo, 5.1, 7.1.
-INSTANTIATE_TEST_SUITE_P(MultiChannelResamplerTest,
-                         MultiChannelResamplerTest,
+INSTANTIATE_TEST_CASE_P(
+    MultiChannelResamplerTest, MultiChannelResamplerTest,
                          testing::Values(1, 2, 6, 8));
 
 }  // namespace media
--- a/media/base/output_device_info.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/output_device_info.h	2019-05-17 18:53:34.092000000 +0300
@@ -57,7 +57,7 @@
   // Returns the device's audio output parameters.
   // The return value is undefined if the device status (as returned by
   // device_status()) is different from OUTPUT_DEVICE_STATUS_OK.
-  const AudioParameters& output_params() const { return output_params_; }
+  const AudioParameters& output_params() const { return output_params_; };
 
   // Returns a human-readable string describing |*this|.  For debugging & test
   // output only.
--- a/media/base/pipeline_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/pipeline_impl_unittest.cc	2019-05-17 18:53:34.092000000 +0300
@@ -1163,25 +1163,25 @@
     RunTest(k##state, k##stop_or_error);                  \
   }
 
-INSTANTIATE_TEARDOWN_TEST(Stop, InitDemuxer)
-INSTANTIATE_TEARDOWN_TEST(Stop, InitRenderer)
-INSTANTIATE_TEARDOWN_TEST(Stop, Flushing)
-INSTANTIATE_TEARDOWN_TEST(Stop, Seeking)
-INSTANTIATE_TEARDOWN_TEST(Stop, Playing)
-INSTANTIATE_TEARDOWN_TEST(Stop, Suspending)
-INSTANTIATE_TEARDOWN_TEST(Stop, Suspended)
-INSTANTIATE_TEARDOWN_TEST(Stop, Resuming)
+INSTANTIATE_TEARDOWN_TEST(Stop, InitDemuxer);
+INSTANTIATE_TEARDOWN_TEST(Stop, InitRenderer);
+INSTANTIATE_TEARDOWN_TEST(Stop, Flushing);
+INSTANTIATE_TEARDOWN_TEST(Stop, Seeking);
+INSTANTIATE_TEARDOWN_TEST(Stop, Playing);
+INSTANTIATE_TEARDOWN_TEST(Stop, Suspending);
+INSTANTIATE_TEARDOWN_TEST(Stop, Suspended);
+INSTANTIATE_TEARDOWN_TEST(Stop, Resuming);
 
-INSTANTIATE_TEARDOWN_TEST(Error, InitDemuxer)
-INSTANTIATE_TEARDOWN_TEST(Error, InitRenderer)
-INSTANTIATE_TEARDOWN_TEST(Error, Flushing)
-INSTANTIATE_TEARDOWN_TEST(Error, Seeking)
-INSTANTIATE_TEARDOWN_TEST(Error, Playing)
-INSTANTIATE_TEARDOWN_TEST(Error, Suspending)
-INSTANTIATE_TEARDOWN_TEST(Error, Suspended)
-INSTANTIATE_TEARDOWN_TEST(Error, Resuming)
+INSTANTIATE_TEARDOWN_TEST(Error, InitDemuxer);
+INSTANTIATE_TEARDOWN_TEST(Error, InitRenderer);
+INSTANTIATE_TEARDOWN_TEST(Error, Flushing);
+INSTANTIATE_TEARDOWN_TEST(Error, Seeking);
+INSTANTIATE_TEARDOWN_TEST(Error, Playing);
+INSTANTIATE_TEARDOWN_TEST(Error, Suspending);
+INSTANTIATE_TEARDOWN_TEST(Error, Suspended);
+INSTANTIATE_TEARDOWN_TEST(Error, Resuming);
 
-INSTANTIATE_TEARDOWN_TEST(ErrorAndStop, Playing)
-INSTANTIATE_TEARDOWN_TEST(ErrorAndStop, Suspended)
+INSTANTIATE_TEARDOWN_TEST(ErrorAndStop, Playing);
+INSTANTIATE_TEARDOWN_TEST(ErrorAndStop, Suspended);
 
 }  // namespace media
--- a/media/base/renderer_factory_selector_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/renderer_factory_selector_unittest.cc	2019-05-17 18:53:34.092000000 +0300
@@ -18,7 +18,7 @@
 
   class FakeFactory : public RendererFactory {
    public:
-    FakeFactory(FactoryType type) : type_(type) {}
+    FakeFactory(FactoryType type) : type_(type){};
 
     std::unique_ptr<Renderer> CreateRenderer(
         const scoped_refptr<base::SingleThreadTaskRunner>& media_task_runner,
@@ -37,10 +37,11 @@
   };
 
   RendererFactorySelectorTest() = default;
+  ;
 
   void AddFactory(FactoryType type) {
     selector_.AddFactory(type, std::make_unique<FakeFactory>(type));
-  }
+  };
 
   FactoryType GetCurrentlySelectedFactoryType() {
     return reinterpret_cast<FakeFactory*>(selector_.GetCurrentFactory())
--- a/media/base/serial_runner.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/serial_runner.cc	2019-05-17 18:53:34.096000000 +0300
@@ -76,9 +76,10 @@
   // Respect both cancellation and calling stack guarantees for |done_cb|
   // when empty.
   if (bound_fns_.empty()) {
-    task_runner_->PostTask(
-        FROM_HERE, base::BindOnce(&SerialRunner::RunNextInSeries,
-                                  weak_factory_.GetWeakPtr(), PIPELINE_OK));
+    task_runner_->PostTask(FROM_HERE,
+                           base::Bind(&SerialRunner::RunNextInSeries,
+                                      weak_factory_.GetWeakPtr(),
+                                      PIPELINE_OK));
     return;
   }
 
--- a/media/base/silent_sink_suspender.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/silent_sink_suspender.cc	2019-05-17 18:53:34.096000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/base/silent_sink_suspender.h"
 
-#include "base/bind.h"
 #include "base/single_thread_task_runner.h"
 #include "base/threading/thread_task_runner_handle.h"
 
@@ -143,17 +142,10 @@
       is_transition_pending_ = false;
       is_using_fake_sink_ = true;
     }
-    fake_sink_.Start(base::BindRepeating(
-        [](SilentSinkSuspender* suspender, base::TimeDelta frozen_delay,
-           base::TimeTicks frozen_delay_timestamp, base::TimeTicks ideal_time,
-           base::TimeTicks now) {
-          // TODO: Seems that the code in Render() might benefit from the two
-          // new timestamps being provided by FakeAudioWorker, in that it's call
-          // to base::TimeTicks::Now() can be eliminated (use |now| instead),
-          // along with its custom delay timestamp calculations.
-          suspender->Render(frozen_delay, frozen_delay_timestamp, 0, nullptr);
-        },
-        this, latest_output_delay_, latest_output_delay_timestamp_));
+    fake_sink_.Start(
+        base::Bind(base::IgnoreResult(&SilentSinkSuspender::Render),
+                   base::Unretained(this), latest_output_delay_,
+                   latest_output_delay_timestamp_, 0, nullptr));
   } else {
     fake_sink_.Stop();
 
--- a/media/base/sinc_resampler_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/sinc_resampler_unittest.cc	2019-05-17 18:53:34.096000000 +0300
@@ -360,7 +360,7 @@
 
 // Thresholds chosen arbitrarily based on what each resampling reported during
 // testing.  All thresholds are in dbFS, http://en.wikipedia.org/wiki/DBFS.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     SincResamplerTest,
     SincResamplerTest,
     testing::Values(
--- a/media/base/supported_types.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/supported_types.cc	2019-05-17 18:53:34.096000000 +0300
@@ -9,6 +9,7 @@
 #include "media/base/media_client.h"
 #include "media/base/media_switches.h"
 #include "media/media_buildflags.h"
+#include "third_party/libaom/av1_buildflags.h"
 #include "ui/display/display_switches.h"
 
 #if BUILDFLAG(ENABLE_LIBVPX)
@@ -250,16 +251,10 @@
     case media::kUnknownVideoCodec:
     case media::kCodecVC1:
     case media::kCodecMPEG2:
+    case media::kCodecMPEG4:
     case media::kCodecHEVC:
     case media::kCodecDolbyVision:
       return false;
-
-    case media::kCodecMPEG4:
-#if defined(OS_CHROMEOS)
-      return true;
-#else
-      return false;
-#endif
   }
 
   NOTREACHED();
--- a/media/base/supported_types_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/supported_types_unittest.cc	2019-05-17 18:53:34.096000000 +0300
@@ -14,12 +14,6 @@
 const bool kPropCodecsEnabled = false;
 #endif
 
-#if defined(OS_CHROMEOS) && BUILDFLAG(USE_PROPRIETARY_CODECS)
-const bool kMpeg4Supported = true;
-#else
-const bool kMpeg4Supported = false;
-#endif
-
 TEST(SupportedTypesTest, IsSupportedVideoTypeBasics) {
   // Default to common 709.
   const media::VideoColorSpace kColorSpace = media::VideoColorSpace::REC709();
@@ -47,6 +41,9 @@
   EXPECT_FALSE(IsSupportedVideoType({media::kCodecMPEG2,
                                      media::VIDEO_CODEC_PROFILE_UNKNOWN,
                                      kUnspecifiedLevel, kColorSpace}));
+  EXPECT_FALSE(IsSupportedVideoType({media::kCodecMPEG4,
+                                     media::VIDEO_CODEC_PROFILE_UNKNOWN,
+                                     kUnspecifiedLevel, kColorSpace}));
   EXPECT_FALSE(IsSupportedVideoType({media::kCodecHEVC,
                                      media::VIDEO_CODEC_PROFILE_UNKNOWN,
                                      kUnspecifiedLevel, kColorSpace}));
@@ -56,10 +53,6 @@
       kPropCodecsEnabled,
       IsSupportedVideoType(
           {media::kCodecH264, media::H264PROFILE_BASELINE, 1, kColorSpace}));
-  EXPECT_EQ(kMpeg4Supported,
-            IsSupportedVideoType({media::kCodecMPEG4,
-                                  media::VIDEO_CODEC_PROFILE_UNKNOWN,
-                                  kUnspecifiedLevel, kColorSpace}));
 }
 
 TEST(SupportedTypesTest, IsSupportedVideoType_VP9TransferFunctions) {
--- a/media/base/test_helpers.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/test_helpers.h	2019-05-17 18:53:34.100000000 +0300
@@ -259,6 +259,12 @@
   return CONTAINS_STRING(arg, "Parsed buffers not in DTS sequence");
 }
 
+MATCHER(ParsedDTSGreaterThanPTS, "") {
+  return CONTAINS_STRING(arg, "Parsed ") &&
+         CONTAINS_STRING(arg, "frame has DTS ") &&
+         CONTAINS_STRING(arg, ", which is after the frame's PTS");
+}
+
 MATCHER_P2(CodecUnsupportedInContainer, codec, container, "") {
   return CONTAINS_STRING(arg, std::string(codec) + "' is not supported for '" +
                                   std::string(container));
@@ -311,8 +317,8 @@
 MATCHER_P2(AudioNonKeyframe, pts_microseconds, dts_microseconds, "") {
   return CONTAINS_STRING(
       arg, std::string("Bytestream with audio frame PTS ") +
-               base::NumberToString(pts_microseconds) + "us and DTS " +
-               base::NumberToString(dts_microseconds) +
+               base::IntToString(pts_microseconds) + "us and DTS " +
+               base::IntToString(dts_microseconds) +
                "us indicated the frame is not a random access point (key "
                "frame). All audio frames are expected to be key frames.");
 }
@@ -323,15 +329,15 @@
            "") {
   return CONTAINS_STRING(
       arg, "Skipping splice frame generation: first new buffer at " +
-               base::NumberToString(new_microseconds) +
+               base::IntToString(new_microseconds) +
                "us begins at or before existing buffer at " +
-               base::NumberToString(existing_microseconds) + "us.");
+               base::IntToString(existing_microseconds) + "us.");
 }
 
 MATCHER_P(SkippingSpliceAlreadySpliced, time_microseconds, "") {
   return CONTAINS_STRING(
       arg, "Skipping splice frame generation: overlapped buffers at " +
-               base::NumberToString(time_microseconds) +
+               base::IntToString(time_microseconds) +
                "us are in a previously buffered splice.");
 }
 
@@ -341,8 +347,8 @@
            "") {
   return CONTAINS_STRING(
       arg, "Skipping audio splice trimming at PTS=" +
-               base::NumberToString(pts_microseconds) + "us. Found only " +
-               base::NumberToString(overlap_microseconds) +
+               base::IntToString(pts_microseconds) + "us. Found only " +
+               base::IntToString(overlap_microseconds) +
                "us of overlap, need at least 1000us. Multiple occurrences may "
                "result in loss of A/V sync.");
 }
@@ -355,7 +361,7 @@
 
 MATCHER_P(WebMSimpleBlockDurationEstimated, estimated_duration_ms, "") {
   return CONTAINS_STRING(arg, "Estimating WebM block duration=" +
-                                  base::NumberToString(estimated_duration_ms));
+                                  base::IntToString(estimated_duration_ms));
 }
 
 MATCHER_P(WebMNegativeTimecodeOffset, timecode_string, "") {
@@ -378,19 +384,17 @@
            trim_duration_us,
            "") {
   return CONTAINS_STRING(
-      arg,
-      "Audio buffer splice at PTS=" + base::NumberToString(splice_time_us) +
+      arg, "Audio buffer splice at PTS=" + base::IntToString(splice_time_us) +
           "us. Trimmed tail of overlapped buffer (PTS=" +
-          base::NumberToString(overlapped_start_us) + "us) by " +
-          base::NumberToString(trim_duration_us));
+               base::IntToString(overlapped_start_us) + "us) by " +
+               base::IntToString(trim_duration_us));
 }
 
 MATCHER_P2(NoSpliceForBadMux, overlapped_buffer_count, splice_time_us, "") {
-  return CONTAINS_STRING(arg,
-                         "Media is badly muxed. Detected " +
-                             base::NumberToString(overlapped_buffer_count) +
+  return CONTAINS_STRING(arg, "Media is badly muxed. Detected " +
+                                  base::IntToString(overlapped_buffer_count) +
                              " overlapping audio buffers at time " +
-                             base::NumberToString(splice_time_us));
+                                  base::IntToString(splice_time_us));
 }
 
 MATCHER_P(BufferingByPtsDts, by_pts_bool, "") {
@@ -401,8 +405,8 @@
 MATCHER_P3(NegativeDtsFailureWhenByDts, frame_type, pts_us, dts_us, "") {
   return CONTAINS_STRING(
       arg, std::string(frame_type) + " frame with PTS " +
-               base::NumberToString(pts_us) + "us has negative DTS " +
-               base::NumberToString(dts_us) +
+               base::IntToString(pts_us) + "us has negative DTS " +
+               base::IntToString(dts_us) +
                "us after applying timestampOffset, handling any discontinuity, "
                "and filtering against append window");
 }
@@ -410,8 +414,8 @@
 MATCHER_P2(DiscardingEmptyFrame, pts_us, dts_us, "") {
   return CONTAINS_STRING(arg,
                          "Discarding empty audio or video coded frame, PTS=" +
-                             base::NumberToString(pts_us) +
-                             "us, DTS=" + base::NumberToString(dts_us) + "us");
+                             base::IntToString(pts_us) +
+                             "us, DTS=" + base::IntToString(dts_us) + "us");
 }
 
 MATCHER_P4(TruncatedFrame,
@@ -431,7 +435,7 @@
 MATCHER_P2(DroppedFrame, frame_type, pts_us, "") {
   return CONTAINS_STRING(arg,
                          "Dropping " + std::string(frame_type) + " frame") &&
-         CONTAINS_STRING(arg, "PTS " + base::NumberToString(pts_us));
+         CONTAINS_STRING(arg, "PTS " + base::IntToString(pts_us));
 }
 
 MATCHER_P3(DroppedFrameCheckAppendWindow,
@@ -443,8 +447,8 @@
                          "Dropping " + std::string(frame_type) + " frame") &&
          CONTAINS_STRING(
              arg, "outside append window [" +
-                      base::NumberToString(append_window_start_us) + "us," +
-                      base::NumberToString(append_window_end_us) + "us");
+                      base::Int64ToString(append_window_start_us) + "us," +
+                      base::Int64ToString(append_window_end_us) + "us");
 }
 
 }  // namespace media
--- a/media/base/test_random.h	2019-05-17 17:45:41.248000000 +0300
+++ b/media/base/test_random.h	2019-05-17 18:53:34.100000000 +0300
@@ -14,7 +14,7 @@
 // numbers, unlike the classes in base/rand_util.h which are meant to generate
 // unpredictable sequences.
 // See
-// https://code.9oo91e.qjz9zk/p/szl/source/browse/trunk/butilities/acmrandom.h
+// https://code.google.com/p/szl/source/browse/trunk/butilities/acmrandom.h
 // for more information.
 
 namespace media {
--- a/media/base/vector_math_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/vector_math_unittest.cc	2019-05-17 18:53:34.100000000 +0300
@@ -278,7 +278,7 @@
   1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0
 };
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     Scenarios,
     VectorMathEWMAAndMaxPowerTest,
     ::testing::Values(
--- a/media/base/video_codecs.h	2019-05-17 17:45:41.248000000 +0300
+++ b/media/base/video_codecs.h	2019-05-17 18:53:34.104000000 +0300
@@ -9,6 +9,7 @@
 #include <string>
 #include "media/base/media_export.h"
 #include "media/media_buildflags.h"
+#include "third_party/libaom/av1_buildflags.h"
 #include "ui/gfx/color_space.h"
 
 namespace media {
@@ -107,7 +108,7 @@
 
 // ParseNewStyleVp9CodecID handles parsing of new style vp9 codec IDs per
 // proposed VP Codec ISO Media File Format Binding specification:
-// https://storage.9oo91eapis.qjz9zk/downloads.webmproject.org/docs/vp9/vp-codec-iso-media-file-format-binding-20160516-draft.pdf
+// https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp-codec-iso-media-file-format-binding-20160516-draft.pdf
 // ParseLegacyVp9CodecID handles parsing of legacy VP9 codec strings defined
 // for WebM.
 // TODO(kqyang): Consolidate the two functions once we address crbug.com/667834
--- a/media/base/video_decoder_config.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_decoder_config.cc	2019-05-17 18:53:34.104000000 +0300
@@ -83,11 +83,6 @@
 
 VideoDecoderConfig::~VideoDecoderConfig() = default;
 
-void VideoDecoderConfig::set_color_space_info(
-    const VideoColorSpace& color_space) {
-  color_space_info_ = color_space;
-}
-
 const VideoColorSpace& VideoDecoderConfig::color_space_info() const {
   return color_space_info_;
 }
--- a/media/base/video_decoder_config.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_decoder_config.h	2019-05-17 18:53:34.104000000 +0300
@@ -137,7 +137,6 @@
   }
 
   // Color space of the image data.
-  void set_color_space_info(const VideoColorSpace& color_space);
   const VideoColorSpace& color_space_info() const;
 
   // Dynamic range of the image data.
--- a/media/base/video_frame.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_frame.cc	2019-05-17 18:53:34.104000000 +0300
@@ -808,14 +808,15 @@
 
 // static
 void VideoFrame::HashFrameForTesting(base::MD5Context* context,
-                                     const VideoFrame& frame) {
+                                     const scoped_refptr<VideoFrame>& frame) {
   DCHECK(context);
-  for (size_t plane = 0; plane < NumPlanes(frame.format()); ++plane) {
-    for (int row = 0; row < frame.rows(plane); ++row) {
-      base::MD5Update(context, base::StringPiece(reinterpret_cast<const char*>(
-                                                     frame.data(plane) +
-                                                     frame.stride(plane) * row),
-                                                 frame.row_bytes(plane)));
+  for (size_t plane = 0; plane < NumPlanes(frame->format()); ++plane) {
+    for (int row = 0; row < frame->rows(plane); ++row) {
+      base::MD5Update(
+          context,
+          base::StringPiece(reinterpret_cast<char*>(frame->data(plane) +
+                                                    frame->stride(plane) * row),
+                            frame->row_bytes(plane)));
     }
   }
 }
@@ -984,7 +985,7 @@
 }
 
 std::string VideoFrame::AsHumanReadableString() {
-  if (metadata()->IsTrue(VideoFrameMetadata::END_OF_STREAM))
+  if (metadata()->IsTrue(media::VideoFrameMetadata::END_OF_STREAM))
     return "end of stream";
 
   std::ostringstream s;
@@ -995,7 +996,7 @@
 }
 
 size_t VideoFrame::BitDepth() const {
-  return media::BitDepth(format());
+  return ::media::BitDepth(format());
 }
 
 // static
--- a/media/base/video_frame.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_frame.h	2019-05-17 18:53:34.104000000 +0300
@@ -346,7 +346,7 @@
   // Used to keep a running hash of seen frames.  Expects an initialized MD5
   // context.  Calls MD5Update with the context and the contents of the frame.
   static void HashFrameForTesting(base::MD5Context* context,
-                                  const VideoFrame& frame);
+                                  const scoped_refptr<VideoFrame>& frame);
 
   // Returns true if |frame| is accesible mapped in the VideoFrame memory space.
   // static
@@ -375,15 +375,8 @@
   VideoPixelFormat format() const { return layout_.format(); }
   StorageType storage_type() const { return storage_type_; }
 
-  // The full dimensions of the video frame data.
   const gfx::Size& coded_size() const { return layout_.coded_size(); }
-  // A subsection of [0, 0, coded_size().width(), coded_size.height()]. This
-  // can be set to "soft-apply" a cropping. It determines the pointers into
-  // the data returned by visible_data().
   const gfx::Rect& visible_rect() const { return visible_rect_; }
-  // Specifies that the |visible_rect| section of the frame is supposed to be
-  // scaled to this size when being presented. This can be used to represent
-  // anamorphic frames, or to "soft-apply" any custom scaling.
   const gfx::Size& natural_size() const { return natural_size_; }
 
   int stride(size_t plane) const {
--- a/media/base/video_frame_layout.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_frame_layout.h	2019-05-17 18:53:34.104000000 +0300
@@ -45,8 +45,9 @@
     bool operator==(const Plane& rhs) const;
     bool operator!=(const Plane& rhs) const;
 
-    // Strides in bytes of a plane. Note that stride can be negative if the
-    // image layout is bottom-up.
+    // Strides of a plane, typically greater or equal to the
+    // width of the surface divided by the horizontal sampling period. Note that
+    // strides can be negative if the image layout is bottom-up.
     int32_t stride = 0;
 
     // Offset of a plane, which stands for the offset of a start point of a
--- a/media/base/video_frame_metadata.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_frame_metadata.cc	2019-05-17 18:53:34.104000000 +0300
@@ -9,8 +9,6 @@
 
 #include "base/logging.h"
 #include "base/strings/string_number_conversions.h"
-#include "base/value_conversions.h"
-#include "ui/gfx/geometry/rect.h"
 
 namespace media {
 
@@ -19,7 +17,7 @@
 // Map enum key to internal std::string key used by base::DictionaryValue.
 inline std::string ToInternalKey(VideoFrameMetadata::Key key) {
   DCHECK_LT(key, VideoFrameMetadata::NUM_KEYS);
-  return base::NumberToString(static_cast<int>(key));
+  return base::IntToString(static_cast<int>(key));
 }
 
 }  // namespace
@@ -79,22 +77,6 @@
   SetTimeValue(key, value, &dictionary_);
 }
 
-void VideoFrameMetadata::SetUnguessableToken(
-    Key key,
-    const base::UnguessableToken& value) {
-  dictionary_.SetKey(ToInternalKey(key),
-                     base::CreateUnguessableTokenValue(value));
-}
-
-void VideoFrameMetadata::SetRect(Key key, const gfx::Rect& value) {
-  base::Value init[] = {base::Value(value.x()), base::Value(value.y()),
-                        base::Value(value.width()),
-                        base::Value(value.height())};
-  SetValue(key, std::make_unique<base::ListValue>(base::Value::ListStorage{
-                    std::make_move_iterator(std::begin(init)),
-                    std::make_move_iterator(std::end(init))}));
-}
-
 void VideoFrameMetadata::SetValue(Key key, std::unique_ptr<base::Value> value) {
   dictionary_.SetWithoutPathExpansion(ToInternalKey(key), std::move(value));
 }
@@ -158,31 +140,6 @@
   return binary_value && ToTimeValue(*binary_value, value);
 }
 
-bool VideoFrameMetadata::GetUnguessableToken(
-    Key key,
-    base::UnguessableToken* value) const {
-  const base::Value* internal_value = dictionary_.FindKey(ToInternalKey(key));
-  if (!internal_value)
-    return false;
-  return base::GetValueAsUnguessableToken(*internal_value, value);
-}
-
-bool VideoFrameMetadata::GetRect(Key key, gfx::Rect* value) const {
-  const base::ListValue* internal_value = GetList(key);
-  if (!internal_value || internal_value->GetList().size() != 4)
-    return false;
-  *value = gfx::Rect(internal_value->GetList()[0].GetInt(),
-                     internal_value->GetList()[1].GetInt(),
-                     internal_value->GetList()[2].GetInt(),
-                     internal_value->GetList()[3].GetInt());
-  return true;
-}
-
-const base::ListValue* VideoFrameMetadata::GetList(Key key) const {
-  return static_cast<const base::ListValue*>(
-      dictionary_.FindKeyOfType(ToInternalKey(key), base::Value::Type::LIST));
-}
-
 const base::Value* VideoFrameMetadata::GetValue(Key key) const {
   return dictionary_.FindKey(ToInternalKey(key));
 }
--- a/media/base/video_frame_metadata.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_frame_metadata.h	2019-05-17 18:53:34.104000000 +0300
@@ -11,16 +11,11 @@
 #include "base/compiler_specific.h"
 #include "base/macros.h"
 #include "base/time/time.h"
-#include "base/unguessable_token.h"
 #include "base/values.h"
 #include "build/build_config.h"
 #include "media/base/media_export.h"
 #include "media/base/video_rotation.h"
 
-namespace gfx {
-class Rect;
-}
-
 namespace media {
 
 class MEDIA_EXPORT VideoFrameMetadata {
@@ -38,24 +33,6 @@
     CAPTURE_BEGIN_TIME,
     CAPTURE_END_TIME,
 
-    // A counter that is increased by the producer of video frames each time
-    // it pushes out a new frame. By looking for gaps in this counter, clients
-    // can determine whether or not any frames have been dropped on the way from
-    // the producer between two consecutively received frames. Note that the
-    // counter may start at arbitrary values, so the absolute value of it has no
-    // meaning.
-    CAPTURE_COUNTER,
-
-    // A base::ListValue containing 4 integers representing x, y, width, height
-    // of the rectangular region of the frame that has changed since the frame
-    // with the directly preceding CAPTURE_COUNTER. If that frame was not
-    // received, typically because it was dropped during transport from the
-    // producer, clients must assume that the entire frame has changed.
-    // The rectangle is relative to the full frame data, i.e. [0, 0,
-    // coded_size().width(), coded_size().height()]. It does not have to be
-    // fully contained within visible_rect().
-    CAPTURE_UPDATE_RECT,
-
     // Indicates that this frame must be copied to a new texture before use,
     // rather than being used directly. Specifically this is required for
     // WebView because of limitations about sharing surface textures between GL
@@ -141,11 +118,6 @@
     // PROTECTED_VIDEO is also set to true.
     HW_PROTECTED,
 
-    // An UnguessableToken that identifies VideoOverlayFactory that created
-    // this VideoFrame. It's used by Cast to help with video hole punch.
-    // Use Get/SetUnguessableToken() for this key.
-    OVERLAY_PLANE_ID,
-
     // Whether this frame was decoded in a power efficient way.
     POWER_EFFICIENT,
 
@@ -179,8 +151,6 @@
   void SetString(Key key, const std::string& value);
   void SetTimeDelta(Key key, const base::TimeDelta& value);
   void SetTimeTicks(Key key, const base::TimeTicks& value);
-  void SetUnguessableToken(Key key, const base::UnguessableToken& value);
-  void SetRect(Key key, const gfx::Rect& value);
   void SetValue(Key key, std::unique_ptr<base::Value> value);
 
   // Getters.  Returns true if |key| is present, and its value has been set.
@@ -191,11 +161,7 @@
   bool GetString(Key key, std::string* value) const WARN_UNUSED_RESULT;
   bool GetTimeDelta(Key key, base::TimeDelta* value) const WARN_UNUSED_RESULT;
   bool GetTimeTicks(Key key, base::TimeTicks* value) const WARN_UNUSED_RESULT;
-  bool GetUnguessableToken(Key key, base::UnguessableToken* value) const
-      WARN_UNUSED_RESULT;
-  bool GetRect(Key key, gfx::Rect* value) const WARN_UNUSED_RESULT;
-  // Returns null if |key| was not present or value was not a ListValue.
-  const base::ListValue* GetList(Key key) const WARN_UNUSED_RESULT;
+
   // Returns null if |key| was not present.
   const base::Value* GetValue(Key key) const WARN_UNUSED_RESULT;
 
@@ -205,7 +171,7 @@
   // For serialization.
   std::unique_ptr<base::DictionaryValue> CopyInternalValues() const;
   void MergeInternalValuesFrom(const base::Value& in);
-  const base::Value& GetInternalValues() const { return dictionary_; }
+  const base::Value& GetInternalValues() const { return dictionary_; };
 
   // Merges internal values from |metadata_source|.
   void MergeMetadataFrom(const VideoFrameMetadata* metadata_source);
--- a/media/base/video_frame_pool_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_frame_pool_unittest.cc	2019-05-17 18:53:34.104000000 +0300
@@ -58,7 +58,7 @@
     EXPECT_EQ(0, frame->data(i)[0]);
 }
 
-INSTANTIATE_TEST_SUITE_P(,
+INSTANTIATE_TEST_CASE_P(,
                          VideoFramePoolTest,
                          testing::Values(PIXEL_FORMAT_I420,
                                          PIXEL_FORMAT_NV12,
--- a/media/base/video_frame_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_frame_unittest.cc	2019-05-17 18:53:34.104000000 +0300
@@ -19,7 +19,6 @@
 #include "base/strings/stringprintf.h"
 #include "build/build_config.h"
 #include "gpu/command_buffer/common/mailbox_holder.h"
-#include "media/base/simple_sync_token_client.h"
 #include "testing/gtest/include/gtest/gtest.h"
 #include "third_party/libyuv/include/libyuv.h"
 
@@ -144,7 +143,7 @@
 
   base::MD5Context context;
   base::MD5Init(&context);
-  VideoFrame::HashFrameForTesting(&context, *frame.get());
+  VideoFrame::HashFrameForTesting(&context, frame);
   base::MD5Digest digest;
   base::MD5Final(&digest, &context);
   EXPECT_EQ(MD5DigestToBase16(digest), expected_hash);
@@ -171,7 +170,7 @@
   base::MD5Digest digest;
   base::MD5Context context;
   base::MD5Init(&context);
-  VideoFrame::HashFrameForTesting(&context, *frame.get());
+  VideoFrame::HashFrameForTesting(&context, frame);
   base::MD5Final(&digest, &context);
   EXPECT_EQ(MD5DigestToBase16(digest), "9065c841d9fca49186ef8b4ef547e79b");
   {
@@ -180,7 +179,7 @@
     ExpectFrameColor(frame.get(), 0xFFFFFFFF);
   }
   base::MD5Init(&context);
-  VideoFrame::HashFrameForTesting(&context, *frame.get());
+  VideoFrame::HashFrameForTesting(&context, frame);
   base::MD5Final(&digest, &context);
   EXPECT_EQ(MD5DigestToBase16(digest), "911991d51438ad2e1a40ed5f6fc7c796");
 
@@ -468,6 +467,24 @@
   EXPECT_FALSE(called_sync_token.HasData());
 }
 
+namespace {
+
+class SyncTokenClientImpl : public VideoFrame::SyncTokenClient {
+ public:
+  explicit SyncTokenClientImpl(const gpu::SyncToken& sync_token)
+      : sync_token_(sync_token) {}
+  ~SyncTokenClientImpl() override = default;
+  void GenerateSyncToken(gpu::SyncToken* sync_token) override {
+    *sync_token = sync_token_;
+  }
+  void WaitSyncToken(const gpu::SyncToken& sync_token) override {}
+
+ private:
+  gpu::SyncToken sync_token_;
+};
+
+}  // namespace
+
 // Verify the gpu::MailboxHolder::ReleaseCallback is called when VideoFrame is
 // destroyed with the release sync point, which was updated by clients.
 // (i.e. the compositor, webgl).
@@ -515,7 +532,7 @@
       EXPECT_EQ(sync_token, mailbox_holder.sync_token);
     }
 
-    SimpleSyncTokenClient client(release_sync_token);
+    SyncTokenClientImpl client(release_sync_token);
     frame->UpdateReleaseSyncToken(&client);
     EXPECT_EQ(sync_token,
               frame->mailbox_holder(VideoFrame::kYPlane).sync_token);
--- a/media/base/video_thumbnail_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_thumbnail_decoder.cc	2019-05-17 18:53:34.108000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/base/video_thumbnail_decoder.h"
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "media/base/decoder_buffer.h"
 #include "media/base/video_frame.h"
--- a/media/base/video_thumbnail_decoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_thumbnail_decoder_unittest.cc	2019-05-17 18:53:34.108000000 +0300
@@ -5,7 +5,6 @@
 #include <memory>
 #include <vector>
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/run_loop.h"
 #include "base/test/scoped_task_environment.h"
--- a/media/base/video_util_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/base/video_util_unittest.cc	2019-05-17 18:53:34.108000000 +0300
@@ -434,8 +434,7 @@
   EXPECT_EQ(memcmp(dest, GetParam().target, size), 0);
 }
 
-INSTANTIATE_TEST_SUITE_P(,
-                         VideoUtilRotationTest,
+INSTANTIATE_TEST_CASE_P(, VideoUtilRotationTest,
                          testing::ValuesIn(kVideoRotationTestData));
 
 // Tests the ComputeLetterboxRegion function.  Also, because of shared code
--- a/media/blink/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/BUILD.gn	2019-05-17 18:53:34.108000000 +0300
@@ -98,6 +98,13 @@
       "webmediaplayer_impl.cc",
       "webmediaplayer_impl.h",
     ]
+    if (is_android) {
+      sources += [
+        "webmediaplayer_cast_android.cc",
+        "webmediaplayer_cast_android.h",
+      ]
+      deps += [ "//gpu/command_buffer/client:gles2_interface" ]
+    }
   }
 }
 
--- a/media/blink/cache_util_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/cache_util_unittest.cc	2019-05-17 18:53:34.112000000 +0300
@@ -36,7 +36,7 @@
 static WebURLResponse CreateResponse(const GRFUTestCase& test) {
   WebURLResponse response;
   response.SetHTTPVersion(test.version);
-  response.SetHttpStatusCode(test.status_code);
+  response.SetHTTPStatusCode(test.status_code);
   for (const std::string& line :
        base::SplitString(test.headers, "\n", base::KEEP_WHITESPACE,
                          base::SPLIT_WANT_NONEMPTY)) {
--- a/media/blink/DEPS	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/DEPS	2019-05-17 18:53:34.108000000 +0300
@@ -15,6 +15,7 @@
   "+services/network/public/cpp",
   "+services/network/public/mojom",
   "+services/service_manager/public/cpp",
+  "+third_party/blink/public/common",
   "+third_party/blink/public/mojom",
   "+third_party/blink/public/platform",
   "+third_party/blink/public/web",
--- a/media/blink/key_system_config_selector.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/key_system_config_selector.cc	2019-05-17 18:53:34.112000000 +0300
@@ -517,7 +517,7 @@
 
   // 3. If the initDataTypes member of candidate configuration is non-empty,
   //    run the following steps:
-  if (!candidate.init_data_types.empty()) {
+  if (!candidate.init_data_types.IsEmpty()) {
     // 3.1. Let supported types be an empty sequence of DOMStrings.
     std::vector<blink::WebEncryptedMediaInitDataType> supported_types;
 
@@ -705,8 +705,8 @@
 
   // 15. If the videoCapabilities and audioCapabilities members in candidate
   //     configuration are both empty, return NotSupported.
-  if (candidate.video_capabilities.empty() &&
-      candidate.audio_capabilities.empty()) {
+  if (candidate.video_capabilities.IsEmpty() &&
+      candidate.audio_capabilities.IsEmpty()) {
     DVLOG(2) << "Rejecting requested configuration because "
              << "neither audioCapabilities nor videoCapabilities is specified";
     return CONFIGURATION_NOT_SUPPORTED;
@@ -715,7 +715,7 @@
   // 16. If the videoCapabilities member in candidate configuration is
   //     non-empty:
   std::vector<blink::WebMediaKeySystemMediaCapability> video_capabilities;
-  if (!candidate.video_capabilities.empty()) {
+  if (!candidate.video_capabilities.IsEmpty()) {
     // 16.1. Let video capabilities be the result of executing the Get
     //       Supported Capabilities for Audio/Video Type algorithm on Video,
     //       candidate configuration's videoCapabilities member, accumulated
@@ -741,7 +741,7 @@
   // 17. If the audioCapabilities member in candidate configuration is
   //     non-empty:
   std::vector<blink::WebMediaKeySystemMediaCapability> audio_capabilities;
-  if (!candidate.audio_capabilities.empty()) {
+  if (!candidate.audio_capabilities.IsEmpty()) {
     // 17.1. Let audio capabilities be the result of executing the Get
     //       Supported Capabilities for Audio/Video Type algorithm on Audio,
     //       candidate configuration's audioCapabilities member, accumulated
--- a/media/blink/key_system_config_selector_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/key_system_config_selector_unittest.cc	2019-05-17 18:53:34.112000000 +0300
@@ -488,9 +488,9 @@
   SelectConfigReturnsConfig();
 
   EXPECT_EQ("", config_.label);
-  EXPECT_TRUE(config_.init_data_types.empty());
+  EXPECT_TRUE(config_.init_data_types.IsEmpty());
   EXPECT_EQ(1u, config_.audio_capabilities.size());
-  EXPECT_TRUE(config_.video_capabilities.empty());
+  EXPECT_TRUE(config_.video_capabilities.IsEmpty());
   EXPECT_EQ(MediaKeysRequirement::kNotAllowed, config_.distinctive_identifier);
   EXPECT_EQ(MediaKeysRequirement::kNotAllowed, config_.persistent_state);
   ASSERT_EQ(1u, config_.session_types.size());
@@ -700,7 +700,7 @@
   configs_.push_back(config);
 
   SelectConfigReturnsConfig();
-  EXPECT_TRUE(config_.session_types.empty());
+  EXPECT_TRUE(config_.session_types.IsEmpty());
 }
 
 TEST_F(KeySystemConfigSelectorTest, SessionTypes_SubsetSupported) {
--- a/media/blink/multibuffer_data_source.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/multibuffer_data_source.cc	2019-05-17 18:53:34.112000000 +0300
@@ -206,7 +206,7 @@
   if (reader_->Available()) {
     render_task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&MultibufferDataSource::StartCallback, weak_ptr_));
+        base::Bind(&MultibufferDataSource::StartCallback, weak_ptr_));
 
     // When the entire file is already in the cache, we won't get any more
     // progress callbacks, which breaks some expectations. Post a task to
@@ -229,7 +229,7 @@
     if (init_cb_) {
       render_task_runner_->PostTask(
           FROM_HERE,
-          base::BindOnce(&MultibufferDataSource::StartCallback, weak_ptr_));
+          base::Bind(&MultibufferDataSource::StartCallback, weak_ptr_));
     } else {
       base::AutoLock auto_lock(lock_);
       StopInternal_Locked();
@@ -252,7 +252,7 @@
       if (reader_->Available()) {
         render_task_runner_->PostTask(
             FROM_HERE,
-            base::BindOnce(&MultibufferDataSource::StartCallback, weak_ptr_));
+            base::Bind(&MultibufferDataSource::StartCallback, weak_ptr_));
       } else {
         reader_->Wait(
             1, base::Bind(&MultibufferDataSource::StartCallback, weak_ptr_));
@@ -289,10 +289,6 @@
   return url_data_->is_cors_cross_origin();
 }
 
-bool MultibufferDataSource::HasAccessControl() const {
-  return url_data_->has_access_control();
-}
-
 UrlData::CorsMode MultibufferDataSource::cors_mode() const {
   return url_data_->cors_mode();
 }
@@ -332,8 +328,8 @@
     }
   }
 
-  render_task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&MultibufferDataSource::StopLoader,
+  render_task_runner_->PostTask(FROM_HERE,
+                                base::Bind(&MultibufferDataSource::StopLoader,
                                 weak_factory_.GetWeakPtr()));
 }
 
@@ -412,7 +408,7 @@
         if (seek_positions_.size() == 1) {
           render_task_runner_->PostDelayedTask(
               FROM_HERE,
-              base::BindOnce(&MultibufferDataSource::SeekTask,
+              base::Bind(&MultibufferDataSource::SeekTask,
                              weak_factory_.GetWeakPtr()),
               kSeekDelay);
         }
@@ -424,9 +420,9 @@
     read_op_.reset(new ReadOperation(position, size, data, read_cb));
   }
 
-  render_task_runner_->PostTask(FROM_HERE,
-                                base::BindOnce(&MultibufferDataSource::ReadTask,
-                                               weak_factory_.GetWeakPtr()));
+  render_task_runner_->PostTask(
+      FROM_HERE,
+      base::Bind(&MultibufferDataSource::ReadTask, weak_factory_.GetWeakPtr()));
 }
 
 bool MultibufferDataSource::GetSize(int64_t* size_out) {
@@ -627,7 +623,7 @@
   }
 
   render_task_runner_->PostTask(FROM_HERE,
-                                base::BindOnce(std::move(init_cb_), success));
+                                base::Bind(std::move(init_cb_), success));
 
   UpdateBufferSizes();
 
--- a/media/blink/multibuffer_data_source.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/multibuffer_data_source.h	2019-05-17 18:53:34.112000000 +0300
@@ -82,10 +82,6 @@
   // This must be called after the response arrives.
   bool IsCorsCrossOrigin() const;
 
-  // Returns true if the response includes an Access-Control-Allow-Origin
-  // header (that is not "null").
-  bool HasAccessControl() const;
-
   // Returns the CorsMode of the underlying UrlData.
   UrlData::CorsMode cors_mode() const;
 
--- a/media/blink/multibuffer_data_source_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/multibuffer_data_source_unittest.cc	2019-05-17 18:53:34.112000000 +0300
@@ -1385,7 +1385,7 @@
   // Server responds with a redirect.
   blink::WebURL url{GURL(kHttpDifferentPathUrl)};
   blink::WebURLResponse response((GURL(kHttpUrl)));
-  response.SetHttpStatusCode(307);
+  response.SetHTTPStatusCode(307);
   data_provider()->WillFollowRedirect(url, response);
   Respond(response_generator_->Generate206(kDataSize));
   ReceiveData(kDataSize);
@@ -1401,7 +1401,7 @@
   // Server responds with a redirect.
   blink::WebURL url{GURL(kHttpDifferentPathUrl)};
   blink::WebURLResponse response((GURL(kHttpUrl)));
-  response.SetHttpStatusCode(307);
+  response.SetHTTPStatusCode(307);
   data_provider()->WillFollowRedirect(url, response);
 
   EXPECT_CALL(host_, SetTotalBytes(response_generator_->content_length()));
@@ -1423,7 +1423,7 @@
   // Server responds with a redirect.
   blink::WebURL url{GURL(kHttpDifferentPathUrl)};
   blink::WebURLResponse response((GURL(kHttpUrl)));
-  response.SetHttpStatusCode(307);
+  response.SetHTTPStatusCode(307);
   data_provider()->WillFollowRedirect(url, response);
 
   EXPECT_CALL(host_, AddBufferedByteRange(0, kDataSize));
@@ -1437,7 +1437,7 @@
   // Server responds with a redirect.
   blink::WebURL url{GURL(kHttpDifferentPathUrl)};
   blink::WebURLResponse response((GURL(kHttpUrl)));
-  response.SetHttpStatusCode(307);
+  response.SetHTTPStatusCode(307);
   data_provider()->WillFollowRedirect(url, response);
 
   Respond(response_generator_->Generate404());
@@ -1475,10 +1475,10 @@
   Initialize(kHttpUrl, true);
   GURL gurl(kHttpUrl);
   blink::WebURLResponse response(gurl);
-  response.SetHttpStatusCode(200);
+  response.SetHTTPStatusCode(200);
   response.SetHTTPHeaderField(
       WebString::FromUTF8("Content-Length"),
-      WebString::FromUTF8(base::NumberToString(kDataSize / 2)));
+      WebString::FromUTF8(base::Int64ToString(kDataSize / 2)));
   response.SetExpectedContentLength(kDataSize / 2);
   Respond(response);
   EXPECT_CALL(host_, AddBufferedByteRange(0, kDataSize / 2));
--- a/media/blink/multibuffer_reader.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/multibuffer_reader.cc	2019-05-17 18:53:34.112000000 +0300
@@ -181,8 +181,7 @@
   if (!progress_callback_.is_null()) {
     base::ThreadTaskRunnerHandle::Get()->PostTask(
         FROM_HERE,
-        base::BindOnce(progress_callback_,
-                       static_cast<int64_t>(range.begin)
+        base::Bind(progress_callback_, static_cast<int64_t>(range.begin)
                            << multibuffer_->block_size_shift(),
                        (static_cast<int64_t>(range.end)
                         << multibuffer_->block_size_shift()) +
--- a/media/blink/resource_multibuffer_data_provider.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/resource_multibuffer_data_provider.cc	2019-05-17 18:53:34.116000000 +0300
@@ -197,8 +197,8 @@
 }
 
 void ResourceMultiBufferDataProvider::DidSendData(
-    uint64_t bytes_sent,
-    uint64_t total_bytes_to_be_sent) {
+    unsigned long long bytes_sent,
+    unsigned long long total_bytes_to_be_sent) {
   NOTIMPLEMENTED();
 }
 
@@ -331,18 +331,6 @@
   destination_url_data->set_is_cors_cross_origin(
       network::cors::IsCorsCrossOriginResponseType(response_type));
 
-  // Only used for metrics.
-  {
-    WebString access_control =
-        response.HttpHeaderField("Access-Control-Allow-Origin");
-    if (!access_control.IsEmpty() && !access_control.Equals("null")) {
-      // Note: When |access_control| is not *, we should verify that it matches
-      // the requesting origin. Instead we just assume that it matches, which is
-      // probably accurate enough for metrics.
-      destination_url_data->set_has_access_control();
-    }
-  }
-
   if (destination_url_data != url_data_) {
     // At this point, we've encountered a redirect, or found a better url data
     // instance for the data that we're about to download.
@@ -433,7 +421,8 @@
   // Beware, this object might be deleted here.
 }
 
-void ResourceMultiBufferDataProvider::DidDownloadData(uint64_t dataLength) {
+void ResourceMultiBufferDataProvider::DidDownloadData(
+    unsigned long long dataLength) {
   NOTIMPLEMENTED();
 }
 
@@ -463,7 +452,7 @@
       retries_++;
       base::ThreadTaskRunnerHandle::Get()->PostDelayedTask(
           FROM_HERE,
-          base::BindOnce(&ResourceMultiBufferDataProvider::Start,
+          base::Bind(&ResourceMultiBufferDataProvider::Start,
                          weak_factory_.GetWeakPtr()),
           base::TimeDelta::FromMilliseconds(kLoaderPartialRetryDelayMs));
       return;
@@ -495,7 +484,7 @@
     retries_++;
     base::ThreadTaskRunnerHandle::Get()->PostDelayedTask(
         FROM_HERE,
-        base::BindOnce(&ResourceMultiBufferDataProvider::Start,
+        base::Bind(&ResourceMultiBufferDataProvider::Start,
                        weak_factory_.GetWeakPtr()),
         base::TimeDelta::FromMilliseconds(
             kLoaderFailedRetryDelayMs + kAdditionalDelayPerRetryMs * retries_));
--- a/media/blink/resource_multibuffer_data_provider.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/resource_multibuffer_data_provider.h	2019-05-17 18:53:34.116000000 +0300
@@ -52,9 +52,10 @@
   bool WillFollowRedirect(
       const blink::WebURL& new_url,
       const blink::WebURLResponse& redirect_response) override;
-  void DidSendData(uint64_t bytesSent, uint64_t totalBytesToBeSent) override;
+  void DidSendData(unsigned long long bytesSent,
+                   unsigned long long totalBytesToBeSent) override;
   void DidReceiveResponse(const blink::WebURLResponse& response) override;
-  void DidDownloadData(uint64_t data_length) override;
+  void DidDownloadData(unsigned long long data_length) override;
   void DidReceiveData(const char* data, int data_length) override;
   void DidReceiveCachedMetadata(const char* data, int dataLength) override;
   void DidFinishLoading() override;
--- a/media/blink/resource_multibuffer_data_provider_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/resource_multibuffer_data_provider_unittest.cc	2019-05-17 18:53:34.116000000 +0300
@@ -111,7 +111,7 @@
         WebString::FromUTF8("Content-Length"),
         WebString::FromUTF8(base::StringPrintf("%" PRId64, instance_size)));
     response.SetExpectedContentLength(instance_size);
-    response.SetHttpStatusCode(kHttpOK);
+    response.SetHTTPStatusCode(kHttpOK);
     loader_->DidReceiveResponse(response);
 
     if (ok) {
@@ -156,7 +156,7 @@
                                   WebString::FromUTF8("bytes"));
     }
 
-    response.SetHttpStatusCode(kHttpPartialContent);
+    response.SetHTTPStatusCode(kHttpPartialContent);
     loader_->DidReceiveResponse(response);
 
     EXPECT_EQ(instance_size, url_data_->length());
@@ -244,7 +244,7 @@
   EXPECT_CALL(*this, RedirectCallback(scoped_refptr<UrlData>(nullptr)));
 
   WebURLResponse response(gurl_);
-  response.SetHttpStatusCode(404);
+  response.SetHTTPStatusCode(404);
   response.SetHTTPStatusText("Not Found\n");
   loader_->DidReceiveResponse(response);
 }
@@ -308,7 +308,7 @@
                                              "%d-%d/%d",
                                              1, 10, 1024)));
   response.SetExpectedContentLength(10);
-  response.SetHttpStatusCode(kHttpPartialContent);
+  response.SetHTTPStatusCode(kHttpPartialContent);
   loader_->DidReceiveResponse(response);
 }
 
--- a/media/blink/test_response_generator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/test_response_generator.cc	2019-05-17 18:53:34.116000000 +0300
@@ -27,11 +27,11 @@
 
 WebURLResponse TestResponseGenerator::Generate200() {
   WebURLResponse response(gurl_);
-  response.SetHttpStatusCode(200);
+  response.SetHTTPStatusCode(200);
 
   response.SetHTTPHeaderField(
       WebString::FromUTF8("Content-Length"),
-      WebString::FromUTF8(base::NumberToString(content_length_)));
+      WebString::FromUTF8(base::Int64ToString(content_length_)));
   response.SetExpectedContentLength(content_length_);
   return response;
 }
@@ -58,7 +58,7 @@
   int64_t range_content_length = content_length_ - first_byte_offset;
 
   WebURLResponse response(gurl_);
-  response.SetHttpStatusCode(206);
+  response.SetHTTPStatusCode(206);
 
   if ((flags & kNoAcceptRanges) == 0) {
     response.SetHTTPHeaderField(WebString::FromUTF8("Accept-Ranges"),
@@ -80,7 +80,7 @@
   if ((flags & kNoContentLength) == 0) {
     response.SetHTTPHeaderField(
         WebString::FromUTF8("Content-Length"),
-        WebString::FromUTF8(base::NumberToString(range_content_length)));
+        WebString::FromUTF8(base::Int64ToString(range_content_length)));
     response.SetExpectedContentLength(range_content_length);
   }
   return response;
@@ -88,7 +88,7 @@
 
 WebURLResponse TestResponseGenerator::GenerateResponse(int code) {
   WebURLResponse response(gurl_);
-  response.SetHttpStatusCode(code);
+  response.SetHTTPStatusCode(code);
   return response;
 }
 
@@ -99,7 +99,7 @@
 WebURLResponse TestResponseGenerator::GenerateFileResponse(
     int64_t first_byte_offset) {
   WebURLResponse response(gurl_);
-  response.SetHttpStatusCode(0);
+  response.SetHTTPStatusCode(0);
 
   if (first_byte_offset >= 0) {
     response.SetExpectedContentLength(content_length_ - first_byte_offset);
--- a/media/blink/texttrack_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/texttrack_impl.cc	2019-05-17 18:53:34.116000000 +0300
@@ -28,8 +28,8 @@
 
 TextTrackImpl::~TextTrackImpl() {
   task_runner_->PostTask(FROM_HERE,
-                         base::BindOnce(&TextTrackImpl::OnRemoveTrack, client_,
-                                        std::move(text_track_)));
+                         base::Bind(&TextTrackImpl::OnRemoveTrack, client_,
+                                    base::Passed(&text_track_)));
 }
 
 void TextTrackImpl::addWebVTTCue(base::TimeDelta start,
--- a/media/blink/url_index.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/url_index.cc	2019-05-17 18:53:34.116000000 +0300
@@ -49,7 +49,6 @@
     : url_(url),
       have_data_origin_(false),
       cors_mode_(cors_mode),
-      has_access_control_(false),
       url_index_(url_index),
       length_(kPositionNotSpecified),
       range_supported_(false),
@@ -91,7 +90,6 @@
     bytes_read_from_cache_ += other->bytes_read_from_cache_;
     // is_cors_corss_origin_ will not relax from true to false.
     set_is_cors_cross_origin(other->is_cors_cross_origin_);
-    has_access_control_ |= other->has_access_control_;
     multibuffer()->MergeFrom(other->multibuffer());
   }
 }
@@ -114,10 +112,6 @@
   is_cors_cross_origin_ = is_cors_cross_origin;
 }
 
-void UrlData::set_has_access_control() {
-  has_access_control_ = true;
-}
-
 void UrlData::RedirectTo(const scoped_refptr<UrlData>& url_data) {
   DCHECK(thread_checker_.CalledOnValidThread());
   // Copy any cached data over to the new location.
--- a/media/blink/url_index.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/url_index.h	2019-05-17 18:53:34.116000000 +0300
@@ -67,8 +67,6 @@
   // Cross-origin access mode
   CorsMode cors_mode() const { return cors_mode_; }
 
-  bool has_access_control() const { return has_access_control_; }
-
   // Are HTTP range requests supported?
   bool range_supported() const { return range_supported_; }
 
@@ -124,7 +122,6 @@
   void set_last_modified(base::Time last_modified);
   void set_etag(const std::string& etag);
   void set_is_cors_cross_origin(bool is_cors_cross_origin);
-  void set_has_access_control();
 
   // A redirect has occured (or we've found a better UrlData for the same
   // resource).
@@ -185,7 +182,6 @@
 
   // Cross-origin access mode.
   const CorsMode cors_mode_;
-  bool has_access_control_;
 
   UrlIndex* const url_index_;
 
@@ -201,7 +197,7 @@
   // Does the server support ranges?
   bool range_supported_;
 
-  // Set to false if we have reason to believe the chrome disk cache
+  // Set to false if we have reason to beleive the chrome disk cache
   // will not cache this url.
   bool cacheable_;
 
--- a/media/blink/video_decode_stats_reporter.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/video_decode_stats_reporter.cc	2019-05-17 18:53:34.116000000 +0300
@@ -7,11 +7,9 @@
 #include <cmath>
 #include <limits>
 
-#include "base/bind.h"
 #include "base/logging.h"
 #include "base/macros.h"
 #include "media/capabilities/bucket_utility.h"
-#include "media/mojo/interfaces/media_types.mojom.h"
 
 namespace media {
 
--- a/media/blink/video_decode_stats_reporter_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/video_decode_stats_reporter_unittest.cc	2019-05-17 18:53:34.116000000 +0300
@@ -4,7 +4,6 @@
 
 #include <memory>
 
-#include "base/bind.h"
 #include "base/memory/ptr_util.h"
 #include "base/memory/ref_counted.h"
 #include "base/message_loop/message_loop_current.h"
@@ -16,7 +15,6 @@
 #include "media/base/video_types.h"
 #include "media/blink/video_decode_stats_reporter.h"
 #include "media/capabilities/bucket_utility.h"
-#include "media/mojo/interfaces/media_types.mojom.h"
 #include "media/mojo/interfaces/video_decode_stats_recorder.mojom.h"
 #include "mojo/public/cpp/bindings/strong_binding.h"
 #include "testing/gmock/include/gmock/gmock.h"
--- a/media/blink/video_frame_compositor.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/video_frame_compositor.cc	2019-05-17 18:53:34.116000000 +0300
@@ -205,7 +205,7 @@
   if (!task_runner_->BelongsToCurrentThread()) {
     task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&VideoFrameCompositor::PaintSingleFrame,
+        base::Bind(&VideoFrameCompositor::PaintSingleFrame,
                        base::Unretained(this), frame, repaint_duplicate_frame));
     return;
   }
--- a/media/blink/video_frame_compositor_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/video_frame_compositor_unittest.cc	2019-05-17 18:53:34.116000000 +0300
@@ -341,7 +341,7 @@
   StopVideoRendererSink(false);
 }
 
-INSTANTIATE_TEST_SUITE_P(SubmitterEnabled,
+INSTANTIATE_TEST_CASE_P(SubmitterEnabled,
                          VideoFrameCompositorTest,
                          ::testing::Bool());
 
--- a/media/blink/watch_time_reporter.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/watch_time_reporter.cc	2019-05-17 18:53:34.120000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/blink/watch_time_reporter.h"
 
-#include "base/bind.h"
 #include "base/power_monitor/power_monitor.h"
 #include "media/base/watch_time_keys.h"
 
--- a/media/blink/watch_time_reporter_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/watch_time_reporter_unittest.cc	2019-05-17 18:53:34.120000000 +0300
@@ -1999,7 +1999,7 @@
   wtr_.reset();
 }
 
-INSTANTIATE_TEST_SUITE_P(WatchTimeReporterTest,
+INSTANTIATE_TEST_CASE_P(WatchTimeReporterTest,
                          WatchTimeReporterTest,
                          testing::ValuesIn({// has_video, has_audio
                                             std::make_tuple(true, true),
@@ -2009,7 +2009,7 @@
                                             std::make_tuple(false, true)}));
 
 // Separate test set since display tests only work with video.
-INSTANTIATE_TEST_SUITE_P(DisplayTypeWatchTimeReporterTest,
+INSTANTIATE_TEST_CASE_P(DisplayTypeWatchTimeReporterTest,
                          DisplayTypeWatchTimeReporterTest,
                          testing::ValuesIn({// has_video, has_audio
                                             std::make_tuple(true, true),
@@ -2017,7 +2017,7 @@
                                             std::make_tuple(true, false)}));
 
 // Separate test set since muted tests only work with audio+video.
-INSTANTIATE_TEST_SUITE_P(MutedWatchTimeReporterTest,
+INSTANTIATE_TEST_CASE_P(MutedWatchTimeReporterTest,
                          MutedWatchTimeReporterTest,
                          testing::ValuesIn({
                              // has_video, has_audio
--- a/media/blink/webmediacapabilitiesclient_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/webmediacapabilitiesclient_impl.cc	2019-05-17 18:53:34.120000000 +0300
@@ -7,17 +7,19 @@
 #include <string>
 #include <vector>
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
+#include "media/base/audio_codecs.h"
 #include "media/base/key_system_names.h"
 #include "media/base/mime_util.h"
 #include "media/base/supported_types.h"
 #include "media/base/video_codecs.h"
 #include "media/base/video_color_space.h"
 #include "media/blink/webcontentdecryptionmoduleaccess_impl.h"
+#include "media/filters/stream_parser_factory.h"
 #include "media/mojo/interfaces/media_types.mojom.h"
 #include "mojo/public/cpp/bindings/associated_interface_ptr.h"
 #include "services/service_manager/public/cpp/connector.h"
+#include "third_party/blink/public/platform/modules/media_capabilities/web_audio_configuration.h"
 #include "third_party/blink/public/platform/modules/media_capabilities/web_media_capabilities_info.h"
 #include "third_party/blink/public/platform/modules/media_capabilities/web_media_decoding_configuration.h"
 #include "third_party/blink/public/platform/modules/media_capabilities/web_video_configuration.h"
@@ -35,6 +37,31 @@
                            mojo::MakeRequest(history_ptr));
 }
 
+bool CheckAudioSupport(const blink::WebAudioConfiguration& audio_config) {
+  bool audio_supported = false;
+  AudioCodec audio_codec = kUnknownAudioCodec;
+  bool is_audio_codec_ambiguous = true;
+
+  if (!ParseAudioCodecString(audio_config.mime_type.Ascii(),
+                             audio_config.codec.Ascii(),
+                             &is_audio_codec_ambiguous, &audio_codec)) {
+    // TODO(chcunningham): Replace this and other DVLOGs here with MEDIA_LOG.
+    // MediaCapabilities may need its own tab in chrome://media-internals.
+    DVLOG(2) << __func__ << " Failed to parse audio contentType: "
+             << audio_config.mime_type.Ascii()
+             << "; codecs=" << audio_config.codec.Ascii();
+    audio_supported = false;
+  } else if (is_audio_codec_ambiguous) {
+    DVLOG(2) << __func__ << " Invalid (ambiguous) audio codec string:"
+             << audio_config.codec.Ascii();
+    audio_supported = false;
+  } else {
+    audio_supported = IsSupportedAudioType({audio_codec});
+  }
+
+  return audio_supported;
+}
+
 bool CheckVideoSupport(const blink::WebVideoConfiguration& video_config,
                        VideoCodecProfile* out_video_profile) {
   bool video_supported = false;
@@ -63,6 +90,54 @@
   return video_supported;
 }
 
+bool CheckMseSupport(const blink::WebMediaConfiguration& configuration) {
+  DCHECK_EQ(blink::MediaConfigurationType::kMediaSource, configuration.type);
+
+  // For MSE queries, we assume the queried audio and video streams will be
+  // placed into separate source buffers.
+  // TODO(chcunningham): Clarify this assumption in the spec.
+
+  // Media MIME API expects a vector of codec strings. We query audio and video
+  // separately, so |codec_string|.size() should always be 1 or 0 (when no
+  // codecs parameter is required for the given mime type).
+  std::vector<std::string> codec_vector;
+
+  if (configuration.audio_configuration) {
+    const blink::WebAudioConfiguration& audio_config =
+        configuration.audio_configuration.value();
+
+    if (!audio_config.codec.Ascii().empty())
+      codec_vector.push_back(audio_config.codec.Ascii());
+
+    if (!media::StreamParserFactory::IsTypeSupported(
+            audio_config.mime_type.Ascii(), codec_vector)) {
+      DVLOG(2) << __func__ << " MSE does not support audio config: "
+               << audio_config.mime_type.Ascii() << " "
+               << (codec_vector.empty() ? "" : codec_vector[1]);
+      return false;
+    }
+  }
+
+  if (configuration.video_configuration) {
+    const blink::WebVideoConfiguration& video_config =
+        configuration.video_configuration.value();
+
+    codec_vector.clear();
+    if (!video_config.codec.Ascii().empty())
+      codec_vector.push_back(video_config.codec.Ascii());
+
+    if (!media::StreamParserFactory::IsTypeSupported(
+            video_config.mime_type.Ascii(), codec_vector)) {
+      DVLOG(2) << __func__ << " MSE does not support video config: "
+               << video_config.mime_type.Ascii() << " "
+               << (codec_vector.empty() ? "" : codec_vector[1]);
+      return false;
+    }
+  }
+
+  return true;
+}
+
 WebMediaCapabilitiesClientImpl::WebMediaCapabilitiesClientImpl() = default;
 
 WebMediaCapabilitiesClientImpl::~WebMediaCapabilitiesClientImpl() = default;
@@ -94,6 +169,32 @@
   std::unique_ptr<blink::WebMediaCapabilitiesDecodingInfo> info(
       new blink::WebMediaCapabilitiesDecodingInfo());
 
+  // MSE support is cheap to check (regex matching). Do it first.
+  if (configuration.type == blink::MediaConfigurationType::kMediaSource &&
+      !CheckMseSupport(configuration)) {
+    info->supported = info->smooth = info->power_efficient = false;
+    callbacks->OnSuccess(std::move(info));
+    return;
+  }
+
+  bool audio_supported = true;
+  if (configuration.audio_configuration)
+    audio_supported =
+        CheckAudioSupport(configuration.audio_configuration.value());
+
+  // No need to check video capabilities if video not included in configuration
+  // or when audio is already known to be unsupported.
+  if (!audio_supported || !configuration.video_configuration) {
+    // Supported audio-only configurations are always considered smooth and
+    // power efficient.
+    info->supported = info->smooth = info->power_efficient = audio_supported;
+    callbacks->OnSuccess(std::move(info));
+    return;
+  }
+
+  // Audio is supported and video configuration is provided in the query; all
+  // that remains is to check video support and performance.
+  DCHECK(audio_supported);
   DCHECK(configuration.video_configuration);
   const blink::WebVideoConfiguration& video_config =
       configuration.video_configuration.value();
--- a/media/blink/webmediacapabilitiesclient_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/webmediacapabilitiesclient_impl_unittest.cc	2019-05-17 18:53:34.120000000 +0300
@@ -7,7 +7,6 @@
 #include "base/run_loop.h"
 #include "base/test/scoped_task_environment.h"
 #include "media/blink/webmediacapabilitiesclient_impl.h"
-#include "media/mojo/interfaces/media_types.mojom.h"
 #include "media/mojo/interfaces/video_decode_perf_history.mojom.h"
 #include "mojo/public/cpp/bindings/binding.h"
 #include "testing/gmock/include/gmock/gmock.h"
--- a/media/blink/webmediaplayer_delegate.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/webmediaplayer_delegate.h	2019-05-17 18:53:34.120000000 +0300
@@ -16,6 +16,10 @@
 class Size;
 }  // namespace gfx
 
+namespace viz {
+class SurfaceId;
+}  // namespace viz
+
 namespace media {
 
 enum class MediaContentType;
@@ -57,7 +61,6 @@
     // Called when external controls are activated.
     virtual void OnPlay() = 0;
     virtual void OnPause() = 0;
-    virtual void OnMuted(bool muted) = 0;
     virtual void OnSeekForward(double seconds) = 0;
     virtual void OnSeekBackward(double seconds) = 0;
 
@@ -73,6 +76,12 @@
     // Called when Picture-in-Picture mode is terminated from the
     // Picture-in-Picture window.
     virtual void OnPictureInPictureModeEnded() = 0;
+
+    // Called when a custom control is clicked on the Picture-in-Picture window.
+    // |control_id| is the identifier for its custom control. This is defined by
+    // the site that calls the web API.
+    virtual void OnPictureInPictureControlClicked(
+        const std::string& control_id) = 0;
   };
 
   // Returns true if the host frame is hidden or closed.
@@ -109,6 +118,38 @@
   // Notify that the muted status of the media player has changed.
   virtual void DidPlayerMutedStatusChange(int delegate_id, bool muted) = 0;
 
+  // Notify that the source media player has entered Picture-in-Picture mode.
+  virtual void DidPictureInPictureModeStart(
+      int delegate_id,
+      const viz::SurfaceId&,
+      const gfx::Size&,
+      blink::WebMediaPlayer::PipWindowOpenedCallback,
+      bool show_play_pause_button) = 0;
+
+  // Notify that the source media player has exited Picture-in-Picture mode.
+  virtual void DidPictureInPictureModeEnd(int delegate_id,
+                                          base::OnceClosure) = 0;
+
+  // Notify that custom controls have been sent to be assigned to the
+  // Picture-in-Picture window.
+  virtual void DidSetPictureInPictureCustomControls(
+      int delegate_id,
+      const std::vector<blink::PictureInPictureControlInfo>&) = 0;
+
+  // Notify that the media player in Picture-in-Picture had a change of surface.
+  virtual void DidPictureInPictureSurfaceChange(
+      int delegate_id,
+      const viz::SurfaceId&,
+      const gfx::Size&,
+      bool show_play_pause_button) = 0;
+
+  // Registers a callback associated with a player that will be called when
+  // receiving a notification from the browser process that the
+  // Picture-in-Picture associated to this player has been resized.
+  virtual void RegisterPictureInPictureWindowResizeCallback(
+      int player_id,
+      blink::WebMediaPlayer::PipWindowResizedCallback) = 0;
+
   // Notify that playback is stopped. This will drop wake locks and remove any
   // external controls.
   //
--- a/media/blink/webmediaplayer_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/webmediaplayer_impl.cc	2019-05-17 18:53:34.124000000 +0300
@@ -57,6 +57,7 @@
 #include "media/filters/memory_data_source.h"
 #include "media/media_buildflags.h"
 #include "net/base/data_url.h"
+#include "third_party/blink/public/common/picture_in_picture/picture_in_picture_control_info.h"
 #include "third_party/blink/public/platform/web_encrypted_media_types.h"
 #include "third_party/blink/public/platform/web_localized_string.h"
 #include "third_party/blink/public/platform/web_media_player_client.h"
@@ -127,6 +128,10 @@
   return base::FeatureList::IsEnabled(kBackgroundVideoPauseOptimization);
 }
 
+bool IsNewRemotePlaybackPipelineEnabled() {
+  return base::FeatureList::IsEnabled(kNewRemotePlaybackPipeline);
+}
+
 bool IsNetworkStateError(blink::WebMediaPlayer::NetworkState state) {
   bool result = state == blink::WebMediaPlayer::kNetworkStateFormatError ||
                 state == blink::WebMediaPlayer::kNetworkStateNetworkError ||
@@ -320,6 +325,9 @@
       context_provider_(params->context_provider()),
       vfc_task_runner_(params->video_frame_compositor_task_runner()),
       compositor_(std::move(compositor)),
+#if defined(OS_ANDROID)  // WMPI_CAST
+      cast_impl_(this, client_, params->context_provider()),
+#endif
       renderer_factory_selector_(std::move(renderer_factory_selector)),
       observer_(params->media_observer()),
       enable_instant_source_buffer_gc_(
@@ -516,8 +524,11 @@
   // disabled.
   // The viz::SurfaceId may be updated when the video begins playback or when
   // the size of the video changes.
-  if (client_)
-    client_->OnPictureInPictureStateChange();
+  if (client_ && IsInPictureInPicture() && !client_->IsInAutoPIP()) {
+    delegate_->DidPictureInPictureSurfaceChange(
+        delegate_id_, surface_id, pipeline_metadata_.natural_size,
+        ShouldShowPlayPauseButtonInPictureInPictureWindow());
+  }
 }
 
 bool WebMediaPlayerImpl::SupportsOverlayFullscreenVideo() {
@@ -732,6 +743,10 @@
     mb_data_source_->Initialize(
         base::Bind(&WebMediaPlayerImpl::DataSourceInitialized, AsWeakPtr()));
   }
+
+#if defined(OS_ANDROID)  // WMPI_CAST
+  cast_impl_.Initialize(url, frame_, delegate_id_);
+#endif
 }
 
 void WebMediaPlayerImpl::Play() {
@@ -742,6 +757,12 @@
   if (blink::WebUserGestureIndicator::IsProcessingUserGesture(frame_))
     video_locked_when_paused_when_hidden_ = false;
 
+#if defined(OS_ANDROID)  // WMPI_CAST
+  if (IsRemote()) {
+    cast_impl_.play();
+    return;
+  }
+#endif
   // TODO(sandersd): Do we want to reset the idle timer here?
   delegate_->SetIdle(delegate_id_, false);
   paused_ = false;
@@ -787,6 +808,13 @@
   if (blink::WebUserGestureIndicator::IsProcessingUserGesture(frame_))
     video_locked_when_paused_when_hidden_ = true;
 
+#if defined(OS_ANDROID)  // WMPI_CAST
+  if (IsRemote()) {
+    cast_impl_.pause();
+    return;
+  }
+#endif
+
   pipeline_controller_.SetPlaybackRate(0.0);
   paused_time_ = pipeline_controller_.GetMediaTime();
 
@@ -807,8 +835,7 @@
 void WebMediaPlayerImpl::Seek(double seconds) {
   DVLOG(1) << __func__ << "(" << seconds << "s)";
   DCHECK(main_task_runner_->BelongsToCurrentThread());
-  media_log_->AddEvent(
-      media_log_->CreateTimeEvent(MediaLogEvent::SEEK, "seek_target", seconds));
+  media_log_->AddEvent(media_log_->CreateSeekEvent(seconds));
   DoSeek(base::TimeDelta::FromSecondsD(seconds), true);
 }
 
@@ -817,6 +844,13 @@
   TRACE_EVENT2("media", "WebMediaPlayerImpl::DoSeek", "target",
                time.InSecondsF(), "id", media_log_->id());
 
+#if defined(OS_ANDROID)  // WMPI_CAST
+  if (IsRemote()) {
+    cast_impl_.seek(time);
+    return;
+  }
+#endif
+
   ReadyState old_state = ready_state_;
   if (ready_state_ > WebMediaPlayer::kReadyStateHaveMetadata)
     SetReadyState(WebMediaPlayer::kReadyStateHaveMetadata);
@@ -901,12 +935,44 @@
   UpdatePlayState();
 }
 
-void WebMediaPlayerImpl::OnRequestPictureInPicture() {
+void WebMediaPlayerImpl::EnterPictureInPicture(
+    blink::WebMediaPlayer::PipWindowOpenedCallback callback) {
   if (!surface_layer_for_video_enabled_)
     ActivateSurfaceLayerForVideo();
 
   DCHECK(bridge_);
-  DCHECK(bridge_->GetSurfaceId().is_valid());
+
+  const viz::SurfaceId& surface_id = bridge_->GetSurfaceId();
+  DCHECK(surface_id.is_valid());
+
+  // Notifies the browser process that the player should now be in
+  // Picture-in-Picture mode.
+  delegate_->DidPictureInPictureModeStart(
+      delegate_id_, surface_id, pipeline_metadata_.natural_size,
+      std::move(callback), ShouldShowPlayPauseButtonInPictureInPictureWindow());
+}
+
+void WebMediaPlayerImpl::ExitPictureInPicture(
+    blink::WebMediaPlayer::PipWindowClosedCallback callback) {
+  // Notifies the browser process that Picture-in-Picture has ended. It will
+  // clear out the states and close the window.
+  delegate_->DidPictureInPictureModeEnd(delegate_id_, std::move(callback));
+
+  // Internal cleanups.
+  OnPictureInPictureModeEnded();
+}
+
+void WebMediaPlayerImpl::SetPictureInPictureCustomControls(
+    const std::vector<blink::PictureInPictureControlInfo>& controls) {
+  delegate_->DidSetPictureInPictureCustomControls(delegate_id_, controls);
+}
+
+void WebMediaPlayerImpl::RegisterPictureInPictureWindowResizeCallback(
+    blink::WebMediaPlayer::PipWindowResizedCallback callback) {
+  DCHECK(IsInPictureInPicture() && !client_->IsInAutoPIP());
+
+  delegate_->RegisterPictureInPictureWindowResizeCallback(delegate_id_,
+                                                          std::move(callback));
 }
 
 void WebMediaPlayerImpl::SetSinkId(
@@ -995,6 +1061,12 @@
 
 bool WebMediaPlayerImpl::Paused() const {
   DCHECK(main_task_runner_->BelongsToCurrentThread());
+
+#if defined(OS_ANDROID)  // WMPI_CAST
+  if (IsRemote())
+    return cast_impl_.IsPaused();
+#endif
+
   return pipeline_controller_.GetPlaybackRate() == 0.0f;
 }
 
@@ -1047,6 +1119,10 @@
   base::TimeDelta current_time;
   if (Seeking())
     current_time = seek_time_;
+#if defined(OS_ANDROID)  // WMPI_CAST
+  else if (IsRemote())
+    current_time = cast_impl_.currentTime();
+#endif
   else if (paused_)
     current_time = paused_time_;
   else
@@ -1472,7 +1548,15 @@
   seek_time_ = base::TimeDelta();
 
   if (paused_) {
+#if defined(OS_ANDROID)  // WMPI_CAST
+    if (IsRemote()) {
+      paused_time_ = cast_impl_.currentTime();
+    } else {
     paused_time_ = pipeline_controller_.GetMediaTime();
+    }
+#else
+    paused_time_ = pipeline_controller_.GetMediaTime();
+#endif
   } else {
     DCHECK(watch_time_reporter_);
     watch_time_reporter_->OnPlaying();
@@ -1530,6 +1614,14 @@
   // Add a log event so the player shows up as "SUSPENDED" in media-internals.
   media_log_->AddEvent(media_log_->CreateEvent(MediaLogEvent::SUSPENDED));
 
+#if defined(OS_ANDROID)
+  if (IsRemote() && !IsNewRemotePlaybackPipelineEnabled()) {
+    scoped_refptr<VideoFrame> frame = cast_impl_.GetCastingBanner();
+    if (frame)
+      compositor_->PaintSingleFrame(frame);
+  }
+#endif
+
   // Tell the data source we have enough data so that it may release the
   // connection.
   if (mb_data_source_)
@@ -1622,17 +1714,11 @@
   // URL, since MediaPlayer doesn't support data:// URLs, fail playback now.
   const bool found_hls = status == PipelineStatus::DEMUXER_ERROR_DETECTED_HLS;
   if (found_hls && mb_data_source_) {
-    demuxer_found_hls_ = true;
-
     UMA_HISTOGRAM_BOOLEAN("Media.WebMediaPlayerImpl.HLS.IsCorsCrossOrigin",
                           mb_data_source_->IsCorsCrossOrigin());
-    if (mb_data_source_->IsCorsCrossOrigin()) {
-      UMA_HISTOGRAM_BOOLEAN("Media.WebMediaPlayerImpl.HLS.HasAccessControl",
-                            mb_data_source_->HasAccessControl());
-    }
-
-    // Note: Does not consider the full redirect chain, which could contain
-    // undetected mixed content.
+    // Note: Does not consider the full redirect chain. Redirecting through
+    // another origin will set WouldTaintOrigin() though, assuming that the
+    // crossorigin attribute is not set.
     bool frame_url_is_cryptographic = url::Origin(frame_->GetSecurityOrigin())
                                           .GetURL()
                                           .SchemeIsCryptographic();
@@ -1642,6 +1728,10 @@
     UMA_HISTOGRAM_BOOLEAN(
         "Media.WebMediaPlayerImpl.HLS.IsMixedContent",
         frame_url_is_cryptographic && !manifest_url_is_cryptographic);
+    UMA_HISTOGRAM_BOOLEAN("Media.WebMediaPlayerImpl.HLS.WouldTaintOrigin",
+                          WouldTaintOrigin());
+    // Note: Affects WouldTaintOrigin().
+    demuxer_found_hls_ = true;
 
     renderer_factory_selector_->SetUseMediaPlayer(true);
 
@@ -2267,10 +2357,6 @@
   client_->RequestPause();
 }
 
-void WebMediaPlayerImpl::OnMuted(bool muted) {
-  client_->RequestMuted(muted);
-}
-
 void WebMediaPlayerImpl::OnSeekForward(double seconds) {
   DCHECK_GE(seconds, 0) << "Attempted to seek by a negative number of seconds";
   client_->RequestSeek(CurrentTime() + seconds);
@@ -2301,6 +2387,14 @@
   client_->PictureInPictureStopped();
 }
 
+void WebMediaPlayerImpl::OnPictureInPictureControlClicked(
+    const std::string& control_id) {
+  if (client_ && IsInPictureInPicture()) {
+    client_->PictureInPictureControlClicked(
+        blink::WebString::FromUTF8(control_id));
+  }
+}
+
 void WebMediaPlayerImpl::SendBytesReceivedUpdate() {
   media_metrics_provider_->AddBytesReceived(bytes_received_since_last_update_);
   bytes_received_since_last_update_ = 0;
@@ -2338,7 +2432,36 @@
     observer_->OnRemotePlaybackDisabled(disabled);
 }
 
-#if defined(OS_ANDROID)
+#if defined(OS_ANDROID)  // WMPI_CAST
+bool WebMediaPlayerImpl::IsRemote() const {
+  return cast_impl_.isRemote();
+}
+
+void WebMediaPlayerImpl::SetMediaPlayerManager(
+    RendererMediaPlayerManagerInterface* media_player_manager) {
+  cast_impl_.SetMediaPlayerManager(media_player_manager);
+}
+
+void WebMediaPlayerImpl::RequestRemotePlayback() {
+  cast_impl_.requestRemotePlayback();
+}
+
+void WebMediaPlayerImpl::RequestRemotePlaybackControl() {
+  cast_impl_.requestRemotePlaybackControl();
+}
+
+void WebMediaPlayerImpl::RequestRemotePlaybackStop() {
+  cast_impl_.requestRemotePlaybackStop();
+}
+
+void WebMediaPlayerImpl::OnRemotePlaybackEnded() {
+  DVLOG(1) << __func__;
+  DCHECK(main_task_runner_->BelongsToCurrentThread());
+
+  ended_ = true;
+  client_->TimeChanged();
+}
+
 void WebMediaPlayerImpl::FlingingStarted() {
   DCHECK(main_task_runner_->BelongsToCurrentThread());
   DCHECK(!disable_pipeline_auto_suspend_);
@@ -2365,17 +2488,65 @@
 
   ScheduleRestart();
 }
-#endif  // defined(OS_ANDROID)
+
+void WebMediaPlayerImpl::OnDisconnectedFromRemoteDevice(double t) {
+  DoSeek(base::TimeDelta::FromSecondsD(t), false);
+
+  // Capabilities reporting can resume now that playback is local.
+  CreateVideoDecodeStatsReporter();
+
+  // |client_| might destroy us in methods below.
+  UpdatePlayState();
+
+  // We already told the delegate we're paused when remoting started.
+  client_->RequestPause();
+  client_->DisconnectedFromRemoteDevice();
+}
+
+void WebMediaPlayerImpl::SuspendForRemote() {
+  // Capabilities reporting should only be performed for local playbacks.
+  video_decode_stats_reporter_.reset();
+
+  if (pipeline_controller_.IsPipelineSuspended() &&
+      !IsNewRemotePlaybackPipelineEnabled()) {
+    scoped_refptr<VideoFrame> frame = cast_impl_.GetCastingBanner();
+    if (frame)
+      compositor_->PaintSingleFrame(frame);
+  }
+
+  UpdatePlayState();
+}
+
+gfx::Size WebMediaPlayerImpl::GetCanvasSize() const {
+  if (!surface_layer_for_video_enabled_) {
+    if (!video_layer_)
+      return pipeline_metadata_.natural_size;
+
+    return video_layer_->bounds();
+  }
+  if (!bridge_->GetCcLayer())
+    return pipeline_metadata_.natural_size;
+
+  return bridge_->GetCcLayer()->bounds();
+}
+
+void WebMediaPlayerImpl::SetDeviceScaleFactor(float scale_factor) {
+  cast_impl_.SetDeviceScaleFactor(scale_factor);
+}
+#endif  // defined(OS_ANDROID)  // WMPI_CAST
 
 void WebMediaPlayerImpl::SetPoster(const blink::WebURL& poster) {
   has_poster_ = !poster.IsEmpty();
+#if defined(OS_ANDROID)  // WMPI_CAST
+  cast_impl_.setPoster(poster);
+#endif  // defined(OS_ANDROID)  // WMPI_CAST
 }
 
 void WebMediaPlayerImpl::DataSourceInitialized(bool success) {
   DVLOG(1) << __func__;
   DCHECK(main_task_runner_->BelongsToCurrentThread());
 
-  if (observer_ && mb_data_source_)
+  if (observer_ && IsNewRemotePlaybackPipelineEnabled() && mb_data_source_)
     observer_->OnDataSourceInitialized(mb_data_source_->GetUrlAfterRedirects());
 
   if (!success) {
@@ -2523,8 +2694,11 @@
   if (demuxer_found_hls_ ||
       renderer_factory_selector_->GetCurrentFactory()
               ->GetRequiredMediaResourceType() == MediaResource::Type::URL) {
-    // MediaPlayerRendererClientFactory is the only factory that a uses
-    // MediaResource::Type::URL for the moment.
+    // MediaPlayerRendererClient factory is the only factory that a
+    // MediaResource::Type::URL for the moment. This might no longer be true
+    // when we remove WebMediaPlayerCast.
+    //
+    // TODO(tguilbert/avayvod): Update this flag when removing |cast_impl_|.
     using_media_player_renderer_ = true;
 
     // MediaPlayerRenderer does not provide pipeline stats, so nuke capabilities
@@ -2655,6 +2829,12 @@
 
 void WebMediaPlayerImpl::UpdatePlayState() {
   DCHECK(main_task_runner_->BelongsToCurrentThread());
+
+#if defined(OS_ANDROID)  // WMPI_CAST
+  bool is_remote = IsRemote();
+  bool can_auto_suspend = true;
+#else
+  bool is_remote = false;
   bool can_auto_suspend = !disable_pipeline_auto_suspend_;
   // For streaming videos, we only allow suspending at the very beginning of the
   // video, and only if we know the length of the video. (If we don't know
@@ -2667,11 +2847,12 @@
     if (!at_beginning || GetPipelineMediaDuration() == kInfiniteDuration)
       can_auto_suspend = false;
   }
+#endif
 
   bool is_suspended = pipeline_controller_.IsSuspended();
   bool is_backgrounded = IsBackgroundSuspendEnabled(this) && IsHidden();
   PlayState state = UpdatePlayState_ComputePlayState(
-      is_flinging_, can_auto_suspend, is_suspended, is_backgrounded);
+      is_remote, is_flinging_, can_auto_suspend, is_suspended, is_backgrounded);
   SetDelegateState(state.delegate_state, state.is_idle);
   SetMemoryReportingState(state.is_memory_reporting_enabled);
   SetSuspendState(state.is_suspended || pending_suspend_resume_cycle_);
@@ -2766,8 +2947,18 @@
   }
 }
 
+// NOTE: |is_remote| and |is_flinging| both indicate that we are in a remote
+// playback session, with the following differences:
+//   - |is_remote| : we are using |cast_impl_|, and most of WMPI's functions
+//     are forwarded to it. This method of remote playback is scheduled
+//     for deprecation soon, in favor of the |is_flinging| path.
+//   - |is_flinging| : we are using the FlingingRenderer, and WMPI should
+//     behave exactly if we are using the DefaultRenderer, except for the
+//     disabling of certain optimizations.
+// See https://crbug.com/790766.
 WebMediaPlayerImpl::PlayState
-WebMediaPlayerImpl::UpdatePlayState_ComputePlayState(bool is_flinging,
+WebMediaPlayerImpl::UpdatePlayState_ComputePlayState(bool is_remote,
+                                                     bool is_flinging,
                                                      bool can_auto_suspend,
                                                      bool is_suspended,
                                                      bool is_backgrounded) {
@@ -2811,10 +3002,11 @@
                             paused_ && !seeking_ && !needs_first_frame_;
 
   // Combined suspend state.
-  result.is_suspended = must_suspend || idle_suspended ||
+  result.is_suspended = is_remote || must_suspend || idle_suspended ||
                         background_suspended || can_stay_suspended;
 
-  DVLOG(3) << __func__ << ": must_suspend=" << must_suspend
+  DVLOG(3) << __func__ << ": is_remote=" << is_remote
+           << ", must_suspend=" << must_suspend
            << ", idle_suspended=" << idle_suspended
            << ", background_suspended=" << background_suspended
            << ", can_stay_suspended=" << can_stay_suspended
@@ -2845,7 +3037,8 @@
   //   - |have_future_data|, since we need to know whether we are paused to
   //     correctly configure the session and also because the tracks and
   //     duration are passed to DidPlay(),
-  //   - |is_flinging| is false (RemotePlayback is not handled by the delegate)
+  //   - |is_remote| and |is_flinging| are false as remote playback is not
+  //     handled by the delegate,
   //   - |has_error| is false as player should have no errors,
   //   - |background_suspended| is false, otherwise |has_remote_controls| must
   //     be true.
@@ -2862,7 +3055,8 @@
   bool can_play = !has_error && have_future_data;
   bool has_remote_controls =
       HasAudio() && !backgrounded_video_has_no_remote_controls;
-  bool alive = can_play && !is_flinging && !must_suspend &&
+  bool in_remote_playback = is_remote || is_flinging;
+  bool alive = can_play && !in_remote_playback && !must_suspend &&
                (!background_suspended || has_remote_controls);
   if (!alive) {
     // Do not mark players as idle when flinging.
@@ -2881,8 +3075,8 @@
 
   // It's not critical if some cases where memory usage can change are missed,
   // since media memory changes are usually gradual.
-  result.is_memory_reporting_enabled = !has_error && can_play && !is_flinging &&
-                                       !result.is_suspended &&
+  result.is_memory_reporting_enabled =
+      !has_error && can_play && !in_remote_playback && !result.is_suspended &&
                                        (!paused_ || seeking_);
 
   return result;
@@ -2973,8 +3167,8 @@
   }
 
 #if defined(OS_ANDROID)
-  // Don't pause videos casted as part of RemotePlayback.
-  if (is_flinging_)
+  // Remote players will be suspended and locally paused.
+  if (IsRemote())
     return;
 #endif
 
@@ -3096,16 +3290,6 @@
   return opaque_;
 }
 
-int WebMediaPlayerImpl::GetDelegateId() {
-  return delegate_id_;
-}
-
-base::Optional<viz::SurfaceId> WebMediaPlayerImpl::GetSurfaceId() {
-  if (!surface_layer_for_video_enabled_)
-    return base::nullopt;
-  return bridge_->GetSurfaceId();
-}
-
 bool WebMediaPlayerImpl::ShouldPauseVideoWhenHidden() const {
   if (!is_background_video_playback_enabled_)
     return true;
@@ -3117,7 +3301,7 @@
       return false;
 
 #if defined(OS_ANDROID)
-    if (is_flinging_)
+    if (IsRemote() || is_flinging_)
       return false;
 #endif
 
@@ -3152,11 +3336,11 @@
   if (IsInPictureInPicture())
     return false;
 
-#if defined(OS_ANDROID)
-  // Don't optimize videos casted as part of RemotePlayback.
-  if (is_flinging_)
+#if defined(OS_ANDROID)  // WMPI_CAST
+  // Don't optimize players being Cast.
+  if (IsRemote())
     return false;
-#endif
+#endif  // defined(OS_ANDROID)
 
   // Don't optimize audio-only or streaming players.
   if (!HasVideo() || IsStreaming())
@@ -3410,6 +3594,11 @@
          WebMediaPlayer::DisplayType::kPictureInPicture;
 }
 
+bool WebMediaPlayerImpl::ShouldShowPlayPauseButtonInPictureInPictureWindow()
+    const {
+  return Duration() != std::numeric_limits<double>::infinity();
+}
+
 void WebMediaPlayerImpl::MaybeSetContainerName() {
   // MSE nor MediaPlayerRenderer provide container information.
   if (chunk_demuxer_ || using_media_player_renderer_)
--- a/media/blink/webmediaplayer_impl.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/webmediaplayer_impl.h	2019-05-17 18:53:34.124000000 +0300
@@ -47,6 +47,11 @@
 #include "third_party/blink/public/platform/web_surface_layer_bridge.h"
 #include "url/gurl.h"
 
+#if defined(OS_ANDROID)  // WMPI_CAST
+// Delete this file when WMPI_CAST is no longer needed.
+#include "media/blink/webmediaplayer_cast_android.h"
+#endif
+
 namespace blink {
 class WebLocalFrame;
 class WebMediaPlayerClient;
@@ -120,7 +125,14 @@
   void Seek(double seconds) override;
   void SetRate(double rate) override;
   void SetVolume(double volume) override;
-  void OnRequestPictureInPicture() override;
+  void EnterPictureInPicture(
+      blink::WebMediaPlayer::PipWindowOpenedCallback callback) override;
+  void ExitPictureInPicture(
+      blink::WebMediaPlayer::PipWindowClosedCallback callback) override;
+  void SetPictureInPictureCustomControls(
+      const std::vector<blink::PictureInPictureControlInfo>&) override;
+  void RegisterPictureInPictureWindowResizeCallback(
+      blink::WebMediaPlayer::PipWindowResizedCallback callback) override;
   void SetSinkId(
       const blink::WebString& sink_id,
       std::unique_ptr<blink::WebSetSinkIdCallbacks> web_callback) override;
@@ -222,25 +234,39 @@
   void OnIdleTimeout() override;
   void OnPlay() override;
   void OnPause() override;
-  void OnMuted(bool muted) override;
   void OnSeekForward(double seconds) override;
   void OnSeekBackward(double seconds) override;
   void OnVolumeMultiplierUpdate(double multiplier) override;
   void OnBecamePersistentVideo(bool value) override;
   void OnPictureInPictureModeEnded() override;
+  void OnPictureInPictureControlClicked(const std::string& control_id) override;
 
   // Callback for when bytes are received by |chunk_demuxer_| or the UrlData
   // being loaded.
   void OnBytesReceived(uint64_t data_length);
 
   void RequestRemotePlaybackDisabled(bool disabled) override;
-
-#if defined(OS_ANDROID)
+#if defined(OS_ANDROID)  // WMPI_CAST
   // TODO(https://crbug.com/839651): Rename Flinging[Started/Stopped] to
   // RemotePlayback[Started/Stopped] once the other RemotePlayback methods have
   // been removed
+  bool IsRemote() const override;
+  void RequestRemotePlayback() override;
+  void RequestRemotePlaybackControl() override;
+  void RequestRemotePlaybackStop() override;
   void FlingingStarted() override;
   void FlingingStopped() override;
+
+  void SetMediaPlayerManager(
+      RendererMediaPlayerManagerInterface* media_player_manager);
+  void OnRemotePlaybackEnded();
+  void OnDisconnectedFromRemoteDevice(double t);
+  void SuspendForRemote();
+  void DisplayCastFrameAfterSuspend(const scoped_refptr<VideoFrame>& new_frame,
+                                    PipelineStatus status);
+  gfx::Size GetCanvasSize() const;
+  void SetDeviceScaleFactor(float scale_factor);
+  void SetUseFallbackPath(bool use_fallback_path);
 #endif
 
   // MediaObserverClient implementation.
@@ -257,16 +283,19 @@
   // paused; see UpdatePlayState_ComputePlayState() for the exact details.
   void ForceStaleStateForTesting(ReadyState target_state) override;
   bool IsSuspendedForTesting() override;
+
   bool DidLazyLoad() const override;
   void OnBecameVisible() override;
   bool IsOpaque() const override;
-  int GetDelegateId() override;
-  base::Optional<viz::SurfaceId> GetSurfaceId() override;
 
   bool IsBackgroundMediaSuspendEnabled() const {
     return is_background_suspend_enabled_;
   }
 
+  // Called from WebMediaPlayerCast.
+  // TODO(hubbe): WMPI_CAST make private.
+  void OnPipelineSeeked(bool time_updated);
+
   // Distinct states that |delegate_| can be in. (Public for testing.)
   enum class DelegateState {
     GONE,
@@ -308,7 +337,6 @@
   void OnPipelineSuspended();
   void OnBeforePipelineResume();
   void OnPipelineResumed();
-  void OnPipelineSeeked(bool time_updated);
   void OnDemuxerOpened();
 
   // Pipeline::Client overrides.
@@ -396,7 +424,7 @@
   //
   // This method should be called any time its dependent values change. These
   // are:
-  //   - is_flinging_,
+  //   - isRemote(), is_flinging_,
   //   - hasVideo(),
   //   - delegate_->IsHidden(),
   //   - network_state_, ready_state_,
@@ -407,7 +435,8 @@
   void UpdatePlayState();
 
   // Methods internal to UpdatePlayState().
-  PlayState UpdatePlayState_ComputePlayState(bool is_flinging,
+  PlayState UpdatePlayState_ComputePlayState(bool is_remote,
+                                             bool is_flinging,
                                              bool can_auto_suspend,
                                              bool is_suspended,
                                              bool is_backgrounded);
@@ -572,6 +601,11 @@
 
   void SendBytesReceivedUpdate();
 
+  // Returns whether the Picture-in-Picture window should contain a play/pause
+  // button. It will return false if video is "live", in other words if duration
+  // is equals to Infinity.
+  bool ShouldShowPlayPauseButtonInPictureInPictureWindow() const;
+
   blink::WebLocalFrame* const frame_;
 
   // The playback state last reported to |delegate_|, to avoid setting duplicate
@@ -737,6 +771,10 @@
   // IsRemote().
   bool is_flinging_ = false;
 
+#if defined(OS_ANDROID)  // WMPI_CAST
+  WebMediaPlayerCast cast_impl_;
+#endif
+
   // The last volume received by setVolume() and the last volume multiplier from
   // OnVolumeMultiplierUpdate().  The multiplier is typical 1.0, but may be less
   // if the WebMediaPlayerDelegate has requested a volume reduction (ducking)
@@ -775,9 +813,11 @@
   GURL loaded_url_;
 
   // NOTE: |using_media_player_renderer_| is set based on the usage of a
-  // MediaResource::Type::URL in StartPipeline(). This works because
-  // MediaPlayerRendererClientFactory is the only factory that uses
-  // MediaResource::Type::URL for now.
+  // MediaResource::Type::URL in StartPipeline(). This currently works because
+  // the MediaPlayerRendererClient factory is the only factory that returns that
+  // Type, but this may no longer be accurate when we remove |cast_impl_| and
+  // WebMediaPlayerCast. This flag should be renamed/updated accordingly when
+  // removing |cast_impl_|.
   bool using_media_player_renderer_ = false;
 
   // Set whenever the demuxer encounters an HLS file.
--- a/media/blink/webmediaplayer_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/webmediaplayer_impl_unittest.cc	2019-05-17 18:53:34.124000000 +0300
@@ -9,7 +9,6 @@
 #include <memory>
 
 #include "base/bind.h"
-#include "base/bind_helpers.h"
 #include "base/callback_helpers.h"
 #include "base/command_line.h"
 #include "base/memory/ref_counted.h"
@@ -46,6 +45,7 @@
 #include "mojo/public/cpp/bindings/strong_binding.h"
 #include "testing/gmock/include/gmock/gmock.h"
 #include "testing/gtest/include/gtest/gtest.h"
+#include "third_party/blink/public/common/picture_in_picture/picture_in_picture_control_info.h"
 #include "third_party/blink/public/mojom/frame/document_interface_broker.mojom.h"
 #include "third_party/blink/public/platform/web_fullscreen_video_status.h"
 #include "third_party/blink/public/platform/web_media_player.h"
@@ -63,6 +63,10 @@
 #include "third_party/blink/public/web/web_widget.h"
 #include "url/gurl.h"
 
+#if defined(OS_ANDROID)
+#include "media/blink/renderer_media_player_interface.h"
+#endif
+
 using ::testing::_;
 using ::testing::AnyNumber;
 using ::testing::Eq;
@@ -97,6 +101,33 @@
   return mojo::MakeRequest(&info).PassMessagePipe();
 }
 
+#if defined(OS_ANDROID)
+class MockRendererMediaPlayerManager
+    : public RendererMediaPlayerManagerInterface {
+ public:
+  MOCK_METHOD7(Initialize,
+               void(MediaPlayerHostMsg_Initialize_Type type,
+                    int player_id,
+                    const GURL& url,
+                    const GURL& site_for_cookies,
+                    const GURL& frame_url,
+                    bool allow_credentials,
+                    int delegate_id));
+  MOCK_METHOD1(Start, void(int player_id));
+  MOCK_METHOD2(Pause, void(int player_id, bool is_media_related_action));
+  MOCK_METHOD2(Seek, void(int player_id, base::TimeDelta time));
+  MOCK_METHOD2(SetVolume, void(int player_id, double volume));
+  MOCK_METHOD2(SetPoster, void(int player_id, const GURL& poster));
+  MOCK_METHOD1(SuspendAndReleaseResources, void(int player_id));
+  MOCK_METHOD1(DestroyPlayer, void(int player_id));
+  MOCK_METHOD1(RequestRemotePlayback, void(int player_id));
+  MOCK_METHOD1(RequestRemotePlaybackControl, void(int player_id));
+  MOCK_METHOD1(RequestRemotePlaybackStop, void(int player_id));
+  MOCK_METHOD1(RegisterMediaPlayer, int(RendererMediaPlayerInterface* player));
+  MOCK_METHOD1(UnregisterMediaPlayer, void(int player_id));
+};
+#endif
+
 class MockWebMediaPlayerClient : public blink::WebMediaPlayerClient {
  public:
   MockWebMediaPlayerClient() = default;
@@ -128,6 +159,12 @@
   MOCK_METHOD1(RemoveTextTrack, void(blink::WebInbandTextTrack*));
   MOCK_METHOD1(MediaSourceOpened, void(blink::WebMediaSource*));
   MOCK_METHOD1(RequestSeek, void(double));
+  MOCK_METHOD1(RemoteRouteAvailabilityChanged,
+               void(blink::WebRemotePlaybackAvailability));
+  MOCK_METHOD0(ConnectedToRemoteDevice, void());
+  MOCK_METHOD0(DisconnectedFromRemoteDevice, void());
+  MOCK_METHOD0(CancelledRemotePlaybackRequest, void());
+  MOCK_METHOD0(RemotePlaybackStarted, void());
   MOCK_METHOD2(RemotePlaybackCompatibilityChanged,
                void(const blink::WebURL&, bool));
   MOCK_METHOD1(OnBecamePersistentVideo, void(bool));
@@ -141,12 +178,12 @@
   MOCK_METHOD1(ActivateViewportIntersectionMonitoring, void(bool));
   MOCK_METHOD1(MediaRemotingStarted, void(const blink::WebString&));
   MOCK_METHOD1(MediaRemotingStopped, void(blink::WebLocalizedString::Name));
+  MOCK_METHOD0(PictureInPictureStarted, void());
   MOCK_METHOD0(PictureInPictureStopped, void());
-  MOCK_METHOD0(OnPictureInPictureStateChange, void());
+  MOCK_METHOD1(PictureInPictureControlClicked, void(const blink::WebString&));
   MOCK_CONST_METHOD0(CouldPlayIfEnoughData, bool());
   MOCK_METHOD0(RequestPlay, void());
   MOCK_METHOD0(RequestPause, void());
-  MOCK_METHOD1(RequestMuted, void(bool));
 
   void set_was_always_muted(bool value) { was_always_muted_ = value; }
 
@@ -208,6 +245,22 @@
     DCHECK_EQ(player_id_, delegate_id);
   }
 
+  MOCK_METHOD5(DidPictureInPictureModeStart,
+               void(int,
+                    const viz::SurfaceId&,
+                    const gfx::Size&,
+                    blink::WebMediaPlayer::PipWindowOpenedCallback,
+                    bool));
+  MOCK_METHOD2(DidPictureInPictureModeEnd,
+               void(int, blink::WebMediaPlayer::PipWindowClosedCallback));
+  MOCK_METHOD2(DidSetPictureInPictureCustomControls,
+               void(int,
+                    const std::vector<blink::PictureInPictureControlInfo>&));
+  MOCK_METHOD4(DidPictureInPictureSurfaceChange,
+               void(int, const viz::SurfaceId&, const gfx::Size&, bool));
+  MOCK_METHOD2(RegisterPictureInPictureWindowResizeCallback,
+               void(int, blink::WebMediaPlayer::PipWindowResizedCallback));
+
   void ClearStaleFlag(int player_id) override {
     DCHECK_EQ(player_id_, player_id);
     is_stale_ = false;
@@ -372,6 +425,10 @@
         web_local_frame_, &client_, &encrypted_client_, &delegate_,
         std::move(factory_selector), url_index_.get(), std::move(compositor),
         std::move(params));
+
+#if defined(OS_ANDROID)
+    wmpi_->SetMediaPlayerManager(&mock_media_player_manager_);
+#endif
   }
 
   ~WebMediaPlayerImplTest() override {
@@ -465,31 +522,43 @@
   WebMediaPlayerImpl::PlayState ComputePlayState() {
     EXPECT_CALL(client_, WasAlwaysMuted())
         .WillRepeatedly(Return(client_.was_always_muted_));
-    return wmpi_->UpdatePlayState_ComputePlayState(false, true, false, false);
+    return wmpi_->UpdatePlayState_ComputePlayState(false, false, true, false,
+                                                   false);
   }
 
   WebMediaPlayerImpl::PlayState ComputePlayState_FrameHidden() {
     EXPECT_CALL(client_, WasAlwaysMuted())
         .WillRepeatedly(Return(client_.was_always_muted_));
-    return wmpi_->UpdatePlayState_ComputePlayState(false, true, false, true);
+    return wmpi_->UpdatePlayState_ComputePlayState(false, false, true, false,
+                                                   true);
   }
 
   WebMediaPlayerImpl::PlayState ComputePlayState_Suspended() {
     EXPECT_CALL(client_, WasAlwaysMuted())
         .WillRepeatedly(Return(client_.was_always_muted_));
-    return wmpi_->UpdatePlayState_ComputePlayState(false, true, true, false);
+    return wmpi_->UpdatePlayState_ComputePlayState(false, false, true, true,
+                                                   false);
+  }
+
+  WebMediaPlayerImpl::PlayState ComputePlayState_Remote() {
+    EXPECT_CALL(client_, WasAlwaysMuted())
+        .WillRepeatedly(Return(client_.was_always_muted_));
+    return wmpi_->UpdatePlayState_ComputePlayState(true, false, true, false,
+                                                   false);
   }
 
   WebMediaPlayerImpl::PlayState ComputePlayState_Flinging() {
     EXPECT_CALL(client_, WasAlwaysMuted())
         .WillRepeatedly(Return(client_.was_always_muted_));
-    return wmpi_->UpdatePlayState_ComputePlayState(true, true, false, false);
+    return wmpi_->UpdatePlayState_ComputePlayState(false, true, true, false,
+                                                   false);
   }
 
   WebMediaPlayerImpl::PlayState ComputePlayState_BackgroundedStreaming() {
     EXPECT_CALL(client_, WasAlwaysMuted())
         .WillRepeatedly(Return(client_.was_always_muted_));
-    return wmpi_->UpdatePlayState_ComputePlayState(false, false, false, true);
+    return wmpi_->UpdatePlayState_ComputePlayState(false, false, false, false,
+                                                   true);
   }
 
   bool IsSuspended() { return wmpi_->pipeline_controller_.IsSuspended(); }
@@ -594,7 +663,7 @@
         blink::WebString::FromUTF8("Content-Length"),
         blink::WebString::FromUTF8(base::NumberToString(data->data_size())));
     response.SetExpectedContentLength(data->data_size());
-    response.SetHttpStatusCode(200);
+    response.SetHTTPStatusCode(200);
     client->DidReceiveResponse(response);
 
     // Copy over the file data and indicate that's everything.
@@ -663,6 +732,10 @@
   NiceMock<MockWebMediaPlayerClient> client_;
   MockWebMediaPlayerEncryptedMediaClient encrypted_client_;
 
+#if defined(OS_ANDROID)
+  NiceMock<MockRendererMediaPlayerManager> mock_media_player_manager_;
+#endif
+
   viz::FrameSinkId frame_sink_id_ = viz::FrameSinkId(1, 1);
   viz::LocalSurfaceId local_surface_id_ =
       viz::LocalSurfaceId(11, base::UnguessableToken::Deserialize(0x111111, 0));
@@ -1178,6 +1251,19 @@
   EXPECT_FALSE(state.is_memory_reporting_enabled);
 }
 
+TEST_F(WebMediaPlayerImplTest, ComputePlayState_Remote) {
+  InitializeWebMediaPlayerImpl();
+  SetMetadata(true, true);
+  SetReadyState(blink::WebMediaPlayer::kReadyStateHaveFutureData);
+
+  // Remote media via wmpi_cast is always suspended.
+  // TODO(sandersd): Decide whether this should count as idle or not.
+  WebMediaPlayerImpl::PlayState state = ComputePlayState_Remote();
+  EXPECT_EQ(WebMediaPlayerImpl::DelegateState::GONE, state.delegate_state);
+  EXPECT_TRUE(state.is_suspended);
+  EXPECT_FALSE(state.is_memory_reporting_enabled);
+}
+
 TEST_F(WebMediaPlayerImplTest, ComputePlayState_Flinging) {
   InitializeWebMediaPlayerImpl();
   SetMetadata(true, true);
@@ -1543,12 +1629,57 @@
   }
 }
 
-// Tests that updating the surface id calls OnPictureInPictureStateChange.
-TEST_F(WebMediaPlayerImplTest, PictureInPictureStateChange) {
+// Tests delegate methods are called when Picture-in-Picture is triggered.
+TEST_F(WebMediaPlayerImplTest, PictureInPictureTriggerCallback) {
+  base::test::ScopedFeatureList feature_list;
+  feature_list.InitFromCommandLine(kUseSurfaceLayerForVideo.name, "");
+
+  InitializeWebMediaPlayerImpl();
+
+  EXPECT_CALL(*surface_layer_bridge_ptr_, CreateSurfaceLayer());
+  EXPECT_CALL(*surface_layer_bridge_ptr_, GetSurfaceId())
+      .WillRepeatedly(ReturnRef(surface_id_));
+  EXPECT_CALL(*surface_layer_bridge_ptr_, GetLocalSurfaceIdAllocationTime())
+      .WillRepeatedly(Return(base::TimeTicks()));
+  EXPECT_CALL(*compositor_, EnableSubmission(_, _, _, _));
+  EXPECT_CALL(*surface_layer_bridge_ptr_, SetContentsOpaque(false));
+
+  PipelineMetadata metadata;
+  metadata.has_video = true;
+  OnMetadata(metadata);
+
+  EXPECT_CALL(client_, DisplayType())
+      .WillRepeatedly(
+          Return(blink::WebMediaPlayer::DisplayType::kPictureInPicture));
+  EXPECT_CALL(delegate_,
+              DidPictureInPictureSurfaceChange(
+                  delegate_.player_id(), surface_id_, GetNaturalSize(), true))
+      .Times(2);
+
+  wmpi_->OnSurfaceIdUpdated(surface_id_);
+
+  EXPECT_CALL(delegate_,
+              DidPictureInPictureModeStart(delegate_.player_id(), surface_id_,
+                                           GetNaturalSize(), _, true));
+
+  wmpi_->EnterPictureInPicture(base::DoNothing());
+  wmpi_->OnSurfaceIdUpdated(surface_id_);
+
+  // Updating SurfaceId should NOT exit Picture-in-Picture.
+  EXPECT_CALL(delegate_, DidPictureInPictureModeEnd(delegate_.player_id(), _))
+      .Times(0);
+  EXPECT_CALL(*surface_layer_bridge_ptr_, ClearObserver());
+}
+
+// Tests delegate methods are called with the appropriate play/pause button
+// state when Picture-in-Picture is triggered and video duration is infinity.
+TEST_F(WebMediaPlayerImplTest,
+       PictureInPictureTriggerWithInfiniteDurationCallback) {
   base::test::ScopedFeatureList feature_list;
   feature_list.InitFromCommandLine(kUseSurfaceLayerForVideo.name, "");
 
   InitializeWebMediaPlayerImpl();
+  SetDuration(kInfiniteDuration);
 
   EXPECT_CALL(*surface_layer_bridge_ptr_, CreateSurfaceLayer());
   EXPECT_CALL(*surface_layer_bridge_ptr_, GetSurfaceId())
@@ -1565,10 +1696,23 @@
   EXPECT_CALL(client_, DisplayType())
       .WillRepeatedly(
           Return(blink::WebMediaPlayer::DisplayType::kPictureInPicture));
-  EXPECT_CALL(client_, OnPictureInPictureStateChange()).Times(1);
+  EXPECT_CALL(delegate_,
+              DidPictureInPictureSurfaceChange(
+                  delegate_.player_id(), surface_id_, GetNaturalSize(), false))
+      .Times(2);
+
+  wmpi_->OnSurfaceIdUpdated(surface_id_);
+
+  EXPECT_CALL(delegate_,
+              DidPictureInPictureModeStart(delegate_.player_id(), surface_id_,
+                                           GetNaturalSize(), _, false));
 
+  wmpi_->EnterPictureInPicture(base::DoNothing());
   wmpi_->OnSurfaceIdUpdated(surface_id_);
 
+  // Updating SurfaceId should NOT exit Picture-in-Picture.
+  EXPECT_CALL(delegate_, DidPictureInPictureModeEnd(delegate_.player_id(), _))
+      .Times(0);
   EXPECT_CALL(*surface_layer_bridge_ptr_, ClearObserver());
 }
 
@@ -1777,7 +1921,7 @@
   EXPECT_TRUE(IsDisableVideoTrackPending());
 }
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     BackgroundBehaviorTestInstances,
     WebMediaPlayerImplBackgroundBehaviorTest,
     ::testing::Combine(
--- a/media/blink/webmediaplayer_util.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/webmediaplayer_util.cc	2019-05-17 18:53:34.124000000 +0300
@@ -9,7 +9,6 @@
 #include <string>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/metrics/histogram_macros.h"
 #include "media/base/bind_to_current_loop.h"
 #include "media/base/media_log.h"
--- a/media/blink/websourcebuffer_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/blink/websourcebuffer_impl.cc	2019-05-17 18:53:34.124000000 +0300
@@ -226,7 +226,7 @@
     trackInfo.track_type = mediaTrackTypeToBlink(track->type());
     trackInfo.id = blink::WebString::FromUTF8(track->id());
     trackInfo.byte_stream_track_id = blink::WebString::FromUTF8(
-        base::NumberToString(track->bytestream_track_id()));
+        base::UintToString(track->bytestream_track_id()));
     trackInfo.kind = blink::WebString::FromUTF8(track->kind());
     trackInfo.label = blink::WebString::FromUTF8(track->label());
     trackInfo.language = blink::WebString::FromUTF8(track->language());
--- a/media/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/BUILD.gn	2019-05-17 18:53:34.016000000 +0300
@@ -22,8 +22,6 @@
     "ENABLE_CBCS_ENCRYPTION_SCHEME=$enable_cbcs_encryption_scheme",
     "ENABLE_CDM_HOST_VERIFICATION=$enable_cdm_host_verification",
     "ENABLE_CDM_STORAGE_ID=$enable_cdm_storage_id",
-    "ENABLE_DAV1D_DECODER=$enable_dav1d_decoder",
-    "ENABLE_AV1_DECODER=$enable_av1_decoder",
     "ENABLE_DOLBY_VISION_DEMUXING=$enable_dolby_vision_demuxing",
     "ENABLE_FFMPEG=$media_use_ffmpeg",
     "ENABLE_FFMPEG_VIDEO_DECODERS=$enable_ffmpeg_video_decoders",
@@ -36,6 +34,7 @@
     "ENABLE_MEDIA_REMOTING_RPC=$enable_media_remoting_rpc",
     "ENABLE_MPEG_H_AUDIO_DEMUXING=$enable_mpeg_h_audio_demuxing",
     "ENABLE_MSE_MPEG2TS_STREAM_PARSER=$enable_mse_mpeg2ts_stream_parser",
+    "ENABLE_RUNTIME_MEDIA_RENDERER_SELECTION=$enable_runtime_media_renderer_selection",
     "USE_PROPRIETARY_CODECS=$proprietary_codecs",
   ]
 }
--- a/media/capabilities/in_memory_video_decode_stats_db_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capabilities/in_memory_video_decode_stats_db_impl.cc	2019-05-17 18:53:34.124000000 +0300
@@ -7,7 +7,6 @@
 #include <memory>
 #include <tuple>
 
-#include "base/bind.h"
 #include "base/files/file_path.h"
 #include "base/format_macros.h"
 #include "base/logging.h"
--- a/media/capabilities/learning_helper.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capabilities/learning_helper.cc	2019-05-17 18:53:34.124000000 +0300
@@ -4,47 +4,24 @@
 
 #include "media/capabilities/learning_helper.h"
 
-#include "base/task/post_task.h"
-#include "media/learning/common/feature_library.h"
 #include "media/learning/common/learning_task.h"
 
 namespace media {
 
-using learning::FeatureLibrary;
-using learning::FeatureProviderFactoryCB;
 using learning::FeatureValue;
 using learning::LabelledExample;
-using learning::LearningSessionImpl;
 using learning::LearningTask;
-using learning::SequenceBoundFeatureProvider;
 using learning::TargetValue;
 
-// Dropped frame ratio, default features, regression tree.
-const char* const kDroppedFrameRatioBaseTreeTaskName =
-    "DroppedFrameRatioBaseTreeTask";
-// Dropped frame ratio, default+FeatureLibrary features, regression tree.
-const char* const kDroppedFrameRatioEnhancedTreeTaskName =
-    "DroppedFrameRatioEnhancedTreeTask";
-// Dropped frame ratio, default features, lookup table.
-const char* const kDroppedFrameRatioBaseTableTaskName =
-    "DroppedFrameRatioBaseTableTask";
-
-LearningHelper::LearningHelper(FeatureProviderFactoryCB feature_factory) {
-  // Create the LearningSession on a background task runner.  In the future,
-  // it's likely that the session will live on the main thread, and handle
-  // delegation of LearningTaskControllers to other threads.  However, for now,
-  // do it here.
-  learning_session_ = base::SequenceBound<LearningSessionImpl>(
-      base::CreateSequencedTaskRunnerWithTraits(
-          {base::TaskPriority::BEST_EFFORT,
-           base::TaskShutdownBehavior::SKIP_ON_SHUTDOWN}));
+const char* kDroppedFrameRatioTaskName = "DroppedFrameRatioTask";
 
+LearningHelper::LearningHelper() {
   // Register a few learning tasks.
   //
   // We only do this here since we own the session.  Normally, whatever creates
   // the session would register all the learning tasks.
   LearningTask dropped_frame_task(
-      kDroppedFrameRatioBaseTableTaskName, LearningTask::Model::kLookupTable,
+      kDroppedFrameRatioTaskName, LearningTask::Model::kExtraTrees,
       {
           {"codec_profile",
            ::media::learning::LearningTask::Ordering::kUnordered},
@@ -54,34 +31,10 @@
       },
       LearningTask::ValueDescription(
           {"dropped_ratio", LearningTask::Ordering::kNumeric}));
-
   // Enable hacky reporting of accuracy.
   dropped_frame_task.uma_hacky_confusion_matrix =
       "Media.Learning.MediaCapabilities.DroppedFrameRatioTask.BaseTree";
-  learning_session_.Post(FROM_HERE, &LearningSessionImpl::RegisterTask,
-                         dropped_frame_task, SequenceBoundFeatureProvider());
-
-  // Modify the task to use ExtraTrees.
-  dropped_frame_task.name = kDroppedFrameRatioBaseTreeTaskName;
-  dropped_frame_task.model = LearningTask::Model::kExtraTrees;
-  dropped_frame_task.uma_hacky_confusion_matrix =
-      "Media.Learning.MediaCapabilities.DroppedFrameRatioTask.BaseTable";
-  learning_session_.Post(FROM_HERE, &LearningSessionImpl::RegisterTask,
-                         dropped_frame_task, SequenceBoundFeatureProvider());
-
-  // Add common features, if we have a factory.
-  if (feature_factory) {
-    dropped_frame_task.name = kDroppedFrameRatioEnhancedTreeTaskName;
-    dropped_frame_task.feature_descriptions.push_back(
-        FeatureLibrary::NetworkType());
-    dropped_frame_task.feature_descriptions.push_back(
-        FeatureLibrary::BatteryPower());
-    dropped_frame_task.uma_hacky_confusion_matrix =
-        "Media.Learning.MediaCapabilities.DroppedFrameRatioTask.EnhancedTree";
-    learning_session_.Post(FROM_HERE, &LearningSessionImpl::RegisterTask,
-                           dropped_frame_task,
-                           feature_factory.Run(dropped_frame_task));
-  }
+  learning_session_.RegisterTask(dropped_frame_task);
 }
 
 LearningHelper::~LearningHelper() = default;
@@ -119,15 +72,7 @@
   example.target_value = TargetValue(
       static_cast<double>(new_stats.frames_dropped) / new_stats.frames_decoded);
   example.weight = new_stats.frames_decoded;
-
-  // Add this example to both tasks.
-  learning_session_.Post(FROM_HERE, &LearningSessionImpl::AddExample,
-                         kDroppedFrameRatioBaseTreeTaskName, example);
-  learning_session_.Post(FROM_HERE, &LearningSessionImpl::AddExample,
-                         kDroppedFrameRatioBaseTableTaskName, example);
-  // Might fail, but that's okay.
-  learning_session_.Post(FROM_HERE, &LearningSessionImpl::AddExample,
-                         kDroppedFrameRatioEnhancedTreeTaskName, example);
+  learning_session_.AddExample(kDroppedFrameRatioTaskName, example);
 }
 
 }  // namespace media
--- a/media/capabilities/learning_helper.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capabilities/learning_helper.h	2019-05-17 18:53:34.124000000 +0300
@@ -6,10 +6,8 @@
 #define MEDIA_CAPABILITIES_LEARNING_HELPER_H_
 
 #include "base/macros.h"
-#include "base/threading/sequence_bound.h"
 #include "media/base/media_export.h"
 #include "media/capabilities/video_decode_stats_db.h"
-#include "media/learning/impl/feature_provider.h"
 #include "media/learning/impl/learning_session_impl.h"
 
 namespace media {
@@ -18,9 +16,7 @@
 // media::learning LearningTask.
 class MEDIA_EXPORT LearningHelper {
  public:
-  // |feature_factory| lets us register FeatureProviders with those
-  // LearningTasks that include standard features.
-  LearningHelper(learning::FeatureProviderFactoryCB feature_factory);
+  LearningHelper();
   ~LearningHelper();
 
   void AppendStats(const VideoDecodeStatsDB::VideoDescKey& video_key,
@@ -31,7 +27,7 @@
   // directly, but would instead get one that's connected to a browser profile.
   // For now, however, we just instantiate one and assume that we'll be
   // destroyed when the profile changes / history is cleared.
-  base::SequenceBound<learning::LearningSessionImpl> learning_session_;
+  learning::LearningSessionImpl learning_session_;
 };
 
 }  // namespace media
--- a/media/capabilities/video_decode_stats_db_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capabilities/video_decode_stats_db_impl.cc	2019-05-17 18:53:34.124000000 +0300
@@ -7,7 +7,6 @@
 #include <memory>
 #include <tuple>
 
-#include "base/bind.h"
 #include "base/files/file_path.h"
 #include "base/logging.h"
 #include "base/memory/ptr_util.h"
@@ -34,7 +33,7 @@
 
 const int kMaxDaysToKeepStatsDefault = 30;
 
-}  // namespace
+};  // namespace
 
 const char VideoDecodeStatsDBImpl::kMaxFramesPerBufferParamName[] =
     "db_frames_buffer_size";
--- a/media/capture/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/BUILD.gn	2019-05-17 18:53:34.128000000 +0300
@@ -256,18 +256,10 @@
       "video/chromeos/camera_hal_dispatcher_impl.h",
       "video/chromeos/camera_metadata_utils.cc",
       "video/chromeos/camera_metadata_utils.h",
-      "video/chromeos/cros_image_capture_impl.cc",
-      "video/chromeos/cros_image_capture_impl.h",
       "video/chromeos/display_rotation_observer.cc",
       "video/chromeos/display_rotation_observer.h",
       "video/chromeos/pixel_format_utils.cc",
       "video/chromeos/pixel_format_utils.h",
-      "video/chromeos/reprocess_manager.cc",
-      "video/chromeos/reprocess_manager.h",
-      "video/chromeos/request_builder.cc",
-      "video/chromeos/request_builder.h",
-      "video/chromeos/request_manager.cc",
-      "video/chromeos/request_manager.h",
       "video/chromeos/stream_buffer_manager.cc",
       "video/chromeos/stream_buffer_manager.h",
       "video/chromeos/video_capture_device_chromeos_halv3.cc",
@@ -399,7 +391,7 @@
       "video/chromeos/mock_camera_module.h",
       "video/chromeos/mock_video_capture_client.cc",
       "video/chromeos/mock_video_capture_client.h",
-      "video/chromeos/request_manager_unittest.cc",
+      "video/chromeos/stream_buffer_manager_unittest.cc",
     ]
     deps += [
       "//build/config/linux/libdrm",
--- a/media/capture/content/android/java/borg/chromium/media/ScreenCapture.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/content/android/java/borg/chromium/media/ScreenCapture.java	2019-05-17 18:53:34.128000000 +0300
@@ -22,7 +22,6 @@
 import android.os.Build;
 import android.os.Handler;
 import android.os.HandlerThread;
-import android.support.annotation.IntDef;
 import android.util.DisplayMetrics;
 import android.view.Display;
 import android.view.Surface;
@@ -34,8 +33,6 @@
 import org.chromium.base.annotations.CalledByNative;
 import org.chromium.base.annotations.JNINamespace;
 
-import java.lang.annotation.Retention;
-import java.lang.annotation.RetentionPolicy;
 import java.nio.ByteBuffer;
 
 /**
@@ -50,29 +47,13 @@
 
     private static final int REQUEST_MEDIA_PROJECTION = 1;
 
-    @IntDef({CaptureState.ATTACHED, CaptureState.ALLOWED, CaptureState.STARTED,
-            CaptureState.STOPPING, CaptureState.STOPPED})
-    @Retention(RetentionPolicy.SOURCE)
-    private @interface CaptureState {
-        int ATTACHED = 0;
-        int ALLOWED = 1;
-        int STARTED = 2;
-        int STOPPING = 3;
-        int STOPPED = 4;
-    }
-
-    @IntDef({DeviceOrientation.PORTRAIT, DeviceOrientation.LANDSCAPE})
-    @Retention(RetentionPolicy.SOURCE)
-    private @interface DeviceOrientation {
-        int PORTRAIT = 0;
-        int LANDSCAPE = 1;
-    }
-
     // Native callback context variable.
     private final long mNativeScreenCaptureMachineAndroid;
 
+    private static enum CaptureState { ATTACHED, ALLOWED, STARTED, STOPPING, STOPPED }
+    private static enum DeviceOrientation { PORTRAIT, LANDSCAPE }
     private final Object mCaptureStateLock = new Object();
-    private @CaptureState int mCaptureState = CaptureState.STOPPED;
+    private CaptureState mCaptureState = CaptureState.STOPPED;
 
     private MediaProjection mMediaProjection;
     private MediaProjectionManager mMediaProjectionManager;
@@ -82,7 +63,7 @@
     private HandlerThread mThread;
     private Handler mBackgroundHandler;
     private Display mDisplay;
-    private @DeviceOrientation int mCurrentOrientation;
+    private DeviceOrientation mCurrentOrientation;
     private Intent mResultData;
 
     private int mScreenDensity;
@@ -374,7 +355,7 @@
                 null);
     }
 
-    private void changeCaptureStateAndNotify(@CaptureState int state) {
+    private void changeCaptureStateAndNotify(CaptureState state) {
         synchronized (mCaptureStateLock) {
             mCaptureState = state;
             mCaptureStateLock.notifyAll();
@@ -398,7 +379,7 @@
         }
     }
 
-    private @DeviceOrientation int getDeviceOrientation(int rotation) {
+    private DeviceOrientation getDeviceOrientation(int rotation) {
         switch (rotation) {
             case 0:
             case 180:
@@ -415,7 +396,7 @@
 
     private boolean maybeDoRotation() {
         final int rotation = getDeviceRotation();
-        final @DeviceOrientation int orientation = getDeviceOrientation(rotation);
+        final DeviceOrientation orientation = getDeviceOrientation(rotation);
         if (orientation == mCurrentOrientation) {
             return false;
         }
@@ -426,7 +407,7 @@
         return true;
     }
 
-    private void rotateCaptureOrientation(@DeviceOrientation int orientation) {
+    private void rotateCaptureOrientation(DeviceOrientation orientation) {
         if ((orientation == DeviceOrientation.LANDSCAPE && mWidth < mHeight)
                 || (orientation == DeviceOrientation.PORTRAIT && mHeight < mWidth)) {
             mWidth += mHeight - (mHeight = mWidth);
--- a/media/capture/content/animated_content_sampler_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/content/animated_content_sampler_unittest.cc	2019-05-17 18:53:34.128000000 +0300
@@ -702,7 +702,7 @@
               max_acceptable_error.InMicroseconds());
 }
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     ,
     AnimatedContentSamplerParameterizedTest,
     ::testing::Values(
--- a/media/capture/content/smooth_event_sampler_unittest.cc	2019-05-17 17:45:41.252000000 +0300
+++ b/media/capture/content/smooth_event_sampler_unittest.cc	2019-05-17 18:53:34.128000000 +0300
@@ -485,7 +485,7 @@
 
 TEST(SmoothEventSamplerTest, DrawingAt60FpsWith60HzVsyncSampledAt30Hertz) {
   // Actual capturing of timing data: WebGL Acquarium demo
-  // (http://webglsamples.9oo91ecode.qjz9zk/hg/aquarium/aquarium.html) which ran
+  // (http://webglsamples.googlecode.com/hg/aquarium/aquarium.html) which ran
   // between 55-60 FPS in the steady-state.
   static const DataPoint data_points[] = {{true, 16.72},
                                           {true, 16.72},
--- a/media/capture/content/video_capture_oracle.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/content/video_capture_oracle.cc	2019-05-17 18:53:34.128000000 +0300
@@ -355,10 +355,6 @@
   min_size_change_period_ = period;
 }
 
-gfx::Size VideoCaptureOracle::capture_size() const {
-  return capture_size_;
-}
-
 // static
 const char* VideoCaptureOracle::EventAsString(Event event) {
   switch (event) {
--- a/media/capture/content/video_capture_oracle.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/content/video_capture_oracle.h	2019-05-17 18:53:34.128000000 +0300
@@ -84,7 +84,7 @@
   // Returns true iff the captured frame should be delivered.  |frame_timestamp|
   // is set to the timestamp that should be provided to the consumer of the
   // frame.
-  virtual bool CompleteCapture(int frame_number,
+  bool CompleteCapture(int frame_number,
                                bool capture_was_successful,
                                base::TimeTicks* frame_timestamp);
 
@@ -118,7 +118,7 @@
   // Returns the capture frame size the client should use.  This is updated by
   // calls to ObserveEventAndDecideCapture().  The oracle prevents too-frequent
   // changes to the capture size, to avoid stressing the end-to-end pipeline.
-  virtual gfx::Size capture_size() const;
+  gfx::Size capture_size() const { return capture_size_; }
 
   // Returns the oracle's estimate of the last time animation was detected.
   base::TimeTicks last_time_animation_was_detected() const {
--- a/media/capture/mojom/video_capture.mojom	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/mojom/video_capture.mojom	2019-05-17 18:53:34.132000000 +0300
@@ -108,11 +108,4 @@
   // Get the format(s) in use by a device referenced by |session_id|.
   GetDeviceFormatsInUse(int32 device_id, int32 session_id)
     => (array<VideoCaptureFormat> formats_in_use);
-
-  // Notifies the host about a frame being dropped.
-  OnFrameDropped(int32 device_id,
-                 media.mojom.VideoCaptureFrameDropReason reason);
-
-  // Sends a log message to the VideoCaptureHost.
-  OnLog(int32 device_id, string message);
 };
--- a/media/capture/mojom/video_capture_types.mojom	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/mojom/video_capture_types.mojom	2019-05-17 18:53:34.132000000 +0300
@@ -228,17 +228,7 @@
   kWinMediaFoundationLockingBufferDelieveredNullptr,
   kWinMediaFoundationGetBufferByIndexReturnedNull,
   kBufferPoolMaxBufferCountExceeded,
-  kBufferPoolBufferAllocationFailed,
-  kVideoCaptureImplNotInStartedState,
-  kVideoCaptureImplFailedToWrapDataAsMediaVideoFrame,
-  kVideoTrackAdapterHasNoResolutionAdapters,
-  kResolutionAdapterFrameIsNotValid,
-  kResolutionAdapterWrappingFrameForCroppingFailed,
-  kResolutionAdapterTimestampTooCloseToPrevious,
-  kResolutionAdapterFrameRateIsHigherThanRequested,
-  kResolutionAdapterHasNoCallbacks,
-  kVideoTrackFrameDelivererNotEnabledReplacingWithBlackFrame,
-  kRendererSinkFrameDelivererIsNotStarted
+  kBufferPoolBufferAllocationFailed
 };
 
 struct VideoCaptureFormat {
--- a/media/capture/mojom/video_capture_types_mojom_traits.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/mojom/video_capture_types_mojom_traits.cc	2019-05-17 18:53:34.132000000 +0300
@@ -1260,43 +1260,6 @@
     case media::VideoCaptureFrameDropReason::kBufferPoolBufferAllocationFailed:
       return media::mojom::VideoCaptureFrameDropReason::
           kBufferPoolBufferAllocationFailed;
-    case media::VideoCaptureFrameDropReason::kVideoCaptureImplNotInStartedState:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kVideoCaptureImplNotInStartedState;
-    case media::VideoCaptureFrameDropReason::
-        kVideoCaptureImplFailedToWrapDataAsMediaVideoFrame:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kVideoCaptureImplFailedToWrapDataAsMediaVideoFrame;
-    case media::VideoCaptureFrameDropReason::
-        kVideoTrackAdapterHasNoResolutionAdapters:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kVideoTrackAdapterHasNoResolutionAdapters;
-    case media::VideoCaptureFrameDropReason::kResolutionAdapterFrameIsNotValid:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kResolutionAdapterFrameIsNotValid;
-    case media::VideoCaptureFrameDropReason::
-        kResolutionAdapterWrappingFrameForCroppingFailed:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kResolutionAdapterWrappingFrameForCroppingFailed;
-    case media::VideoCaptureFrameDropReason::
-        kResolutionAdapterTimestampTooCloseToPrevious:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kResolutionAdapterTimestampTooCloseToPrevious;
-    case media::VideoCaptureFrameDropReason::
-        kResolutionAdapterFrameRateIsHigherThanRequested:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kResolutionAdapterFrameRateIsHigherThanRequested;
-    case media::VideoCaptureFrameDropReason::kResolutionAdapterHasNoCallbacks:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kResolutionAdapterHasNoCallbacks;
-    case media::VideoCaptureFrameDropReason::
-        kVideoTrackFrameDelivererNotEnabledReplacingWithBlackFrame:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kVideoTrackFrameDelivererNotEnabledReplacingWithBlackFrame;
-    case media::VideoCaptureFrameDropReason::
-        kRendererSinkFrameDelivererIsNotStarted:
-      return media::mojom::VideoCaptureFrameDropReason::
-          kRendererSinkFrameDelivererIsNotStarted;
   }
   NOTREACHED();
   return media::mojom::VideoCaptureFrameDropReason::kNone;
@@ -1382,56 +1345,6 @@
       *output =
           media::VideoCaptureFrameDropReason::kBufferPoolBufferAllocationFailed;
       return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kVideoCaptureImplNotInStartedState:
-      *output = media::VideoCaptureFrameDropReason::
-          kVideoCaptureImplNotInStartedState;
-      return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kVideoCaptureImplFailedToWrapDataAsMediaVideoFrame:
-      *output = media::VideoCaptureFrameDropReason::
-          kVideoCaptureImplFailedToWrapDataAsMediaVideoFrame;
-      return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kVideoTrackAdapterHasNoResolutionAdapters:
-      *output = media::VideoCaptureFrameDropReason::
-          kVideoTrackAdapterHasNoResolutionAdapters;
-      return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kResolutionAdapterFrameIsNotValid:
-      *output =
-          media::VideoCaptureFrameDropReason::kResolutionAdapterFrameIsNotValid;
-      return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kResolutionAdapterWrappingFrameForCroppingFailed:
-      *output = media::VideoCaptureFrameDropReason::
-          kResolutionAdapterWrappingFrameForCroppingFailed;
-      return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kResolutionAdapterTimestampTooCloseToPrevious:
-      *output = media::VideoCaptureFrameDropReason::
-          kResolutionAdapterTimestampTooCloseToPrevious;
-      return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kResolutionAdapterFrameRateIsHigherThanRequested:
-      *output = media::VideoCaptureFrameDropReason::
-          kResolutionAdapterFrameRateIsHigherThanRequested;
-      return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kResolutionAdapterHasNoCallbacks:
-      *output =
-          media::VideoCaptureFrameDropReason::kResolutionAdapterHasNoCallbacks;
-      return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kVideoTrackFrameDelivererNotEnabledReplacingWithBlackFrame:
-      *output = media::VideoCaptureFrameDropReason::
-          kVideoTrackFrameDelivererNotEnabledReplacingWithBlackFrame;
-      return true;
-    case media::mojom::VideoCaptureFrameDropReason::
-        kRendererSinkFrameDelivererIsNotStarted:
-      *output = media::VideoCaptureFrameDropReason::
-          kRendererSinkFrameDelivererIsNotStarted;
-      return true;
   }
   NOTREACHED();
   return false;
--- a/media/capture/video/android/java/borg/chromium/media/VideoCaptureCamera2.java	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/android/java/borg/chromium/media/VideoCaptureCamera2.java	2019-05-17 18:53:34.132000000 +0300
@@ -26,7 +26,6 @@
 import android.os.Handler;
 import android.os.HandlerThread;
 import android.os.Looper;
-import android.support.annotation.IntDef;
 import android.util.Range;
 import android.util.Size;
 import android.util.SparseIntArray;
@@ -37,8 +36,6 @@
 import org.chromium.base.TraceEvent;
 import org.chromium.base.annotations.JNINamespace;
 
-import java.lang.annotation.Retention;
-import java.lang.annotation.RetentionPolicy;
 import java.nio.ByteBuffer;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -925,15 +922,7 @@
         COLOR_TEMPERATURES_MAP.append(7000, CameraMetadata.CONTROL_AWB_MODE_SHADE);
     };
 
-    @IntDef({CameraState.OPENING, CameraState.CONFIGURING, CameraState.STARTED,
-            CameraState.STOPPED})
-    @Retention(RetentionPolicy.SOURCE)
-    private @interface CameraState {
-        int OPENING = 0;
-        int CONFIGURING = 1;
-        int STARTED = 2;
-        int STOPPED = 3;
-    }
+    private static enum CameraState { OPENING, CONFIGURING, STARTED, STOPPED }
 
     private final Object mCameraStateLock = new Object();
 
@@ -951,7 +940,7 @@
     private ConditionVariable mWaitForDeviceClosedConditionVariable = new ConditionVariable();
 
     private Range<Integer> mAeFpsRange;
-    private @CameraState int mCameraState = CameraState.STOPPED;
+    private CameraState mCameraState = CameraState.STOPPED;
     private float mMaxZoom = 1.0f;
     private Rect mCropRect = new Rect();
     private int mPhotoWidth;
@@ -1206,7 +1195,7 @@
         }
     }
 
-    private void changeCameraStateAndNotify(@CameraState int state) {
+    private void changeCameraStateAndNotify(CameraState state) {
         synchronized (mCameraStateLock) {
             mCameraState = state;
             mCameraStateLock.notifyAll();
--- a/media/capture/video/android/video_capture_device_android.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/android/video_capture_device_android.cc	2019-05-17 18:53:34.136000000 +0300
@@ -10,7 +10,6 @@
 #include "base/android/jni_android.h"
 #include "base/android/jni_array.h"
 #include "base/android/jni_string.h"
-#include "base/bind.h"
 #include "base/numerics/safe_conversions.h"
 #include "base/strings/string_number_conversions.h"
 #include "base/threading/thread_task_runner_handle.h"
@@ -344,7 +343,7 @@
   const int y_plane_length = width * height;
   const int uv_plane_length = y_plane_length / 4;
   const int buffer_length = y_plane_length + uv_plane_length * 2;
-  std::unique_ptr<uint8_t[]> buffer(new uint8_t[buffer_length]);
+  std::unique_ptr<uint8_t> buffer(new uint8_t[buffer_length]);
 
   libyuv::Android420ToI420(y_src, y_stride, u_src, uv_row_stride, v_src,
                            uv_row_stride, uv_pixel_stride, buffer.get(), width,
--- a/media/capture/video/android/video_capture_device_android.h	2019-05-17 17:45:41.252000000 +0300
+++ b/media/capture/video/android/video_capture_device_android.h	2019-05-17 18:53:34.136000000 +0300
@@ -37,7 +37,7 @@
   // GENERATED_JAVA_ENUM_PACKAGE: org.chromium.media
   enum AndroidImageFormat {
     // Android graphics ImageFormat mapping, see reference in:
-    // http://developer.8n6r01d.qjz9zk/reference/android/graphics/ImageFormat.html
+    // http://developer.android.com/reference/android/graphics/ImageFormat.html
     ANDROID_IMAGE_FORMAT_NV21 = 17,
     ANDROID_IMAGE_FORMAT_YUV_420_888 = 35,
     ANDROID_IMAGE_FORMAT_YV12 = 842094169,
--- a/media/capture/video/android/video_capture_device_factory_android.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/android/video_capture_device_factory_android.cc	2019-05-17 18:53:34.136000000 +0300
@@ -74,7 +74,7 @@
         Java_VideoCaptureFactory_getFacingMode(env, camera_id);
     const std::string display_name =
         base::android::ConvertJavaStringToUTF8(device_name);
-    const std::string device_id = base::NumberToString(camera_id);
+    const std::string device_id = base::IntToString(camera_id);
 
     // Android cameras are not typically USB devices, and the model_id is
     // currently only used for USB model identifiers, so this implementation
--- a/media/capture/video/chromeos/camera_3a_controller.cc	2019-05-17 17:45:41.252000000 +0300
+++ b/media/capture/video/chromeos/camera_3a_controller.cc	2019-05-17 18:53:34.136000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/capture/video/chromeos/camera_3a_controller.h"
 
-#include "base/bind.h"
 #include "base/numerics/ranges.h"
 
 #include "media/capture/video/chromeos/camera_metadata_utils.h"
@@ -229,7 +228,7 @@
     // The sensor timestamp might not be monotonically increasing. The result
     // metadata from zero-shutter-lag request may be out of order compared to
     // previous regular requests.
-    // https://developer.8n6r01d.qjz9zk/reference/android/hardware/camera2/CaptureResult#CONTROL_ENABLE_ZSL
+    // https://developer.android.com/reference/android/hardware/camera2/CaptureResult#CONTROL_ENABLE_ZSL
     latest_sensor_timestamp_ =
         std::max(latest_sensor_timestamp_,
                  base::TimeDelta::FromNanoseconds(sensor_timestamp[0]));
--- a/media/capture/video/chromeos/camera_3a_controller.h	2019-05-17 17:45:41.256000000 +0300
+++ b/media/capture/video/chromeos/camera_3a_controller.h	2019-05-17 18:53:34.136000000 +0300
@@ -10,14 +10,14 @@
 #include "base/cancelable_callback.h"
 #include "media/base/media_export.h"
 #include "media/capture/video/chromeos/mojo/camera3.mojom.h"
-#include "media/capture/video/chromeos/request_manager.h"
+#include "media/capture/video/chromeos/stream_buffer_manager.h"
 
 namespace media {
 
 // A class to control the auto-exposure, auto-focus, and auto-white-balancing
 // operations and modes of the camera.  For the detailed state transitions for
 // auto-exposure, auto-focus, and auto-white-balancing, see
-// https://source.8n6r01d.qjz9zk/devices/camera/camera3_3Amodes
+// https://source.android.com/devices/camera/camera3_3Amodes
 class CAPTURE_EXPORT Camera3AController
     : public CaptureMetadataDispatcher::ResultMetadataObserver {
  public:
--- a/media/capture/video/chromeos/camera_3a_controller_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/camera_3a_controller_unittest.cc	2019-05-17 18:53:34.136000000 +0300
@@ -4,13 +4,10 @@
 
 #include "media/capture/video/chromeos/camera_3a_controller.h"
 
-#include <functional>
-
-#include "base/bind.h"
 #include "base/synchronization/waitable_event.h"
 #include "base/threading/thread.h"
 #include "media/capture/video/chromeos/camera_metadata_utils.h"
-#include "media/capture/video/chromeos/request_builder.h"
+#include "media/capture/video/chromeos/stream_buffer_manager.h"
 #include "testing/gmock/include/gmock/gmock.h"
 #include "testing/gtest/include/gtest/gtest.h"
 
@@ -72,7 +69,7 @@
     thread_.task_runner()->PostTask(
         location,
         base::BindOnce(&Camera3AControllerTest::RunOnThread,
-                       base::Unretained(this), std::cref(location),
+                       base::Unretained(this), base::ConstRef(location),
                        base::Passed(&closure), base::Unretained(&done)));
     done.Wait();
   }
@@ -82,7 +79,8 @@
     RunOnThreadSync(
         FROM_HERE,
         base::BindOnce(&Camera3AControllerTest::Reset3AControllerOnThread,
-                       base::Unretained(this), std::cref(static_metadata)));
+                       base::Unretained(this),
+                       base::ConstRef(static_metadata)));
   }
 
   template <typename Value>
@@ -321,7 +319,7 @@
   RunOnThreadSync(FROM_HERE,
                   base::BindOnce(&Camera3AController::OnResultMetadataAvailable,
                                  base::Unretained(camera_3a_controller_.get()),
-                                 std::cref(result_metadata)));
+                                 base::ConstRef(result_metadata)));
 
   // |camera_3a_controller_| should call the registered callback once 3A are
   // stabilized.
@@ -340,7 +338,7 @@
   RunOnThreadSync(FROM_HERE,
                   base::BindOnce(&Camera3AController::OnResultMetadataAvailable,
                                  base::Unretained(camera_3a_controller_.get()),
-                                 std::cref(result_metadata)));
+                                 base::ConstRef(result_metadata)));
   done.Wait();
 }
 
--- a/media/capture/video/chromeos/camera_device_context.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/camera_device_context.h	2019-05-17 18:53:34.136000000 +0300
@@ -122,7 +122,7 @@
   int GetCameraFrameOrientation();
 
  private:
-  friend class RequestManagerTest;
+  friend class StreamBufferManagerTest;
 
   SEQUENCE_CHECKER(sequence_checker_);
 
--- a/media/capture/video/chromeos/camera_device_delegate.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/camera_device_delegate.cc	2019-05-17 18:53:34.136000000 +0300
@@ -9,8 +9,6 @@
 #include <utility>
 #include <vector>
 
-#include "base/bind.h"
-#include "base/bind_helpers.h"
 #include "base/numerics/ranges.h"
 #include "base/posix/safe_strerror.h"
 #include "base/trace_event/trace_event.h"
@@ -22,21 +20,16 @@
 #include "media/capture/video/chromeos/camera_device_context.h"
 #include "media/capture/video/chromeos/camera_hal_delegate.h"
 #include "media/capture/video/chromeos/camera_metadata_utils.h"
-#include "media/capture/video/chromeos/reprocess_manager.h"
-#include "media/capture/video/chromeos/request_manager.h"
+#include "media/capture/video/chromeos/stream_buffer_manager.h"
 
 namespace media {
 
 namespace {
 
-// The result of max_width and max_height could be zero if the stream
-// is not in the pre-defined configuration.
-void GetMaxStreamResolution(
+void GetMaxBlobStreamResolution(
     const cros::mojom::CameraMetadataPtr& static_metadata,
-    cros::mojom::Camera3StreamType stream_type,
-    cros::mojom::HalPixelFormat stream_format,
-    int32_t* max_width,
-    int32_t* max_height) {
+    int32_t* max_blob_width,
+    int32_t* max_blob_height) {
   const cros::mojom::CameraMetadataEntryPtr* stream_configurations =
       GetMetadataEntry(static_metadata,
                        cros::mojom::CameraMetadataTag::
@@ -51,8 +44,8 @@
   const size_t kStreamConfigurationSize = 4;
   int32_t* iter =
       reinterpret_cast<int32_t*>((*stream_configurations)->data.data());
-  *max_width = 0;
-  *max_height = 0;
+  *max_blob_width = 0;
+  *max_blob_height = 0;
   for (size_t i = 0; i < (*stream_configurations)->count;
        i += kStreamConfigurationSize) {
     auto format =
@@ -63,17 +56,17 @@
         static_cast<cros::mojom::Camera3StreamType>(iter[kStreamTypeOffset]);
     iter += kStreamConfigurationSize;
 
-    if (type != stream_type || format != stream_format) {
+    if (type != cros::mojom::Camera3StreamType::CAMERA3_STREAM_OUTPUT ||
+        format != cros::mojom::HalPixelFormat::HAL_PIXEL_FORMAT_BLOB) {
       continue;
     }
-
-    // TODO(wtlee): Once we have resolution settings mechanism, we could set
-    // stream resolution based on user's settings.
-    if (width > *max_width && height > *max_height) {
-      *max_width = width;
-      *max_height = height;
+    if (width > *max_blob_width && height > *max_blob_height) {
+      *max_blob_width = width;
+      *max_blob_height = height;
     }
   }
+  DCHECK_GT(*max_blob_width, 0);
+  DCHECK_GT(*max_blob_height, 0);
 }
 
 // VideoCaptureDevice::TakePhotoCallback is given by the application and is used
@@ -90,17 +83,6 @@
 
 }  // namespace
 
-StreamType StreamIdToStreamType(uint64_t stream_id) {
-  switch (stream_id) {
-    case 0:
-      return StreamType::kPreview;
-    case 1:
-      return StreamType::kStillCapture;
-    default:
-      return StreamType::kUnknown;
-  }
-}  // namespace media
-
 std::string StreamTypeToString(StreamType stream_type) {
   switch (stream_type) {
     case StreamType::kPreview:
@@ -128,6 +110,21 @@
       base::WeakPtr<CameraDeviceDelegate> camera_device_delegate)
       : camera_device_delegate_(std::move(camera_device_delegate)) {}
 
+  void RegisterBuffer(uint64_t buffer_id,
+                      cros::mojom::Camera3DeviceOps::BufferType type,
+                      uint32_t drm_format,
+                      cros::mojom::HalPixelFormat hal_pixel_format,
+                      uint32_t width,
+                      uint32_t height,
+                      std::vector<StreamCaptureInterface::Plane> planes,
+                      base::OnceCallback<void(int32_t)> callback) final {
+    if (camera_device_delegate_) {
+      camera_device_delegate_->RegisterBuffer(
+          buffer_id, type, drm_format, hal_pixel_format, width, height,
+          std::move(planes), std::move(callback));
+    }
+  }
+
   void ProcessCaptureRequest(cros::mojom::Camera3CaptureRequestPtr request,
                              base::OnceCallback<void(int32_t)> callback) final {
     if (camera_device_delegate_) {
@@ -149,13 +146,11 @@
 CameraDeviceDelegate::CameraDeviceDelegate(
     VideoCaptureDeviceDescriptor device_descriptor,
     scoped_refptr<CameraHalDelegate> camera_hal_delegate,
-    scoped_refptr<base::SingleThreadTaskRunner> ipc_task_runner,
-    ReprocessManager* reprocess_manager)
+    scoped_refptr<base::SingleThreadTaskRunner> ipc_task_runner)
     : device_descriptor_(device_descriptor),
       camera_id_(std::stoi(device_descriptor.device_id)),
       camera_hal_delegate_(std::move(camera_hal_delegate)),
       ipc_task_runner_(std::move(ipc_task_runner)),
-      reprocess_manager_(reprocess_manager),
       weak_ptr_factory_(this) {}
 
 CameraDeviceDelegate::~CameraDeviceDelegate() = default;
@@ -182,10 +177,10 @@
   if (!device_context_ ||
       device_context_->GetState() == CameraDeviceContext::State::kStopped ||
       (device_context_->GetState() == CameraDeviceContext::State::kError &&
-       !request_manager_)) {
+       !stream_buffer_manager_)) {
     // In case of Mojo connection error the device may be stopped before
     // StopAndDeAllocate is called; in case of device open failure, the state
-    // is set to kError and |request_manager_| is uninitialized.
+    // is set to kError and |stream_buffer_manager_| is uninitialized.
     std::move(device_close_callback).Run();
     return;
   }
@@ -200,7 +195,7 @@
     // The device delegate is in the process of opening the camera device.
     return;
   }
-  request_manager_->StopPreview(base::NullCallback());
+  stream_buffer_manager_->StopPreview(base::NullCallback());
   device_ops_->Close(
       base::BindOnce(&CameraDeviceDelegate::OnClosed, GetWeakPtr()));
 }
@@ -236,10 +231,8 @@
   }
 
   int32_t max_blob_width = 0, max_blob_height = 0;
-  GetMaxStreamResolution(static_metadata_,
-                         cros::mojom::Camera3StreamType::CAMERA3_STREAM_OUTPUT,
-                         cros::mojom::HalPixelFormat::HAL_PIXEL_FORMAT_BLOB,
-                         &max_blob_width, &max_blob_height);
+  GetMaxBlobStreamResolution(static_metadata_, &max_blob_width,
+                             &max_blob_height);
   photo_state->width->current = max_blob_width;
   photo_state->width->min = max_blob_width;
   photo_state->width->max = max_blob_width;
@@ -268,8 +261,8 @@
     return;
   }
 
-  if (request_manager_->GetNumberOfStreams() < kMaxConfiguredStreams) {
-    request_manager_->StopPreview(
+  if (stream_buffer_manager_->GetStreamNumber() < kMaxConfiguredStreams) {
+    stream_buffer_manager_->StopPreview(
         base::BindOnce(&CameraDeviceDelegate::OnFlushed, GetWeakPtr()));
     set_photo_option_callback_ = std::move(callback);
   } else {
@@ -295,7 +288,7 @@
       base::BindOnce(&CameraDeviceDelegate::ConstructDefaultRequestSettings,
                      GetWeakPtr(), StreamType::kStillCapture);
 
-  if (request_manager_->GetNumberOfStreams() >= kMaxConfiguredStreams) {
+  if (stream_buffer_manager_->GetStreamNumber() >= kMaxConfiguredStreams) {
     camera_3a_controller_->Stabilize3AForStillCapture(
         std::move(construct_request_cb));
     return;
@@ -322,8 +315,8 @@
     OnClosed(0);
   } else {
     // The Mojo channel terminated unexpectedly.
-    if (request_manager_) {
-      request_manager_->StopPreview(base::NullCallback());
+    if (stream_buffer_manager_) {
+      stream_buffer_manager_->StopPreview(base::NullCallback());
     }
     device_context_->SetState(CameraDeviceContext::State::kStopped);
     device_context_->SetErrorState(
@@ -368,7 +361,7 @@
 
   device_ops_.reset();
   camera_3a_controller_.reset();
-  request_manager_.reset();
+  stream_buffer_manager_.reset();
 }
 
 void CameraDeviceDelegate::OnGotCameraInfo(
@@ -450,13 +443,13 @@
   cros::mojom::Camera3CallbackOpsPtr callback_ops_ptr;
   cros::mojom::Camera3CallbackOpsRequest callback_ops_request =
       mojo::MakeRequest(&callback_ops_ptr);
-  request_manager_ = std::make_unique<RequestManager>(
+  stream_buffer_manager_ = std::make_unique<StreamBufferManager>(
       std::move(callback_ops_request),
       std::make_unique<StreamCaptureInterfaceImpl>(GetWeakPtr()),
       device_context_, std::make_unique<CameraBufferFactory>(),
       base::BindRepeating(&RotateAndBlobify), ipc_task_runner_);
   camera_3a_controller_ = std::make_unique<Camera3AController>(
-      static_metadata_, request_manager_.get(), ipc_task_runner_);
+      static_metadata_, stream_buffer_manager_.get(), ipc_task_runner_);
   device_ops_->Initialize(
       std::move(callback_ops_ptr),
       base::BindOnce(&CameraDeviceDelegate::OnInitialized, GetWeakPtr()));
@@ -514,9 +507,7 @@
   // should be configured dynamically per the photo options.
   if (require_photo) {
     int32_t max_blob_width = 0, max_blob_height = 0;
-    GetMaxStreamResolution(
-        static_metadata_, cros::mojom::Camera3StreamType::CAMERA3_STREAM_OUTPUT,
-        cros::mojom::HalPixelFormat::HAL_PIXEL_FORMAT_BLOB, &max_blob_width,
+    GetMaxBlobStreamResolution(static_metadata_, &max_blob_width,
         &max_blob_height);
 
     cros::mojom::Camera3StreamPtr still_capture_stream =
@@ -572,7 +563,7 @@
     return;
   }
 
-  request_manager_->SetUpStreamsAndBuffers(
+  stream_buffer_manager_->SetUpStreamsAndBuffers(
       chrome_capture_params_.requested_format, static_metadata_,
       std::move(updated_config->streams));
 
@@ -622,7 +613,7 @@
   }
   device_context_->SetState(CameraDeviceContext::State::kCapturing);
   camera_3a_controller_->SetAutoFocusModeForStillCapture();
-  request_manager_->StartPreview(std::move(settings));
+  stream_buffer_manager_->StartPreview(std::move(settings));
 
   if (!take_photo_callbacks_.empty()) {
     TakePhotoImpl();
@@ -638,7 +629,7 @@
   DCHECK(ipc_task_runner_->BelongsToCurrentThread());
 
   while (!take_photo_callbacks_.empty()) {
-    request_manager_->TakePhoto(
+    stream_buffer_manager_->TakePhoto(
         settings.Clone(),
         base::BindOnce(
             &TakePhotoCallbackBundle, std::move(take_photo_callbacks_.front()),
@@ -648,6 +639,36 @@
   }
 }
 
+void CameraDeviceDelegate::RegisterBuffer(
+    uint64_t buffer_id,
+    cros::mojom::Camera3DeviceOps::BufferType type,
+    uint32_t drm_format,
+    cros::mojom::HalPixelFormat hal_pixel_format,
+    uint32_t width,
+    uint32_t height,
+    std::vector<StreamCaptureInterface::Plane> planes,
+    base::OnceCallback<void(int32_t)> callback) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+
+  if (device_context_->GetState() != CameraDeviceContext::State::kCapturing) {
+    DCHECK_EQ(device_context_->GetState(),
+              CameraDeviceContext::State::kStopping);
+    return;
+  }
+  size_t num_planes = planes.size();
+  std::vector<mojo::ScopedHandle> fds(num_planes);
+  std::vector<uint32_t> strides(num_planes);
+  std::vector<uint32_t> offsets(num_planes);
+  for (size_t i = 0; i < num_planes; ++i) {
+    fds[i] = std::move(planes[i].fd);
+    strides[i] = planes[i].stride;
+    offsets[i] = planes[i].offset;
+  }
+  device_ops_->RegisterBuffer(
+      buffer_id, type, std::move(fds), drm_format, hal_pixel_format, width,
+      height, std::move(strides), std::move(offsets), std::move(callback));
+}
+
 void CameraDeviceDelegate::ProcessCaptureRequest(
     cros::mojom::Camera3CaptureRequestPtr request,
     base::OnceCallback<void(int32_t)> callback) {
--- a/media/capture/video/chromeos/camera_device_delegate.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/camera_device_delegate.h	2019-05-17 18:53:34.136000000 +0300
@@ -20,8 +20,7 @@
 class Camera3AController;
 class CameraDeviceContext;
 class CameraHalDelegate;
-class ReprocessManager;
-class RequestManager;
+class StreamBufferManager;
 
 enum class StreamType : uint64_t {
   kPreview = 0,
@@ -29,8 +28,6 @@
   kUnknown,
 };
 
-StreamType StreamIdToStreamType(uint64_t stream_id);
-
 std::string StreamTypeToString(StreamType stream_type);
 
 std::ostream& operator<<(std::ostream& os, StreamType stream_type);
@@ -49,6 +46,16 @@
 
   virtual ~StreamCaptureInterface() {}
 
+  // Registers a buffer to the camera HAL.
+  virtual void RegisterBuffer(uint64_t buffer_id,
+                              cros::mojom::Camera3DeviceOps::BufferType type,
+                              uint32_t drm_format,
+                              cros::mojom::HalPixelFormat hal_pixel_format,
+                              uint32_t width,
+                              uint32_t height,
+                              std::vector<Plane> planes,
+                              base::OnceCallback<void(int32_t)> callback) = 0;
+
   // Sends a capture request to the camera HAL.
   virtual void ProcessCaptureRequest(
       cros::mojom::Camera3CaptureRequestPtr request,
@@ -67,8 +74,7 @@
   CameraDeviceDelegate(
       VideoCaptureDeviceDescriptor device_descriptor,
       scoped_refptr<CameraHalDelegate> camera_hal_delegate,
-      scoped_refptr<base::SingleThreadTaskRunner> ipc_task_runner,
-      ReprocessManager* reprocess_manager);
+      scoped_refptr<base::SingleThreadTaskRunner> ipc_task_runner);
 
   ~CameraDeviceDelegate();
 
@@ -147,6 +153,14 @@
 
   // StreamCaptureInterface implementations.  These methods are called by
   // |stream_buffer_manager_| on |ipc_task_runner_|.
+  void RegisterBuffer(uint64_t buffer_id,
+                      cros::mojom::Camera3DeviceOps::BufferType type,
+                      uint32_t drm_format,
+                      cros::mojom::HalPixelFormat hal_pixel_format,
+                      uint32_t width,
+                      uint32_t height,
+                      std::vector<StreamCaptureInterface::Plane> planes,
+                      base::OnceCallback<void(int32_t)> callback);
   void ProcessCaptureRequest(cros::mojom::Camera3CaptureRequestPtr request,
                              base::OnceCallback<void(int32_t)> callback);
   void Flush(base::OnceCallback<void(int32_t)> callback);
@@ -166,7 +180,7 @@
 
   std::queue<VideoCaptureDevice::TakePhotoCallback> take_photo_callbacks_;
 
-  std::unique_ptr<RequestManager> request_manager_;
+  std::unique_ptr<StreamBufferManager> stream_buffer_manager_;
 
   std::unique_ptr<Camera3AController> camera_3a_controller_;
 
@@ -184,8 +198,6 @@
 
   VideoCaptureDevice::SetPhotoOptionsCallback set_photo_option_callback_;
 
-  ReprocessManager* reprocess_manager_;  // weak
-
   base::WeakPtrFactory<CameraDeviceDelegate> weak_ptr_factory_;
 
   DISALLOW_IMPLICIT_CONSTRUCTORS(CameraDeviceDelegate);
--- a/media/capture/video/chromeos/camera_device_delegate_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/camera_device_delegate_unittest.cc	2019-05-17 18:53:34.136000000 +0300
@@ -10,7 +10,6 @@
 #include <memory>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/run_loop.h"
 #include "base/test/scoped_task_environment.h"
 #include "media/base/bind_to_current_loop.h"
@@ -19,7 +18,6 @@
 #include "media/capture/video/chromeos/camera_hal_delegate.h"
 #include "media/capture/video/chromeos/mock_camera_module.h"
 #include "media/capture/video/chromeos/mock_video_capture_client.h"
-#include "media/capture/video/chromeos/reprocess_manager.h"
 #include "media/capture/video/chromeos/video_capture_device_factory_chromeos.h"
 #include "media/capture/video/mock_gpu_memory_buffer_manager.h"
 #include "testing/gmock/include/gmock/gmock.h"
@@ -89,7 +87,21 @@
                       uint32_t height,
                       const std::vector<uint32_t>& strides,
                       const std::vector<uint32_t>& offsets,
-                      RegisterBufferCallback callback) override {}
+                      RegisterBufferCallback callback) override {
+    DoRegisterBuffer(buffer_id, type, fds, drm_format, hal_pixel_format, width,
+                     height, strides, offsets, callback);
+  }
+  MOCK_METHOD10(DoRegisterBuffer,
+                void(uint64_t buffer_id,
+                     cros::mojom::Camera3DeviceOps::BufferType type,
+                     std::vector<mojo::ScopedHandle>& fds,
+                     uint32_t drm_format,
+                     cros::mojom::HalPixelFormat hal_pixel_format,
+                     uint32_t width,
+                     uint32_t height,
+                     const std::vector<uint32_t>& strides,
+                     const std::vector<uint32_t>& offsets,
+                     RegisterBufferCallback& callback));
 
   void Close(CloseCallback callback) override { DoClose(callback); }
   MOCK_METHOD1(DoClose, void(CloseCallback& callback));
@@ -121,13 +133,11 @@
     hal_delegate_thread_.Start();
     camera_hal_delegate_ =
         new CameraHalDelegate(hal_delegate_thread_.task_runner());
-    reprocess_manager_ = std::make_unique<ReprocessManager>();
     camera_hal_delegate_->SetCameraModule(
         mock_camera_module_.GetInterfacePtrInfo());
   }
 
   void TearDown() override {
-    reprocess_manager_.reset();
     camera_hal_delegate_->Reset();
     hal_delegate_thread_.Stop();
   }
@@ -137,8 +147,8 @@
     ASSERT_FALSE(camera_device_delegate_);
     device_delegate_thread_.Start();
     camera_device_delegate_ = std::make_unique<CameraDeviceDelegate>(
-        descriptor, camera_hal_delegate_, device_delegate_thread_.task_runner(),
-        reprocess_manager_.get());
+        descriptor, camera_hal_delegate_,
+        device_delegate_thread_.task_runner());
     num_streams_ = 0;
   }
 
@@ -254,6 +264,19 @@
     std::move(callback).Run(std::move(fake_settings));
   }
 
+  void RegisterBuffer(uint64_t buffer_id,
+                      cros::mojom::Camera3DeviceOps::BufferType type,
+                      std::vector<mojo::ScopedHandle>& fds,
+                      uint32_t drm_format,
+                      cros::mojom::HalPixelFormat hal_pixel_format,
+                      uint32_t width,
+                      uint32_t height,
+                      const std::vector<uint32_t>& strides,
+                      const std::vector<uint32_t>& offsets,
+                      base::OnceCallback<void(int32_t)>& callback) {
+    std::move(callback).Run(0);
+  }
+
   void ProcessCaptureRequest(cros::mojom::Camera3CaptureRequestPtr& request,
                              base::OnceCallback<void(int32_t)>& callback) {
     std::move(callback).Run(0);
@@ -354,6 +377,12 @@
   }
 
   void SetUpExpectationForCaptureLoop() {
+    EXPECT_CALL(mock_camera_device_,
+                DoRegisterBuffer(_, _, _, _, _, _, _, _, _, _))
+        .Times(AtLeast(1))
+        .WillOnce(Invoke(this, &CameraDeviceDelegateTest::RegisterBuffer))
+        .WillRepeatedly(
+            Invoke(this, &CameraDeviceDelegateTest::RegisterBuffer));
     EXPECT_CALL(mock_camera_device_, DoProcessCaptureRequest(_, _))
         .Times(AtLeast(1))
         .WillOnce(
@@ -436,8 +465,6 @@
   mojo::Binding<cros::mojom::Camera3DeviceOps> mock_camera_device_binding_;
   cros::mojom::Camera3CallbackOpsPtr callback_ops_;
 
-  std::unique_ptr<ReprocessManager> reprocess_manager_;
-
   base::Thread device_delegate_thread_;
 
   std::unique_ptr<CameraDeviceContext> device_context_;
--- a/media/capture/video/chromeos/camera_hal_delegate.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/camera_hal_delegate.cc	2019-05-17 18:53:34.136000000 +0300
@@ -20,7 +20,6 @@
 #include "media/capture/video/chromeos/camera_buffer_factory.h"
 #include "media/capture/video/chromeos/camera_hal_dispatcher_impl.h"
 #include "media/capture/video/chromeos/camera_metadata_utils.h"
-#include "media/capture/video/chromeos/reprocess_manager.h"
 #include "media/capture/video/chromeos/video_capture_device_chromeos_halv3.h"
 
 namespace media {
@@ -125,8 +124,7 @@
 
 std::unique_ptr<VideoCaptureDevice> CameraHalDelegate::CreateDevice(
     scoped_refptr<base::SingleThreadTaskRunner> task_runner_for_screen_observer,
-    const VideoCaptureDeviceDescriptor& device_descriptor,
-    ReprocessManager* reprocess_manager) {
+    const VideoCaptureDeviceDescriptor& device_descriptor) {
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   std::unique_ptr<VideoCaptureDevice> capture_device;
   if (!UpdateBuiltInCameraInfo()) {
@@ -138,8 +136,7 @@
     return capture_device;
   }
   capture_device.reset(new VideoCaptureDeviceChromeOSHalv3(
-      std::move(task_runner_for_screen_observer), device_descriptor, this,
-      reprocess_manager));
+      std::move(task_runner_for_screen_observer), device_descriptor, this));
   return capture_device;
 }
 
--- a/media/capture/video/chromeos/camera_hal_delegate.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/camera_hal_delegate.h	2019-05-17 18:53:34.136000000 +0300
@@ -24,7 +24,6 @@
 namespace media {
 
 class CameraBufferFactory;
-class ReprocessManager;
 
 // CameraHalDelegate is the component which does Mojo IPCs to the camera HAL
 // process on Chrome OS to access the module-level camera functionalities such
@@ -57,8 +56,7 @@
   std::unique_ptr<VideoCaptureDevice> CreateDevice(
       scoped_refptr<base::SingleThreadTaskRunner>
           task_runner_for_screen_observer,
-      const VideoCaptureDeviceDescriptor& device_descriptor,
-      ReprocessManager* reprocess_manager);
+      const VideoCaptureDeviceDescriptor& device_descriptor);
   void GetSupportedFormats(
       const VideoCaptureDeviceDescriptor& device_descriptor,
       VideoCaptureFormats* supported_formats);
--- a/media/capture/video/chromeos/camera_hal_dispatcher_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/camera_hal_dispatcher_impl.cc	2019-05-17 18:53:34.136000000 +0300
@@ -11,7 +11,6 @@
 
 #include <vector>
 
-#include "base/bind.h"
 #include "base/files/file_path.h"
 #include "base/files/file_util.h"
 #include "base/posix/eintr_wrapper.h"
@@ -21,7 +20,6 @@
 #include "base/strings/string_number_conversions.h"
 #include "base/synchronization/waitable_event.h"
 #include "base/trace_event/trace_event.h"
-#include "media/capture/video/chromeos/mojo/camera_common.mojom.h"
 #include "mojo/public/cpp/platform/named_platform_channel.h"
 #include "mojo/public/cpp/platform/platform_channel.h"
 #include "mojo/public/cpp/platform/socket_utils_posix.h"
--- a/media/capture/video/chromeos/camera_hal_dispatcher_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/camera_hal_dispatcher_impl_unittest.cc	2019-05-17 18:53:34.136000000 +0300
@@ -7,11 +7,9 @@
 #include <memory>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/run_loop.h"
 #include "base/single_thread_task_runner.h"
 #include "base/test/scoped_task_environment.h"
-#include "media/capture/video/chromeos/mojo/camera_common.mojom.h"
 #include "media/capture/video/chromeos/mojo/cros_camera_service.mojom.h"
 #include "mojo/public/cpp/bindings/strong_binding.h"
 #include "testing/gmock/include/gmock/gmock.h"
@@ -174,7 +172,7 @@
 
   // Wait until the clients gets the newly established Mojo channel.
   DoLoop();
-}
+};
 
 // Test that the CameraHalDisptcherImpl correctly re-establishes a Mojo channel
 // for the client when the client reconnects after crash.
@@ -223,6 +221,6 @@
 
   // Wait until the clients gets the newly established Mojo channel.
   DoLoop();
-}
+};
 
 }  // namespace media
--- a/media/capture/video/chromeos/local_gpu_memory_buffer_manager.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/local_gpu_memory_buffer_manager.cc	2019-05-17 18:53:34.136000000 +0300
@@ -71,9 +71,9 @@
     handle_.type = gfx::NATIVE_PIXMAP;
     // Set a dummy id since this is for testing only.
     handle_.id = gfx::GpuMemoryBufferId(0);
-    for (size_t i = 0; i < gbm_bo_get_num_planes(buffer_object); ++i) {
       handle_.native_pixmap_handle.fds.push_back(
-          base::FileDescriptor(gbm_bo_get_plane_fd(buffer_object, i), true));
+        base::FileDescriptor(gbm_bo_get_fd(buffer_object), false));
+    for (size_t i = 0; i < gbm_bo_get_num_planes(buffer_object); ++i) {
       handle_.native_pixmap_handle.planes.push_back(
           gfx::NativePixmapPlane(gbm_bo_get_plane_stride(buffer_object, i),
                                  gbm_bo_get_plane_offset(buffer_object, i),
@@ -85,12 +85,7 @@
     if (mapped_) {
       Unmap();
     }
-
-    for (const auto& fd : handle_.native_pixmap_handle.fds) {
-      // Close fds.
-      DCHECK(fd.auto_close);
-      close(fd.fd);
-    }
+    close(gbm_bo_get_fd(buffer_object_));
     gbm_bo_destroy(buffer_object_);
   }
 
@@ -117,7 +112,7 @@
     }
     mapped_ = true;
     return true;
-  }
+  };
 
   void* memory(size_t plane) override {
     if (!mapped_) {
--- a/media/capture/video/chromeos/mock_camera_module.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/mock_camera_module.cc	2019-05-17 18:53:34.140000000 +0300
@@ -3,7 +3,6 @@
 // found in the LICENSE file.
 
 #include "media/capture/video/chromeos/mock_camera_module.h"
-#include "base/bind.h"
 
 #include <memory>
 #include <utility>
--- a/media/capture/video/chromeos/mojo/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/mojo/BUILD.gn	2019-05-17 18:53:34.140000000 +0300
@@ -11,11 +11,9 @@
     "camera_metadata.mojom",
     "camera_metadata_tags.mojom",
     "cros_camera_service.mojom",
-    "cros_image_capture.mojom",
   ]
 
   deps = [
-    "//media/capture/mojom:image_capture",
     "//media/mojo/interfaces",
   ]
 }
--- a/media/capture/video/chromeos/mojo/camera3.mojom	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/mojo/camera3.mojom	2019-05-17 18:53:34.140000000 +0300
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Next min version: 3
+// Next min version: 2
 
 module cros.mojom;
 
@@ -73,25 +73,12 @@
   CAMERA3_BUFFER_STATUS_ERROR = 1,
 };
 
-// Structure that contains needed information about a camera buffer.
-struct CameraBufferHandle {
-  uint64 buffer_id;
-  array<handle> fds;
-  uint32 drm_format;
-  HalPixelFormat hal_pixel_format;
-  uint32 width;
-  uint32 height;
-  array<uint32> strides;
-  array<uint32> offsets;
-};
-
 struct Camera3StreamBuffer {
   uint64 stream_id;
   uint64 buffer_id;
   Camera3BufferStatus status;
   handle? acquire_fence;
   handle? release_fence;
-  [MinVersion=2] CameraBufferHandle? buffer_handle;
 };
 
 enum Camera3MsgType {
@@ -171,8 +158,9 @@
 };
 
 // Camera3DeviceOps is mostly a translation of the camera3_device_ops_t API from
-// Android camera HAL v3.  This is the interface to interact with a camera
-// device in the camera HAL.
+// Android camera HAL v3, with the additional RegisterBuffer() function to pass
+// buffer handles across processes.  This is the interface to interact with a
+// camera device in the camera HAL.
 //
 // The work flow of the Camera3DeviceOps is:
 //
@@ -190,15 +178,21 @@
 //   4. Start the capture loop. The capture loop is composed of a series of
 //      capture requests and results.
 //
-//      The capture loop shall call ProcessCaptureRequest() to request capturing
-//      each frame.  A request may contain multiple streams and the camera HAL
-//      would fill the buffers of each streams per requirements specified in
-//      ConfigureStreams().  For example, the camera HAL may fill a frame to a
-//      still capture buffer with the native capture resolution, and down-scale
-//      the same frame to a lower resolution for the preview buffer.
-//
-//      The client may continue calling ProcessCaptureRequest() up to the
-//      pipe-line depth configured in ConfigureStreams().
+//      For each capture request:
+//        a. Call RegisterBuffer() for each buffer associated with the request
+//           to register the buffers which will later be filled by the camera
+//           HAL with capture result.  For example, the client may register one
+//           small buffer for the low-resolution preview stream and one large
+//           buffer for the high-resolution still capture stream.
+//        b. Call ProcessCaptureRequest() to request capturing one frame.  A
+//           request may contain multiple streams and the camera HAL would fill
+//           the buffers of each streams per requirements specified in
+//           ConfigureStreams() and RegisterBuffer().  For example, the camera
+//           HAL may fill a frame to a still capture buffer with the native
+//           capture resolution, and down-scale the same frame to a lower
+//           resolution for the preview buffer.
+//      The client may continue with RegisterBuffer() -> ProcessCaptureRequest()
+//      up to the pipe-line depth configured in ConfigureStreams().
 //
 //      When the camera HAL is done with a capture request, the capture result
 //      is sent back to the client through the callbacks in Camera3CallbackOps.
@@ -253,8 +247,6 @@
   // camera HAL to idle state.
   Flush@5() => (int32 result);
 
-  // [Deprecated in version 2]
-  //
   // The type of buffers the CrOS camera service currently supports.
   // GRALLOC is for the platform-specific gralloc buffer allocated by Android.
   // SHM is for the shared memory buffer allocated by Chrome.
@@ -264,8 +256,6 @@
     // Add DMABUF when needed.
   };
 
-  // [Deprecated in version 2]
-  //
   // RegisterBuffer() is called to register a buffer with the camera HAL.  The
   // registered buffer can then be specified in ProcessCaptureRequest() for the
   // camera HAL to fill captured frame.  RegisterBuffer() is not part of the
--- a/media/capture/video/chromeos/stream_buffer_manager.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/stream_buffer_manager.cc	2019-05-17 18:53:34.140000000 +0300
@@ -4,29 +4,67 @@
 
 #include "media/capture/video/chromeos/stream_buffer_manager.h"
 
+#include <sync/sync.h>
 #include <memory>
-#include <string>
 
-#include "base/bind.h"
 #include "base/posix/safe_strerror.h"
 #include "base/trace_event/trace_event.h"
 #include "media/capture/video/chromeos/camera_buffer_factory.h"
 #include "media/capture/video/chromeos/camera_device_context.h"
 #include "media/capture/video/chromeos/camera_metadata_utils.h"
-#include "media/capture/video/chromeos/request_builder.h"
 #include "mojo/public/cpp/platform/platform_handle.h"
 #include "mojo/public/cpp/system/platform_handle.h"
 
 namespace media {
 
+namespace {
+
+size_t GetBufferIndex(uint64_t buffer_id) {
+  return buffer_id & 0xFFFFFFFF;
+}
+
+StreamType StreamIdToStreamType(uint64_t stream_id) {
+  switch (stream_id) {
+    case 0:
+      return StreamType::kPreview;
+    case 1:
+      return StreamType::kStillCapture;
+    default:
+      return StreamType::kUnknown;
+  }
+}
+
+}  // namespace
+
 StreamBufferManager::StreamBufferManager(
+    cros::mojom::Camera3CallbackOpsRequest callback_ops_request,
+    std::unique_ptr<StreamCaptureInterface> capture_interface,
     CameraDeviceContext* device_context,
-    std::unique_ptr<CameraBufferFactory> camera_buffer_factory)
-    : device_context_(device_context),
+    std::unique_ptr<CameraBufferFactory> camera_buffer_factory,
+    base::RepeatingCallback<
+        mojom::BlobPtr(const uint8_t* buffer,
+                       const uint32_t bytesused,
+                       const VideoCaptureFormat& capture_format,
+                       int screen_rotation)> blobify_callback,
+    scoped_refptr<base::SingleThreadTaskRunner> ipc_task_runner)
+    : callback_ops_(this, std::move(callback_ops_request)),
+      capture_interface_(std::move(capture_interface)),
+      device_context_(device_context),
       camera_buffer_factory_(std::move(camera_buffer_factory)),
-      weak_ptr_factory_(this) {}
+      blobify_callback_(std::move(blobify_callback)),
+      ipc_task_runner_(std::move(ipc_task_runner)),
+      capturing_(false),
+      frame_number_(0),
+      partial_result_count_(1),
+      first_frame_shutter_time_(base::TimeTicks()),
+      weak_ptr_factory_(this) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  DCHECK(callback_ops_.is_bound());
+  DCHECK(device_context_);
+}
 
 StreamBufferManager::~StreamBufferManager() {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
   for (const auto& iter : stream_context_) {
     if (iter.second) {
       for (const auto& buf : iter.second->buffers) {
@@ -38,39 +76,24 @@
   }
 }
 
-gfx::GpuMemoryBuffer* StreamBufferManager::GetBufferById(StreamType stream_type,
-                                                         uint64_t buffer_id) {
-  size_t buffer_index = GetBufferIndex(buffer_id);
-  if (buffer_index >= stream_context_[stream_type]->buffers.size()) {
-    LOG(ERROR) << "Invalid buffer index: " << buffer_index
-               << " for stream: " << stream_type;
-    return nullptr;
-  }
-  return stream_context_[stream_type]->buffers[buffer_index].get();
-}
-
-VideoCaptureFormat StreamBufferManager::GetStreamCaptureFormat(
-    StreamType stream_type) {
-  return stream_context_[stream_type]->capture_format;
-}
-
-bool StreamBufferManager::HasFreeBuffers(
-    const std::set<StreamType>& stream_types) {
-  for (auto stream_type : stream_types) {
-    if (stream_context_[stream_type]->free_buffers.empty()) {
-      return false;
-    }
-  }
-  return true;
-}
-
 void StreamBufferManager::SetUpStreamsAndBuffers(
     VideoCaptureFormat capture_format,
     const cros::mojom::CameraMetadataPtr& static_metadata,
     std::vector<cros::mojom::Camera3StreamPtr> streams) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+
+  // The partial result count metadata is optional; defaults to 1 in case it
+  // is not set in the static metadata.
+  const cros::mojom::CameraMetadataEntryPtr* partial_count = GetMetadataEntry(
+      static_metadata,
+      cros::mojom::CameraMetadataTag::ANDROID_REQUEST_PARTIAL_RESULT_COUNT);
+  if (partial_count) {
+    partial_result_count_ =
+        *reinterpret_cast<int32_t*>((*partial_count)->data.data());
+  }
+
   for (auto& stream : streams) {
     DVLOG(2) << "Stream " << stream->id
-             << " stream_type: " << stream->stream_type
              << " configured: usage=" << stream->usage
              << " max_buffers=" << stream->max_buffers;
 
@@ -89,7 +112,14 @@
 
     // A better way to tell the stream type here would be to check on the usage
     // flags of the stream.
-    StreamType stream_type = StreamIdToStreamType(stream->id);
+    StreamType stream_type;
+    if (stream->format ==
+        cros::mojom::HalPixelFormat::HAL_PIXEL_FORMAT_YCbCr_420_888) {
+      stream_type = StreamType::kPreview;
+    } else {  // stream->format ==
+              // cros::mojom::HalPixelFormat::HAL_PIXEL_FORMAT_BLOB
+      stream_type = StreamType::kStillCapture;
+    }
     stream_context_[stream_type] = std::make_unique<StreamContext>();
     stream_context_[stream_type]->capture_format = capture_format;
     stream_context_[stream_type]->stream = std::move(stream);
@@ -104,25 +134,16 @@
     size_t num_buffers = stream_context_[stream_type]->stream->max_buffers;
     stream_context_[stream_type]->buffers.resize(num_buffers);
     int32_t buffer_width, buffer_height;
-    switch (stream_type) {
-      case StreamType::kPreview: {
+    if (stream_type == StreamType::kPreview) {
         buffer_width = stream_context_[stream_type]->stream->width;
         buffer_height = stream_context_[stream_type]->stream->height;
-        break;
-      }
-      case StreamType::kStillCapture: {
+    } else {  // StreamType::kStillCapture
         const cros::mojom::CameraMetadataEntryPtr* jpeg_max_size =
             GetMetadataEntry(
                 static_metadata,
                 cros::mojom::CameraMetadataTag::ANDROID_JPEG_MAX_SIZE);
-        buffer_width =
-            *reinterpret_cast<int32_t*>((*jpeg_max_size)->data.data());
+      buffer_width = *reinterpret_cast<int32_t*>((*jpeg_max_size)->data.data());
         buffer_height = 1;
-        break;
-      }
-      default: {
-        NOTREACHED();
-      }
     }
     for (size_t j = 0; j < num_buffers; ++j) {
       auto buffer = camera_buffer_factory_->CreateGpuMemoryBuffer(
@@ -151,6 +172,30 @@
   }
 }
 
+void StreamBufferManager::StartPreview(
+    cros::mojom::CameraMetadataPtr preview_settings) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  DCHECK(stream_context_[StreamType::kPreview]);
+  DCHECK(repeating_request_settings_.is_null());
+
+  capturing_ = true;
+  repeating_request_settings_ = std::move(preview_settings);
+  // We cannot use a loop to register all the free buffers in one shot here
+  // because the camera HAL v3 API specifies that the client cannot call
+  // ProcessCaptureRequest before the previous one returns.
+  RegisterBuffer(StreamType::kPreview);
+}
+
+void StreamBufferManager::StopPreview(
+    base::OnceCallback<void(int32_t)> callback) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  capturing_ = false;
+  repeating_request_settings_.reset();
+  if (callback) {
+    capture_interface_->Flush(std::move(callback));
+  }
+}
+
 cros::mojom::Camera3StreamPtr StreamBufferManager::GetStreamConfiguration(
     StreamType stream_type) {
   if (!stream_context_.count(stream_type)) {
@@ -159,8 +204,140 @@
   return stream_context_[stream_type]->stream.Clone();
 }
 
-base::Optional<BufferInfo> StreamBufferManager::RequestBuffer(
-    StreamType stream_type) {
+void StreamBufferManager::TakePhoto(
+    cros::mojom::CameraMetadataPtr settings,
+    VideoCaptureDevice::TakePhotoCallback callback) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  DCHECK(stream_context_[StreamType::kStillCapture]);
+
+  still_capture_callbacks_yet_to_be_processed_.push(std::move(callback));
+
+  std::vector<uint8_t> frame_orientation(sizeof(int32_t));
+  *reinterpret_cast<int32_t*>(frame_orientation.data()) =
+      base::checked_cast<int32_t>(device_context_->GetCameraFrameOrientation());
+  cros::mojom::CameraMetadataEntryPtr e =
+      cros::mojom::CameraMetadataEntry::New();
+  e->tag = cros::mojom::CameraMetadataTag::ANDROID_JPEG_ORIENTATION;
+  e->type = cros::mojom::EntryType::TYPE_INT32;
+  e->count = 1;
+  e->data = std::move(frame_orientation);
+  AddOrUpdateMetadataEntry(&settings, std::move(e));
+
+  oneshot_request_settings_.push(std::move(settings));
+  RegisterBuffer(StreamType::kStillCapture);
+}
+
+size_t StreamBufferManager::GetStreamNumber() {
+  return stream_context_.size();
+}
+
+void StreamBufferManager::AddResultMetadataObserver(
+    ResultMetadataObserver* observer) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  DCHECK(!result_metadata_observers_.count(observer));
+
+  result_metadata_observers_.insert(observer);
+}
+
+void StreamBufferManager::RemoveResultMetadataObserver(
+    ResultMetadataObserver* observer) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  DCHECK(result_metadata_observers_.count(observer));
+
+  result_metadata_observers_.erase(observer);
+}
+
+void StreamBufferManager::SetCaptureMetadata(cros::mojom::CameraMetadataTag tag,
+                                             cros::mojom::EntryType type,
+                                             size_t count,
+                                             std::vector<uint8_t> value) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+
+  cros::mojom::CameraMetadataEntryPtr setting =
+      cros::mojom::CameraMetadataEntry::New();
+
+  setting->tag = tag;
+  setting->type = type;
+  setting->count = count;
+  setting->data = std::move(value);
+
+  capture_settings_override_.push_back(std::move(setting));
+}
+
+void StreamBufferManager::SetRepeatingCaptureMetadata(
+    cros::mojom::CameraMetadataTag tag,
+    cros::mojom::EntryType type,
+    size_t count,
+    std::vector<uint8_t> value) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  cros::mojom::CameraMetadataEntryPtr setting =
+      cros::mojom::CameraMetadataEntry::New();
+
+  setting->tag = tag;
+  setting->type = type;
+  setting->count = count;
+  setting->data = std::move(value);
+
+  capture_settings_repeating_override_[tag] = std::move(setting);
+}
+
+void StreamBufferManager::UnsetRepeatingCaptureMetadata(
+    cros::mojom::CameraMetadataTag tag) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  auto it = capture_settings_repeating_override_.find(tag);
+  if (it == capture_settings_repeating_override_.end()) {
+    LOG(ERROR) << "Unset a non-existent metadata: " << tag;
+    return;
+  }
+  capture_settings_repeating_override_.erase(it);
+}
+
+// static
+uint64_t StreamBufferManager::GetBufferIpcId(StreamType stream_type,
+                                             size_t index) {
+  uint64_t id = 0;
+  id |= static_cast<uint64_t>(stream_type) << 32;
+  id |= index;
+  return id;
+}
+
+void StreamBufferManager::ApplyCaptureSettings(
+    cros::mojom::CameraMetadataPtr* capture_settings) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+
+  if (capture_settings_override_.empty() &&
+      capture_settings_repeating_override_.empty()) {
+    return;
+  }
+
+  for (const auto& setting : capture_settings_repeating_override_) {
+    AddOrUpdateMetadataEntry(capture_settings, setting.second.Clone());
+  }
+
+  for (auto& s : capture_settings_override_) {
+    AddOrUpdateMetadataEntry(capture_settings, std::move(s));
+  }
+  capture_settings_override_.clear();
+  SortCameraMetadata(capture_settings);
+}
+
+void StreamBufferManager::RegisterBuffer(StreamType stream_type) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  DCHECK(stream_context_[stream_type]);
+
+  if (!capturing_) {
+    return;
+  }
+
+  if (stream_context_[stream_type]->free_buffers.empty()) {
+    return;
+  }
+
+  uint64_t buffer_id = stream_context_[stream_type]->free_buffers.front();
+  stream_context_[stream_type]->free_buffers.pop();
+  const gfx::GpuMemoryBuffer* buffer =
+      stream_context_[stream_type]->buffers[GetBufferIndex(buffer_id)].get();
+
   VideoPixelFormat buffer_format =
       stream_context_[stream_type]->capture_format.pixel_format;
   uint32_t drm_format = PixFormatVideoToDrm(buffer_format);
@@ -171,45 +348,516 @@
         FROM_HERE,
         std::string("Unsupported video pixel format") +
             VideoPixelFormatToString(buffer_format));
-    return {};
+    return;
   }
+  cros::mojom::HalPixelFormat hal_pixel_format =
+      stream_context_[stream_type]->stream->format;
 
-  BufferInfo buffer_info;
-  buffer_info.id = stream_context_[stream_type]->free_buffers.front();
-  stream_context_[stream_type]->free_buffers.pop();
-  buffer_info.gpu_memory_buffer = stream_context_[stream_type]
-                                      ->buffers[GetBufferIndex(buffer_info.id)]
-                                      .get();
-  buffer_info.hal_pixel_format = stream_context_[stream_type]->stream->format;
-  buffer_info.drm_format = drm_format;
-  return buffer_info;
+  gfx::NativePixmapHandle buffer_handle =
+      buffer->CloneHandle().native_pixmap_handle;
+  // Take ownership of FD at index 0.
+  base::ScopedFD fd(buffer_handle.fds[0].fd);
+  // There should be only one FD. Close all remaining FDs if there are any.
+  DCHECK_EQ(buffer_handle.fds.size(), 1U);
+  for (size_t i = 1; i < buffer_handle.fds.size(); ++i)
+    base::ScopedFD scoped_fd(buffer_handle.fds[i].fd);
+
+  size_t num_planes = buffer_handle.planes.size();
+  std::vector<StreamCaptureInterface::Plane> planes(num_planes);
+  for (size_t i = 0; i < num_planes; ++i) {
+    int dup_fd = dup(fd.get());
+    if (dup_fd == -1) {
+      device_context_->SetErrorState(
+          media::VideoCaptureError::kCrosHalV3BufferManagerFailedToDupFd,
+          FROM_HERE, "Failed to dup fd");
+      return;
+    }
+    planes[i].fd =
+        mojo::WrapPlatformHandle(mojo::PlatformHandle(base::ScopedFD(dup_fd)));
+    if (!planes[i].fd.is_valid()) {
+      device_context_->SetErrorState(
+          media::VideoCaptureError::
+              kCrosHalV3BufferManagerFailedToWrapGpuMemoryHandle,
+          FROM_HERE, "Failed to wrap gpu memory handle");
+      return;
+    }
+    planes[i].stride = buffer_handle.planes[i].stride;
+    planes[i].offset = buffer_handle.planes[i].offset;
+  }
+  if (stream_type == StreamType::kStillCapture) {
+    still_capture_callbacks_currently_processing_.push(
+        std::move(still_capture_callbacks_yet_to_be_processed_.front()));
+    still_capture_callbacks_yet_to_be_processed_.pop();
+  }
+  // We reuse BufferType::GRALLOC here since on ARC++ we are using DMA-buf-based
+  // gralloc buffers.
+  capture_interface_->RegisterBuffer(
+      buffer_id, cros::mojom::Camera3DeviceOps::BufferType::GRALLOC, drm_format,
+      hal_pixel_format, buffer->GetSize().width(), buffer->GetSize().height(),
+      std::move(planes),
+      base::BindOnce(&StreamBufferManager::OnRegisteredBuffer,
+                     weak_ptr_factory_.GetWeakPtr(), stream_type, buffer_id));
+  DVLOG(2) << "Registered buffer " << buffer_id;
+}
+
+void StreamBufferManager::OnRegisteredBuffer(StreamType stream_type,
+                                             uint64_t buffer_id,
+                                             int32_t result) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  DCHECK(stream_context_[stream_type]);
+
+  if (!capturing_) {
+    return;
+  }
+  if (result) {
+    device_context_->SetErrorState(
+        media::VideoCaptureError::kCrosHalV3BufferManagerFailedToRegisterBuffer,
+        FROM_HERE,
+        std::string("Failed to register buffer: ") +
+            base::safe_strerror(-result));
+    return;
+  }
+  stream_context_[stream_type]->registered_buffers.push(buffer_id);
+  ProcessCaptureRequest();
 }
 
-void StreamBufferManager::ReleaseBuffer(StreamType stream_type,
-                                        uint64_t buffer_id) {
-  stream_context_[stream_type]->free_buffers.push(buffer_id);
+void StreamBufferManager::ProcessCaptureRequest() {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  DCHECK(stream_context_[StreamType::kPreview]);
+
+  cros::mojom::Camera3CaptureRequestPtr request =
+      cros::mojom::Camera3CaptureRequest::New();
+  request->frame_number = frame_number_;
+
+  CaptureResult& pending_result = pending_results_[frame_number_];
+
+  if (!stream_context_[StreamType::kPreview]->registered_buffers.empty()) {
+    cros::mojom::Camera3StreamBufferPtr buffer =
+        cros::mojom::Camera3StreamBuffer::New();
+    buffer->stream_id = static_cast<uint64_t>(StreamType::kPreview);
+    buffer->buffer_id =
+        stream_context_[StreamType::kPreview]->registered_buffers.front();
+    stream_context_[StreamType::kPreview]->registered_buffers.pop();
+    buffer->status = cros::mojom::Camera3BufferStatus::CAMERA3_BUFFER_STATUS_OK;
+
+    DVLOG(2) << "Requested capture for stream " << StreamType::kPreview
+             << " in frame " << frame_number_;
+    request->settings = repeating_request_settings_.Clone();
+    request->output_buffers.push_back(std::move(buffer));
+  }
+
+  if (stream_context_.count(StreamType::kStillCapture) &&
+      !stream_context_[StreamType::kStillCapture]->registered_buffers.empty()) {
+    DCHECK(!still_capture_callbacks_currently_processing_.empty());
+    cros::mojom::Camera3StreamBufferPtr buffer =
+        cros::mojom::Camera3StreamBuffer::New();
+    buffer->stream_id = static_cast<uint64_t>(StreamType::kStillCapture);
+    buffer->buffer_id =
+        stream_context_[StreamType::kStillCapture]->registered_buffers.front();
+    stream_context_[StreamType::kStillCapture]->registered_buffers.pop();
+    buffer->status = cros::mojom::Camera3BufferStatus::CAMERA3_BUFFER_STATUS_OK;
+
+    DVLOG(2) << "Requested capture for stream " << StreamType::kStillCapture
+             << " in frame " << frame_number_;
+    // Use the still capture settings and override the preview ones.
+    request->settings = std::move(oneshot_request_settings_.front());
+    oneshot_request_settings_.pop();
+    pending_result.still_capture_callback =
+        std::move(still_capture_callbacks_currently_processing_.front());
+    still_capture_callbacks_currently_processing_.pop();
+    request->output_buffers.push_back(std::move(buffer));
+  }
+
+  pending_result.unsubmitted_buffer_count = request->output_buffers.size();
+
+  ApplyCaptureSettings(&request->settings);
+  capture_interface_->ProcessCaptureRequest(
+      std::move(request),
+      base::BindOnce(&StreamBufferManager::OnProcessedCaptureRequest,
+                     weak_ptr_factory_.GetWeakPtr()));
+  frame_number_++;
 }
 
-size_t StreamBufferManager::GetNumberOfStreams() {
-  return stream_context_.size();
+void StreamBufferManager::OnProcessedCaptureRequest(int32_t result) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+
+  if (!capturing_) {
+    return;
+  }
+  if (result) {
+    device_context_->SetErrorState(
+        media::VideoCaptureError::
+            kCrosHalV3BufferManagerProcessCaptureRequestFailed,
+        FROM_HERE,
+        std::string("Process capture request failed: ") +
+            base::safe_strerror(-result));
+    return;
+  }
+  // Keeps the preview stream going.
+  RegisterBuffer(StreamType::kPreview);
 }
 
-// static
-uint64_t StreamBufferManager::GetBufferIpcId(StreamType stream_type,
-                                             size_t index) {
-  uint64_t id = 0;
-  id |= static_cast<uint64_t>(stream_type) << 32;
-  id |= index;
-  return id;
+void StreamBufferManager::ProcessCaptureResult(
+    cros::mojom::Camera3CaptureResultPtr result) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+
+  if (!capturing_) {
+    return;
+  }
+  uint32_t frame_number = result->frame_number;
+  // A new partial result may be created in either ProcessCaptureResult or
+  // Notify.
+  CaptureResult& pending_result = pending_results_[frame_number];
+
+  // |result->pending_result| is set to 0 if the capture result contains only
+  // the result buffer handles and no result metadata.
+  if (result->partial_result) {
+    uint32_t result_id = result->partial_result;
+    if (result_id > partial_result_count_) {
+      device_context_->SetErrorState(
+          media::VideoCaptureError::
+              kCrosHalV3BufferManagerInvalidPendingResultId,
+          FROM_HERE,
+          std::string("Invalid pending_result id: ") +
+              std::to_string(result_id));
+      return;
+    }
+    if (pending_result.partial_metadata_received.count(result_id)) {
+      device_context_->SetErrorState(
+          media::VideoCaptureError::
+              kCrosHalV3BufferManagerReceivedDuplicatedPartialMetadata,
+          FROM_HERE,
+          std::string("Received duplicated partial metadata: ") +
+              std::to_string(result_id));
+      return;
+    }
+    DVLOG(2) << "Received partial result " << result_id << " for frame "
+             << frame_number;
+    pending_result.partial_metadata_received.insert(result_id);
+    MergeMetadata(&pending_result.metadata, result->result);
+  }
+
+  if (result->output_buffers) {
+    if (result->output_buffers->size() > kMaxConfiguredStreams) {
+      device_context_->SetErrorState(
+          media::VideoCaptureError::
+              kCrosHalV3BufferManagerIncorrectNumberOfOutputBuffersReceived,
+          FROM_HERE,
+          std::string("Incorrect number of output buffers received: ") +
+              std::to_string(result->output_buffers->size()));
+      return;
+    }
+    for (auto& stream_buffer : result->output_buffers.value()) {
+      DVLOG(2) << "Received capture result for frame " << frame_number
+               << " stream_id: " << stream_buffer->stream_id;
+      StreamType stream_type = StreamIdToStreamType(stream_buffer->stream_id);
+      if (stream_type == StreamType::kUnknown) {
+        device_context_->SetErrorState(
+            media::VideoCaptureError::
+                kCrosHalV3BufferManagerInvalidTypeOfOutputBuffersReceived,
+            FROM_HERE,
+            std::string("Invalid type of output buffers received: ") +
+                std::to_string(stream_buffer->stream_id));
+        return;
+      }
+
+      // The camera HAL v3 API specifies that only one capture result can carry
+      // the result buffer for any given frame number.
+      if (stream_context_[stream_type]->capture_results_with_buffer.count(
+              frame_number)) {
+        device_context_->SetErrorState(
+            media::VideoCaptureError::
+                kCrosHalV3BufferManagerReceivedMultipleResultBuffersForFrame,
+            FROM_HERE,
+            std::string("Received multiple result buffers for frame ") +
+                std::to_string(frame_number) + std::string(" for stream ") +
+                std::to_string(stream_buffer->stream_id));
+        return;
+      }
+
+      pending_result.buffers[stream_type] = std::move(stream_buffer);
+      stream_context_[stream_type]->capture_results_with_buffer[frame_number] =
+          &pending_result;
+      if (pending_result.buffers[stream_type]->status ==
+          cros::mojom::Camera3BufferStatus::CAMERA3_BUFFER_STATUS_ERROR) {
+        // If the buffer is marked as error, its content is discarded for this
+        // frame.  Send the buffer to the free list directly through
+        // SubmitCaptureResult.
+        SubmitCaptureResult(frame_number, stream_type);
+      }
+    }
+  }
+
+  for (const auto& iter : stream_context_) {
+    TRACE_EVENT1("camera", "Capture Result", "frame_number", frame_number);
+    SubmitCaptureResultIfComplete(frame_number, iter.first);
+  }
 }
 
-// static
-size_t StreamBufferManager::GetBufferIndex(uint64_t buffer_id) {
-  return buffer_id & 0xFFFFFFFF;
+void StreamBufferManager::Notify(cros::mojom::Camera3NotifyMsgPtr message) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+
+  if (!capturing_) {
+    return;
+  }
+  if (message->type == cros::mojom::Camera3MsgType::CAMERA3_MSG_ERROR) {
+    uint32_t frame_number = message->message->get_error()->frame_number;
+    uint64_t error_stream_id = message->message->get_error()->error_stream_id;
+    StreamType stream_type = StreamIdToStreamType(error_stream_id);
+    if (stream_type == StreamType::kUnknown) {
+      device_context_->SetErrorState(
+          media::VideoCaptureError::
+              kCrosHalV3BufferManagerUnknownStreamInCamera3NotifyMsg,
+          FROM_HERE,
+          std::string("Unknown stream in Camera3NotifyMsg: ") +
+              std::to_string(error_stream_id));
+      return;
+    }
+    cros::mojom::Camera3ErrorMsgCode error_code =
+        message->message->get_error()->error_code;
+    HandleNotifyError(frame_number, stream_type, error_code);
+  } else {  // cros::mojom::Camera3MsgType::CAMERA3_MSG_SHUTTER
+    uint32_t frame_number = message->message->get_shutter()->frame_number;
+    uint64_t shutter_time = message->message->get_shutter()->timestamp;
+    DVLOG(2) << "Received shutter time for frame " << frame_number;
+    if (!shutter_time) {
+      device_context_->SetErrorState(
+          media::VideoCaptureError::
+              kCrosHalV3BufferManagerReceivedInvalidShutterTime,
+          FROM_HERE,
+          std::string("Received invalid shutter time: ") +
+              std::to_string(shutter_time));
+      return;
+    }
+    CaptureResult& pending_result = pending_results_[frame_number];
+    // Shutter timestamp is in ns.
+    base::TimeTicks reference_time =
+        base::TimeTicks::FromInternalValue(shutter_time / 1000);
+    pending_result.reference_time = reference_time;
+    if (first_frame_shutter_time_.is_null()) {
+      // Record the shutter time of the first frame for calculating the
+      // timestamp.
+      first_frame_shutter_time_ = reference_time;
+    }
+    pending_result.timestamp = reference_time - first_frame_shutter_time_;
+    for (const auto& iter : stream_context_) {
+      SubmitCaptureResultIfComplete(frame_number, iter.first);
+    }
+  }
+}
+
+void StreamBufferManager::HandleNotifyError(
+    uint32_t frame_number,
+    StreamType stream_type,
+    cros::mojom::Camera3ErrorMsgCode error_code) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+
+  std::string warning_msg;
+
+  switch (error_code) {
+    case cros::mojom::Camera3ErrorMsgCode::CAMERA3_MSG_ERROR_DEVICE:
+      // Fatal error and no more frames will be produced by the device.
+      device_context_->SetErrorState(
+          media::VideoCaptureError::kCrosHalV3BufferManagerFatalDeviceError,
+          FROM_HERE, "Fatal device error");
+      return;
+
+    case cros::mojom::Camera3ErrorMsgCode::CAMERA3_MSG_ERROR_REQUEST:
+      // An error has occurred in processing the request; the request
+      // specified by |frame_number| has been dropped by the camera device.
+      // Subsequent requests are unaffected.
+      //
+      // The HAL will call ProcessCaptureResult with the buffers' state set to
+      // STATUS_ERROR.  The content of the buffers will be dropped and the
+      // buffers will be reused in SubmitCaptureResult.
+      warning_msg =
+          std::string("An error occurred while processing request for frame ") +
+          std::to_string(frame_number);
+      break;
+
+    case cros::mojom::Camera3ErrorMsgCode::CAMERA3_MSG_ERROR_RESULT:
+      // An error has occurred in producing the output metadata buffer for a
+      // result; the output metadata will not be available for the frame
+      // specified by |frame_number|.  Subsequent requests are unaffected.
+      warning_msg = std::string(
+                        "An error occurred while producing result "
+                        "metadata for frame ") +
+                    std::to_string(frame_number);
+      break;
+
+    case cros::mojom::Camera3ErrorMsgCode::CAMERA3_MSG_ERROR_BUFFER:
+      // An error has occurred in placing the output buffer into a stream for
+      // a request. |frame_number| specifies the request for which the buffer
+      // was dropped, and |stream_type| specifies the stream that dropped
+      // the buffer.
+      //
+      // The HAL will call ProcessCaptureResult with the buffer's state set to
+      // STATUS_ERROR.  The content of the buffer will be dropped and the
+      // buffer will be reused in SubmitCaptureResult.
+      warning_msg =
+          std::string(
+              "An error occurred while filling output buffer of stream ") +
+          StreamTypeToString(stream_type) + std::string(" in frame ") +
+          std::to_string(frame_number);
+      break;
+
+    default:
+      // To eliminate the warning for not handling CAMERA3_MSG_NUM_ERRORS
+      break;
+  }
+
+  LOG(WARNING) << warning_msg << stream_type;
+  device_context_->LogToClient(warning_msg);
+  // If the buffer is already returned by the HAL, submit it and we're done.
+  if (pending_results_.count(frame_number) &&
+      pending_results_[frame_number].buffers.count(stream_type)) {
+    SubmitCaptureResult(frame_number, stream_type);
+  }
+}
+
+void StreamBufferManager::SubmitCaptureResultIfComplete(
+    uint32_t frame_number,
+    StreamType stream_type) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+
+  if (!pending_results_.count(frame_number)) {
+    // The capture result may be discarded in case of error.
+    return;
+  }
+
+  CaptureResult& pending_result = pending_results_[frame_number];
+  if (!stream_context_[stream_type]->capture_results_with_buffer.count(
+          frame_number) ||
+      *pending_result.partial_metadata_received.rbegin() <
+          partial_result_count_ ||
+      pending_result.reference_time == base::TimeTicks()) {
+    // We can only submit the result buffer of |frame_number| for |stream_type|
+    // when:
+    //   1. The result buffer for |stream_type| is received, and
+    //   2. Received partial result id equals to partial result count, and
+    //   3. The shutter time is received.
+    return;
+  }
+  SubmitCaptureResult(frame_number, stream_type);
+}
+
+void StreamBufferManager::SubmitCaptureResult(uint32_t frame_number,
+                                              StreamType stream_type) {
+  DCHECK(ipc_task_runner_->BelongsToCurrentThread());
+  DCHECK(pending_results_.count(frame_number));
+  DCHECK(stream_context_[stream_type]->capture_results_with_buffer.count(
+      frame_number));
+
+  CaptureResult& pending_result =
+      *stream_context_[stream_type]->capture_results_with_buffer[frame_number];
+  if (stream_context_[stream_type]
+          ->capture_results_with_buffer.begin()
+          ->first != frame_number) {
+    device_context_->SetErrorState(
+        media::VideoCaptureError::
+            kCrosHalV3BufferManagerReceivedFrameIsOutOfOrder,
+        FROM_HERE,
+        std::string("Received frame is out-of-order; expect ") +
+            std::to_string(pending_results_.begin()->first) +
+            std::string(" but got ") + std::to_string(frame_number));
+    return;
+  }
+
+  DVLOG(2) << "Submit capture result of frame " << frame_number
+           << " for stream " << static_cast<int>(stream_type);
+  for (auto* iter : result_metadata_observers_) {
+    iter->OnResultMetadataAvailable(pending_result.metadata);
+  }
+
+  DCHECK(pending_result.buffers[stream_type]);
+  const cros::mojom::Camera3StreamBufferPtr& stream_buffer =
+      pending_result.buffers[stream_type];
+  uint64_t buffer_id = stream_buffer->buffer_id;
+
+  // Wait on release fence before delivering the result buffer to client.
+  if (stream_buffer->release_fence.is_valid()) {
+    const int kSyncWaitTimeoutMs = 1000;
+    mojo::PlatformHandle fence =
+        mojo::UnwrapPlatformHandle(std::move(stream_buffer->release_fence));
+    if (!fence.is_valid()) {
+      device_context_->SetErrorState(
+          media::VideoCaptureError::
+              kCrosHalV3BufferManagerFailedToUnwrapReleaseFenceFd,
+          FROM_HERE, "Failed to unwrap release fence fd");
+      return;
+    }
+    if (!sync_wait(fence.GetFD().get(), kSyncWaitTimeoutMs)) {
+      device_context_->SetErrorState(
+          media::VideoCaptureError::
+              kCrosHalV3BufferManagerSyncWaitOnReleaseFenceTimedOut,
+          FROM_HERE, "Sync wait on release fence timed out");
+      return;
+    }
+  }
+
+  // Deliver the captured data to client.
+  if (stream_buffer->status !=
+      cros::mojom::Camera3BufferStatus::CAMERA3_BUFFER_STATUS_ERROR) {
+    size_t buffer_index = GetBufferIndex(buffer_id);
+    gfx::GpuMemoryBuffer* buffer =
+        stream_context_[stream_type]->buffers[buffer_index].get();
+    if (stream_type == StreamType::kPreview) {
+      device_context_->SubmitCapturedData(
+          buffer, stream_context_[StreamType::kPreview]->capture_format,
+          pending_result.reference_time, pending_result.timestamp);
+      ipc_task_runner_->PostTask(
+          FROM_HERE,
+          base::BindOnce(&StreamBufferManager::RegisterBuffer,
+                         weak_ptr_factory_.GetWeakPtr(), StreamType::kPreview));
+    } else {  // StreamType::kStillCapture
+      DCHECK(pending_result.still_capture_callback);
+      const Camera3JpegBlob* header = reinterpret_cast<Camera3JpegBlob*>(
+          reinterpret_cast<uintptr_t>(buffer->memory(0)) +
+          buffer->GetSize().width() - sizeof(Camera3JpegBlob));
+      if (header->jpeg_blob_id != kCamera3JpegBlobId) {
+        device_context_->SetErrorState(
+            media::VideoCaptureError::kCrosHalV3BufferManagerInvalidJpegBlob,
+            FROM_HERE, "Invalid JPEG blob");
+        return;
+      }
+      // Still capture result from HALv3 already has orientation info in EXIF,
+      // so just provide 0 as screen rotation in |blobify_callback_| parameters.
+      mojom::BlobPtr blob = blobify_callback_.Run(
+          reinterpret_cast<uint8_t*>(buffer->memory(0)), header->jpeg_size,
+          stream_context_[stream_type]->capture_format, 0);
+      if (blob) {
+        std::move(pending_result.still_capture_callback).Run(std::move(blob));
+      } else {
+        LOG(ERROR) << "Failed to blobify the captured JPEG image";
+      }
+    }
+  }
+
+  stream_context_[stream_type]->free_buffers.push(buffer_id);
+  stream_context_[stream_type]->capture_results_with_buffer.erase(frame_number);
+  pending_result.unsubmitted_buffer_count--;
+  if (!pending_result.unsubmitted_buffer_count) {
+    pending_results_.erase(frame_number);
+  }
+
+  if (stream_type == StreamType::kPreview) {
+    // Always keep the preview stream running.
+    RegisterBuffer(StreamType::kPreview);
+  } else {  // stream_type == StreamType::kStillCapture
+    if (!still_capture_callbacks_yet_to_be_processed_.empty()) {
+      RegisterBuffer(StreamType::kStillCapture);
+    }
+  }
 }
 
 StreamBufferManager::StreamContext::StreamContext() = default;
 
 StreamBufferManager::StreamContext::~StreamContext() = default;
 
+StreamBufferManager::CaptureResult::CaptureResult()
+    : metadata(cros::mojom::CameraMetadata::New()),
+      unsubmitted_buffer_count(0) {}
+
+StreamBufferManager::CaptureResult::~CaptureResult() = default;
+
 }  // namespace media
--- a/media/capture/video/chromeos/stream_buffer_manager.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/stream_buffer_manager.h	2019-05-17 18:53:34.140000000 +0300
@@ -5,17 +5,13 @@
 #ifndef MEDIA_CAPTURE_VIDEO_CHROMEOS_STREAM_BUFFER_MANAGER_H_
 #define MEDIA_CAPTURE_VIDEO_CHROMEOS_STREAM_BUFFER_MANAGER_H_
 
-#include <cstring>
-#include <initializer_list>
 #include <memory>
 #include <queue>
-#include <set>
 #include <unordered_map>
 #include <vector>
 
 #include "base/containers/queue.h"
 #include "base/memory/weak_ptr.h"
-#include "base/optional.h"
 #include "base/single_thread_task_runner.h"
 #include "media/capture/video/chromeos/camera_device_delegate.h"
 #include "media/capture/video/chromeos/mojo/camera3.mojom.h"
@@ -26,33 +22,70 @@
 
 class GpuMemoryBuffer;
 
-}  // namespace gfx
+}  // namespace base
 
 namespace media {
 
 class CameraBufferFactory;
 class CameraDeviceContext;
 
-struct BufferInfo;
+// One stream for preview, one stream for still capture.
+constexpr size_t kMaxConfiguredStreams = 2;
+
+// The JPEG transport header as defined by Android camera HAL v3 API.  The JPEG
+// transport header is at the end of the blob buffer filled by the HAL.
+constexpr uint16_t kCamera3JpegBlobId = 0x00FF;
+struct Camera3JpegBlob {
+  uint16_t jpeg_blob_id;
+  uint32_t jpeg_size;
+};
+
+class CAPTURE_EXPORT CaptureMetadataDispatcher {
+ public:
+  class ResultMetadataObserver {
+   public:
+    virtual ~ResultMetadataObserver() {}
+    virtual void OnResultMetadataAvailable(
+        const cros::mojom::CameraMetadataPtr&) = 0;
+  };
+
+  virtual ~CaptureMetadataDispatcher() {}
+  virtual void AddResultMetadataObserver(ResultMetadataObserver* observer) = 0;
+  virtual void RemoveResultMetadataObserver(
+      ResultMetadataObserver* observer) = 0;
+  virtual void SetCaptureMetadata(cros::mojom::CameraMetadataTag tag,
+                                  cros::mojom::EntryType type,
+                                  size_t count,
+                                  std::vector<uint8_t> value) = 0;
+  virtual void SetRepeatingCaptureMetadata(cros::mojom::CameraMetadataTag tag,
+                                           cros::mojom::EntryType type,
+                                           size_t count,
+                                           std::vector<uint8_t> value) = 0;
+  virtual void UnsetRepeatingCaptureMetadata(
+      cros::mojom::CameraMetadataTag tag) = 0;
+};
 
 // StreamBufferManager is responsible for managing the buffers of the
 // stream.  StreamBufferManager allocates buffers according to the given
-// stream configuration.
-class CAPTURE_EXPORT StreamBufferManager final {
+// stream configuration, and circulates the buffers along with capture
+// requests and results between Chrome and the camera HAL process.
+class CAPTURE_EXPORT StreamBufferManager final
+    : public cros::mojom::Camera3CallbackOps,
+      public CaptureMetadataDispatcher {
  public:
   StreamBufferManager(
+      cros::mojom::Camera3CallbackOpsRequest callback_ops_request,
+      std::unique_ptr<StreamCaptureInterface> capture_interface,
       CameraDeviceContext* device_context,
-      std::unique_ptr<CameraBufferFactory> camera_buffer_factory);
-  ~StreamBufferManager();
+      std::unique_ptr<CameraBufferFactory> camera_buffer_factory,
+      base::RepeatingCallback<
+          mojom::BlobPtr(const uint8_t* buffer,
+                         const uint32_t bytesused,
+                         const VideoCaptureFormat& capture_format,
+                         int screen_rotation)> blobify_callback,
+      scoped_refptr<base::SingleThreadTaskRunner> ipc_task_runner);
 
-  gfx::GpuMemoryBuffer* GetBufferById(StreamType stream_type,
-                                      uint64_t buffer_id);
-
-  VideoCaptureFormat GetStreamCaptureFormat(StreamType stream_type);
-
-  // Checks if all streams are available. For output stream, it is available if
-  // it has free buffers.
-  bool HasFreeBuffers(const std::set<StreamType>& stream_types);
+  ~StreamBufferManager() override;
 
   // Sets up the stream context and allocate buffers according to the
   // configuration specified in |stream|.
@@ -61,22 +94,154 @@
       const cros::mojom::CameraMetadataPtr& static_metadata,
       std::vector<cros::mojom::Camera3StreamPtr> streams);
 
+  // StartPreview is the entry point to starting the video capture.  The way
+  // the video capture loop works is:
+  //
+  //  (1) If there is a free buffer, RegisterBuffer registers the buffer with
+  //      the camera HAL.
+  //  (2) Once the free buffer is registered, ProcessCaptureRequest is called
+  //      to issue a capture request which will eventually fill the registered
+  //      buffer.  Goto (1) to register the remaining free buffers.
+  //  (3) The camera HAL returns the shutter time of a capture request through
+  //      Notify, and the filled buffer through ProcessCaptureResult.
+  //  (4) Once all the result metadata are collected,
+  //      SubmitCaptureResultIfComplete is called to deliver the filled buffer
+  //      to Chrome.  After the buffer is consumed by Chrome it is enqueued back
+  //      to the free buffer queue.  Goto (1) to start another capture loop.
+  //
+  // When TakePhoto() is called, an additional BLOB buffer is queued in step (2)
+  // to let the HAL fill the still capture JPEG image.  When the JPEG image is
+  // returned in (4), it's passed to upper layer through the TakePhotoCallback.
+  void StartPreview(cros::mojom::CameraMetadataPtr preview_settings);
+
+  // Stops the capture loop.  After StopPreview is called |callback_ops_| is
+  // unbound, so no new capture request or result will be processed.
+  void StopPreview(base::OnceCallback<void(int32_t)> callback);
+
   cros::mojom::Camera3StreamPtr GetStreamConfiguration(StreamType stream_type);
 
-  // Requests buffer for specific stream type.
-  base::Optional<BufferInfo> RequestBuffer(StreamType stream_type);
+  void TakePhoto(cros::mojom::CameraMetadataPtr settings,
+                 VideoCaptureDevice::TakePhotoCallback callback);
 
-  // Releases buffer by marking it as free buffer.
-  void ReleaseBuffer(StreamType stream_type, uint64_t buffer_id);
+  size_t GetStreamNumber();
 
-  size_t GetNumberOfStreams();
+  // CaptureMetadataDispatcher implementations.
+  void AddResultMetadataObserver(ResultMetadataObserver* observer) override;
+  void RemoveResultMetadataObserver(ResultMetadataObserver* observer) override;
+
+  // Queues a capture setting that will be send along with the earliest next
+  // capture request.
+  void SetCaptureMetadata(cros::mojom::CameraMetadataTag tag,
+                          cros::mojom::EntryType type,
+                          size_t count,
+                          std::vector<uint8_t> value) override;
+
+  void SetRepeatingCaptureMetadata(cros::mojom::CameraMetadataTag tag,
+                                   cros::mojom::EntryType type,
+                                   size_t count,
+                                   std::vector<uint8_t> value) override;
 
- private:
-  friend class RequestManagerTest;
+  void UnsetRepeatingCaptureMetadata(
+      cros::mojom::CameraMetadataTag tag) override;
 
   static uint64_t GetBufferIpcId(StreamType stream_type, size_t index);
 
-  static size_t GetBufferIndex(uint64_t buffer_id);
+ private:
+  friend class StreamBufferManagerTest;
+
+  // Registers a free buffer, if any, for the give |stream_type| to the camera
+  // HAL.
+  void RegisterBuffer(StreamType stream_type);
+
+  // Calls ProcessCaptureRequest if the buffer specified by |buffer_id| is
+  // successfully registered.
+  void OnRegisteredBuffer(StreamType stream_type,
+                          uint64_t buffer_id,
+                          int32_t result);
+
+  // The capture request contains the buffer handles waiting to be filled.
+  void ProcessCaptureRequest();
+  // Calls RegisterBuffer to attempt to register any remaining free buffers.
+  void OnProcessedCaptureRequest(int32_t result);
+
+  // Camera3CallbackOps implementations.
+
+  // ProcessCaptureResult receives the result metadata as well as the filled
+  // buffer from camera HAL.  The result metadata may be divided and delivered
+  // in several stages.  Before all the result metadata is received the
+  // partial results are kept in |pending_results_|.
+  void ProcessCaptureResult(
+      cros::mojom::Camera3CaptureResultPtr result) override;
+
+  // Notify receives the shutter time of capture requests and various errors
+  // from camera HAL.  The shutter time is used as the timestamp in the video
+  // frame delivered to Chrome.
+  void Notify(cros::mojom::Camera3NotifyMsgPtr message) override;
+  void HandleNotifyError(uint32_t frame_number,
+                         StreamType stream_type,
+                         cros::mojom::Camera3ErrorMsgCode error_code);
+
+  // Submits the captured buffer of frame |frame_number_| for the give
+  // |stream_type| to Chrome if all the required metadata and the captured
+  // buffer are received.  After the buffer is submitted the function then
+  // enqueues the buffer to free buffer queue for the next capture request.
+  void SubmitCaptureResultIfComplete(uint32_t frame_number,
+                                     StreamType stream_type);
+  void SubmitCaptureResult(uint32_t frame_number, StreamType stream_type);
+
+  void ApplyCaptureSettings(cros::mojom::CameraMetadataPtr* capture_settings);
+
+  mojo::Binding<cros::mojom::Camera3CallbackOps> callback_ops_;
+
+  std::unique_ptr<StreamCaptureInterface> capture_interface_;
+
+  CameraDeviceContext* device_context_;
+
+  std::unique_ptr<CameraBufferFactory> camera_buffer_factory_;
+
+  base::RepeatingCallback<mojom::BlobPtr(
+      const uint8_t* buffer,
+      const uint32_t bytesused,
+      const VideoCaptureFormat& capture_format,
+      int screen_rotation)>
+      blobify_callback_;
+
+  // Where all the Mojo IPC calls takes place.
+  const scoped_refptr<base::SingleThreadTaskRunner> ipc_task_runner_;
+
+  // A flag indicating whether the capture loops is running.
+  bool capturing_;
+
+  // The frame number.  Increased by one for each capture request sent; reset
+  // to zero in AllocateAndStart.
+  uint32_t frame_number_;
+
+  // CaptureResult is used to hold the pending capture results for each frame.
+  struct CaptureResult {
+    CaptureResult();
+    ~CaptureResult();
+    // |reference_time| and |timestamp| are derived from the shutter time of
+    // this frame.  They are be passed to |client_->OnIncomingCapturedData|
+    // along with the |buffers| when the captured frame is submitted.
+    base::TimeTicks reference_time;
+    base::TimeDelta timestamp;
+    // The result metadata.  Contains various information about the captured
+    // frame.
+    cros::mojom::CameraMetadataPtr metadata;
+    // The buffer handles that hold the captured data of this frame.
+    std::unordered_map<StreamType, cros::mojom::Camera3StreamBufferPtr> buffers;
+    // The set of the partial metadata received.  For each capture result, the
+    // total number of partial metadata should equal to
+    // |partial_result_count_|.
+    std::set<uint32_t> partial_metadata_received;
+    // Incremented for every stream buffer requested for the given frame.
+    // StreamBufferManager destructs the CaptureResult when
+    // |unsubmitted_buffer_count| drops to zero.
+    size_t unsubmitted_buffer_count;
+    // The callback used to return the captured still capture JPEG buffer.  Set
+    // if and only if the capture request was sent with a still capture buffer.
+    VideoCaptureDevice::TakePhotoCallback still_capture_callback;
+  };
 
   struct StreamContext {
     StreamContext();
@@ -90,15 +255,60 @@
     // The free buffers of this stream.  The queue stores indices into the
     // |buffers| vector.
     std::queue<uint64_t> free_buffers;
+    // The buffers that are registered to the HAL, which can be used as the
+    // output buffers for capture requests.
+    std::queue<uint64_t> registered_buffers;
+    // The pointers to the pending capture results that have unsubmitted result
+    // buffers.
+    std::map<uint32_t, CaptureResult*> capture_results_with_buffer;
   };
 
   // The context for the set of active streams.
   std::unordered_map<StreamType, std::unique_ptr<StreamContext>>
       stream_context_;
 
-  CameraDeviceContext* device_context_;
-
-  std::unique_ptr<CameraBufferFactory> camera_buffer_factory_;
+  // The repeating request settings.  The settings come from the default preview
+  // request settings reported by the HAL.  |repeating_request_settings_| is the
+  // default settings for each capture request.
+  cros::mojom::CameraMetadataPtr repeating_request_settings_;
+
+  // A queue of oneshot request settings.  These are the request settings for
+  // each still capture requests.  |oneshot_request_settings_| overrides
+  // |repeating_request_settings_| if present.
+  std::queue<cros::mojom::CameraMetadataPtr> oneshot_request_settings_;
+
+  // The pending callbacks for the TakePhoto requests.
+  std::queue<VideoCaptureDevice::TakePhotoCallback>
+      still_capture_callbacks_yet_to_be_processed_;
+  std::queue<VideoCaptureDevice::TakePhotoCallback>
+      still_capture_callbacks_currently_processing_;
+
+  // The number of partial stages.  |partial_result_count_| is learned by
+  // querying |static_metadata_|.  In case the result count is absent in
+  // |static_metadata_|, it defaults to one which means all the result
+  // metadata and captured buffer of a frame are returned together in one
+  // shot.
+  uint32_t partial_result_count_;
+
+  // The shutter time of the first frame.  We derive the |timestamp| of a
+  // frame using the difference between the frame's shutter time and
+  // |first_frame_shutter_time_|.
+  base::TimeTicks first_frame_shutter_time_;
+
+  // Stores the pending capture results of the current in-flight frames.
+  std::map<uint32_t, CaptureResult> pending_results_;
+
+  // StreamBufferManager does not own the ResultMetadataObservers.  The
+  // observers are responsible for removing itself before self-destruction.
+  std::unordered_set<ResultMetadataObserver*> result_metadata_observers_;
+
+  // The list of settings to set/override once in the capture request.
+  std::vector<cros::mojom::CameraMetadataEntryPtr> capture_settings_override_;
+
+  // The settings to set/override repeatedly in the capture request.  In
+  // conflict with |capture_settings_override_|, this one has lower priority.
+  std::map<cros::mojom::CameraMetadataTag, cros::mojom::CameraMetadataEntryPtr>
+      capture_settings_repeating_override_;
 
   base::WeakPtrFactory<StreamBufferManager> weak_ptr_factory_;
 
--- a/media/capture/video/chromeos/video_capture_device_chromeos_halv3.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/video_capture_device_chromeos_halv3.cc	2019-05-17 18:53:34.140000000 +0300
@@ -8,7 +8,6 @@
 #include <string>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/location.h"
 #include "base/synchronization/waitable_event.h"
@@ -19,7 +18,6 @@
 #include "media/capture/video/chromeos/camera_device_context.h"
 #include "media/capture/video/chromeos/camera_device_delegate.h"
 #include "media/capture/video/chromeos/camera_hal_delegate.h"
-#include "media/capture/video/chromeos/reprocess_manager.h"
 #include "ui/display/display.h"
 #include "ui/display/display_observer.h"
 #include "ui/display/screen.h"
@@ -29,8 +27,7 @@
 VideoCaptureDeviceChromeOSHalv3::VideoCaptureDeviceChromeOSHalv3(
     scoped_refptr<base::SingleThreadTaskRunner> task_runner_for_screen_observer,
     const VideoCaptureDeviceDescriptor& device_descriptor,
-    scoped_refptr<CameraHalDelegate> camera_hal_delegate,
-    ReprocessManager* reprocess_manager)
+    scoped_refptr<CameraHalDelegate> camera_hal_delegate)
     : device_descriptor_(device_descriptor),
       camera_hal_delegate_(std::move(camera_hal_delegate)),
       capture_task_runner_(base::ThreadTaskRunnerHandle::Get()),
@@ -46,16 +43,17 @@
       rotates_with_device_(lens_facing_ !=
                            VideoFacingMode::MEDIA_VIDEO_FACING_NONE),
       rotation_(0),
-      reprocess_manager_(reprocess_manager),
       weak_ptr_factory_(this) {
-  chromeos::PowerManagerClient::Get()->AddObserver(this);
+  chromeos::DBusThreadManager::Get()->GetPowerManagerClient()->AddObserver(
+      this);
 }
 
 VideoCaptureDeviceChromeOSHalv3::~VideoCaptureDeviceChromeOSHalv3() {
   DCHECK(capture_task_runner_->BelongsToCurrentThread());
   DCHECK(!camera_device_ipc_thread_.IsRunning());
   screen_observer_delegate_->RemoveObserver();
-  chromeos::PowerManagerClient::Get()->RemoveObserver(this);
+  chromeos::DBusThreadManager::Get()->GetPowerManagerClient()->RemoveObserver(
+      this);
 }
 
 // VideoCaptureDevice implementation.
@@ -77,7 +75,7 @@
   device_context_ = std::make_unique<CameraDeviceContext>(std::move(client));
   camera_device_delegate_ = std::make_unique<CameraDeviceDelegate>(
       device_descriptor_, camera_hal_delegate_,
-      camera_device_ipc_thread_.task_runner(), reprocess_manager_);
+      camera_device_ipc_thread_.task_runner());
   OpenDevice();
 }
 
@@ -129,7 +127,8 @@
       base::BindOnce(
           &VideoCaptureDeviceChromeOSHalv3::CloseDevice,
           weak_ptr_factory_.GetWeakPtr(),
-          BindToCurrentLoop(chromeos::PowerManagerClient::Get()
+          BindToCurrentLoop(chromeos::DBusThreadManager::Get()
+                                ->GetPowerManagerClient()
                                 ->GetSuspendReadinessCallback(FROM_HERE))));
 }
 
@@ -151,7 +150,7 @@
   // sure |device_context_| outlives |camera_device_delegate_|.
   camera_device_ipc_thread_.task_runner()->PostTask(
       FROM_HERE,
-      base::BindOnce(&CameraDeviceDelegate::AllocateAndStart,
+      base::Bind(&CameraDeviceDelegate::AllocateAndStart,
                      camera_device_delegate_->GetWeakPtr(), capture_params_,
                      base::Unretained(device_context_.get())));
   camera_device_ipc_thread_.task_runner()->PostTask(
@@ -239,7 +238,7 @@
   if (camera_device_ipc_thread_.IsRunning()) {
     camera_device_ipc_thread_.task_runner()->PostTask(
         FROM_HERE,
-        base::BindOnce(&CameraDeviceDelegate::SetRotation,
+        base::Bind(&CameraDeviceDelegate::SetRotation,
                        camera_device_delegate_->GetWeakPtr(), rotation_));
   }
 }
--- a/media/capture/video/chromeos/video_capture_device_chromeos_halv3.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/video_capture_device_chromeos_halv3.h	2019-05-17 18:53:34.140000000 +0300
@@ -28,7 +28,6 @@
 class CameraHalDelegate;
 class CameraDeviceContext;
 class CameraDeviceDelegate;
-class ReprocessManager;
 
 // Implementation of VideoCaptureDevice for ChromeOS with CrOS camera HALv3.
 class CAPTURE_EXPORT VideoCaptureDeviceChromeOSHalv3 final
@@ -40,8 +39,7 @@
       scoped_refptr<base::SingleThreadTaskRunner>
           task_runner_for_screen_observer,
       const VideoCaptureDeviceDescriptor& device_descriptor,
-      scoped_refptr<CameraHalDelegate> camera_hal_delegate,
-      ReprocessManager* reprocess_manager);
+      scoped_refptr<CameraHalDelegate> camera_hal_delegate);
 
   ~VideoCaptureDeviceChromeOSHalv3() final;
 
@@ -101,8 +99,6 @@
   const bool rotates_with_device_;
   int rotation_;
 
-  ReprocessManager* reprocess_manager_;  // weak
-
   base::WeakPtrFactory<VideoCaptureDeviceChromeOSHalv3> weak_ptr_factory_;
 
   DISALLOW_IMPLICIT_CONSTRUCTORS(VideoCaptureDeviceChromeOSHalv3);
--- a/media/capture/video/chromeos/video_capture_device_factory_chromeos.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/video_capture_device_factory_chromeos.cc	2019-05-17 18:53:34.140000000 +0300
@@ -4,12 +4,8 @@
 
 #include "media/capture/video/chromeos/video_capture_device_factory_chromeos.h"
 
-#include <utility>
-
 #include "base/memory/ptr_util.h"
 #include "media/capture/video/chromeos/camera_hal_dispatcher_impl.h"
-#include "media/capture/video/chromeos/cros_image_capture_impl.h"
-#include "media/capture/video/chromeos/reprocess_manager.h"
 
 namespace media {
 
@@ -23,8 +19,6 @@
     scoped_refptr<base::SingleThreadTaskRunner> task_runner_for_screen_observer)
     : task_runner_for_screen_observer_(task_runner_for_screen_observer),
       camera_hal_ipc_thread_("CameraHalIpcThread"),
-      reprocess_manager_(new ReprocessManager),
-      cros_image_capture_(new CrosImageCaptureImpl(reprocess_manager_.get())),
       initialized_(Init()) {}
 
 VideoCaptureDeviceFactoryChromeOS::~VideoCaptureDeviceFactoryChromeOS() {
@@ -40,8 +34,7 @@
     return std::unique_ptr<VideoCaptureDevice>();
   }
   return camera_hal_delegate_->CreateDevice(task_runner_for_screen_observer_,
-                                            device_descriptor,
-                                            reprocess_manager_.get());
+                                            device_descriptor);
 }
 
 void VideoCaptureDeviceFactoryChromeOS::GetSupportedFormats(
@@ -90,9 +83,4 @@
   return true;
 }
 
-void VideoCaptureDeviceFactoryChromeOS::BindCrosImageCaptureRequest(
-    cros::mojom::CrosImageCaptureRequest request) {
-  cros_image_capture_->BindRequest(std::move(request));
-}
-
 }  // namespace media
--- a/media/capture/video/chromeos/video_capture_device_factory_chromeos.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/chromeos/video_capture_device_factory_chromeos.h	2019-05-17 18:53:34.140000000 +0300
@@ -10,14 +10,10 @@
 #include "base/macros.h"
 #include "base/single_thread_task_runner.h"
 #include "media/capture/video/chromeos/camera_hal_delegate.h"
-#include "media/capture/video/chromeos/mojo/cros_image_capture.mojom.h"
 #include "media/capture/video/video_capture_device_factory.h"
 
 namespace media {
 
-class CrosImageCaptureImpl;
-class ReprocessManager;
-
 class CAPTURE_EXPORT VideoCaptureDeviceFactoryChromeOS final
     : public VideoCaptureDeviceFactory {
  public:
@@ -39,9 +35,6 @@
   static gpu::GpuMemoryBufferManager* GetBufferManager();
   static void SetGpuBufferManager(gpu::GpuMemoryBufferManager* buffer_manager);
 
-  void BindCrosImageCaptureRequest(
-      cros::mojom::CrosImageCaptureRequest request);
-
  private:
   // Initializes the factory. The factory is functional only after this call
   // succeeds.
@@ -60,10 +53,6 @@
   // |camera_hal_ipc_thread_|.
   scoped_refptr<CameraHalDelegate> camera_hal_delegate_;
 
-  std::unique_ptr<ReprocessManager> reprocess_manager_;
-
-  std::unique_ptr<CrosImageCaptureImpl> cros_image_capture_;
-
   bool initialized_;
 
   DISALLOW_COPY_AND_ASSIGN(VideoCaptureDeviceFactoryChromeOS);
--- a/media/capture/video/fake_video_capture_device.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/fake_video_capture_device.cc	2019-05-17 18:53:34.140000000 +0300
@@ -695,7 +695,7 @@
   const base::TimeDelta delay = next_execution_time - current_time;
   base::ThreadTaskRunnerHandle::Get()->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&FakeVideoCaptureDevice::OnNextFrameDue,
+      base::Bind(&FakeVideoCaptureDevice::OnNextFrameDue,
                      weak_factory_.GetWeakPtr(), next_execution_time,
                      current_session_id_),
       delay);
--- a/media/capture/video/fake_video_capture_device_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/fake_video_capture_device_unittest.cc	2019-05-17 18:53:34.140000000 +0300
@@ -98,7 +98,7 @@
       buffer_id, arbitrary_frame_feedback_id,
       std::make_unique<StubBufferHandleProvider>(mapped_size, buffer),
       std::make_unique<StubReadWritePermission>(buffer));
-}
+};
 
 class ImageCaptureClient : public base::RefCounted<ImageCaptureClient> {
  public:
@@ -262,7 +262,7 @@
   }
 }
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     ,
     FakeVideoCaptureDeviceTest,
     Combine(
@@ -566,7 +566,7 @@
   }
 }
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     ,
     FakeVideoCaptureDeviceFactoryTest,
     Values(CommandLineTestData{"fps=-1",
@@ -617,4 +617,4 @@
                1u,
                FakeVideoCaptureDevice::DisplayMediaType::BROWSER,
                {PIXEL_FORMAT_I420}}));
-}  // namespace media
+};  // namespace media
--- a/media/capture/video/file_video_capture_device.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/file_video_capture_device.cc	2019-05-17 18:53:34.140000000 +0300
@@ -319,8 +319,8 @@
   capture_thread_.Start();
   capture_thread_.task_runner()->PostTask(
       FROM_HERE,
-      base::BindOnce(&FileVideoCaptureDevice::OnAllocateAndStart,
-                     base::Unretained(this), params, std::move(client)));
+      base::Bind(&FileVideoCaptureDevice::OnAllocateAndStart,
+                 base::Unretained(this), params, base::Passed(&client)));
 }
 
 void FileVideoCaptureDevice::StopAndDeAllocate() {
--- a/media/capture/video/file_video_capture_device_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/file_video_capture_device_factory.cc	2019-05-17 18:53:34.140000000 +0300
@@ -29,8 +29,7 @@
 std::unique_ptr<VideoCaptureDevice> FileVideoCaptureDeviceFactory::CreateDevice(
     const VideoCaptureDeviceDescriptor& device_descriptor) {
   DCHECK(thread_checker_.CalledOnValidThread());
-  base::ScopedBlockingCall scoped_blocking_call(FROM_HERE,
-                                                base::BlockingType::MAY_BLOCK);
+  base::ScopedBlockingCall scoped_blocking_call(base::BlockingType::MAY_BLOCK);
 #if defined(OS_WIN)
   return std::unique_ptr<VideoCaptureDevice>(new FileVideoCaptureDevice(
       base::FilePath(base::SysUTF8ToWide(device_descriptor.display_name()))));
@@ -68,8 +67,7 @@
     const VideoCaptureDeviceDescriptor& device_descriptor,
     VideoCaptureFormats* supported_formats) {
   DCHECK(thread_checker_.CalledOnValidThread());
-  base::ScopedBlockingCall scoped_blocking_call(FROM_HERE,
-                                                base::BlockingType::MAY_BLOCK);
+  base::ScopedBlockingCall scoped_blocking_call(base::BlockingType::MAY_BLOCK);
 
   VideoCaptureFormat capture_format;
   if (!FileVideoCaptureDevice::GetVideoCaptureFormat(
--- a/media/capture/video/file_video_capture_device_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/file_video_capture_device_unittest.cc	2019-05-17 18:53:34.144000000 +0300
@@ -8,7 +8,6 @@
 #include <memory>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/test/scoped_task_environment.h"
 #include "media/base/bind_to_current_loop.h"
 #include "media/base/test_data_util.h"
--- a/media/capture/video/linux/v4l2_capture_delegate.cc	2019-05-17 17:45:41.256000000 +0300
+++ b/media/capture/video/linux/v4l2_capture_delegate.cc	2019-05-17 18:53:34.144000000 +0300
@@ -78,7 +78,7 @@
     // that method.
     {V4L2_PIX_FMT_MJPEG, PIXEL_FORMAT_MJPEG, 1},
     // JPEG works as MJPEG on some gspca webcams from field reports, see
-    // https://code.9oo91e.qjz9zk/p/webrtc/issues/detail?id=529, put it as the
+    // https://code.google.com/p/webrtc/issues/detail?id=529, put it as the
     // least preferred format.
     {V4L2_PIX_FMT_JPEG, PIXEL_FORMAT_MJPEG, 1},
 };
@@ -234,8 +234,7 @@
     V4L2CaptureDevice* v4l2,
     const VideoCaptureDeviceDescriptor& device_descriptor,
     const scoped_refptr<base::SingleThreadTaskRunner>& v4l2_task_runner,
-    int power_line_frequency,
-    int rotation)
+    int power_line_frequency)
     : v4l2_(v4l2),
       v4l2_task_runner_(v4l2_task_runner),
       device_descriptor_(device_descriptor),
@@ -243,7 +242,7 @@
       device_fd_(v4l2),
       is_capturing_(false),
       timeout_count_(0),
-      rotation_(rotation),
+      rotation_(0),
       weak_factory_(this) {}
 
 void V4L2CaptureDelegate::AllocateAndStart(
--- a/media/capture/video/linux/v4l2_capture_delegate.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/linux/v4l2_capture_delegate.h	2019-05-17 18:53:34.144000000 +0300
@@ -51,8 +51,7 @@
       V4L2CaptureDevice* v4l2,
       const VideoCaptureDeviceDescriptor& device_descriptor,
       const scoped_refptr<base::SingleThreadTaskRunner>& v4l2_task_runner,
-      int power_line_frequency,
-      int rotation);
+      int power_line_frequency);
   ~V4L2CaptureDelegate();
 
   // Forward-to versions of VideoCaptureDevice virtual methods.
--- a/media/capture/video/linux/v4l2_capture_delegate_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/linux/v4l2_capture_delegate_unittest.cc	2019-05-17 18:53:34.144000000 +0300
@@ -182,8 +182,7 @@
             v4l2_.get(),
             device_descriptor_,
             base::ThreadTaskRunnerHandle::Get(),
-            50,
-            0)) {}
+            50)) {}
   ~V4L2CaptureDelegateTest() override = default;
 
   base::test::ScopedTaskEnvironment scoped_task_environment_;
@@ -253,4 +252,4 @@
   }
 }
 
-}  // namespace media
+};  // namespace media
--- a/media/capture/video/linux/video_capture_device_chromeos.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/linux/video_capture_device_chromeos.cc	2019-05-17 18:53:34.144000000 +0300
@@ -28,12 +28,10 @@
           ScreenObserverDelegate::Create(this, ui_task_runner)) {}
 
 VideoCaptureDeviceChromeOS::~VideoCaptureDeviceChromeOS() {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   screen_observer_delegate_->RemoveObserver();
 }
 
 void VideoCaptureDeviceChromeOS::SetRotation(int rotation) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   if (!camera_config_.rotates_with_device) {
     rotation = 0;
   } else if (camera_config_.lens_facing ==
@@ -79,7 +77,6 @@
 
 void VideoCaptureDeviceChromeOS::SetDisplayRotation(
     const display::Display& display) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   if (display.IsInternal())
     SetRotation(display.rotation() * 90);
 }
--- a/media/capture/video/linux/video_capture_device_chromeos.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/linux/video_capture_device_chromeos.h	2019-05-17 18:53:34.144000000 +0300
@@ -51,12 +51,8 @@
  private:
   // DisplayRotationObserver implementation.
   void SetDisplayRotation(const display::Display& display) override;
-
   const ChromeOSDeviceCameraConfig camera_config_;
   scoped_refptr<ScreenObserverDelegate> screen_observer_delegate_;
-
-  SEQUENCE_CHECKER(sequence_checker_);
-
   DISALLOW_IMPLICIT_CONSTRUCTORS(VideoCaptureDeviceChromeOS);
 };
 
--- a/media/capture/video/linux/video_capture_device_factory_linux_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/linux/video_capture_device_factory_linux_unittest.cc	2019-05-17 18:53:34.144000000 +0300
@@ -143,4 +143,4 @@
   device->StopAndDeAllocate();
 }
 
-}  // namespace media
+};  // namespace media
--- a/media/capture/video/linux/video_capture_device_linux.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/linux/video_capture_device_linux.cc	2019-05-17 18:53:34.144000000 +0300
@@ -56,11 +56,9 @@
     const VideoCaptureDeviceDescriptor& device_descriptor)
     : device_descriptor_(device_descriptor),
       v4l2_(std::move(v4l2)),
-      v4l2_thread_("V4L2CaptureThread"),
-      rotation_(0) {}
+      v4l2_thread_("V4L2CaptureThread") {}
 
 VideoCaptureDeviceLinux::~VideoCaptureDeviceLinux() {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   // Check if the thread is running.
   // This means that the device has not been StopAndDeAllocate()d properly.
   DCHECK(!v4l2_thread_.IsRunning());
@@ -70,7 +68,6 @@
 void VideoCaptureDeviceLinux::AllocateAndStart(
     const VideoCaptureParams& params,
     std::unique_ptr<VideoCaptureDevice::Client> client) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   DCHECK(!capture_impl_);
   if (v4l2_thread_.IsRunning())
     return;  // Wrong state.
@@ -80,7 +77,7 @@
       TranslatePowerLineFrequencyToV4L2(GetPowerLineFrequency(params));
   capture_impl_ = std::make_unique<V4L2CaptureDelegate>(
       v4l2_.get(), device_descriptor_, v4l2_thread_.task_runner(),
-      line_frequency, rotation_);
+      line_frequency);
   if (!capture_impl_) {
     client->OnError(VideoCaptureError::
                         kDeviceCaptureLinuxFailedToCreateVideoCaptureDelegate,
@@ -89,11 +86,11 @@
   }
   v4l2_thread_.task_runner()->PostTask(
       FROM_HERE,
-      base::BindOnce(&V4L2CaptureDelegate::AllocateAndStart,
+      base::Bind(&V4L2CaptureDelegate::AllocateAndStart,
                      capture_impl_->GetWeakPtr(),
                      params.requested_format.frame_size.width(),
                      params.requested_format.frame_size.height(),
-                     params.requested_format.frame_rate, std::move(client)));
+                 params.requested_format.frame_rate, base::Passed(&client)));
 
   for (auto& request : photo_requests_queue_)
     v4l2_thread_.task_runner()->PostTask(FROM_HERE, std::move(request));
@@ -101,7 +98,6 @@
 }
 
 void VideoCaptureDeviceLinux::StopAndDeAllocate() {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   if (!v4l2_thread_.IsRunning())
     return;  // Wrong state.
   v4l2_thread_.task_runner()->PostTask(
@@ -114,11 +110,10 @@
 }
 
 void VideoCaptureDeviceLinux::TakePhoto(TakePhotoCallback callback) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   DCHECK(capture_impl_);
   auto functor =
       base::BindOnce(&V4L2CaptureDelegate::TakePhoto,
-                     capture_impl_->GetWeakPtr(), std::move(callback));
+                     capture_impl_->GetWeakPtr(), base::Passed(&callback));
   if (!v4l2_thread_.IsRunning()) {
     // We have to wait until we get the device AllocateAndStart()ed.
     photo_requests_queue_.push_back(std::move(functor));
@@ -128,10 +123,9 @@
 }
 
 void VideoCaptureDeviceLinux::GetPhotoState(GetPhotoStateCallback callback) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   auto functor =
       base::BindOnce(&V4L2CaptureDelegate::GetPhotoState,
-                     capture_impl_->GetWeakPtr(), std::move(callback));
+                     capture_impl_->GetWeakPtr(), base::Passed(&callback));
   if (!v4l2_thread_.IsRunning()) {
     // We have to wait until we get the device AllocateAndStart()ed.
     photo_requests_queue_.push_back(std::move(functor));
@@ -143,10 +137,9 @@
 void VideoCaptureDeviceLinux::SetPhotoOptions(
     mojom::PhotoSettingsPtr settings,
     SetPhotoOptionsCallback callback) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
-  auto functor = base::BindOnce(&V4L2CaptureDelegate::SetPhotoOptions,
-                                capture_impl_->GetWeakPtr(),
-                                std::move(settings), std::move(callback));
+  auto functor = base::BindOnce(
+      &V4L2CaptureDelegate::SetPhotoOptions, capture_impl_->GetWeakPtr(),
+      base::Passed(&settings), base::Passed(&callback));
   if (!v4l2_thread_.IsRunning()) {
     // We have to wait until we get the device AllocateAndStart()ed.
     photo_requests_queue_.push_back(std::move(functor));
@@ -156,8 +149,6 @@
 }
 
 void VideoCaptureDeviceLinux::SetRotation(int rotation) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
-  rotation_ = rotation;
   if (v4l2_thread_.IsRunning()) {
     v4l2_thread_.task_runner()->PostTask(
         FROM_HERE, base::BindOnce(&V4L2CaptureDelegate::SetRotation,
--- a/media/capture/video/linux/video_capture_device_linux.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/linux/video_capture_device_linux.h	2019-05-17 18:53:34.144000000 +0300
@@ -67,13 +67,6 @@
 
   base::Thread v4l2_thread_;  // Thread used for reading data from the device.
 
-  // SetRotation() may get called even when the device is not started. When that
-  // is the case we remember the value here and use it as soon as the device
-  // gets started.
-  int rotation_;
-
-  SEQUENCE_CHECKER(sequence_checker_);
-
   DISALLOW_IMPLICIT_CONSTRUCTORS(VideoCaptureDeviceLinux);
 };
 
--- a/media/capture/video/mac/video_capture_device_factory_mac_unittest.mm	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/mac/video_capture_device_factory_mac_unittest.mm	2019-05-17 18:53:34.148000000 +0300
@@ -2,7 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 #include "media/capture/video/mac/video_capture_device_factory_mac.h"
-#include "base/bind.h"
 #include "base/run_loop.h"
 #include "base/test/scoped_task_environment.h"
 #include "media/capture/video/mac/video_capture_device_mac.h"
@@ -45,4 +44,4 @@
   }));
 }
 
-}  // namespace media
+};  // namespace media
--- a/media/capture/video/mock_device.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/mock_device.cc	2019-05-17 18:53:34.148000000 +0300
@@ -25,10 +25,6 @@
       frame_feedback_id);
 }
 
-void MockDevice::SendOnStarted() {
-  client_->OnStarted();
-}
-
 void MockDevice::AllocateAndStart(const media::VideoCaptureParams& params,
                                   std::unique_ptr<Client> client) {
   client_ = std::move(client);
--- a/media/capture/video/mock_device_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/mock_device_factory.cc	2019-05-17 18:53:34.148000000 +0300
@@ -57,10 +57,6 @@
   devices_[descriptor] = device;
 }
 
-void MockDeviceFactory::RemoveAllDevices() {
-  devices_.clear();
-}
-
 std::unique_ptr<media::VideoCaptureDevice> MockDeviceFactory::CreateDevice(
     const media::VideoCaptureDeviceDescriptor& device_descriptor) {
   if (devices_.find(device_descriptor) == devices_.end())
--- a/media/capture/video/mock_device_factory.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/mock_device_factory.h	2019-05-17 18:53:34.148000000 +0300
@@ -21,7 +21,6 @@
 
   void AddMockDevice(media::VideoCaptureDevice* device,
                      const media::VideoCaptureDeviceDescriptor& descriptor);
-  void RemoveAllDevices();
 
   // media::VideoCaptureDeviceFactory implementation.
   std::unique_ptr<media::VideoCaptureDevice> CreateDevice(
--- a/media/capture/video/mock_device.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/mock_device.h	2019-05-17 18:53:34.148000000 +0300
@@ -21,7 +21,6 @@
   void SendStubFrame(const media::VideoCaptureFormat& format,
                      int rotation,
                      int frame_feedback_id);
-  void SendOnStarted();
 
   // media::VideoCaptureDevice implementation.
   MOCK_METHOD2(DoAllocateAndStart,
--- a/media/capture/video/mock_gpu_memory_buffer_manager.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/mock_gpu_memory_buffer_manager.cc	2019-05-17 18:53:34.148000000 +0300
@@ -9,7 +9,7 @@
 #include "build/build_config.h"
 
 #if defined(OS_CHROMEOS)
-#include "media/capture/video/chromeos/request_manager.h"
+#include "media/capture/video/chromeos/stream_buffer_manager.h"
 #endif
 
 using ::testing::Return;
@@ -37,15 +37,12 @@
 
 #if defined(OS_CHROMEOS)
     // Set a dummy fd since this is for testing only.
-    handle_.native_pixmap_handle.fds.push_back(base::FileDescriptor(0, true));
+    handle_.native_pixmap_handle.fds.push_back(base::FileDescriptor(0, false));
     handle_.native_pixmap_handle.planes.push_back(
         gfx::NativePixmapPlane(size_.width(), 0, y_plane_size));
-    if (format == gfx::BufferFormat::YUV_420_BIPLANAR) {
-      handle_.native_pixmap_handle.fds.push_back(base::FileDescriptor(0, true));
       handle_.native_pixmap_handle.planes.push_back(gfx::NativePixmapPlane(
           size_.width(), handle_.native_pixmap_handle.planes[0].size,
           uv_plane_size));
-    }
 
     // For faking a valid JPEG blob buffer.
     if (base::checked_cast<size_t>(size_.width()) >= sizeof(Camera3JpegBlob)) {
--- a/media/capture/video/mock_video_frame_receiver.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/mock_video_frame_receiver.h	2019-05-17 18:53:34.148000000 +0300
@@ -29,7 +29,6 @@
   MOCK_METHOD1(OnBufferRetired, void(int buffer_id));
   MOCK_METHOD0(OnStarted, void());
   MOCK_METHOD0(OnStartedUsingGpuDecode, void());
-  MOCK_METHOD0(OnStopped, void());
 
   void OnNewBuffer(int buffer_id,
                    media::mojom::VideoBufferHandlePtr buffer_handle) override {
--- a/media/capture/video/video_capture_buffer_tracker.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/video_capture_buffer_tracker.h	2019-05-17 18:53:34.148000000 +0300
@@ -26,7 +26,7 @@
   virtual bool Init(const gfx::Size& dimensions,
                     VideoPixelFormat format,
                     const mojom::PlaneStridesPtr& strides) = 0;
-  virtual ~VideoCaptureBufferTracker() {}
+  virtual ~VideoCaptureBufferTracker(){};
 
   bool held_by_producer() const { return held_by_producer_; }
   void set_held_by_producer(bool value) { held_by_producer_ = value; }
--- a/media/capture/video/video_capture_device_client.cc	2019-05-17 17:45:41.260000000 +0300
+++ b/media/capture/video/video_capture_device_client.cc	2019-05-17 18:53:34.148000000 +0300
@@ -115,7 +115,6 @@
 VideoCaptureDeviceClient::~VideoCaptureDeviceClient() {
   for (int buffer_id : buffer_ids_known_by_receiver_)
     receiver_->OnBufferRetired(buffer_id);
-  receiver_->OnStopped();
 }
 
 // static
@@ -236,7 +235,7 @@
 // Linux RGB24 defines red at lowest byte address,
 // see http://linuxtv.org/downloads/v4l-dvb-apis/packed-rgb.html.
 // Windows RGB24 defines blue at lowest byte,
-// see https://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/dd407253
+// see https://msdn.microsoft.com/en-us/library/windows/desktop/dd407253
 #if defined(OS_LINUX)
       origin_colorspace = libyuv::FOURCC_RAW;
 #elif defined(OS_WIN)
--- a/media/capture/video/video_capture_device_unittest.cc	2019-05-17 17:45:41.260000000 +0300
+++ b/media/capture/video/video_capture_device_unittest.cc	2019-05-17 18:53:34.152000000 +0300
@@ -185,12 +185,11 @@
     if (strcmp("image/jpeg", blob->mime_type.c_str()) == 0) {
       ASSERT_GT(blob->data.size(), 4u);
       // Check some bytes that univocally identify |data| as a JPEG File.
-      // The first two bytes must be the SOI marker.
-      // The next two bytes must be a marker, such as APPn, DTH etc.
-      // cf. Section B.2 at https://www.w3.org/Graphics/JPEG/itu-t81.pdf
+      // https://en.wikipedia.org/wiki/JPEG_File_Interchange_Format#File_format_structure
       EXPECT_EQ(0xFF, blob->data[0]);         // First SOI byte
       EXPECT_EQ(0xD8, blob->data[1]);         // Second SOI byte
-      EXPECT_EQ(0xFF, blob->data[2]);         // First byte of the next marker
+      EXPECT_EQ(0xFF, blob->data[2]);         // First JFIF-APP0 byte
+      EXPECT_EQ(0xE0, blob->data[3] & 0xF0);  // Second JFIF-APP0 byte
       OnCorrectPhotoTaken();
     } else if (strcmp("image/png", blob->mime_type.c_str()) == 0) {
       ASSERT_GT(blob->data.size(), 4u);
@@ -279,7 +278,8 @@
 
   void SetUp() override {
 #if defined(OS_CHROMEOS)
-    chromeos::PowerManagerClient::Initialize();
+    dbus_setter_->SetPowerManagerClient(
+        std::make_unique<chromeos::FakePowerManagerClient>());
 #endif
 #if defined(OS_ANDROID)
     static_cast<VideoCaptureDeviceFactoryAndroid*>(
@@ -292,12 +292,6 @@
 #endif
   }
 
-  void TearDown() override {
-#if defined(OS_CHROMEOS)
-    chromeos::PowerManagerClient::Shutdown();
-#endif
-  }
-
 #if defined(OS_WIN)
   bool UseWinMediaFoundation() {
     return std::get<1>(GetParam()) == WIN_MEDIA_FOUNDATION;
@@ -376,7 +370,7 @@
     }
 #if defined(OS_WIN)
     // Dump the camera model to help debugging.
-    // TODO(alaoui.rda@9ma1l.qjz9zk): remove after http://crbug.com/730068 is
+    // TODO(alaoui.rda@gmail.com): remove after http://crbug.com/730068 is
     // fixed.
     LOG(INFO) << "Using camera "
               << device_descriptors_->front().GetNameAndModel();
@@ -574,7 +568,7 @@
 #endif
 };
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     VideoCaptureDeviceTests,
     VideoCaptureDeviceTest,
     testing::Combine(testing::ValuesIn(kCaptureSizes),
@@ -886,4 +880,4 @@
 }
 #endif
 
-}  // namespace media
+};  // namespace media
--- a/media/capture/video/video_capture_jpeg_decoder_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/video_capture_jpeg_decoder_impl.cc	2019-05-17 18:53:34.152000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/capture/video/video_capture_jpeg_decoder_impl.h"
 
-#include "base/bind.h"
 #include "base/metrics/histogram_macros.h"
 #include "media/base/media_switches.h"
 
--- a/media/capture/video/video_capture_system.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/video_capture_system.h	2019-05-17 18:53:34.152000000 +0300
@@ -8,10 +8,6 @@
 #include "media/capture/video/video_capture_device_factory.h"
 #include "media/capture/video/video_capture_device_info.h"
 
-#if defined(OS_CHROMEOS)
-#include "media/capture/video/chromeos/mojo/cros_image_capture.mojom.h"
-#endif  // defined(OS_CHROMEOS)
-
 namespace media {
 
 // GetDeviceInfosAsync() should be called at least once before calling
@@ -33,12 +29,6 @@
   // wrong.
   virtual std::unique_ptr<VideoCaptureDevice> CreateDevice(
       const std::string& device_id) = 0;
-
-#if defined(OS_CHROMEOS)
-  // Pass the mojo request to bind with DeviceFactory for Chrome OS.
-  virtual void BindCrosImageCaptureRequest(
-      cros::mojom::CrosImageCaptureRequest request) = 0;
-#endif  // defined(OS_CHROMEOS)
 };
 
 }  // namespace media
--- a/media/capture/video/video_capture_system_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/video_capture_system_impl.cc	2019-05-17 18:53:34.152000000 +0300
@@ -4,18 +4,11 @@
 
 #include "media/capture/video/video_capture_system_impl.h"
 
-#include <utility>
-
 #include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "build/build_config.h"
 #include "media/base/bind_to_current_loop.h"
 
-#if defined(OS_CHROMEOS)
-#include "media/capture/video/chromeos/public/cros_features.h"
-#include "media/capture/video/chromeos/video_capture_device_factory_chromeos.h"
-#endif  // defined(OS_CHROMEOS)
-
 namespace {
 
 // Compares two VideoCaptureFormat by checking smallest frame_size area, then
@@ -163,16 +156,4 @@
   ProcessDeviceInfoRequest();
 }
 
-#if defined(OS_CHROMEOS)
-void VideoCaptureSystemImpl::BindCrosImageCaptureRequest(
-    cros::mojom::CrosImageCaptureRequest request) {
-  CHECK(factory_);
-
-  if (media::ShouldUseCrosCameraService()) {
-    static_cast<VideoCaptureDeviceFactoryChromeOS*>(factory_.get())
-        ->BindCrosImageCaptureRequest(std::move(request));
-  }
-}
-#endif  // defined(OS_CHROMEOS)
-
 }  // namespace media
--- a/media/capture/video/video_capture_system_impl.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/video_capture_system_impl.h	2019-05-17 18:53:34.152000000 +0300
@@ -7,10 +7,6 @@
 
 #include "media/capture/video/video_capture_system.h"
 
-#if defined(OS_CHROMEOS)
-#include "media/capture/video/chromeos/mojo/cros_image_capture.mojom.h"
-#endif  // defined(OS_CHROMEOS)
-
 namespace media {
 
 // Layer on top of VideoCaptureDeviceFactory that translates device descriptors
@@ -26,11 +22,6 @@
   std::unique_ptr<VideoCaptureDevice> CreateDevice(
       const std::string& device_id) override;
 
-#if defined(OS_CHROMEOS)
-  void BindCrosImageCaptureRequest(
-      cros::mojom::CrosImageCaptureRequest request) override;
-#endif  // defined(OS_CHROMEOS)
-
  private:
   using DeviceEnumQueue = std::list<DeviceInfoCallback>;
 
--- a/media/capture/video/video_frame_receiver.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/video_frame_receiver.h	2019-05-17 18:53:34.152000000 +0300
@@ -16,7 +16,7 @@
 // clients.
 class CAPTURE_EXPORT VideoFrameReceiver {
  public:
-  virtual ~VideoFrameReceiver() {}
+  virtual ~VideoFrameReceiver(){};
 
   // Tells the VideoFrameReceiver that the producer is going to subsequently use
   // the provided buffer as one of possibly many for frame delivery via
@@ -56,7 +56,6 @@
   virtual void OnLog(const std::string& message) = 0;
   virtual void OnStarted() = 0;
   virtual void OnStartedUsingGpuDecode() = 0;
-  virtual void OnStopped() = 0;
 };
 
 }  // namespace media
--- a/media/capture/video/video_frame_receiver_on_task_runner.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/video_frame_receiver_on_task_runner.cc	2019-05-17 18:53:34.152000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/capture/video/video_frame_receiver_on_task_runner.h"
 
-#include "base/bind.h"
 #include "base/single_thread_task_runner.h"
 
 namespace media {
@@ -40,8 +39,8 @@
 
 void VideoFrameReceiverOnTaskRunner::OnBufferRetired(int buffer_id) {
   task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&VideoFrameReceiver::OnBufferRetired, receiver_,
-                                buffer_id));
+      FROM_HERE,
+      base::Bind(&VideoFrameReceiver::OnBufferRetired, receiver_, buffer_id));
 }
 
 void VideoFrameReceiverOnTaskRunner::OnError(VideoCaptureError error) {
@@ -72,9 +71,4 @@
       base::BindOnce(&VideoFrameReceiver::OnStartedUsingGpuDecode, receiver_));
 }
 
-void VideoFrameReceiverOnTaskRunner::OnStopped() {
-  task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&VideoFrameReceiver::OnStopped, receiver_));
-}
-
 }  // namespace media
--- a/media/capture/video/video_frame_receiver_on_task_runner.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/video_frame_receiver_on_task_runner.h	2019-05-17 18:53:34.152000000 +0300
@@ -38,7 +38,6 @@
   void OnLog(const std::string& message) override;
   void OnStarted() override;
   void OnStartedUsingGpuDecode() override;
-  void OnStopped() override;
 
  private:
   const base::WeakPtr<VideoFrameReceiver> receiver_;
--- a/media/capture/video/win/sink_filter_win.h	2019-05-17 17:45:41.264000000 +0300
+++ b/media/capture/video/win/sink_filter_win.h	2019-05-17 18:53:34.152000000 +0300
@@ -22,7 +22,7 @@
 
 // Define GUID for I420. This is the color format we would like to support but
 // it is not defined in the DirectShow SDK.
-// http://msdn.m1cr050ft.qjz9zk/en-us/library/dd757532.aspx
+// http://msdn.microsoft.com/en-us/library/dd757532.aspx
 // 30323449-0000-0010-8000-00AA00389B71.
 const GUID kMediaSubTypeI420 = {
     0x30323449,
@@ -38,6 +38,7 @@
     0x0000,
     0x0010,
     {0x80, 0x00, 0x00, 0xaa, 0x00, 0x38, 0x9b, 0x71}};
+;
 
 // 16-bit grey-scale single plane formats provided by some depth cameras.
 const GUID kMediaSubTypeZ16 = {
--- a/media/capture/video/win/video_capture_device_factory_win.cc	2019-05-17 17:45:41.264000000 +0300
+++ b/media/capture/video/win/video_capture_device_factory_win.cc	2019-05-17 18:53:34.152000000 +0300
@@ -13,7 +13,6 @@
 #include <wrl.h>
 #include <wrl/client.h>
 
-#include "base/bind.h"
 #include "base/command_line.h"
 #include "base/feature_list.h"
 #include "base/metrics/histogram_macros.h"
@@ -130,7 +129,7 @@
     const std::vector<std::pair<GUID, GUID>>& attributes_data,
     int count,
     IMFAttributes** attributes) {
-  // Once https://bugs.ch40m1um.qjz9zk/p/chromium/issues/detail?id=791615 is fixed,
+  // Once https://bugs.chromium.org/p/chromium/issues/detail?id=791615 is fixed,
   // we must make sure that this method succeeds in capture_unittests context
   // when MediaFoundation is enabled.
   if (!VideoCaptureDeviceFactoryWin::PlatformSupportsMediaFoundation() ||
--- a/media/capture/video/win/video_capture_device_factory_win_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/win/video_capture_device_factory_win_unittest.cc	2019-05-17 18:53:34.152000000 +0300
@@ -9,7 +9,6 @@
 #include <mferror.h>
 #include <stddef.h>
 
-#include "base/bind.h"
 #include "base/strings/sys_string_conversions.h"
 #include "media/capture/video/win/video_capture_device_factory_win.h"
 #include "testing/gmock/include/gmock/gmock.h"
--- a/media/capture/video/win/video_capture_device_mf_win.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/win/video_capture_device_mf_win.cc	2019-05-17 18:53:34.152000000 +0300
@@ -12,7 +12,6 @@
 #include <thread>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/location.h"
 #include "base/memory/ref_counted.h"
 #include "base/strings/stringprintf.h"
@@ -581,8 +580,7 @@
       retry_delay_in_ms_(50),
       source_(source),
       engine_(engine),
-      is_started_(false),
-      has_sent_on_started_to_client_(false) {
+      is_started_(false) {
   DETACH_FROM_SEQUENCE(sequence_checker_);
 }
 
@@ -762,15 +760,6 @@
     return;
   }
 
-  // Note, that it is not sufficient to wait for
-  // MF_CAPTURE_ENGINE_PREVIEW_STARTED as an indicator that starting capture has
-  // succeeded. If the capture device is already in use by a different
-  // application, MediaFoundation will still emit
-  // MF_CAPTURE_ENGINE_PREVIEW_STARTED, and only after that raise an error
-  // event. For the lack of any other events indicating success, we have to wait
-  // for the first video frame to arrive before sending our |OnStarted| event to
-  // |client_|.
-  has_sent_on_started_to_client_ = false;
   hr = engine_->StartPreview();
   if (FAILED(hr)) {
     OnError(VideoCaptureError::kWinMediaFoundationEngineStartPreviewFailed,
@@ -781,6 +770,7 @@
   selected_video_capability_.reset(
       new CapabilityWin(best_match_video_capability));
 
+  client_->OnStarted();
   is_started_ = true;
 }
 
@@ -995,14 +985,7 @@
   base::AutoLock lock(lock_);
   DCHECK(data);
 
-  SendOnStartedIfNotYetSent();
-
   if (client_.get()) {
-    if (!has_sent_on_started_to_client_) {
-      has_sent_on_started_to_client_ = true;
-      client_->OnStarted();
-    }
-
     client_->OnIncomingCapturedData(
         data, length, selected_video_capability_->supported_format,
         GetCameraRotation(facing_mode_), reference_time, timestamp);
@@ -1034,9 +1017,6 @@
 void VideoCaptureDeviceMFWin::OnFrameDropped(
     VideoCaptureFrameDropReason reason) {
   base::AutoLock lock(lock_);
-
-  SendOnStartedIfNotYetSent();
-
   if (client_.get()) {
     client_->OnFrameDropped(reason);
   }
@@ -1069,11 +1049,4 @@
                    base::StringPrintf("VideoCaptureDeviceMFWin: %s", message));
 }
 
-void VideoCaptureDeviceMFWin::SendOnStartedIfNotYetSent() {
-  if (!client_ || has_sent_on_started_to_client_)
-    return;
-  has_sent_on_started_to_client_ = true;
-  client_->OnStarted();
-}
-
 }  // namespace media
--- a/media/capture/video/win/video_capture_device_mf_win.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/win/video_capture_device_mf_win.h	2019-05-17 18:53:34.152000000 +0300
@@ -115,7 +115,6 @@
   void OnError(VideoCaptureError error,
                const base::Location& from_here,
                const char* message);
-  void SendOnStartedIfNotYetSent();
 
   VideoFacingMode facing_mode_;
   CreateMFPhotoCallbackCB create_mf_photo_callback_;
@@ -136,7 +135,6 @@
   CapabilityList photo_capabilities_;
   std::unique_ptr<CapabilityWin> selected_photo_capability_;
   bool is_started_;
-  bool has_sent_on_started_to_client_;
   base::queue<TakePhotoCallback> video_stream_take_photo_callbacks_;
 
   SEQUENCE_CHECKER(sequence_checker_);
--- a/media/capture/video/win/video_capture_device_mf_win_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video/win/video_capture_device_mf_win_unittest.cc	2019-05-17 18:53:34.156000000 +0300
@@ -7,7 +7,6 @@
 #include <stddef.h>
 #include <wincodec.h>
 
-#include "base/bind.h"
 #include "media/capture/video/win/sink_filter_win.h"
 #include "media/capture/video/win/video_capture_device_factory_win.h"
 #include "media/capture/video/win/video_capture_device_mf_win.h"
@@ -780,17 +779,11 @@
 
   STDMETHOD(CopyAllItems)(IMFAttributes* pDest) override { return E_NOTIMPL; }
 
-  STDMETHOD(GetType)(MediaEventType* pmet) override {
-    *pmet = DoGetType();
-    return S_OK;
-  }
-  MOCK_METHOD0(DoGetType, MediaEventType());
+  STDMETHOD(GetType)(MediaEventType* pmet) override { return E_NOTIMPL; }
 
   STDMETHOD(GetExtendedType)(GUID* pguidExtendedType) override {
-    *pguidExtendedType = DoGetExtendedType();
-    return S_OK;
+    return E_NOTIMPL;
   }
-  MOCK_METHOD0(DoGetExtendedType, GUID());
 
   STDMETHOD(GetStatus)(HRESULT* status) override {
     *status = DoGetStatus();
@@ -1065,7 +1058,7 @@
   EXPECT_CALL(*(engine_.Get()), OnStopPreview());
 
   device_->AllocateAndStart(VideoCaptureParams(), std::move(client_));
-  capture_preview_sink_->sample_callback->OnSample(nullptr);
+
   device_->StopAndDeAllocate();
 }
 
@@ -1083,7 +1076,6 @@
   EXPECT_CALL(*media_event_error, DoGetStatus()).WillRepeatedly(Return(E_FAIL));
 
   device_->AllocateAndStart(VideoCaptureParams(), std::move(client_));
-  capture_preview_sink_->sample_callback->OnSample(nullptr);
   engine_->event_callback->OnEvent(media_event_error.get());
 }
 
@@ -1125,12 +1117,11 @@
         return S_OK;
       }));
 
-  auto mock_sink = base::MakeRefCounted<MockCapturePreviewSink>();
   EXPECT_CALL(*(engine_.Get()),
               DoGetSink(MF_CAPTURE_ENGINE_SINK_TYPE_PREVIEW, _))
-      .WillRepeatedly(Invoke([&mock_sink](MF_CAPTURE_ENGINE_SINK_TYPE sink_type,
-                                          IMFCaptureSink** sink) {
-        *sink = mock_sink.get();
+      .WillRepeatedly(Invoke(
+          [](MF_CAPTURE_ENGINE_SINK_TYPE sink_type, IMFCaptureSink** sink) {
+            *sink = new MockCapturePreviewSink();
         (*sink)->AddRef();
         return S_OK;
       }));
@@ -1138,7 +1129,6 @@
   EXPECT_CALL(*(engine_.Get()), OnStartPreview());
   EXPECT_CALL(*client_, OnStarted());
   device_->AllocateAndStart(VideoCaptureParams(), std::move(client_));
-  mock_sink->sample_callback->OnSample(nullptr);
 }
 
 // Allocates device with methods always failing with MF_E_INVALIDREQUEST and
@@ -1154,36 +1144,6 @@
   device_->AllocateAndStart(VideoCaptureParams(), std::move(client_));
 }
 
-TEST_F(VideoCaptureDeviceMFWinTest,
-       SendsOnErrorWithoutOnStartedIfDeviceIsBusy) {
-  if (ShouldSkipTest())
-    return;
-
-  PrepareMFDeviceWithOneVideoStream(MFVideoFormat_MJPG);
-
-  EXPECT_CALL(*(engine_.Get()), OnStartPreview());
-  EXPECT_CALL(*client_, OnStarted()).Times(0);
-  EXPECT_CALL(*client_, OnError(_, _, _));
-
-  scoped_refptr<MockMFMediaEvent> media_event_preview_started =
-      new MockMFMediaEvent();
-  ON_CALL(*media_event_preview_started, DoGetStatus())
-      .WillByDefault(Return(S_OK));
-  ON_CALL(*media_event_preview_started, DoGetType())
-      .WillByDefault(Return(MEExtendedType));
-  ON_CALL(*media_event_preview_started, DoGetExtendedType())
-      .WillByDefault(Return(MF_CAPTURE_ENGINE_PREVIEW_STARTED));
-
-  scoped_refptr<MockMFMediaEvent> media_event_error = new MockMFMediaEvent();
-  EXPECT_CALL(*media_event_error, DoGetStatus()).WillRepeatedly(Return(E_FAIL));
-
-  device_->AllocateAndStart(VideoCaptureParams(), std::move(client_));
-  // Even if the device is busy, MediaFoundation sends
-  // MF_CAPTURE_ENGINE_PREVIEW_STARTED before sending an error event.
-  engine_->event_callback->OnEvent(media_event_preview_started.get());
-  engine_->event_callback->OnEvent(media_event_error.get());
-}
-
 // Given an |IMFCaptureSource| offering a video stream without photo stream to
 // |VideoCaptureDevice|, when asking the photo state from |VideoCaptureDevice|
 // then expect the returned state to match the video resolution
@@ -1197,7 +1157,6 @@
   EXPECT_CALL(*client_, OnStarted());
 
   device_->AllocateAndStart(VideoCaptureParams(), std::move(client_));
-  capture_preview_sink_->sample_callback->OnSample(nullptr);
 
   VideoCaptureDevice::GetPhotoStateCallback get_photo_state_callback =
       base::BindOnce(&MockImageCaptureClient::DoOnGetPhotoState,
@@ -1227,8 +1186,6 @@
   EXPECT_CALL(*client_, OnStarted());
 
   device_->AllocateAndStart(VideoCaptureParams(), std::move(client_));
-  capture_preview_sink_->sample_callback->OnSample(nullptr);
-
   VideoCaptureDevice::GetPhotoStateCallback get_photo_state_callback =
       base::BindOnce(&MockImageCaptureClient::DoOnGetPhotoState,
                      image_capture_client_);
@@ -1259,7 +1216,6 @@
   EXPECT_CALL(*(engine_.Get()), OnTakePhoto());
 
   device_->AllocateAndStart(VideoCaptureParams(), std::move(client_));
-  capture_preview_sink_->sample_callback->OnSample(nullptr);
   VideoCaptureDevice::TakePhotoCallback take_photo_callback = base::BindOnce(
       &MockImageCaptureClient::DoOnPhotoTaken, image_capture_client_);
   device_->TakePhoto(std::move(take_photo_callback));
@@ -1275,7 +1231,7 @@
     {kMediaSubTypeINVZ, true, false},
     {MFVideoFormat_D16, true, true}};
 
-INSTANTIATE_TEST_SUITE_P(DepthCameraDeviceMFWinTests,
+INSTANTIATE_TEST_CASE_P(DepthCameraDeviceMFWinTests,
                          DepthCameraDeviceMFWinTest,
                          testing::ValuesIn(kDepthCamerasParams));
 
@@ -1320,7 +1276,6 @@
   VideoCaptureParams video_capture_params;
   video_capture_params.requested_format = format;
   device_->AllocateAndStart(video_capture_params, std::move(client_));
-  capture_preview_sink_->sample_callback->OnSample(nullptr);
 }
 
 }  // namespace media
--- a/media/capture/video/win/video_capture_device_utils_win.cc	2019-05-17 17:45:41.264000000 +0300
+++ b/media/capture/video/win/video_capture_device_utils_win.cc	2019-05-17 18:53:34.156000000 +0300
@@ -76,7 +76,7 @@
     if (get_rotation_state(&auto_rotation_state)) {
       // AR_ENABLED is defined as '0x0', while AR_STATE enumeration is defined
       // as bitwise. See the example codes in
-      // https://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/dn629263(v=vs.85).aspx.
+      // https://msdn.microsoft.com/en-us/library/windows/desktop/dn629263(v=vs.85).aspx.
       if (auto_rotation_state == AR_ENABLED) {
         return true;
       }
--- a/media/capture/video/win/video_capture_device_win.cc	2019-05-17 17:45:41.268000000 +0300
+++ b/media/capture/video/win/video_capture_device_win.cc	2019-05-17 18:53:34.156000000 +0300
@@ -360,7 +360,7 @@
 }
 
 // Release the format block for a media type.
-// http://msdn.m1cr050ft.qjz9zk/en-us/library/dd375432(VS.85).aspx
+// http://msdn.microsoft.com/en-us/library/dd375432(VS.85).aspx
 void VideoCaptureDeviceWin::ScopedMediaType::FreeMediaType(AM_MEDIA_TYPE* mt) {
   if (mt->cbFormat != 0) {
     CoTaskMemFree(mt->pbFormat);
@@ -376,7 +376,7 @@
 }
 
 // Delete a media type structure that was allocated on the heap.
-// http://msdn.m1cr050ft.qjz9zk/en-us/library/dd375432(VS.85).aspx
+// http://msdn.microsoft.com/en-us/library/dd375432(VS.85).aspx
 void VideoCaptureDeviceWin::ScopedMediaType::DeleteMediaType(
     AM_MEDIA_TYPE* mt) {
   if (mt != NULL) {
--- a/media/capture/video_capturer_source.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video_capturer_source.h	2019-05-17 18:53:34.156000000 +0300
@@ -107,12 +107,6 @@
   // may still occur after this call, so the caller must take care to
   // use refcounted or weak references in |new_frame_callback|.
   virtual void StopCapture() = 0;
-
-  // Indicates to the source that a frame has been dropped.
-  virtual void OnFrameDropped(media::VideoCaptureFrameDropReason reason) {}
-
-  // Sends a log message to the source.
-  virtual void OnLog(const std::string& message) {}
 };
 
 }  // namespace media
--- a/media/capture/video_capture_types.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/capture/video_capture_types.h	2019-05-17 18:53:34.156000000 +0300
@@ -201,17 +201,7 @@
   kWinMediaFoundationGetBufferByIndexReturnedNull = 14,
   kBufferPoolMaxBufferCountExceeded = 15,
   kBufferPoolBufferAllocationFailed = 16,
-  kVideoCaptureImplNotInStartedState = 17,
-  kVideoCaptureImplFailedToWrapDataAsMediaVideoFrame = 18,
-  kVideoTrackAdapterHasNoResolutionAdapters = 19,
-  kResolutionAdapterFrameIsNotValid = 20,
-  kResolutionAdapterWrappingFrameForCroppingFailed = 21,
-  kResolutionAdapterTimestampTooCloseToPrevious = 22,
-  kResolutionAdapterFrameRateIsHigherThanRequested = 23,
-  kResolutionAdapterHasNoCallbacks = 24,
-  kVideoTrackFrameDelivererNotEnabledReplacingWithBlackFrame = 25,
-  kRendererSinkFrameDelivererIsNotStarted = 26,
-  kMaxValue = 26
+  kMaxValue = 16
 };
 
 // Assert that the int:frequency mapping is correct.
--- a/media/cast/net/cast_transport.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/net/cast_transport.h	2019-05-17 18:53:34.164000000 +0300
@@ -76,7 +76,7 @@
   // from CastTransport.
   class Client {
    public:
-    virtual ~Client() {}
+    virtual ~Client(){};
 
     // Audio and Video transport status change is reported on this callback.
     virtual void OnStatusChanged(CastTransportStatus status) = 0;
--- a/media/cast/net/cast_transport_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/net/cast_transport_impl.cc	2019-05-17 18:53:34.164000000 +0300
@@ -9,7 +9,6 @@
 #include <string>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/single_thread_task_runner.h"
 #include "build/build_config.h"
 #include "media/cast/net/cast_transport_defines.h"
@@ -291,8 +290,7 @@
 
   transport_task_runner_->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&CastTransportImpl::SendRawEvents,
-                     weak_factory_.GetWeakPtr()),
+      base::Bind(&CastTransportImpl::SendRawEvents, weak_factory_.GetWeakPtr()),
       logging_flush_interval_);
 }
 
--- a/media/cast/net/cast_transport_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/net/cast_transport_impl_unittest.cc	2019-05-17 18:53:34.164000000 +0300
@@ -135,13 +135,13 @@
       CastTransportImplTest* cast_transport_sender_impl_test)
       : cast_transport_sender_impl_test_(cast_transport_sender_impl_test) {}
 
-  void OnStatusChanged(CastTransportStatus status) final {}
+  void OnStatusChanged(CastTransportStatus status) final{};
   void OnLoggingEventsReceived(
       std::unique_ptr<std::vector<FrameEvent>> frame_events,
       std::unique_ptr<std::vector<PacketEvent>> packet_events) final {
     CHECK(cast_transport_sender_impl_test_);
     cast_transport_sender_impl_test_->ReceivedLoggingEvents();
-  }
+  };
   void ProcessRtpPacket(std::unique_ptr<Packet> packet) final {}
 
  private:
--- a/media/cast/net/udp_packet_pipe.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/net/udp_packet_pipe.cc	2019-05-17 18:53:34.168000000 +0300
@@ -6,7 +6,6 @@
 
 #include <cstring>
 
-#include "base/bind.h"
 #include "base/callback.h"
 #include "base/logging.h"
 #include "base/memory/ptr_util.h"
--- a/media/cast/OWNERS	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/OWNERS	2019-05-17 18:53:34.156000000 +0300
@@ -1,3 +1,4 @@
 miu@chromium.org
+xjz@chromium.org
 
 # COMPONENT: Internals>Cast>Streaming
--- a/media/cast/receiver/audio_decoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/receiver/audio_decoder_unittest.cc	2019-05-17 18:53:34.172000000 +0300
@@ -239,10 +239,11 @@
 }
 
 #if !defined(OS_ANDROID)  // https://crbug.com/831999
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     AudioDecoderTestScenarios,
     AudioDecoderTest,
-    ::testing::Values(TestScenario(CODEC_AUDIO_PCM16, 1, 8000),
+    ::testing::Values(
+         TestScenario(CODEC_AUDIO_PCM16, 1, 8000),
                       TestScenario(CODEC_AUDIO_PCM16, 2, 48000),
                       TestScenario(CODEC_AUDIO_OPUS, 1, 8000),
                       TestScenario(CODEC_AUDIO_OPUS, 2, 48000)));
--- a/media/cast/receiver/video_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/receiver/video_decoder.cc	2019-05-17 18:53:34.172000000 +0300
@@ -199,8 +199,9 @@
     // Make sure this is a JSON string.
     if (!len || data[0] != '{')
       return NULL;
-    std::unique_ptr<base::Value> values(base::JSONReader::ReadDeprecated(
-        base::StringPiece(reinterpret_cast<char*>(data), len)));
+    base::JSONReader reader;
+    std::unique_ptr<base::Value> values(
+        reader.Read(base::StringPiece(reinterpret_cast<char*>(data), len)));
     if (!values)
       return NULL;
     base::DictionaryValue* dict = NULL;
--- a/media/cast/receiver/video_decoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/receiver/video_decoder_unittest.cc	2019-05-17 18:53:34.172000000 +0300
@@ -235,7 +235,7 @@
   WaitForAllVideoToBeDecoded();
 }
 
-INSTANTIATE_TEST_SUITE_P(,
+INSTANTIATE_TEST_CASE_P(,
                          VideoDecoderTest,
                          ::testing::Values(CODEC_VIDEO_VP8));
 
--- a/media/cast/sender/audio_encoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/sender/audio_encoder_unittest.cc	2019-05-17 18:53:34.172000000 +0300
@@ -225,7 +225,7 @@
 static const int64_t kMixedUnderruns[] = {31, -64, 4, 15, 9,  26, -53, 5,   8,
                                           -9, 7,   9, 32, 38, -4, 62,  -64, 3};
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     AudioEncoderTestScenarios,
     AudioEncoderTest,
     ::testing::Values(
--- a/media/cast/sender/audio_sender_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/sender/audio_sender_unittest.cc	2019-05-17 18:53:34.172000000 +0300
@@ -43,11 +43,11 @@
 
   void OnStatusChanged(CastTransportStatus status) final {
     EXPECT_EQ(TRANSPORT_STREAM_INITIALIZED, status);
-  }
+  };
   void OnLoggingEventsReceived(
       std::unique_ptr<std::vector<FrameEvent>> frame_events,
-      std::unique_ptr<std::vector<PacketEvent>> packet_events) final {}
-  void ProcessRtpPacket(std::unique_ptr<Packet> packet) final {}
+      std::unique_ptr<std::vector<PacketEvent>> packet_events) final{};
+  void ProcessRtpPacket(std::unique_ptr<Packet> packet) final{};
 
   DISALLOW_COPY_AND_ASSIGN(TransportClient);
 };
--- a/media/cast/sender/congestion_control_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/sender/congestion_control_unittest.cc	2019-05-17 18:53:34.172000000 +0300
@@ -55,10 +55,10 @@
       congestion_control_->UpdateRtt(rtt);
       congestion_control_->SendFrameToTransport(
           frame_id_, frame_size, testing_clock_.NowTicks());
-      task_runner_->PostDelayedTask(
-          FROM_HERE,
-          base::BindOnce(&CongestionControlTest::AckFrame,
-                         base::Unretained(this), frame_id_),
+      task_runner_->PostDelayedTask(FROM_HERE,
+                                    base::Bind(&CongestionControlTest::AckFrame,
+                                               base::Unretained(this),
+                                               frame_id_),
           ack_time);
       task_runner_->Sleep(frame_delay);
     }
--- a/media/cast/sender/external_video_encoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/sender/external_video_encoder.cc	2019-05-17 18:53:34.172000000 +0300
@@ -430,15 +430,16 @@
 
   // Note: This method can be called on any thread.
   void OnCreateSharedMemory(std::unique_ptr<base::SharedMemory> memory) {
-    task_runner_->PostTask(
-        FROM_HERE, base::BindOnce(&VEAClientImpl::OnReceivedSharedMemory, this,
-                                  std::move(memory)));
+    task_runner_->PostTask(FROM_HERE,
+                           base::Bind(&VEAClientImpl::OnReceivedSharedMemory,
+                                      this,
+                                      base::Passed(&memory)));
   }
 
   void OnCreateInputSharedMemory(std::unique_ptr<base::SharedMemory> memory) {
     task_runner_->PostTask(
         FROM_HERE, base::BindOnce(&VEAClientImpl::OnReceivedInputSharedMemory,
-                                  this, std::move(memory)));
+                                  this, base::Passed(&memory)));
   }
 
   void OnReceivedSharedMemory(std::unique_ptr<base::SharedMemory> memory) {
@@ -662,10 +663,13 @@
   if (!client_ || video_frame->visible_rect().size() != frame_size_)
     return false;
 
-  client_->task_runner()->PostTask(
-      FROM_HERE, base::BindOnce(&VEAClientImpl::EncodeVideoFrame, client_,
-                                video_frame, reference_time,
-                                key_frame_requested_, frame_encoded_callback));
+  client_->task_runner()->PostTask(FROM_HERE,
+                                   base::Bind(&VEAClientImpl::EncodeVideoFrame,
+                                              client_,
+                                              video_frame,
+                                              reference_time,
+                                              key_frame_requested_,
+                                              frame_encoded_callback));
   key_frame_requested_ = false;
   return true;
 }
@@ -758,10 +762,13 @@
                               std::move(vea), video_config.max_frame_rate,
                               std::move(wrapped_status_change_cb),
                               create_video_encode_memory_cb_);
-  client_->task_runner()->PostTask(
-      FROM_HERE,
-      base::BindOnce(&VEAClientImpl::Initialize, client_, frame_size_,
-                     codec_profile, bit_rate_, first_frame_id));
+  client_->task_runner()->PostTask(FROM_HERE,
+                                   base::Bind(&VEAClientImpl::Initialize,
+                                              client_,
+                                              frame_size_,
+                                              codec_profile,
+                                              bit_rate_,
+                                              first_frame_id));
 }
 
 SizeAdaptableExternalVideoEncoder::SizeAdaptableExternalVideoEncoder(
--- a/media/cast/sender/frame_sender.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/sender/frame_sender.cc	2019-05-17 18:53:34.172000000 +0300
@@ -10,7 +10,6 @@
 #include <utility>
 #include <vector>
 
-#include "base/bind.h"
 #include "base/macros.h"
 #include "base/numerics/safe_conversions.h"
 #include "base/trace_event/trace_event.h"
--- a/media/cast/sender/h264_vt_encoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/sender/h264_vt_encoder_unittest.cc	2019-05-17 18:53:34.176000000 +0300
@@ -269,7 +269,7 @@
 scoped_refptr<media::VideoFrame> H264VideoToolboxEncoderTest::frame_;
 FrameSenderConfig H264VideoToolboxEncoderTest::video_sender_config_;
 
-// Failed on mac-rel trybot. http://crbug.com/627260
+// Failed on mac_chromium_rel_ng trybot. http://crbug.com/627260
 TEST_F(H264VideoToolboxEncoderTest, DISABLED_CheckFrameMetadataSequence) {
   scoped_refptr<MetadataRecorder> metadata_recorder(new MetadataRecorder());
   VideoEncoder::FrameEncodedCallback cb = base::Bind(
@@ -299,7 +299,7 @@
 }
 
 #if BUILDFLAG(USE_PROPRIETARY_CODECS)
-// Failed on mac-rel trybot. http://crbug.com/627260
+// Failed on mac_chromium_rel_ng trybot. http://crbug.com/627260
 TEST_F(H264VideoToolboxEncoderTest, DISABLED_CheckFramesAreDecodable) {
   VideoDecoderConfig config(
       kCodecH264, H264PROFILE_MAIN, frame_->format(), VideoColorSpace(),
--- a/media/cast/sender/video_encoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/sender/video_encoder_unittest.cc	2019-05-17 18:53:34.176000000 +0300
@@ -379,9 +379,8 @@
 }
 }  // namespace
 
-INSTANTIATE_TEST_SUITE_P(,
-                         VideoEncoderTest,
-                         ::testing::ValuesIn(DetermineEncodersToTest()));
+INSTANTIATE_TEST_CASE_P(
+    , VideoEncoderTest, ::testing::ValuesIn(DetermineEncodersToTest()));
 
 }  // namespace cast
 }  // namespace media
--- a/media/cast/sender/video_sender_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/sender/video_sender_unittest.cc	2019-05-17 18:53:34.176000000 +0300
@@ -135,11 +135,11 @@
 
   void OnStatusChanged(CastTransportStatus status) final {
     EXPECT_EQ(TRANSPORT_STREAM_INITIALIZED, status);
-  }
+  };
   void OnLoggingEventsReceived(
       std::unique_ptr<std::vector<FrameEvent>> frame_events,
-      std::unique_ptr<std::vector<PacketEvent>> packet_events) final {}
-  void ProcessRtpPacket(std::unique_ptr<Packet> packet) final {}
+      std::unique_ptr<std::vector<PacketEvent>> packet_events) final{};
+  void ProcessRtpPacket(std::unique_ptr<Packet> packet) final{};
 
   DISALLOW_COPY_AND_ASSIGN(TransportClient);
 };
--- a/media/cast/test/cast_benchmarks.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/cast_benchmarks.cc	2019-05-17 18:53:34.176000000 +0300
@@ -453,14 +453,14 @@
 
   void OnStatusChanged(CastTransportStatus status) final {
     EXPECT_EQ(TRANSPORT_STREAM_INITIALIZED, status);
-  }
+  };
   void OnLoggingEventsReceived(
       std::unique_ptr<std::vector<FrameEvent>> frame_events,
-      std::unique_ptr<std::vector<PacketEvent>> packet_events) final {}
+      std::unique_ptr<std::vector<PacketEvent>> packet_events) final{};
   void ProcessRtpPacket(std::unique_ptr<Packet> packet) final {
     if (run_one_benchmark_)
       run_one_benchmark_->ReceivePacket(std::move(packet));
-  }
+  };
 
  private:
   RunOneBenchmark* const run_one_benchmark_;
@@ -651,8 +651,10 @@
       SearchVector ac = a.blend(c, static_cast<double>(x) / max);
       SearchVector v = ab.blend(ac, x == y ? 1.0 : static_cast<double>(y) / x);
       thread_num++;
-      (*threads)[thread_num % threads->size()]->task_runner()->PostTask(
-          FROM_HERE, base::BindOnce(&CastBenchmark::BinarySearch,
+      (*threads)[thread_num % threads->size()]
+          ->task_runner()
+          ->PostTask(FROM_HERE,
+                     base::Bind(&CastBenchmark::BinarySearch,
                                     base::Unretained(this), v, accuracy));
     } else {
       skip *= 2;
--- a/media/cast/test/end2end_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/end2end_unittest.cc	2019-05-17 18:53:34.176000000 +0300
@@ -892,17 +892,17 @@
 
   void OnStatusChanged(media::cast::CastTransportStatus status) final {
     EXPECT_EQ(TRANSPORT_STREAM_INITIALIZED, status);
-  }
+  };
   void OnLoggingEventsReceived(
       std::unique_ptr<std::vector<FrameEvent>> frame_events,
       std::unique_ptr<std::vector<PacketEvent>> packet_events) final {
     log_event_dispatcher_->DispatchBatchOfEvents(std::move(frame_events),
                                                  std::move(packet_events));
-  }
+  };
   void ProcessRtpPacket(std::unique_ptr<Packet> packet) final {
     if (e2e_test_)
       e2e_test_->ReceivePacket(std::move(packet));
-  }
+  };
 
  private:
   LogEventDispatcher* const log_event_dispatcher_;  // Not owned by this class.
--- a/media/cast/test/fake_media_source.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/fake_media_source.cc	2019-05-17 18:53:34.180000000 +0300
@@ -240,8 +240,9 @@
 
   if (!is_transcoding_audio() && !is_transcoding_video()) {
     // Send fake patterns.
-    task_runner_->PostTask(FROM_HERE,
-                           base::BindOnce(&FakeMediaSource::SendNextFakeFrame,
+    task_runner_->PostTask(
+        FROM_HERE,
+        base::Bind(&FakeMediaSource::SendNextFakeFrame,
                                           weak_factory_.GetWeakPtr()));
     return;
   }
@@ -260,9 +261,9 @@
   audio_converter_.reset(new media::AudioConverter(
       source_audio_params_, output_audio_params_, true));
   audio_converter_->AddInput(this);
-  task_runner_->PostTask(FROM_HERE,
-                         base::BindOnce(&FakeMediaSource::SendNextFrame,
-                                        weak_factory_.GetWeakPtr()));
+  task_runner_->PostTask(
+      FROM_HERE,
+      base::Bind(&FakeMediaSource::SendNextFrame, weak_factory_.GetWeakPtr()));
 }
 
 void FakeMediaSource::SendNextFakeFrame() {
@@ -311,7 +312,7 @@
 
   task_runner_->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&FakeMediaSource::SendNextFakeFrame,
+      base::Bind(&FakeMediaSource::SendNextFakeFrame,
                      weak_factory_.GetWeakPtr()),
       video_time - elapsed_time);
 }
@@ -408,8 +409,7 @@
   // Send next send.
   task_runner_->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&FakeMediaSource::SendNextFrame,
-                     weak_factory_.GetWeakPtr()),
+      base::Bind(&FakeMediaSource::SendNextFrame, weak_factory_.GetWeakPtr()),
       base::TimeDelta::FromMilliseconds(kAudioFrameMs));
 }
 
--- a/media/cast/test/receiver.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/receiver.cc	2019-05-17 18:53:34.180000000 +0300
@@ -16,7 +16,6 @@
 #include <utility>
 
 #include "base/at_exit.h"
-#include "base/bind.h"
 #include "base/command_line.h"
 #include "base/containers/circular_deque.h"
 #include "base/logging.h"
@@ -217,7 +216,7 @@
   void Start() final {
     AudioManager::Get()->GetTaskRunner()->PostTask(
         FROM_HERE,
-        base::BindOnce(&NaivePlayer::StartAudioOutputOnAudioManagerThread,
+        base::Bind(&NaivePlayer::StartAudioOutputOnAudioManagerThread,
                        base::Unretained(this)));
     // Note: No need to wait for audio polling to start since the push-and-pull
     // mechanism is synchronized via the |audio_playout_queue_|.
@@ -231,8 +230,9 @@
     DCHECK(!AudioManager::Get()->GetTaskRunner()->BelongsToCurrentThread());
     AudioManager::Get()->GetTaskRunner()->PostTask(
         FROM_HERE,
-        base::BindOnce(&NaivePlayer::StopAudioOutputOnAudioManagerThread,
-                       base::Unretained(this), &done));
+        base::Bind(&NaivePlayer::StopAudioOutputOnAudioManagerThread,
+                   base::Unretained(this),
+                   &done));
     done.Wait();
 
     // Now, stop receiving new frames.
--- a/media/cast/test/sender.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/sender.cc	2019-05-17 18:53:34.180000000 +0300
@@ -12,7 +12,6 @@
 
 #include "base/at_exit.h"
 #include "base/base_paths.h"
-#include "base/bind.h"
 #include "base/command_line.h"
 #include "base/files/file_path.h"
 #include "base/json/json_writer.h"
@@ -175,7 +174,7 @@
 
   void OnStatusChanged(media::cast::CastTransportStatus status) final {
     VLOG(1) << "Transport status: " << status;
-  }
+  };
   void OnLoggingEventsReceived(
       std::unique_ptr<std::vector<media::cast::FrameEvent>> frame_events,
       std::unique_ptr<std::vector<media::cast::PacketEvent>> packet_events)
@@ -183,7 +182,7 @@
     DCHECK(log_event_dispatcher_);
     log_event_dispatcher_->DispatchBatchOfEvents(std::move(frame_events),
                                                  std::move(packet_events));
-  }
+  };
   void ProcessRtpPacket(std::unique_ptr<media::cast::Packet> packet) final {}
 
  private:
@@ -315,18 +314,21 @@
   const int logging_duration_seconds = 10;
   io_message_loop.task_runner()->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&WriteLogsToFileAndDestroySubscribers, cast_environment,
-                     std::move(video_event_subscriber),
-                     std::move(audio_event_subscriber),
-                     std::move(video_log_file), std::move(audio_log_file)),
+      base::Bind(&WriteLogsToFileAndDestroySubscribers,
+                 cast_environment,
+                 base::Passed(&video_event_subscriber),
+                 base::Passed(&audio_event_subscriber),
+                 base::Passed(&video_log_file),
+                 base::Passed(&audio_log_file)),
       base::TimeDelta::FromSeconds(logging_duration_seconds));
 
   io_message_loop.task_runner()->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&WriteStatsAndDestroySubscribers, cast_environment,
-                     std::move(video_stats_subscriber),
-                     std::move(audio_stats_subscriber),
-                     std::move(offset_estimator)),
+      base::Bind(&WriteStatsAndDestroySubscribers,
+                 cast_environment,
+                 base::Passed(&video_stats_subscriber),
+                 base::Passed(&audio_stats_subscriber),
+                 base::Passed(&offset_estimator)),
       base::TimeDelta::FromSeconds(logging_duration_seconds));
 
   // CastSender initialization.
@@ -334,7 +336,7 @@
       media::cast::CastSender::Create(cast_environment, transport_sender.get());
   io_message_loop.task_runner()->PostTask(
       FROM_HERE,
-      base::BindOnce(&media::cast::CastSender::InitializeVideo,
+      base::Bind(&media::cast::CastSender::InitializeVideo,
                      base::Unretained(cast_sender.get()),
                      fake_media_source->get_video_config(),
                      base::Bind(&QuitLoopOnInitializationResult),
--- a/media/cast/test/simulator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/simulator.cc	2019-05-17 18:53:34.180000000 +0300
@@ -42,7 +42,6 @@
 
 #include "base/at_exit.h"
 #include "base/base_paths.h"
-#include "base/bind.h"
 #include "base/command_line.h"
 #include "base/containers/queue.h"
 #include "base/files/file_path.h"
@@ -145,18 +144,18 @@
 
   void OnStatusChanged(CastTransportStatus status) final {
     LOG(INFO) << "Cast transport status: " << status;
-  }
+  };
   void OnLoggingEventsReceived(
       std::unique_ptr<std::vector<FrameEvent>> frame_events,
       std::unique_ptr<std::vector<PacketEvent>> packet_events) final {
     DCHECK(log_event_dispatcher_);
     log_event_dispatcher_->DispatchBatchOfEvents(std::move(frame_events),
                                                  std::move(packet_events));
-  }
+  };
   void ProcessRtpPacket(std::unique_ptr<Packet> packet) final {
     if (packet_proxy_)
       packet_proxy_->ReceivePacket(std::move(packet));
-  }
+  };
 
  private:
   LogEventDispatcher* const log_event_dispatcher_;  // Not owned by this class.
--- a/media/cast/test/utility/in_process_receiver.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/utility/in_process_receiver.cc	2019-05-17 18:53:34.180000000 +0300
@@ -7,7 +7,6 @@
 #include <memory>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/memory/ptr_util.h"
 #include "base/synchronization/waitable_event.h"
--- a/media/cast/test/utility/tap_proxy.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/utility/tap_proxy.cc	2019-05-17 18:53:34.184000000 +0300
@@ -211,7 +211,8 @@
     last_printout = now;
   }
   base::ThreadTaskRunnerHandle::Get()->PostDelayedTask(
-      FROM_HERE, base::BindOnce(&CheckByteCounters),
+      FROM_HERE,
+      base::Bind(&CheckByteCounters),
       base::TimeDelta::FromMilliseconds(100));
 }
 
--- a/media/cast/test/utility/udp_proxy.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/utility/udp_proxy.cc	2019-05-17 18:53:34.184000000 +0300
@@ -9,7 +9,6 @@
 #include <utility>
 #include <vector>
 
-#include "base/bind.h"
 #include "base/containers/circular_deque.h"
 #include "base/logging.h"
 #include "base/macros.h"
@@ -82,7 +81,7 @@
     int64_t microseconds = static_cast<int64_t>(seconds * 1E6);
     task_runner_->PostDelayedTask(
         FROM_HERE,
-        base::BindOnce(&Buffer::ProcessBuffer, weak_factory_.GetWeakPtr()),
+        base::Bind(&Buffer::ProcessBuffer, weak_factory_.GetWeakPtr()),
         base::TimeDelta::FromMicroseconds(microseconds));
   }
 
@@ -147,8 +146,8 @@
     double seconds = GetDelay();
     task_runner_->PostDelayedTask(
         FROM_HERE,
-        base::BindOnce(&SimpleDelayBase::SendInternal,
-                       weak_factory_.GetWeakPtr(), std::move(packet)),
+        base::Bind(&SimpleDelayBase::SendInternal, weak_factory_.GetWeakPtr(),
+                   base::Passed(&packet)),
         base::TimeDelta::FromMicroseconds(static_cast<int64_t>(seconds * 1E6)));
   }
  protected:
@@ -248,7 +247,7 @@
     int64_t microseconds = static_cast<int64_t>(seconds * 1E6);
     task_runner_->PostDelayedTask(
         FROM_HERE,
-        base::BindOnce(&RandomSortedDelay::CauseExtraDelay,
+        base::Bind(&RandomSortedDelay::CauseExtraDelay,
                        weak_factory_.GetWeakPtr()),
         base::TimeDelta::FromMicroseconds(microseconds));
   }
@@ -278,7 +277,7 @@
     if (!buffer_.empty()) {
       task_runner_->PostDelayedTask(
           FROM_HERE,
-          base::BindOnce(&RandomSortedDelay::ProcessBuffer,
+          base::Bind(&RandomSortedDelay::ProcessBuffer,
                          weak_factory_.GetWeakPtr()),
           next_send_ - now);
     }
@@ -330,7 +329,7 @@
     int64_t microseconds = static_cast<int64_t>(seconds * 1E6);
     task_runner_->PostDelayedTask(
         FROM_HERE,
-        base::BindOnce(&NetworkGlitchPipe::Flip, weak_factory_.GetWeakPtr()),
+        base::Bind(&NetworkGlitchPipe::Flip, weak_factory_.GetWeakPtr()),
         base::TimeDelta::FromMicroseconds(microseconds));
   }
 
@@ -487,7 +486,7 @@
   rate_index_ = (rate_index_ + 1) % average_rates_.size();
   task_runner_->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&InterruptedPoissonProcess::UpdateRates,
+      base::Bind(&InterruptedPoissonProcess::UpdateRates,
                      weak_factory_.GetWeakPtr()),
       base::TimeDelta::FromSeconds(1));
 }
@@ -496,7 +495,7 @@
   on_state_ = false;
   task_runner_->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&InterruptedPoissonProcess::SwitchOn,
+      base::Bind(&InterruptedPoissonProcess::SwitchOn,
                      weak_factory_.GetWeakPtr()),
       NextEvent(switch_on_rate_));
 }
@@ -505,7 +504,7 @@
   on_state_ = true;
   task_runner_->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&InterruptedPoissonProcess::SwitchOff,
+      base::Bind(&InterruptedPoissonProcess::SwitchOff,
                      weak_factory_.GetWeakPtr()),
       NextEvent(switch_off_rate_));
 }
@@ -513,7 +512,7 @@
 void InterruptedPoissonProcess::SendPacket() {
   task_runner_->PostDelayedTask(
       FROM_HERE,
-      base::BindOnce(&InterruptedPoissonProcess::SendPacket,
+      base::Bind(&InterruptedPoissonProcess::SendPacket,
                      weak_factory_.GetWeakPtr()),
       NextEvent(send_rate_));
 
@@ -702,8 +701,11 @@
         base::WaitableEvent::ResetPolicy::AUTOMATIC,
         base::WaitableEvent::InitialState::NOT_SIGNALED);
     proxy_thread_.task_runner()->PostTask(
-        FROM_HERE, base::BindOnce(&UDPProxyImpl::Start, base::Unretained(this),
-                                  base::Unretained(&start_event), net_log));
+        FROM_HERE,
+        base::Bind(&UDPProxyImpl::Start,
+                   base::Unretained(this),
+                   base::Unretained(&start_event),
+                   net_log));
     start_event.Wait();
   }
 
@@ -713,7 +715,9 @@
         base::WaitableEvent::ResetPolicy::AUTOMATIC,
         base::WaitableEvent::InitialState::NOT_SIGNALED);
     proxy_thread_.task_runner()->PostTask(
-        FROM_HERE, base::BindOnce(&UDPProxyImpl::Stop, base::Unretained(this),
+        FROM_HERE,
+        base::Bind(&UDPProxyImpl::Stop,
+                   base::Unretained(this),
                                   base::Unretained(&stop_event)));
     stop_event.Wait();
     proxy_thread_.Stop();
--- a/media/cast/test/utility/udp_proxy.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/utility/udp_proxy.h	2019-05-17 18:53:34.184000000 +0300
@@ -21,11 +21,11 @@
 
 namespace net {
 class NetLog;
-}
+};
 
 namespace base {
 class TickClock;
-}
+};
 
 namespace media {
 namespace cast {
--- a/media/cast/test/utility/udp_proxy_main.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cast/test/utility/udp_proxy_main.cc	2019-05-17 18:53:34.184000000 +0300
@@ -126,7 +126,8 @@
     counter->last_printout = now;
   }
   base::ThreadTaskRunnerHandle::Get()->PostDelayedTask(
-      FROM_HERE, base::BindOnce(&CheckByteCounters),
+      FROM_HERE,
+      base::Bind(&CheckByteCounters),
       base::TimeDelta::FromMilliseconds(100));
 }
 
--- a/media/cdm/aes_decryptor_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/aes_decryptor_unittest.cc	2019-05-17 18:53:34.184000000 +0300
@@ -59,8 +59,7 @@
 }
 MATCHER(IsJSONDictionary, "") {
   std::string result(arg.begin(), arg.end());
-  std::unique_ptr<base::Value> root(
-      base::JSONReader().ReadToValueDeprecated(result));
+  std::unique_ptr<base::Value> root(base::JSONReader().ReadToValue(result));
   return (root.get() && root->type() == base::Value::Type::DICTIONARY);
 }
 MATCHER(IsNullTime, "") {
@@ -1084,12 +1083,12 @@
   EXPECT_EQ(kNumIterations, seen_sessions.size());
 }
 
-INSTANTIATE_TEST_SUITE_P(AesDecryptor,
+INSTANTIATE_TEST_CASE_P(AesDecryptor,
                          AesDecryptorTest,
                          testing::Values(TestType::kAesDecryptor));
 
 #if BUILDFLAG(ENABLE_LIBRARY_CDMS)
-INSTANTIATE_TEST_SUITE_P(CdmAdapter,
+INSTANTIATE_TEST_CASE_P(CdmAdapter,
                          AesDecryptorTest,
                          testing::Values(TestType::kCdmAdapter));
 #endif
--- a/media/cdm/cdm_adapter.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/cdm_adapter.cc	2019-05-17 18:53:34.184000000 +0300
@@ -14,7 +14,6 @@
 #include "base/metrics/histogram_functions.h"
 #include "base/metrics/histogram_macros.h"
 #include "base/numerics/safe_conversions.h"
-#include "base/strings/string_number_conversions.h"
 #include "base/threading/thread_task_runner_handle.h"
 #include "base/time/time.h"
 #include "base/trace_event/trace_event.h"
@@ -77,37 +76,25 @@
 ASSERT_ENUM_EQ(OutputProtection::ProtectionType::NONE, cdm::kProtectionNone);
 ASSERT_ENUM_EQ(OutputProtection::ProtectionType::HDCP, cdm::kProtectionHDCP);
 
-std::string CdmStatusToString(cdm::Status status) {
+inline std::ostream& operator<<(std::ostream& out, cdm::Status status) {
   switch (status) {
     case cdm::kSuccess:
-      return "kSuccess";
+      return out << "kSuccess";
     case cdm::kNoKey:
-      return "kNoKey";
+      return out << "kNoKey";
     case cdm::kNeedMoreData:
-      return "kNeedMoreData";
+      return out << "kNeedMoreData";
     case cdm::kDecryptError:
-      return "kDecryptError";
+      return out << "kDecryptError";
     case cdm::kDecodeError:
-      return "kDecodeError";
+      return out << "kDecodeError";
     case cdm::kInitializationError:
-      return "kInitializationError";
+      return out << "kInitializationError";
     case cdm::kDeferredInitialization:
-      return "kDeferredInitialization";
+      return out << "kDeferredInitialization";
   }
-
   NOTREACHED();
-  return "Invalid Status!";
-}
-
-inline std::ostream& operator<<(std::ostream& out, cdm::Status status) {
-  return out << CdmStatusToString(status);
-}
-
-std::string GetHexKeyId(const cdm::InputBuffer_2& buffer) {
-  if (buffer.key_id_size == 0)
-    return "N/A";
-
-  return base::HexEncode(buffer.key_id, buffer.key_id_size);
+  return out << "Invalid Status!";
 }
 
 void* GetCdmHost(int host_interface_version, void* user_data) {
@@ -449,6 +436,7 @@
                          const DecryptCB& decrypt_cb) {
   DVLOG(3) << __func__ << ": " << encrypted->AsHumanReadableString();
   DCHECK(task_runner_->BelongsToCurrentThread());
+  TRACE_EVENT1("media", "CdmAdapter::Decrypt", "stream_type", stream_type);
 
   ScopedCrashKeyString scoped_crash_key(&g_origin_crash_key, origin_string_);
 
@@ -456,13 +444,8 @@
   std::vector<cdm::SubsampleEntry> subsamples;
   std::unique_ptr<DecryptedBlockImpl> decrypted_block(new DecryptedBlockImpl());
 
-  TRACE_EVENT_BEGIN1("media", "CdmAdapter::Decrypt", "stream_type",
-                     stream_type);
   ToCdmInputBuffer(*encrypted, &subsamples, &input_buffer);
   cdm::Status status = cdm_->Decrypt(input_buffer, decrypted_block.get());
-  TRACE_EVENT_END2("media", "CdmAdapter::Decrypt", "key ID",
-                   GetHexKeyId(input_buffer), "status",
-                   CdmStatusToString(status));
 
   if (status != cdm::kSuccess) {
     DVLOG(1) << __func__ << ": status = " << status;
@@ -560,6 +543,7 @@
                                        const AudioDecodeCB& audio_decode_cb) {
   DVLOG(3) << __func__ << ": " << encrypted->AsHumanReadableString();
   DCHECK(task_runner_->BelongsToCurrentThread());
+  TRACE_EVENT0("media", "CdmAdapter::DecryptAndDecodeAudio");
 
   ScopedCrashKeyString scoped_crash_key(&g_origin_crash_key, origin_string_);
 
@@ -567,13 +551,9 @@
   std::vector<cdm::SubsampleEntry> subsamples;
   std::unique_ptr<AudioFramesImpl> audio_frames(new AudioFramesImpl());
 
-  TRACE_EVENT_BEGIN0("media", "CdmAdapter::DecryptAndDecodeAudio");
   ToCdmInputBuffer(*encrypted, &subsamples, &input_buffer);
   cdm::Status status =
       cdm_->DecryptAndDecodeSamples(input_buffer, audio_frames.get());
-  TRACE_EVENT_END2("media", "CdmAdapter::DecryptAndDecodeAudio", "key ID",
-                   GetHexKeyId(input_buffer), "status",
-                   CdmStatusToString(status));
 
   const Decryptor::AudioFrames empty_frames;
   if (status != cdm::kSuccess) {
@@ -598,6 +578,11 @@
                                        const VideoDecodeCB& video_decode_cb) {
   DVLOG(3) << __func__ << ": " << encrypted->AsHumanReadableString();
   DCHECK(task_runner_->BelongsToCurrentThread());
+  TRACE_EVENT1(
+      "media", "CdmAdapter::DecryptAndDecodeVideo", "buffer type",
+      encrypted->end_of_stream()
+          ? "end of stream"
+          : (encrypted->is_key_frame() ? "key frame" : "non-key frame"));
 
   ScopedCrashKeyString scoped_crash_key(&g_origin_crash_key, origin_string_);
 
@@ -605,17 +590,9 @@
   std::vector<cdm::SubsampleEntry> subsamples;
   std::unique_ptr<VideoFrameImpl> video_frame = helper_->CreateCdmVideoFrame();
 
-  TRACE_EVENT_BEGIN1(
-      "media", "CdmAdapter::DecryptAndDecodeVideo", "buffer type",
-      encrypted->end_of_stream()
-          ? "end of stream"
-          : (encrypted->is_key_frame() ? "key frame" : "non-key frame"));
   ToCdmInputBuffer(*encrypted, &subsamples, &input_buffer);
   cdm::Status status =
       cdm_->DecryptAndDecodeFrame(input_buffer, video_frame.get());
-  TRACE_EVENT_END2("media", "CdmAdapter::DecryptAndDecodeVideo", "key ID",
-                   GetHexKeyId(input_buffer), "status",
-                   CdmStatusToString(status));
 
   if (status != cdm::kSuccess) {
     DVLOG(1) << __func__ << ": status = " << status;
@@ -677,17 +654,19 @@
 }
 
 void CdmAdapter::SetTimer(int64_t delay_ms, void* context) {
-  DCHECK(task_runner_->BelongsToCurrentThread());
+  // TODO(crbug.com/887761): Use CHECKs for bug investigation. Change back to
+  // DCHECK after it's completed.
+  CHECK(task_runner_);
+  CHECK(task_runner_->BelongsToCurrentThread());
 
   auto delay = base::TimeDelta::FromMilliseconds(delay_ms);
   DVLOG(3) << __func__ << ": delay = " << delay << ", context = " << context;
   TRACE_EVENT2("media", "CdmAdapter::SetTimer", "delay_ms", delay_ms, "context",
                context);
 
-  task_runner_->PostDelayedTask(
-      FROM_HERE,
-      base::BindOnce(&CdmAdapter::TimerExpired, weak_factory_.GetWeakPtr(),
-                     context),
+  task_runner_->PostDelayedTask(FROM_HERE,
+                                base::Bind(&CdmAdapter::TimerExpired,
+                                           weak_factory_.GetWeakPtr(), context),
       delay);
 }
 
@@ -764,9 +743,6 @@
   DVLOG(2) << __func__ << ": session_id = " << session_id_str;
   DCHECK(task_runner_->BelongsToCurrentThread());
 
-  TRACE_EVENT2("media", "CdmAdapter::OnSessionMessage", "session_id",
-               session_id_str, "message_type", message_type);
-
   const uint8_t* message_ptr = reinterpret_cast<const uint8_t*>(message);
   session_message_cb_.Run(
       session_id_str, ToMediaMessageType(message_type),
@@ -782,10 +758,6 @@
   DVLOG(2) << __func__ << ": session_id = " << session_id_str;
   DCHECK(task_runner_->BelongsToCurrentThread());
 
-  TRACE_EVENT2("media", "CdmAdapter::OnSessionKeysChange", "session_id",
-               session_id_str, "has_additional_usable_key",
-               has_additional_usable_key);
-
   CdmKeysInfo keys;
   keys.reserve(keys_info_count);
   for (uint32_t i = 0; i < keys_info_count; ++i) {
@@ -816,20 +788,15 @@
            << ", new_expiry_time = " << new_expiry_time;
   DCHECK(task_runner_->BelongsToCurrentThread());
 
-  base::Time expiration = base::Time::FromDoubleT(new_expiry_time);
-  TRACE_EVENT2("media", "CdmAdapter::OnExpirationChange", "session_id",
-               session_id_str, "new_expiry_time", expiration);
-  session_expiration_update_cb_.Run(session_id_str, expiration);
+  session_expiration_update_cb_.Run(session_id_str,
+                                    base::Time::FromDoubleT(new_expiry_time));
 }
 
 void CdmAdapter::OnSessionClosed(const char* session_id,
                                  uint32_t session_id_size) {
   DCHECK(task_runner_->BelongsToCurrentThread());
 
-  std::string session_id_str(session_id, session_id_size);
-  TRACE_EVENT1("media", "CdmAdapter::OnSessionClosed", "session_id",
-               session_id_str);
-  session_closed_cb_.Run(session_id_str);
+  session_closed_cb_.Run(std::string(session_id, session_id_size));
 }
 
 void CdmAdapter::SendPlatformChallenge(const char* service_id,
--- a/media/cdm/cdm_adapter_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/cdm_adapter_factory.cc	2019-05-17 18:53:34.184000000 +0300
@@ -42,8 +42,8 @@
       CdmModule::GetInstance()->GetCreateCdmFunc();
   if (!create_cdm_func) {
     base::ThreadTaskRunnerHandle::Get()->PostTask(
-        FROM_HERE, base::BindOnce(cdm_created_cb, nullptr,
-                                  "CreateCdmFunc not available."));
+        FROM_HERE,
+        base::Bind(cdm_created_cb, nullptr, "CreateCdmFunc not available."));
     return;
   }
 
@@ -51,7 +51,7 @@
   if (!cdm_helper) {
     base::ThreadTaskRunnerHandle::Get()->PostTask(
         FROM_HERE,
-        base::BindOnce(cdm_created_cb, nullptr, "CDM helper creation failed."));
+        base::Bind(cdm_created_cb, nullptr, "CDM helper creation failed."));
     return;
   }
 
--- a/media/cdm/cdm_adapter_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/cdm_adapter_unittest.cc	2019-05-17 18:53:34.184000000 +0300
@@ -121,7 +121,7 @@
   CdmAdapterTestBase() {
     base::CommandLine::ForCurrentProcess()->AppendSwitchASCII(
         switches::kOverrideEnabledCdmInterfaceVersion,
-        base::NumberToString(GetCdmInterfaceVersion()));
+        base::IntToString(GetCdmInterfaceVersion()));
   }
 
   ~CdmAdapterTestBase() override { CdmModule::ResetInstanceForTesting(); }
@@ -360,13 +360,13 @@
 
 // Instantiate test cases
 
-INSTANTIATE_TEST_SUITE_P(CDM_9, CdmAdapterTestWithClearKeyCdm, Values(9));
-INSTANTIATE_TEST_SUITE_P(CDM_10, CdmAdapterTestWithClearKeyCdm, Values(10));
-INSTANTIATE_TEST_SUITE_P(CDM_11, CdmAdapterTestWithClearKeyCdm, Values(11));
-
-INSTANTIATE_TEST_SUITE_P(CDM_9, CdmAdapterTestWithMockCdm, Values(9));
-INSTANTIATE_TEST_SUITE_P(CDM_10, CdmAdapterTestWithMockCdm, Values(10));
-INSTANTIATE_TEST_SUITE_P(CDM_11, CdmAdapterTestWithMockCdm, Values(11));
+INSTANTIATE_TEST_CASE_P(CDM_9, CdmAdapterTestWithClearKeyCdm, Values(9));
+INSTANTIATE_TEST_CASE_P(CDM_10, CdmAdapterTestWithClearKeyCdm, Values(10));
+INSTANTIATE_TEST_CASE_P(CDM_11, CdmAdapterTestWithClearKeyCdm, Values(11));
+
+INSTANTIATE_TEST_CASE_P(CDM_9, CdmAdapterTestWithMockCdm, Values(9));
+INSTANTIATE_TEST_CASE_P(CDM_10, CdmAdapterTestWithMockCdm, Values(10));
+INSTANTIATE_TEST_CASE_P(CDM_11, CdmAdapterTestWithMockCdm, Values(11));
 
 // CdmAdapterTestWithClearKeyCdm Tests
 
--- a/media/cdm/cdm_proxy.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/cdm_proxy.h	2019-05-17 18:53:34.188000000 +0300
@@ -34,11 +34,8 @@
    public:
     Client();
     virtual ~Client();
-
-    // Called when there is a hardware reset. When hardware reset happens, all
-    // the hardware context is lost and all crypto sessions are destroyed. The
-    // CdmProxy returns to an uninitialized state and the caller must call
-    // Initialize() on the CdmProxy again to be able to continue using it.
+    // Called when there is a hardware reset and all the hardware context is
+    // lost.
     virtual void NotifyHardwareReset() = 0;
   };
 
@@ -85,10 +82,7 @@
       void(Status status, Protocol protocol, uint32_t crypto_session_id)>;
 
   // Initializes the proxy. The status and the return values of the call is
-  // reported to |init_cb|. All other methods should only be called after the
-  // proxy is fully initialized. Otherwise they may fail.
-  // Note: The proxy also needs to be reinitialized after hardware reset. See
-  // Client::NotifyHardwareReset() for details.
+  // reported to |init_cb|.
   virtual void Initialize(Client* client, InitializeCB init_cb) = 0;
 
   // Callback for Process(). |output_data| is the output of processing.
--- a/media/cdm/default_cdm_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/default_cdm_factory.cc	2019-05-17 18:53:34.188000000 +0300
@@ -50,7 +50,7 @@
   if (!ShouldCreateAesDecryptor(key_system)) {
     base::ThreadTaskRunnerHandle::Get()->PostTask(
         FROM_HERE,
-        base::BindOnce(cdm_created_cb, nullptr, "Unsupported key system."));
+        base::Bind(cdm_created_cb, nullptr, "Unsupported key system."));
     return;
   }
 
--- a/media/cdm/json_web_key.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/json_web_key.cc	2019-05-17 18:53:34.192000000 +0300
@@ -172,8 +172,7 @@
     return false;
   }
 
-  std::unique_ptr<base::Value> root(
-      base::JSONReader().ReadToValueDeprecated(jwk_set));
+  std::unique_ptr<base::Value> root(base::JSONReader().ReadToValue(jwk_set));
   if (!root.get() || root->type() != base::Value::Type::DICTIONARY) {
     DVLOG(1) << "Not valid JSON: " << jwk_set << ", root: " << root.get();
     return false;
@@ -242,8 +241,7 @@
     return false;
   }
 
-  std::unique_ptr<base::Value> root(
-      base::JSONReader().ReadToValueDeprecated(input));
+  std::unique_ptr<base::Value> root(base::JSONReader().ReadToValue(input));
   if (!root.get() || root->type() != base::Value::Type::DICTIONARY) {
     error_message->assign("Not valid JSON: ");
     error_message->append(ShortenTo64Characters(input));
@@ -377,7 +375,7 @@
   }
 
   std::unique_ptr<base::Value> root(
-      base::JSONReader().ReadToValueDeprecated(license_as_str));
+      base::JSONReader().ReadToValue(license_as_str));
   if (!root.get() || root->type() != base::Value::Type::DICTIONARY) {
     DVLOG(1) << "Not valid JSON: " << license_as_str;
     return false;
--- a/media/cdm/library_cdm/clear_key_cdm/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/library_cdm/clear_key_cdm/BUILD.gn	2019-05-17 18:53:34.192000000 +0300
@@ -38,7 +38,6 @@
     "//media:shared_memory_support",  # For media::AudioBus.
     "//media/cdm:cdm_api",  # For content_decryption_module.h
     "//media/cdm/library_cdm:cdm_host_proxy",
-    "//third_party/libaom:libaom_buildflags",
     "//third_party/libyuv",
     "//url",
   ]
--- a/media/cdm/library_cdm/clear_key_cdm/cdm_video_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/library_cdm/clear_key_cdm/cdm_video_decoder.cc	2019-05-17 18:53:34.192000000 +0300
@@ -25,21 +25,17 @@
 #include "media/cdm/cdm_type_conversion.h"
 #include "media/cdm/library_cdm/cdm_host_proxy.h"
 #include "media/media_buildflags.h"
-#include "third_party/libaom/libaom_buildflags.h"
+#include "third_party/libaom/av1_buildflags.h"
 #include "third_party/libyuv/include/libyuv/planar_functions.h"
 
 #if BUILDFLAG(ENABLE_LIBVPX)
 #include "media/filters/vpx_video_decoder.h"
 #endif
 
-#if BUILDFLAG(ENABLE_LIBAOM_DECODER)
+#if BUILDFLAG(ENABLE_AV1_DECODER)
 #include "media/filters/aom_video_decoder.h"
 #endif
 
-#if BUILDFLAG(ENABLE_DAV1D_DECODER)
-#include "media/filters/dav1d_video_decoder.h"
-#endif
-
 #if BUILDFLAG(ENABLE_FFMPEG)
 #include "media/filters/ffmpeg_video_decoder.h"
 #endif
@@ -304,10 +300,7 @@
     video_decoder.reset(new VpxVideoDecoder());
 #endif
 
-#if BUILDFLAG(ENABLE_DAV1D_DECODER)
-  if (config.codec == cdm::kCodecAv1)
-    video_decoder.reset(new Dav1dVideoDecoder(null_media_log.get()));
-#elif BUILDFLAG(ENABLE_LIBAOM_DECODER)
+#if BUILDFLAG(ENABLE_AV1_DECODER)
   if (config.codec == cdm::kCodecAv1)
     video_decoder.reset(new AomVideoDecoder(null_media_log.get()));
 #endif
--- a/media/cdm/library_cdm/clear_key_cdm/clear_key_cdm.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/cdm/library_cdm/clear_key_cdm/clear_key_cdm.cc	2019-05-17 18:53:34.192000000 +0300
@@ -945,7 +945,7 @@
   }
 #endif
   OnUnitTestComplete(true);
-}
+};
 
 void ClearKeyCdm::OnStorageId(uint32_t version,
                               const uint8_t* storage_id,
--- a/media/DEPS	2019-05-01 01:22:52.000000000 +0300
+++ b/media/DEPS	2019-05-17 18:53:34.016000000 +0300
@@ -12,7 +12,6 @@
   "+mojo/public/cpp/system/platform_handle.h",
   "+services/ws/public/cpp/gpu/context_provider_command_buffer.h",
   "+skia/ext",
-  "+third_party/dav1d",
   "+third_party/ffmpeg",
   "+third_party/libaom",
   "+third_party/libvpx",
@@ -33,8 +32,5 @@
 specific_include_rules = {
   "audio_manager_unittest.cc": [
     "+chromeos/dbus"
-  ],
-  "gpu_memory_buffer_video_frame_pool_unittest.cc": [
-    "+components/viz/test/test_context_provider.h",
   ]
 }
--- a/media/device_monitors/device_monitor_udev.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/device_monitors/device_monitor_udev.cc	2019-05-17 18:53:34.196000000 +0300
@@ -10,7 +10,6 @@
 
 #include <string>
 
-#include "base/bind.h"
 #include "base/macros.h"
 #include "base/system/system_monitor.h"
 #include "device/udev_linux/udev.h"
@@ -42,7 +41,7 @@
     : io_task_runner_(io_task_runner) {
   io_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&DeviceMonitorLinux::Initialize, base::Unretained(this)));
+      base::Bind(&DeviceMonitorLinux::Initialize, base::Unretained(this)));
 }
 
 DeviceMonitorLinux::~DeviceMonitorLinux() = default;
--- a/media/ffmpeg/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/ffmpeg/BUILD.gn	2019-05-17 18:53:34.196000000 +0300
@@ -30,7 +30,6 @@
   deps = [
     "//base",
     "//media/base",
-    "//media/formats",
     "//third_party/ffmpeg",
     "//third_party/ffmpeg:ffmpeg_features",
   ]
--- a/media/ffmpeg/ffmpeg_common.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/ffmpeg/ffmpeg_common.cc	2019-05-17 18:53:34.196000000 +0300
@@ -16,7 +16,6 @@
 #include "media/base/media_util.h"
 #include "media/base/video_decoder_config.h"
 #include "media/base/video_util.h"
-#include "media/formats/mp4/box_definitions.h"
 #include "media/media_buildflags.h"
 
 namespace media {
@@ -480,26 +479,11 @@
   // actually handle capabilities requests correctly. http://crbug.com/784610
   VideoCodecProfile profile = VIDEO_CODEC_PROFILE_UNKNOWN;
   switch (codec) {
-#if BUILDFLAG(USE_PROPRIETARY_CODECS)
-    case kCodecH264: {
-      profile = ProfileIDToVideoCodecProfile(codec_context->profile);
-      // if the profile is still unknown, try to extract it from
-      // the extradata using the internal parser
-      if (profile == VIDEO_CODEC_PROFILE_UNKNOWN && codec_context->extradata &&
-          codec_context->extradata_size) {
-        mp4::AVCDecoderConfigurationRecord avc_config;
-        if (avc_config.Parse(codec_context->extradata,
-                             codec_context->extradata_size)) {
-          profile = ProfileIDToVideoCodecProfile(avc_config.profile_indication);
-        }
-      }
-      // All the heuristics failed, let's assign a default profile
-      if (profile == VIDEO_CODEC_PROFILE_UNKNOWN)
-        profile = H264PROFILE_BASELINE;
-
+#if !BUILDFLAG(ENABLE_FFMPEG_VIDEO_DECODERS)
+    case kCodecH264:
       format = PIXEL_FORMAT_I420;
+      profile = H264PROFILE_BASELINE;
       break;
-    }
 #endif
     case kCodecVP8:
 #if !BUILDFLAG(ENABLE_FFMPEG_VIDEO_DECODERS)
--- a/media/ffmpeg/ffmpeg_common.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/ffmpeg/ffmpeg_common.h	2019-05-17 18:53:34.196000000 +0300
@@ -25,7 +25,7 @@
 // Include FFmpeg header files.
 extern "C" {
 // Temporarily disable possible loss of data warning.
-MSVC_PUSH_DISABLE_WARNING(4244)
+MSVC_PUSH_DISABLE_WARNING(4244);
 #include <libavcodec/avcodec.h>
 #include <libavformat/avformat.h>
 #include <libavformat/avio.h>
@@ -34,7 +34,7 @@
 #include <libavutil/log.h>
 #include <libavutil/mathematics.h>
 #include <libavutil/opt.h>
-MSVC_POP_WARNING()
+MSVC_POP_WARNING();
 }  // extern "C"
 
 namespace media {
--- a/media/ffmpeg/ffmpeg_common_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/ffmpeg/ffmpeg_common_unittest.cc	2019-05-17 18:53:34.196000000 +0300
@@ -296,31 +296,5 @@
   printf("</enum>\n");
 #endif
 }
-#if BUILDFLAG(USE_PROPRIETARY_CODECS)
-TEST_F(FFmpegCommonTest, VerifyH264Profile) {
-  // Open a file to get a real AVStreams from FFmpeg.
-  base::MemoryMappedFile file;
-  ASSERT_TRUE(file.Initialize(GetTestDataFilePath("bear-1280x720.mp4")));
-  InMemoryUrlProtocol protocol(file.data(), file.length(), false);
-  FFmpegGlue glue(&protocol);
-  ASSERT_TRUE(glue.OpenContext());
-  AVFormatContext* format_context = glue.format_context();
-
-  for (size_t i = 0; i < format_context->nb_streams; ++i) {
-    AVStream* stream = format_context->streams[i];
-    AVCodecParameters* codec_parameters = stream->codecpar;
-    AVMediaType codec_type = codec_parameters->codec_type;
-
-    if (codec_type == AVMEDIA_TYPE_VIDEO) {
-      VideoDecoderConfig video_config;
-      EXPECT_TRUE(AVStreamToVideoDecoderConfig(stream, &video_config));
-      EXPECT_EQ(H264PROFILE_HIGH, video_config.profile());
-    } else {
-      // Only process video.
-      continue;
-    }
-  }
-}
-#endif
 
 }  // namespace media
--- a/media/ffmpeg/ffmpeg_regression_tests.cc	2019-05-17 17:45:41.268000000 +0300
+++ b/media/ffmpeg/ffmpeg_regression_tests.cc	2019-05-17 18:53:34.196000000 +0300
@@ -5,12 +5,12 @@
 // Regression tests for FFmpeg.  Test files can be found in the internal media
 // test data directory:
 //
-//    https://chrome-internal.9oo91esource.qjz9zk/chrome/data/media
+//    https://chrome-internal.googlesource.com/chrome/data/media
 //
 // Simply add the custom_dep below to your gclient and sync:
 //
 //    "bmedia/test/data/internal":
-//        "https://chrome-internal.9oo91esource.qjz9zk/chrome/data/media"
+//        "https://chrome-internal.googlesource.com/chrome/data/media"
 //
 // Many of the files here do not cause issues outside of tooling, so you'll need
 // to run this test under ASAN, TSAN, and Valgrind to ensure that all issues are
@@ -69,16 +69,16 @@
 };
 
 #define FFMPEG_TEST_CASE_SEEKING(name, fn, init_status, end_status, seek_time) \
-  INSTANTIATE_TEST_SUITE_P(name, FFmpegRegressionTest,                         \
+  INSTANTIATE_TEST_CASE_P(name, FFmpegRegressionTest,                          \
                            testing::Values(RegressionTestData(                 \
-                               fn, init_status, end_status, seek_time)))
+                              fn, init_status, end_status, seek_time)));
 
 #define FFMPEG_TEST_CASE(name, fn, init_status, end_status) \
   FFMPEG_TEST_CASE_SEEKING(name, fn, init_status, end_status, kNoTimestamp)
 
 #define FLAKY_FFMPEG_TEST_CASE(name, fn)                            \
-  INSTANTIATE_TEST_SUITE_P(FLAKY_##name, FlakyFFmpegRegressionTest, \
-                           testing::Values(FlakyRegressionTestData(fn)))
+    INSTANTIATE_TEST_CASE_P(FLAKY_##name, FlakyFFmpegRegressionTest, \
+                            testing::Values(FlakyRegressionTestData(fn)));
 
 // Test cases from issues.
 FFMPEG_TEST_CASE(Cr47325, "security/47325.mp4", PIPELINE_OK, PIPELINE_OK);
@@ -171,8 +171,8 @@
                  DEMUXER_ERROR_COULD_NOT_OPEN);
 FFMPEG_TEST_CASE(Cr447860,
                  "security/447860.webm",
-                 PIPELINE_ERROR_DECODE,
-                 PIPELINE_ERROR_DECODE);
+                 PIPELINE_OK,
+                 PIPELINE_OK);
 FFMPEG_TEST_CASE(Cr449958,
                  "security/449958.webm",
                  PIPELINE_OK,
@@ -183,8 +183,8 @@
                  PIPELINE_ERROR_DECODE);
 FFMPEG_TEST_CASE(Cr532967,
                  "security/532967.webm",
-                 PIPELINE_ERROR_DECODE,
-                 PIPELINE_ERROR_DECODE);
+                 PIPELINE_OK,
+                 PIPELINE_OK);
 // TODO(tguilbert): update PIPELINE_ERROR_DECODE to
 // AUDIO_RENDERER_ERROR_IMPLICIT_CONFIG_CHANGE once the status is created.
 FFMPEG_TEST_CASE(Cr599625,
--- a/media/filters/android/media_codec_audio_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/android/media_codec_audio_decoder.cc	2019-05-17 18:53:34.196000000 +0300
@@ -8,7 +8,6 @@
 
 #include "base/android/build_info.h"
 #include "base/bind.h"
-#include "base/bind_helpers.h"
 #include "base/callback_helpers.h"
 #include "base/logging.h"
 #include "base/single_thread_task_runner.h"
--- a/media/filters/android/video_frame_extractor_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/android/video_frame_extractor_unittest.cc	2019-05-17 18:53:34.200000000 +0300
@@ -6,7 +6,6 @@
 
 #include <memory>
 
-#include "base/bind.h"
 #include "base/files/file_util.h"
 #include "base/files/scoped_temp_dir.h"
 #include "base/run_loop.h"
--- a/media/filters/audio_decoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/audio_decoder_unittest.cc	2019-05-17 18:53:34.200000000 +0300
@@ -363,7 +363,7 @@
     // Generate a lossy hash of the audio used for comparison across platforms.
     AudioHash audio_hash;
     audio_hash.Update(output.get(), output->frames());
-    EXPECT_TRUE(audio_hash.IsEquivalent(sample_info.hash, 0.03))
+    EXPECT_TRUE(audio_hash.IsEquivalent(sample_info.hash, 0.02))
         << "Audio hashes differ. Expected: " << sample_info.hash
         << " Actual: " << audio_hash.ToString();
 
@@ -642,12 +642,12 @@
   EXPECT_EQ(DecodeStatus::DECODE_ERROR, last_decode_status());
 }
 
-INSTANTIATE_TEST_SUITE_P(FFmpeg,
+INSTANTIATE_TEST_CASE_P(FFmpeg,
                          AudioDecoderTest,
                          Combine(Values(FFMPEG), ValuesIn(kFFmpegTestParams)));
 
 #if defined(OS_ANDROID)
-INSTANTIATE_TEST_SUITE_P(MediaCodec,
+INSTANTIATE_TEST_CASE_P(MediaCodec,
                          AudioDecoderTest,
                          Combine(Values(MEDIA_CODEC),
                                  ValuesIn(kMediaCodecTestParams)));
--- a/media/filters/audio_timestamp_validator_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/audio_timestamp_validator_unittest.cc	2019-05-17 18:53:34.200000000 +0300
@@ -246,7 +246,7 @@
 // Test with cartesian product of various output delay, codec delay, and front
 // discard values. These simulate configurations for different containers/codecs
 // which present different challenges when building timestamp expectations.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     ,
     AudioTimestampValidatorTest,
     ::testing::Combine(
--- a/media/filters/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/BUILD.gn	2019-05-17 18:53:34.196000000 +0300
@@ -47,6 +47,8 @@
     "frame_processor.h",
     "gpu_video_decoder.cc",
     "gpu_video_decoder.h",
+    "jpeg_parser.cc",
+    "jpeg_parser.h",
     "memory_data_source.cc",
     "memory_data_source.h",
     "offloading_video_decoder.cc",
@@ -94,19 +96,15 @@
     "//media:subcomponent_config",
   ]
 
-  public_deps = [
-    ":jpeg_parser",
-  ]
-
   deps = [
     "//cc/base",  # For MathUtil.
     "//gpu/command_buffer/common",
-    "//media:media_buildflags",
     "//media/base",
     "//media/cdm",
     "//media/formats",
     "//media/video",
     "//skia",
+    "//third_party/libaom:av1_buildflags",
     "//third_party/libyuv",
   ]
 
@@ -155,7 +153,7 @@
     deps += [ "//third_party/libvpx" ]
   }
 
-  if (enable_libaom_decoder) {
+  if (enable_av1_decoder) {
     sources += [
       "aom_video_decoder.cc",
       "aom_video_decoder.h",
@@ -163,14 +161,6 @@
     deps += [ "//third_party/libaom" ]
   }
 
-  if (enable_dav1d_decoder) {
-    sources += [
-      "dav1d_video_decoder.cc",
-      "dav1d_video_decoder.h",
-    ]
-    deps += [ "//third_party/dav1d" ]
-  }
-
   if (media_use_ffmpeg) {
     if (proprietary_codecs) {
       sources += [
@@ -224,41 +214,10 @@
       "fuchsia/fuchsia_video_decoder.cc",
       "fuchsia/fuchsia_video_decoder.h",
     ]
-    deps += [
-      "//third_party/fuchsia-sdk/sdk:media",
-      "//third_party/fuchsia-sdk/sdk:mediacodec",
-    ]
+    deps += [ "//third_party/fuchsia-sdk/sdk:mediacodec" ]
   }
 }
 
-# This component allows other targets to use the JPEG parser as a standalone,
-# general-purpose utility without having to pull all of //media as a dependency
-# (which could potentially result in cycles).
-component("jpeg_parser") {
-  output_name = "media_filters_jpeg_parser"
-  sources = [
-    "jpeg_parser.cc",
-    "jpeg_parser.h",
-  ]
-  defines = [ "IS_JPEG_PARSER_IMPL" ]
-  deps = [
-    "//base",
-  ]
-}
-
-source_set("jpeg_parser_unit_tests") {
-  testonly = true
-  sources = [
-    "jpeg_parser_unittest.cc",
-  ]
-  deps = [
-    ":jpeg_parser",
-    "//base",
-    "//media:test_support",
-    "//testing/gtest",
-  ]
-}
-
 source_set("perftests") {
   testonly = true
   sources = []
@@ -318,6 +277,7 @@
     "frame_buffer_pool_unittest.cc",
     "frame_processor_unittest.cc",
     "ivf_parser_unittest.cc",
+    "jpeg_parser_unittest.cc",
     "memory_data_source_unittest.cc",
     "offloading_video_decoder_unittest.cc",
     "pipeline_controller_unittest.cc",
@@ -333,7 +293,6 @@
   ]
 
   deps = [
-    ":jpeg_parser_unit_tests",
     "//base/test:test_support",
     "//media:test_support",
     "//testing/gmock",
@@ -399,14 +358,10 @@
     sources += [ "vpx_video_decoder_unittest.cc" ]
   }
 
-  if (enable_libaom_decoder) {
+  if (enable_av1_decoder) {
     sources += [ "aom_video_decoder_unittest.cc" ]
   }
 
-  if (enable_dav1d_decoder) {
-    sources += [ "dav1d_video_decoder_unittest.cc" ]
-  }
-
   if (current_cpu != "arm" && is_linux) {
     sources += [ "h264_bitstream_buffer_unittest.cc" ]
   }
--- a/media/filters/chunk_demuxer.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/chunk_demuxer.cc	2019-05-17 18:53:34.200000000 +0300
@@ -1337,7 +1337,7 @@
 // static
 MediaTrack::Id ChunkDemuxer::GenerateMediaTrackId() {
   static unsigned g_track_count = 0;
-  return base::NumberToString(++g_track_count);
+  return base::UintToString(++g_track_count);
 }
 
 ChunkDemuxerStream* ChunkDemuxer::CreateDemuxerStream(
--- a/media/filters/chunk_demuxer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/chunk_demuxer_unittest.cc	2019-05-17 18:53:34.204000000 +0300
@@ -4727,10 +4727,10 @@
 
 // Though most of these ChunkDemuxerTests use WebM (where PTS==DTS), we still
 // need to ensure that both versions of the buffering API work.
-INSTANTIATE_TEST_SUITE_P(LegacyByDts,
+INSTANTIATE_TEST_CASE_P(LegacyByDts,
                          ChunkDemuxerTest,
                          Values(BufferingApi::kLegacyByDts));
-INSTANTIATE_TEST_SUITE_P(NewByPts,
+INSTANTIATE_TEST_CASE_P(NewByPts,
                          ChunkDemuxerTest,
                          Values(BufferingApi::kNewByPts));
 
--- a/media/filters/decoder_selector_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/decoder_selector_unittest.cc	2019-05-17 18:53:34.204000000 +0300
@@ -309,7 +309,7 @@
 using DecoderSelectorTestParams =
     ::testing::Types<AudioDecoderSelectorTestParam,
                      VideoDecoderSelectorTestParam>;
-TYPED_TEST_SUITE(DecoderSelectorTest, DecoderSelectorTestParams);
+TYPED_TEST_CASE(DecoderSelectorTest, DecoderSelectorTestParams);
 
 // Tests for clear streams. CDM will not be used for clear streams so
 // DecryptorCapability doesn't really matter.
--- a/media/filters/decoder_stream.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/decoder_stream.cc	2019-05-17 18:53:34.204000000 +0300
@@ -346,21 +346,14 @@
     DCHECK(decoder_);
   }
 
-  auto* original_stream = stream_;
-  bool is_decrypting_demuxer_stream_selected = !!decrypting_demuxer_stream;
-
+  decoder_ = std::move(selected_decoder);
   if (decrypting_demuxer_stream) {
-    // Override |stream_| with the decrypted stream provided by
-    // DecryptingDemuxerStream.
     decrypting_demuxer_stream_ = std::move(decrypting_demuxer_stream);
     stream_ = decrypting_demuxer_stream_.get();
-
     // Also clear |cdm_context_|, it shouldn't be passed during reinitialize for
-    // a stream that isn't encrypted.
+    // a sream that isn't encrypted.
     cdm_context_ = nullptr;
   }
-
-  decoder_ = std::move(selected_decoder);
   if (decoder_change_observer_cb_)
     decoder_change_observer_cb_.Run(decoder_.get());
 
@@ -396,13 +389,6 @@
       "is_platform_" + GetStreamTypeString() + "_decoder",
       decoder_->IsPlatformDecoder());
 
-  if (is_decrypting_demuxer_stream_selected) {
-    MEDIA_LOG(INFO, media_log_)
-        << "Selected DecryptingDemuxerStream for " << GetStreamTypeString()
-        << " decryption, config: "
-        << traits_->GetDecoderConfig(original_stream).AsHumanReadableString();
-  }
-
   MEDIA_LOG(INFO, media_log_)
       << "Selected " << decoder_->GetDisplayName() << " for "
       << GetStreamTypeString() << " decoding, config: "
@@ -958,14 +944,13 @@
   if (ready_outputs_.size() >= static_cast<size_t>(GetMaxDecodeRequests()))
     return;
 
-  // Retain a copy to avoid dangling reference in OnPreparedOutputReady().
-  const scoped_refptr<Output> output = unprepared_outputs_.front();
-  TRACE_EVENT_ASYNC_BEGIN1("media", GetPrepareTraceString<StreamType>(), this,
-                           "timestamp_us",
-                           output->timestamp().InMicroseconds());
+  TRACE_EVENT_ASYNC_BEGIN1(
+      "media", GetPrepareTraceString<StreamType>(), this, "timestamp_us",
+      unprepared_outputs_.front()->timestamp().InMicroseconds());
   preparing_output_ = true;
   prepare_cb_.Run(
-      output, base::BindOnce(&DecoderStream<StreamType>::OnPreparedOutputReady,
+      unprepared_outputs_.front(),
+      base::BindOnce(&DecoderStream<StreamType>::OnPreparedOutputReady,
                              prepare_weak_factory_.GetWeakPtr()));
 }
 
--- a/media/filters/decoder_stream_traits.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/decoder_stream_traits.cc	2019-05-17 18:53:34.204000000 +0300
@@ -167,7 +167,7 @@
     DemuxerStream* stream) {
   DCHECK(stream);
   last_keyframe_timestamp_ = base::TimeDelta();
-  frame_metadata_.clear();
+  frames_to_drop_.clear();
 }
 
 void DecoderStreamTraits<DemuxerStream::VIDEO>::OnDecode(
@@ -177,10 +177,8 @@
     return;
   }
 
-  frame_metadata_[buffer.timestamp()] = {
-      buffer.discard_padding().first == kInfiniteDuration,  // should_drop
-      buffer.duration(),                                    // duration
-  };
+  if (buffer.discard_padding().first == kInfiniteDuration)
+    frames_to_drop_.insert(buffer.timestamp());
 
   if (!buffer.is_key_frame())
     return;
@@ -199,31 +197,17 @@
 
 PostDecodeAction DecoderStreamTraits<DemuxerStream::VIDEO>::OnDecodeDone(
     const scoped_refptr<OutputType>& buffer) {
-  auto it = frame_metadata_.find(buffer->timestamp());
-
-  // If the frame isn't in |frame_metadata_| it probably was erased below on a
-  // previous cycle. We could drop these, but today our video algorithm will put
-  // them back into sorted order or drop the frame if a later frame has already
-  // been rendered.
-  if (it == frame_metadata_.end())
-    return PostDecodeAction::DELIVER;
-
-  auto action = it->second.should_drop ? PostDecodeAction::DROP
-                                       : PostDecodeAction::DELIVER;
-
-  // Provide duration information to help the rendering algorithm on the very
-  // first and very last frames.
-  if (it->second.duration != kNoTimestamp) {
-    buffer->metadata()->SetTimeDelta(VideoFrameMetadata::FRAME_DURATION,
-                                     it->second.duration);
+  auto it = frames_to_drop_.find(buffer->timestamp());
+  if (it != frames_to_drop_.end()) {
+    // We erase from the beginning onward to our target frame since frames
+    // should be returned in presentation order. It's possible to accumulate
+    // entries in this queue if playback begins at a non-keyframe; those frames
+    // may never be returned from the decoder.
+    frames_to_drop_.erase(frames_to_drop_.begin(), it + 1);
+    return PostDecodeAction::DROP;
   }
 
-  // We erase from the beginning onward to our target frame since frames should
-  // be returned in presentation order. It's possible to accumulate entries in
-  // this queue if playback begins at a non-keyframe; those frames may never be
-  // returned from the decoder.
-  frame_metadata_.erase(frame_metadata_.begin(), it + 1);
-  return action;
+  return PostDecodeAction::DELIVER;
 }
 
 }  // namespace media
--- a/media/filters/decoder_stream_traits.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/decoder_stream_traits.h	2019-05-17 18:53:34.204000000 +0300
@@ -5,7 +5,7 @@
 #ifndef MEDIA_FILTERS_DECODER_STREAM_TRAITS_H_
 #define MEDIA_FILTERS_DECODER_STREAM_TRAITS_H_
 
-#include "base/containers/flat_map.h"
+#include "base/containers/flat_set.h"
 #include "base/time/time.h"
 #include "media/base/audio_decoder.h"
 #include "media/base/audio_decoder_config.h"
@@ -104,14 +104,7 @@
  private:
   base::TimeDelta last_keyframe_timestamp_;
   MovingAverage keyframe_distance_average_;
-
-  // Tracks the duration of incoming packets over time.
-  struct FrameMetadata {
-    bool should_drop = false;
-    base::TimeDelta duration = kNoTimestamp;
-  };
-  base::flat_map<base::TimeDelta, FrameMetadata> frame_metadata_;
-
+  base::flat_set<base::TimeDelta> frames_to_drop_;
   PipelineStatistics stats_;
 };
 
--- a/media/filters/decrypting_demuxer_stream.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/decrypting_demuxer_stream.h	2019-05-17 18:53:34.204000000 +0300
@@ -64,48 +64,10 @@
   bool SupportsConfigChanges() override;
 
  private:
-  // See this link for a detailed state diagram: http://shortn/_1nXgoVIrps
-  // Each line has a number that corresponds to an action, status or function
-  // that results in a state change. These actions, etc are all listed below.
-  // NOTE: invoking Reset() will cause a transition from any state except
-  //       kUninitialized to the kIdle state.
-  //
-  //    +----------------+         +---------------------------------+
-  //    | kUninitialized |         | Any State Except kUninitialized |
-  //    +----------------+         +---------------------------------+
-  //             |                                  |
-  //             0                                  7
-  //             v                                  v
-  //         +-------+                          +-------+
-  //         | kIdle |<-------+-+               | kIdle |
-  //         +-------+        | |               +-------+
-  //             |            | |
-  //             1            4 5
-  //             v            | |
-  //  +---------------------+ | |
-  //  | kPendingDemuxerRead |-+ |
-  //  +---------------------+   |
-  //             |              |
-  //             2              |
-  //             v              |
-  //    +-----------------+     |
-  // +->| kPendingDecrypt |-----+
-  // |  +-----------------+
-  // |           |
-  // 6           3
-  // |           v
-  // |   +----------------+
-  // +---| kWaitingForKey |
-  //     +----------------+
-  //
-  // 1) Read()
-  // 2) Has encrypted buffer
-  // 3) kNoKey
-  // 4) kConfigChanged, kAborted, has clear buffer or end of stream
-  // 5) kSuccess or kAborted
-  // 6) OnKeyAdded()
-  // 7) Reset()
-
+  // For a detailed state diagram please see this link: http://goo.gl/8jAok
+  // TODO(xhwang): Add a ASCII state diagram in this file after this class
+  // stabilizes.
+  // TODO(xhwang): Update this diagram for DecryptingDemuxerStream.
   enum State {
     kUninitialized = 0,
     kIdle,
--- a/media/filters/demuxer_perftest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/demuxer_perftest.cc	2019-05-17 18:53:34.208000000 +0300
@@ -227,7 +227,7 @@
 
 // For simplicity we only test containers with above 2% daily usage as measured
 // by the Media.DetectedContainer histogram.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     /* no prefix */,
     DemuxerPerfTest,
     testing::ValuesIn(kDemuxerTestFiles));
--- a/media/filters/fake_video_decoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/fake_video_decoder_unittest.cc	2019-05-17 18:53:34.208000000 +0300
@@ -241,11 +241,11 @@
   DISALLOW_COPY_AND_ASSIGN(FakeVideoDecoderTest);
 };
 
-INSTANTIATE_TEST_SUITE_P(NoParallelDecode,
+INSTANTIATE_TEST_CASE_P(NoParallelDecode,
                          FakeVideoDecoderTest,
                          ::testing::Values(FakeVideoDecoderTestParams(9, 1),
                                            FakeVideoDecoderTestParams(0, 1)));
-INSTANTIATE_TEST_SUITE_P(ParallelDecode,
+INSTANTIATE_TEST_CASE_P(ParallelDecode,
                          FakeVideoDecoderTest,
                          ::testing::Values(FakeVideoDecoderTestParams(9, 3),
                                            FakeVideoDecoderTestParams(0, 3)));
--- a/media/filters/ffmpeg_audio_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/ffmpeg_audio_decoder.cc	2019-05-17 18:53:34.208000000 +0300
@@ -6,9 +6,6 @@
 
 #include <stdint.h>
 
-#include <functional>
-
-#include "base/bind.h"
 #include "base/callback_helpers.h"
 #include "base/single_thread_task_runner.h"
 #include "media/base/audio_buffer.h"
@@ -167,12 +164,12 @@
   }
 
   bool decoded_frame_this_loop = false;
-  // base::Unretained and std::cref are safe to use with the callback given
+  // base::Unretained and base::ConstRef are safe to use with the callback given
   // to DecodePacket() since that callback is only used the function call.
   switch (decoding_loop_->DecodePacket(
-      &packet, base::BindRepeating(&FFmpegAudioDecoder::OnNewFrame,
-                                   base::Unretained(this), std::cref(buffer),
-                                   &decoded_frame_this_loop))) {
+      &packet, base::BindRepeating(
+                   &FFmpegAudioDecoder::OnNewFrame, base::Unretained(this),
+                   base::ConstRef(buffer), &decoded_frame_this_loop))) {
     case FFmpegDecodingLoop::DecodeStatus::kSendPacketFailed:
       MEDIA_LOG(ERROR, media_log_)
           << "Failed to send audio packet for decoding: "
--- a/media/filters/ffmpeg_bitstream_converter.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/ffmpeg_bitstream_converter.h	2019-05-17 18:53:34.208000000 +0300
@@ -14,7 +14,7 @@
 // Interface for classes that allow reformating of FFmpeg bitstreams
 class MEDIA_EXPORT FFmpegBitstreamConverter {
  public:
-  virtual ~FFmpegBitstreamConverter() {}
+  virtual ~FFmpegBitstreamConverter() {};
 
   // Reads the data in packet, and then overwrites this data with the
   // converted version of packet
--- a/media/filters/ffmpeg_demuxer.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/ffmpeg_demuxer.cc	2019-05-17 18:53:34.208000000 +0300
@@ -33,9 +33,7 @@
 #include "media/base/limits.h"
 #include "media/base/media_log.h"
 #include "media/base/media_tracks.h"
-#include "media/base/media_types.h"
 #include "media/base/sample_rates.h"
-#include "media/base/supported_types.h"
 #include "media/base/timestamp_constants.h"
 #include "media/base/video_codecs.h"
 #include "media/base/webvtt_util.h"
@@ -254,15 +252,16 @@
   if (stream->codecpar->codec_type == AVMEDIA_TYPE_AUDIO) {
     audio_config.reset(new AudioDecoderConfig());
 
+    // IsValidConfig() checks that the codec is supported and that the channel
+    // layout and sample format are valid.
+    //
     // TODO(chcunningham): Change AVStreamToAudioDecoderConfig to check
     // IsValidConfig internally and return a null scoped_ptr if not valid.
     if (!AVStreamToAudioDecoderConfig(stream, audio_config.get()) ||
-        !audio_config->IsValidConfig() ||
-        !IsSupportedAudioType(AudioType::FromDecoderConfig(*audio_config))) {
+        !audio_config->IsValidConfig()) {
       MEDIA_LOG(DEBUG, media_log) << "Warning, FFmpegDemuxer failed to create "
-                                     "a valid/supported audio decoder "
-                                     "configuration from muxed stream, config:"
-                                  << audio_config->AsHumanReadableString();
+                                     "a valid audio decoder configuration from "
+                                     "muxed stream";
       return nullptr;
     }
 
@@ -271,15 +270,16 @@
   } else if (stream->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) {
     video_config.reset(new VideoDecoderConfig());
 
+    // IsValidConfig() checks that the codec is supported and that the channel
+    // layout and sample format are valid.
+    //
     // TODO(chcunningham): Change AVStreamToVideoDecoderConfig to check
     // IsValidConfig internally and return a null scoped_ptr if not valid.
     if (!AVStreamToVideoDecoderConfig(stream, video_config.get()) ||
-        !video_config->IsValidConfig() ||
-        !IsSupportedVideoType(VideoType::FromDecoderConfig(*video_config))) {
+        !video_config->IsValidConfig()) {
       MEDIA_LOG(DEBUG, media_log) << "Warning, FFmpegDemuxer failed to create "
-                                     "a valid/supported video decoder "
-                                     "configuration from muxed stream, config:"
-                                  << video_config->AsHumanReadableString();
+                                     "a valid video decoder configuration from "
+                                     "muxed stream";
       return nullptr;
     }
 
@@ -570,8 +570,7 @@
   const base::TimeDelta stream_timestamp =
       ConvertStreamTimestamp(stream_->time_base, packet->pts);
 
-  if (stream_timestamp == kNoTimestamp ||
-      stream_timestamp == kInfiniteDuration) {
+  if (stream_timestamp == kNoTimestamp) {
     MEDIA_LOG(ERROR, media_log_) << "FFmpegDemuxer: PTS is not defined";
     demuxer_->NotifyDemuxerError(DEMUXER_ERROR_COULD_NOT_PARSE);
     return;
@@ -1033,7 +1032,7 @@
   // Aborting the read may cause EOF to be marked, undo this.
   blocking_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&UnmarkEndOfStreamAndClearError, glue_->format_context()));
+      base::Bind(&UnmarkEndOfStreamAndClearError, glue_->format_context()));
   pending_read_ = false;
 
   // TODO(dalecurtis): We probably should report PIPELINE_ERROR_ABORT here
@@ -1419,9 +1418,6 @@
                               base::TimeDelta());
     }
 
-    // TODO(chcunningham): Remove the IsValidConfig() checks below. If the
-    // config isn't valid we shouldn't have created a demuxer stream nor
-    // an entry in |media_tracks|, so the check should always be true.
     if ((codec_type == AVMEDIA_TYPE_AUDIO &&
          media_tracks->getAudioConfig(track_id).IsValidConfig()) ||
         (codec_type == AVMEDIA_TYPE_VIDEO &&
@@ -1441,7 +1437,7 @@
 
       media_track = media_tracks->AddAudioTrack(audio_config, track_id, "main",
                                                 track_label, track_language);
-      media_track->set_id(base::NumberToString(track_id));
+      media_track->set_id(base::UintToString(track_id));
       DCHECK(track_id_to_demux_stream_map_.find(media_track->id()) ==
              track_id_to_demux_stream_map_.end());
       track_id_to_demux_stream_map_[media_track->id()] = streams_[i].get();
@@ -1453,7 +1449,7 @@
 
       media_track = media_tracks->AddVideoTrack(video_config, track_id, "main",
                                                 track_label, track_language);
-      media_track->set_id(base::NumberToString(track_id));
+      media_track->set_id(base::UintToString(track_id));
       DCHECK(track_id_to_demux_stream_map_.find(media_track->id()) ==
              track_id_to_demux_stream_map_.end());
       track_id_to_demux_stream_map_[media_track->id()] = streams_[i].get();
@@ -1606,7 +1602,7 @@
       ++audio_track_count;
       std::string suffix = "";
       if (audio_track_count > 1)
-        suffix = "_track" + base::NumberToString(audio_track_count);
+        suffix = "_track" + base::IntToString(audio_track_count);
       const AVCodecParameters* audio_parameters = avctx->streams[i]->codecpar;
       const AudioDecoderConfig& audio_config = stream->audio_decoder_config();
       params.SetString("audio_codec_name" + suffix,
@@ -1621,7 +1617,7 @@
       ++video_track_count;
       std::string suffix = "";
       if (video_track_count > 1)
-        suffix = "_track" + base::NumberToString(video_track_count);
+        suffix = "_track" + base::IntToString(video_track_count);
       const AVStream* video_av_stream = avctx->streams[i];
       const AVCodecParameters* video_parameters = video_av_stream->codecpar;
       const VideoDecoderConfig& video_config = stream->video_decoder_config();
--- a/media/filters/ffmpeg_demuxer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/ffmpeg_demuxer_unittest.cc	2019-05-17 18:53:34.208000000 +0300
@@ -22,11 +22,9 @@
 #include "base/threading/thread_task_runner_handle.h"
 #include "build/build_config.h"
 #include "media/base/decrypt_config.h"
-#include "media/base/media_client.h"
 #include "media/base/media_tracks.h"
 #include "media/base/media_util.h"
 #include "media/base/mock_demuxer_host.h"
-#include "media/base/mock_filters.h"
 #include "media/base/mock_media_log.h"
 #include "media/base/test_helpers.h"
 #include "media/base/timestamp_constants.h"
@@ -37,12 +35,9 @@
 #include "media/formats/mp4/bitstream_converter.h"
 #include "media/media_buildflags.h"
 #include "testing/gtest/include/gtest/gtest.h"
-#include "ui/gfx/color_space.h"
 
-using ::testing::_;
 using ::testing::AnyNumber;
 using ::testing::DoAll;
-using ::testing::Eq;
 using ::testing::Exactly;
 using ::testing::InSequence;
 using ::testing::Invoke;
@@ -52,6 +47,7 @@
 using ::testing::SetArgPointee;
 using ::testing::StrictMock;
 using ::testing::WithArgs;
+using ::testing::_;
 
 namespace media {
 
@@ -70,9 +66,7 @@
 
 MATCHER_P(FailedToCreateValidDecoderConfigFromStream, stream_type, "") {
   return CONTAINS_STRING(
-      arg,
-      "\"debug\":\"Warning, FFmpegDemuxer failed to create a "
-      "valid/supported " +
+      arg, "\"debug\":\"Warning, FFmpegDemuxer failed to create a valid " +
           std::string(stream_type) +
           " decoder configuration from muxed stream");
 }
@@ -103,7 +97,8 @@
   EXPECT_TRUE(buffer->data());
   EXPECT_GT(buffer->data_size(), 0u);
   *got_eos_buffer = false;
-}
+};
+
 
 // Fixture class to facilitate writing tests.  Takes care of setting up the
 // FFmpeg, pipeline and filter host mocks.
@@ -1216,8 +1211,7 @@
 // MP3s should seek quickly without sequentially reading up to the seek point.
 // VBR vs CBR and the presence/absence of TOC influence the seeking algorithm.
 // See http://crbug.com/530043 and FFmpeg flag AVFMT_FLAG_FAST_SEEK.
-INSTANTIATE_TEST_SUITE_P(,
-                         Mp3SeekFFmpegDemuxerTest,
+INSTANTIATE_TEST_CASE_P(, Mp3SeekFFmpegDemuxerTest,
                          ::testing::Values("bear-audio-10s-CBR-has-TOC.mp3",
                                            "bear-audio-10s-CBR-no-TOC.mp3",
                                            "bear-audio-10s-VBR-has-TOC.mp3",
@@ -1253,7 +1247,7 @@
   }
 
   stream->Read(base::Bind(&ValidateAnnexB, stream));
-}
+};
 
 TEST_F(FFmpegDemuxerTest, IsValidAnnexB) {
   const char* files[] = {
@@ -1349,20 +1343,6 @@
 TEST_F(FFmpegDemuxerTest, HEVC_in_MP4_container) {
   CreateDemuxer("bear-hevc-frag.mp4");
 #if BUILDFLAG(ENABLE_HEVC_DEMUXING)
-  // HEVC is not supported by default media platform. Embedders who add support
-  // must declare it via MediaClient.
-  MockMediaClient media_client;
-  SetMediaClient(&media_client);
-
-  VideoColorSpace color_space(VideoColorSpace::PrimaryID::SMPTE170M,
-                              VideoColorSpace::TransferID::SMPTE170M,
-                              VideoColorSpace::MatrixID::SMPTE170M,
-                              gfx::ColorSpace::RangeID::LIMITED);
-  VideoType hevc_type = {VideoCodec::kCodecHEVC,
-                         VideoCodecProfile::HEVCPROFILE_MAIN, 10, color_space};
-  EXPECT_CALL(media_client, IsSupportedVideoType(Eq(hevc_type)))
-      .WillRepeatedly(Return(true));
-
   InitializeDemuxer();
 
   DemuxerStream* video = GetStream(DemuxerStream::VIDEO);
@@ -1373,8 +1353,6 @@
 
   video->Read(NewReadCB(FROM_HERE, 1042, 200200, false));
   base::RunLoop().Run();
-
-  SetMediaClient(nullptr);
 #else
   InitializeDemuxerAndExpectPipelineStatus(DEMUXER_ERROR_NO_SUPPORTED_STREAMS);
 #endif
@@ -1383,15 +1361,6 @@
 TEST_F(FFmpegDemuxerTest, Read_AC3_Audio) {
   CreateDemuxer("bear-ac3-only-frag.mp4");
 #if BUILDFLAG(ENABLE_AC3_EAC3_AUDIO_DEMUXING)
-  // AC3 is not supported by default media platform. Embedders who add support
-  // must declare it via MediaClient.
-  MockMediaClient media_client;
-  SetMediaClient(&media_client);
-
-  AudioType ac3_type = {AudioCodec::kCodecAC3};
-  EXPECT_CALL(media_client, IsSupportedAudioType(Eq(ac3_type)))
-      .WillRepeatedly(Return(true));
-
   InitializeDemuxer();
 
   // Attempt a read from the audio stream and run the message loop until done.
@@ -1403,8 +1372,6 @@
 
   audio->Read(NewReadCB(FROM_HERE, 836, 34830, true));
   base::RunLoop().Run();
-
-  SetMediaClient(nullptr);
 #else
   InitializeDemuxerAndExpectPipelineStatus(DEMUXER_ERROR_NO_SUPPORTED_STREAMS);
 #endif
@@ -1413,15 +1380,6 @@
 TEST_F(FFmpegDemuxerTest, Read_EAC3_Audio) {
   CreateDemuxer("bear-eac3-only-frag.mp4");
 #if BUILDFLAG(ENABLE_AC3_EAC3_AUDIO_DEMUXING)
-  // EAC3 is not supported by default media platform. Embedders who add support
-  // must declare it via MediaClient.
-  MockMediaClient media_client;
-  SetMediaClient(&media_client);
-
-  AudioType eac3_type = {AudioCodec::kCodecEAC3};
-  EXPECT_CALL(media_client, IsSupportedAudioType(Eq(eac3_type)))
-      .WillRepeatedly(Return(true));
-
   InitializeDemuxer();
 
   // Attempt a read from the audio stream and run the message loop until done.
@@ -1433,8 +1391,6 @@
 
   audio->Read(NewReadCB(FROM_HERE, 872, 34830, true));
   base::RunLoop().Run();
-
-  SetMediaClient(nullptr);
 #else
   InitializeDemuxerAndExpectPipelineStatus(DEMUXER_ERROR_NO_SUPPORTED_STREAMS);
 #endif
--- a/media/filters/frame_processor.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/frame_processor.cc	2019-05-17 18:53:34.212000000 +0300
@@ -16,6 +16,7 @@
 namespace media {
 
 const int kMaxDroppedPrerollWarnings = 10;
+const int kMaxDtsBeyondPtsWarnings = 10;
 const int kMaxAudioNonKeyframeWarnings = 10;
 const int kMaxNumKeyframeTimeGreaterThanDependantWarnings = 1;
 const int kMaxMuxedSequenceModeWarnings = 1;
@@ -757,14 +758,20 @@
                                    << " frame";
       return false;
     }
-
-    // TODO(wolenetz): Determine whether any DTS>PTS logging is needed. See
+    if (decode_timestamp.ToPresentationTime() > presentation_timestamp) {
+      // TODO(wolenetz): Determine whether DTS>PTS should really be allowed. See
     // http://crbug.com/354518.
-    DVLOG_IF(2, decode_timestamp.ToPresentationTime() > presentation_timestamp)
-        << __func__ << ": WARNING: Frame DTS("
+      LIMITED_MEDIA_LOG(DEBUG, media_log_, num_dts_beyond_pts_warnings_,
+                        kMaxDtsBeyondPtsWarnings)
+          << "Parsed " << frame->GetTypeName() << " frame has DTS "
+          << decode_timestamp.InMicroseconds()
+          << "us, which is after the frame's PTS "
+          << presentation_timestamp.InMicroseconds() << "us";
+      DVLOG(2) << __func__ << ": WARNING: Frame DTS("
         << decode_timestamp.InMicroseconds() << "us) > PTS("
         << presentation_timestamp.InMicroseconds()
         << "us), frame type=" << frame->GetTypeName();
+    }
 
     // All stream parsers must emit valid (non-negative) frame durations.
     // Note that duration of 0 can occur for at least WebM alt-ref frames.
--- a/media/filters/frame_processor.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/frame_processor.h	2019-05-17 18:53:34.212000000 +0300
@@ -187,6 +187,7 @@
 
   // Counters that limit spam to |media_log_| for frame processor warnings.
   int num_dropped_preroll_warnings_ = 0;
+  int num_dts_beyond_pts_warnings_ = 0;
   int num_audio_non_keyframe_warnings_ = 0;
   int num_muxed_sequence_mode_warnings_ = 0;
   int num_skipped_empty_frame_warnings_ = 0;
--- a/media/filters/frame_processor_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/frame_processor_unittest.cc	2019-05-17 18:53:34.212000000 +0300
@@ -966,10 +966,13 @@
   AddTestTracks(HAS_AUDIO);
 
   if (use_sequence_mode_) {
+    EXPECT_MEDIA_LOG(ParsedDTSGreaterThanPTS()).Times(2);
     frame_processor_->SetSequenceMode(true);
     EXPECT_CALL(callbacks_, PossibleDurationIncrease(Milliseconds(20)));
   } else {
+    EXPECT_MEDIA_LOG(ParsedDTSGreaterThanPTS());
     EXPECT_MEDIA_LOG(TruncatedFrame(-7000, 3000, "start", 0));
+    EXPECT_MEDIA_LOG(ParsedDTSGreaterThanPTS());
     EXPECT_CALL(callbacks_, PossibleDurationIncrease(Milliseconds(13)));
   }
 
@@ -1151,6 +1154,7 @@
 
   CheckExpectedRangesByTimestamp(video_.get(), "{ [50,70) }");
 
+  EXPECT_MEDIA_LOG(ParsedDTSGreaterThanPTS());
   EXPECT_CALL(callbacks_,
               OnParseWarning(
                   SourceBufferParseWarning::kKeyframeTimeGreaterThanDependant));
@@ -2031,22 +2035,22 @@
   CheckReadsThenReadStalls(audio_.get(), "0 10 20 30");
 }
 
-INSTANTIATE_TEST_SUITE_P(SequenceModeLegacyByDts,
+INSTANTIATE_TEST_CASE_P(SequenceModeLegacyByDts,
                          FrameProcessorTest,
                          Values(FrameProcessorTestParams(
                              true,
                              ChunkDemuxerStream::RangeApi::kLegacyByDts)));
-INSTANTIATE_TEST_SUITE_P(SegmentsModeLegacyByDts,
+INSTANTIATE_TEST_CASE_P(SegmentsModeLegacyByDts,
                          FrameProcessorTest,
                          Values(FrameProcessorTestParams(
                              false,
                              ChunkDemuxerStream::RangeApi::kLegacyByDts)));
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     SequenceModeNewByPts,
     FrameProcessorTest,
     Values(FrameProcessorTestParams(true,
                                     ChunkDemuxerStream::RangeApi::kNewByPts)));
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     SegmentsModeNewByPts,
     FrameProcessorTest,
     Values(FrameProcessorTestParams(false,
--- a/media/filters/fuchsia/fuchsia_video_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/fuchsia/fuchsia_video_decoder.cc	2019-05-17 18:53:34.212000000 +0300
@@ -4,14 +4,13 @@
 
 #include "media/filters/fuchsia/fuchsia_video_decoder.h"
 
-#include <fuchsia/media/cpp/fidl.h>
 #include <fuchsia/mediacodec/cpp/fidl.h>
 #include <zircon/rights.h>
 
 #include "base/bind.h"
 #include "base/callback_helpers.h"
+#include "base/fuchsia/component_context.h"
 #include "base/fuchsia/fuchsia_logging.h"
-#include "base/fuchsia/service_directory_client.h"
 #include "base/location.h"
 #include "base/logging.h"
 #include "base/macros.h"
@@ -103,7 +102,8 @@
  public:
   CodecBuffer() = default;
 
-  bool Initialize(const fuchsia::media::StreamBufferConstraints& constraints) {
+  bool Initialize(
+      const fuchsia::mediacodec::CodecBufferConstraints& constraints) {
     size_ = constraints.per_packet_buffer_bytes_recommended;
 
     if (constraints.is_physically_contiguous_required) {
@@ -121,7 +121,7 @@
   bool ToFidlCodecBuffer(uint64_t buffer_lifetime_ordinal,
                          uint32_t buffer_index,
                          bool read_only,
-                         fuchsia::media::StreamBuffer* buffer) {
+                         fuchsia::mediacodec::CodecBuffer* buffer) {
     zx::vmo vmo_dup;
     zx_status_t status = vmo_.duplicate(
         read_only ? kReadOnlyVmoRights : ZX_RIGHT_SAME_RIGHTS, &vmo_dup);
@@ -130,7 +130,7 @@
       return false;
     }
 
-    fuchsia::media::StreamBufferDataVmo buf_data;
+    fuchsia::mediacodec::CodecBufferDataVmo buf_data;
     buf_data.vmo_handle = std::move(vmo_dup);
 
     buf_data.vmo_usable_start = 0;
@@ -156,7 +156,8 @@
 
   ~InputBuffer() { CallDecodeCallbackIfAny(DecodeStatus::ABORTED); }
 
-  bool Initialize(const fuchsia::media::StreamBufferConstraints& constraints) {
+  bool Initialize(
+      const fuchsia::mediacodec::CodecBufferConstraints& constraints) {
     return buffer_.Initialize(constraints);
   }
 
@@ -217,14 +218,15 @@
  public:
   OutputBuffer() = default;
 
-  bool Initialize(const fuchsia::media::StreamBufferConstraints& constraints) {
+  bool Initialize(
+      const fuchsia::mediacodec::CodecBufferConstraints& constraints) {
     if (!buffer_.Initialize(constraints)) {
       return false;
     }
 
     zx_status_t status = zx::vmar::root_self()->map(
         /*vmar_offset=*/0, buffer_.vmo(), 0, buffer_.size(),
-        ZX_VM_REQUIRE_NON_RESIZABLE | ZX_VM_PERM_READ, &mapped_memory_);
+        ZX_VM_REQUIRE_NON_RESIZABLE | ZX_VM_FLAG_PERM_READ, &mapped_memory_);
 
     if (status != ZX_OK) {
       ZX_DLOG(ERROR, status) << "zx_vmar_map";
@@ -288,10 +290,11 @@
   // Event handlers for |codec_|.
   void OnStreamFailed(uint64_t stream_lifetime_ordinal);
   void OnInputConstraints(
-      fuchsia::media::StreamBufferConstraints input_constraints);
-  void OnFreeInputPacket(fuchsia::media::PacketHeader free_input_packet);
-  void OnOutputConfig(fuchsia::media::StreamOutputConfig output_config);
-  void OnOutputPacket(fuchsia::media::Packet output_packet,
+      fuchsia::mediacodec::CodecBufferConstraints input_constraints);
+  void OnFreeInputPacket(
+      fuchsia::mediacodec::CodecPacketHeader free_input_packet);
+  void OnOutputConfig(fuchsia::mediacodec::CodecOutputConfig output_config);
+  void OnOutputPacket(fuchsia::mediacodec::CodecPacket output_packet,
                       bool error_detected_before,
                       bool error_detected_during);
   void OnOutputEndOfStream(uint64_t stream_lifetime_ordinal,
@@ -301,14 +304,14 @@
 
   // Called by OnInputConstraints() to initialize input buffers.
   bool InitializeInputBuffers(
-      fuchsia::media::StreamBufferConstraints constraints);
+      fuchsia::mediacodec::CodecBufferConstraints constraints);
 
   // Pumps |pending_decodes_| to the decoder.
   void PumpInput();
 
   // Called by OnInputConstraints() to initialize input buffers.
   bool InitializeOutputBuffers(
-      fuchsia::media::StreamBufferConstraints constraints);
+      fuchsia::mediacodec::CodecBufferConstraints constraints);
 
   // Destruction callback for the output VideoFrame instances.
   void OnFrameDestroyed(scoped_refptr<OutputBuffer> buffer,
@@ -323,7 +326,7 @@
   // value is used only if the aspect ratio is not specified in the bitstream.
   float container_pixel_aspect_ratio_ = 1.0;
 
-  fuchsia::media::StreamProcessorPtr codec_;
+  fuchsia::mediacodec::CodecPtr codec_;
 
   uint64_t stream_lifetime_ordinal_ = 1;
 
@@ -336,7 +339,7 @@
   std::vector<InputBuffer> input_buffers_;
   int num_used_input_buffers_ = 0;
 
-  fuchsia::media::VideoUncompressedFormat output_format_;
+  fuchsia::mediacodec::VideoUncompressedFormat output_format_;
   uint64_t output_buffer_lifetime_ordinal_ = 1;
   std::vector<scoped_refptr<OutputBuffer>> output_buffers_;
   int num_used_output_buffers_ = 0;
@@ -402,7 +405,7 @@
   codec_params.require_hw = !enable_sw_decoding_;
 
   auto codec_factory =
-      base::fuchsia::ServiceDirectoryClient::ForCurrentProcess()
+      base::fuchsia::ComponentContext::GetDefault()
           ->ConnectToService<fuchsia::mediacodec::CodecFactory>();
   codec_factory->CreateDecoder(std::move(codec_params), codec_.NewRequest());
 
@@ -491,7 +494,7 @@
 }
 
 void FuchsiaVideoDecoder::OnInputConstraints(
-    fuchsia::media::StreamBufferConstraints input_constraints) {
+    fuchsia::mediacodec::CodecBufferConstraints input_constraints) {
   if (!InitializeInputBuffers(std::move(input_constraints))) {
     DLOG(ERROR) << "Failed to initialize input buffers.";
     OnError();
@@ -502,7 +505,7 @@
 }
 
 void FuchsiaVideoDecoder::OnFreeInputPacket(
-    fuchsia::media::PacketHeader free_input_packet) {
+    fuchsia::mediacodec::CodecPacketHeader free_input_packet) {
   if (free_input_packet.buffer_lifetime_ordinal !=
       input_buffer_lifetime_ordinal_) {
     return;
@@ -527,7 +530,7 @@
 }
 
 void FuchsiaVideoDecoder::OnOutputConfig(
-    fuchsia::media::StreamOutputConfig output_config) {
+    fuchsia::mediacodec::CodecOutputConfig output_config) {
   if (output_config.stream_lifetime_ordinal != stream_lifetime_ordinal_)
     return;
 
@@ -550,7 +553,8 @@
   output_format_ = std::move(format.domain->video().uncompressed());
 }
 
-void FuchsiaVideoDecoder::OnOutputPacket(fuchsia::media::Packet output_packet,
+void FuchsiaVideoDecoder::OnOutputPacket(
+    fuchsia::mediacodec::CodecPacket output_packet,
                                          bool error_detected_before,
                                          bool error_detected_during) {
   if (output_packet.header.buffer_lifetime_ordinal !=
@@ -689,7 +693,7 @@
 }
 
 bool FuchsiaVideoDecoder::InitializeInputBuffers(
-    fuchsia::media::StreamBufferConstraints constraints) {
+    fuchsia::mediacodec::CodecBufferConstraints constraints) {
   input_buffer_lifetime_ordinal_ += 2;
 
   auto settings = constraints.default_settings;
@@ -698,11 +702,11 @@
   codec_->SetInputBufferSettings(settings);
 
   int total_buffers =
-      settings.packet_count_for_server + settings.packet_count_for_client;
+      settings.packet_count_for_codec + settings.packet_count_for_client;
   std::vector<InputBuffer> new_buffers(total_buffers);
 
   for (int i = 0; i < total_buffers; ++i) {
-    fuchsia::media::StreamBuffer codec_buffer;
+    fuchsia::mediacodec::CodecBuffer codec_buffer;
 
     if (!new_buffers[i].Initialize(constraints) ||
         !new_buffers[i].buffer().ToFidlCodecBuffer(
@@ -754,7 +758,7 @@
     size_t bytes_filled =
         input_buffer->FillFromDecodeBuffer(&pending_decodes_.front());
 
-    fuchsia::media::Packet packet;
+    fuchsia::mediacodec::CodecPacket packet;
     packet.header.buffer_lifetime_ordinal = input_buffer_lifetime_ordinal_;
     packet.header.packet_index = input_buffer - input_buffers_.begin();
     packet.buffer_index = packet.header.packet_index;
@@ -775,7 +779,7 @@
 }
 
 bool FuchsiaVideoDecoder::InitializeOutputBuffers(
-    fuchsia::media::StreamBufferConstraints constraints) {
+    fuchsia::mediacodec::CodecBufferConstraints constraints) {
   // mediacodec API expects odd buffer lifetime ordinal, which is incremented by
   // 2 for each buffer generation.
   output_buffer_lifetime_ordinal_ += 2;
@@ -790,11 +794,11 @@
   codec_->SetOutputBufferSettings(std::move(settings));
 
   int total_buffers =
-      settings.packet_count_for_server + settings.packet_count_for_client;
+      settings.packet_count_for_codec + settings.packet_count_for_client;
   std::vector<scoped_refptr<OutputBuffer>> new_buffers(total_buffers);
 
   for (int i = 0; i < total_buffers; ++i) {
-    fuchsia::media::StreamBuffer codec_buffer;
+    fuchsia::mediacodec::CodecBuffer codec_buffer;
     new_buffers[i] = new OutputBuffer();
     if (!new_buffers[i]->Initialize(constraints) ||
         !new_buffers[i]->buffer().ToFidlCodecBuffer(
@@ -821,8 +825,8 @@
   if (buffer_lifetime_ordinal == output_buffer_lifetime_ordinal_) {
     DCHECK_GT(num_used_output_buffers_, 0);
     num_used_output_buffers_--;
-    codec_->RecycleOutputPacket(
-        fuchsia::media::PacketHeader{buffer_lifetime_ordinal, packet_index});
+    codec_->RecycleOutputPacket(fuchsia::mediacodec::CodecPacketHeader{
+        buffer_lifetime_ordinal, packet_index});
   }
 }
 
--- a/media/filters/h264_bitstream_buffer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/h264_bitstream_buffer_unittest.cc	2019-05-17 18:53:34.212000000 +0300
@@ -51,7 +51,7 @@
   EXPECT_EQ(got, expected) << std::hex << "0x" << got << " vs 0x" << expected;
 }
 
-INSTANTIATE_TEST_SUITE_P(AppendNumBits,
+INSTANTIATE_TEST_CASE_P(AppendNumBits,
                          H264BitstreamBufferAppendBitsTest,
                          ::testing::Range(static_cast<uint64_t>(1),
                                           static_cast<uint64_t>(65)));
--- a/media/filters/jpeg_parser.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/jpeg_parser.h	2019-05-17 18:53:34.212000000 +0300
@@ -7,8 +7,7 @@
 
 #include <stddef.h>
 #include <stdint.h>
-
-#include "base/component_export.h"
+#include "media/base/media_export.h"
 
 namespace media {
 
@@ -81,12 +80,12 @@
 };
 
 // K.3.3.1 "Specification of typical tables for DC difference coding"
-COMPONENT_EXPORT(JPEG_PARSER)
-extern const JpegHuffmanTable kDefaultDcTable[kJpegMaxHuffmanTableNumBaseline];
+MEDIA_EXPORT extern const JpegHuffmanTable
+    kDefaultDcTable[kJpegMaxHuffmanTableNumBaseline];
 
 // K.3.3.2 "Specification of typical tables for AC coefficient coding"
-COMPONENT_EXPORT(JPEG_PARSER)
-extern const JpegHuffmanTable kDefaultAcTable[kJpegMaxHuffmanTableNumBaseline];
+MEDIA_EXPORT extern const JpegHuffmanTable
+    kDefaultAcTable[kJpegMaxHuffmanTableNumBaseline];
 
 // Parsing result of JPEG DQT marker.
 struct JpegQuantizationTable {
@@ -94,12 +93,11 @@
   uint8_t value[kDctSize];  // baseline only supports 8 bits quantization table
 };
 
-COMPONENT_EXPORT(JPEG_PARSER) extern const uint8_t kZigZag8x8[64];
+MEDIA_EXPORT extern const uint8_t kZigZag8x8[64];
 
 // Table K.1 Luminance quantization table
 // Table K.2 Chrominance quantization table
-COMPONENT_EXPORT(JPEG_PARSER)
-extern const JpegQuantizationTable kDefaultQuantTable[2];
+MEDIA_EXPORT extern const JpegQuantizationTable kDefaultQuantTable[2];
 
 // Parsing result of a JPEG component.
 struct JpegComponent {
@@ -146,16 +144,14 @@
 // Parses JPEG picture in |buffer| with |length|.  Returns true iff header is
 // valid and JPEG baseline sequential process is present. If parsed
 // successfully, |result| is the parsed result.
-COMPONENT_EXPORT(JPEG_PARSER)
-bool ParseJpegPicture(const uint8_t* buffer,
+MEDIA_EXPORT bool ParseJpegPicture(const uint8_t* buffer,
                       size_t length,
                       JpegParseResult* result);
 
 // Parses the first image of JPEG stream in |buffer| with |length|.  Returns
 // true iff header is valid and JPEG baseline sequential process is present.
 // If parsed successfully, |result| is the parsed result.
-COMPONENT_EXPORT(JPEG_PARSER)
-bool ParseJpegStream(const uint8_t* buffer,
+MEDIA_EXPORT bool ParseJpegStream(const uint8_t* buffer,
                      size_t length,
                      JpegParseResult* result);
 
--- a/media/filters/offloading_video_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/offloading_video_decoder.cc	2019-05-17 18:53:34.216000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/filters/offloading_video_decoder.h"
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/sequenced_task_runner.h"
 #include "base/synchronization/atomic_flag.h"
--- a/media/filters/source_buffer_state_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/source_buffer_state_unittest.cc	2019-05-17 18:53:34.216000000 +0300
@@ -146,7 +146,7 @@
   ChunkDemuxerStream* CreateDemuxerStream(DemuxerStream::Type type) {
     static unsigned track_id = 0;
     demuxer_streams_.push_back(base::WrapUnique(new ChunkDemuxerStream(
-        type, base::NumberToString(++track_id), range_api_)));
+        type, base::UintToString(++track_id), range_api_)));
     return demuxer_streams_.back().get();
   }
 
@@ -350,10 +350,10 @@
   AppendDataAndReportTracks(sbs, std::move(tracks2));
 }
 
-INSTANTIATE_TEST_SUITE_P(LegacyByDts,
+INSTANTIATE_TEST_CASE_P(LegacyByDts,
                          SourceBufferStateTest,
                          Values(ChunkDemuxerStream::RangeApi::kLegacyByDts));
-INSTANTIATE_TEST_SUITE_P(NewByPts,
+INSTANTIATE_TEST_CASE_P(NewByPts,
                          SourceBufferStateTest,
                          Values(ChunkDemuxerStream::RangeApi::kNewByPts));
 
--- a/media/filters/source_buffer_stream_unittest.cc	2019-05-17 17:45:41.276000000 +0300
+++ b/media/filters/source_buffer_stream_unittest.cc	2019-05-17 18:53:34.220000000 +0300
@@ -70,7 +70,7 @@
                          "Media append that overlapped current playback "
                          "position caused time gap in playing VIDEO stream "
                          "because the next keyframe is " +
-                             base::NumberToString(skip_milliseconds) +
+                             base::IntToString(skip_milliseconds) +
                              "ms beyond last overlapped frame. Media may "
                              "appear temporarily frozen.");
 }
@@ -3589,7 +3589,7 @@
 TEST_P(SourceBufferStreamTest, SetExplicitDuration_EdgeCase2) {
   // This test requires specific relative proportions for fudge room, append
   // size, and duration truncation amounts. See details at:
-  // https://codereview.ch40m1um.qjz9zk/2385423002
+  // https://codereview.chromium.org/2385423002
 
   // Append buffers with first buffer establishing max_inter_buffer_distance
   // of 5 ms. This translates to a fudge room (2 x max_interbuffer_distance) of
@@ -3614,7 +3614,7 @@
 TEST_P(SourceBufferStreamTest, RemoveWithinFudgeRoom) {
   // This test requires specific relative proportions for fudge room, append
   // size, and removal amounts. See details at:
-  // https://codereview.ch40m1um.qjz9zk/2385423002
+  // https://codereview.chromium.org/2385423002
 
   // Append buffers with first buffer establishing max_inter_buffer_distance
   // of 5 ms. This translates to a fudge room (2 x max_interbuffer_distance) of
@@ -5332,7 +5332,7 @@
 
 TEST_P(SourceBufferStreamTest, BFrames_WithoutEditList) {
   // Simulates B-frame content where MP4 edit lists are not used to shift PTS so
-  // it matches DTS. From acolwell@ch40m1um.qjz9zk in https://crbug.com/398130
+  // it matches DTS. From acolwell@chromium.org in https://crbug.com/398130
   Seek(0);
 
   if (buffering_api_ == BufferingApi::kLegacyByDts) {
@@ -5904,10 +5904,10 @@
   CheckNoNextBuffer();
 }
 
-INSTANTIATE_TEST_SUITE_P(LegacyByDts,
+INSTANTIATE_TEST_CASE_P(LegacyByDts,
                          SourceBufferStreamTest,
                          Values(BufferingApi::kLegacyByDts));
-INSTANTIATE_TEST_SUITE_P(NewByPts,
+INSTANTIATE_TEST_CASE_P(NewByPts,
                          SourceBufferStreamTest,
                          Values(BufferingApi::kNewByPts));
 
--- a/media/filters/stream_parser_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/stream_parser_factory.cc	2019-05-17 18:53:34.220000000 +0300
@@ -23,6 +23,7 @@
 #include "media/formats/mpeg/mpeg1_audio_stream_parser.h"
 #include "media/formats/webm/webm_stream_parser.h"
 #include "media/media_buildflags.h"
+#include "third_party/libaom/av1_buildflags.h"
 
 #if defined(OS_ANDROID)
 #include "media/base/android/media_codec_util.h"
--- a/media/filters/video_decoder_stream_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/video_decoder_stream_unittest.cc	2019-05-17 18:53:34.220000000 +0300
@@ -44,7 +44,7 @@
 const int kNumBuffersInOneConfig = 5;
 
 static std::string GetDecoderName(int i) {
-  return std::string("VideoDecoder") + base::NumberToString(i);
+  return std::string("VideoDecoder") + base::IntToString(i);
 }
 
 struct VideoDecoderStreamTestParams {
@@ -307,13 +307,8 @@
     DCHECK(pending_read_);
     frame_read_ = frame;
     last_read_status_ = status;
-    if (frame &&
+    if (frame.get() &&
         !frame->metadata()->IsTrue(VideoFrameMetadata::END_OF_STREAM)) {
-      base::TimeDelta metadata_frame_duration;
-      EXPECT_TRUE(frame->metadata()->GetTimeDelta(
-          VideoFrameMetadata::FRAME_DURATION, &metadata_frame_duration));
-      EXPECT_EQ(metadata_frame_duration, demuxer_stream_->duration());
-
       num_decoded_frames_++;
     }
     pending_read_ = false;
@@ -501,7 +496,7 @@
   DISALLOW_COPY_AND_ASSIGN(VideoDecoderStreamTest);
 };
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     Clear,
     VideoDecoderStreamTest,
     ::testing::Values(VideoDecoderStreamTestParams(false, false, false, 0, 1),
@@ -510,19 +505,19 @@
                       VideoDecoderStreamTestParams(false, false, true, 0, 1),
                       VideoDecoderStreamTestParams(false, false, true, 3, 1)));
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     EncryptedWithDecryptor,
     VideoDecoderStreamTest,
     ::testing::Values(VideoDecoderStreamTestParams(true, true, false, 7, 1),
                       VideoDecoderStreamTestParams(true, true, true, 7, 1)));
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     EncryptedWithoutDecryptor,
     VideoDecoderStreamTest,
     ::testing::Values(VideoDecoderStreamTestParams(true, false, false, 7, 1),
                       VideoDecoderStreamTestParams(true, false, true, 7, 1)));
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     Clear_Parallel,
     VideoDecoderStreamTest,
     ::testing::Values(VideoDecoderStreamTestParams(false, false, false, 0, 3),
--- a/media/filters/video_renderer_algorithm.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/video_renderer_algorithm.cc	2019-05-17 18:53:34.220000000 +0300
@@ -476,86 +476,70 @@
   for (const auto& ready_frame : frame_queue_)
     media_timestamps.push_back(ready_frame.frame->timestamp());
 
-  // If available, always use the last frame's metadata duration to estimate the
-  // end time for that frame. This is useful when playback ends on long frame
-  // duration content.
+  // If there are not enough frames to estimate duration based on end time, ask
+  // the WallClockTimeCB to convert the estimated frame duration into wall clock
+  // time.
+  //
+  // Note: This duration value is not compensated for playback rate and
+  // thus is different than |average_frame_duration_| which is compensated.
   //
   // Note: Not all frames have duration. E.g., this class is used with WebRTC
   // which does not provide duration information for its frames.
-  bool have_metadata_duration = false;
-  {
-    const auto& last_frame = frame_queue_.back().frame;
     base::TimeDelta metadata_frame_duration;
-    if (last_frame->metadata()->GetTimeDelta(VideoFrameMetadata::FRAME_DURATION,
-                                             &metadata_frame_duration) &&
-        metadata_frame_duration > base::TimeDelta()) {
-      have_metadata_duration = true;
-      media_timestamps.push_back(last_frame->timestamp() +
+  const bool use_frame_duration_metadata =
+      !frame_duration_calculator_.count() && frame_queue_.size() == 1u &&
+      frame_queue_.front().frame->metadata()->GetTimeDelta(
+          VideoFrameMetadata::FRAME_DURATION, &metadata_frame_duration) &&
+      metadata_frame_duration > base::TimeDelta();
+  if (use_frame_duration_metadata) {
+    media_timestamps.push_back(frame_queue_.front().frame->timestamp() +
                                  metadata_frame_duration);
     }
-  }
 
   std::vector<base::TimeTicks> wall_clock_times;
   was_time_moving_ =
       wall_clock_time_cb_.Run(media_timestamps, &wall_clock_times);
-  DCHECK_EQ(wall_clock_times.size(),
-            frame_queue_.size() + (have_metadata_duration ? 1 : 0));
 
-  // Transfer the converted wall clock times into our frame queue. Never process
-  // the last frame in this loop; the last frame timing is handled below.
+  base::TimeDelta deviation;
+  if (!use_frame_duration_metadata) {
+    // Transfer the converted wall clock times into our frame queue.
+    DCHECK_EQ(wall_clock_times.size(), frame_queue_.size());
   for (size_t i = 0; i < frame_queue_.size() - 1; ++i) {
     ReadyFrame& frame = frame_queue_[i];
-
-    // Whenever a frame is added to the queue, |has_estimated_end_time| is true;
-    // this remains true until we receive a later frame -- from which we use its
-    // timestamp to assign the true |end_time| for the previous frame.
-    //
-    // So a new sample can always be determined by the |has_estimated_end_time|
-    // flag and the fact that the frame is being processed in this loop which
-    // never processes the last (and thus always estimated) frame.
     const bool new_sample = frame.has_estimated_end_time;
-
     frame.start_time = wall_clock_times[i];
     frame.end_time = wall_clock_times[i + 1];
     frame.has_estimated_end_time = false;
     if (new_sample)
       frame_duration_calculator_.AddSample(frame.end_time - frame.start_time);
   }
+    frame_queue_.back().start_time = wall_clock_times.back();
+
+    if (!frame_duration_calculator_.count())
+      return;
 
-  base::TimeDelta deviation;
-  if (frame_duration_calculator_.count()) {
     // Compute |average_frame_duration_|, a moving average of the last few
     // frames; see kMovingAverageSamples for the exact number.
     average_frame_duration_ = frame_duration_calculator_.Average();
     deviation = frame_duration_calculator_.Deviation();
-  }
-
-  if (have_metadata_duration) {
-    auto& frame = frame_queue_.back();
-    frame.start_time = wall_clock_times.end()[-2];
-    frame.end_time = wall_clock_times.end()[-1];
-
-    // This path will be taken for frames after the very first, but we only want
-    // to use our estimate of |average_frame_duration_| when we have no samples
-    // in |frame_duration_calculator_| -- since it's a more accurate reflection
-    // of the per-frame on screen time.
-    if (!frame_duration_calculator_.count()) {
-      average_frame_duration_ = frame.end_time - frame.start_time;
-      if (average_frame_duration_.is_zero())
-        return;
-    }
-  } else {
-    frame_queue_.back().start_time = wall_clock_times.back();
-
-    // If |have_metadata_duration| is false and we don't have any subsequent
-    // frames, we can't continue processing since the cadence estimate requires
-    // |average_frame_duration_| and |deviation| to be non-zero.
-    if (!frame_duration_calculator_.count())
-      return;
 
     // Update the frame end time for the last frame based on the average.
     frame_queue_.back().end_time =
         frame_queue_.back().start_time + average_frame_duration_;
+  } else {
+    DCHECK_EQ(frame_duration_calculator_.count(), 0u);
+    DCHECK_EQ(wall_clock_times.size(), 2u);
+
+    ReadyFrame& frame = frame_queue_.front();
+    frame.start_time = wall_clock_times[0];
+    frame.end_time = wall_clock_times[1];
+    frame.has_estimated_end_time = true;
+
+    // Note: This may be called multiple times, so we don't want to update the
+    // frame duration calculator with our estimate.
+    average_frame_duration_ = frame.end_time - frame.start_time;
+    if (average_frame_duration_.is_zero())
+      return;
   }
 
   // ITU-R BR.265 recommends a maximum acceptable drift of +/- half of the frame
--- a/media/filters/video_renderer_algorithm.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/video_renderer_algorithm.h	2019-05-17 18:53:34.220000000 +0300
@@ -144,14 +144,6 @@
     return average_frame_duration_;
   }
 
-  // End time of the last frame.
-  base::TimeTicks last_frame_end_time() const {
-    return frame_queue_.back().end_time;
-  }
-
-  // Current render interval.
-  base::TimeDelta render_interval() const { return render_interval_; }
-
   // Method used for testing which disables frame dropping, in this mode the
   // algorithm will never drop frames and instead always return every frame
   // for display at least once.
--- a/media/filters/video_renderer_algorithm_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/video_renderer_algorithm_unittest.cc	2019-05-17 18:53:34.220000000 +0300
@@ -208,7 +208,6 @@
       const base::TimeTicks deadline_max = display_tg->step();
       scoped_refptr<VideoFrame> frame =
           algorithm_.Render(deadline_min, deadline_max, &frames_dropped);
-      EXPECT_EQ(deadline_max - deadline_min, algorithm_.render_interval());
 
       render_test_func(frame, frames_dropped);
       tick_clock_->Advance(display_tg->current() - tick_clock_->NowTicks());
@@ -1379,7 +1378,7 @@
     60,       72, 90,    100, 120,      144, 240,      300,
 };
 
-INSTANTIATE_TEST_SUITE_P(,
+INSTANTIATE_TEST_CASE_P(,
                          VideoRendererAlgorithmCadenceTest,
                          ::testing::Combine(::testing::ValuesIn(kDisplayRates),
                                             ::testing::ValuesIn(kTestRates)));
@@ -1608,32 +1607,4 @@
   EXPECT_TRUE(algorithm_.average_frame_duration().is_zero());
 }
 
-TEST_F(VideoRendererAlgorithmTest, UsesFrameDuration) {
-  TickGenerator tg(tick_clock_->NowTicks(), 50);
-
-  auto frame = CreateFrame(tg.interval(0));
-  frame->metadata()->SetTimeDelta(VideoFrameMetadata::FRAME_DURATION,
-                                  tg.interval(1));
-  algorithm_.EnqueueFrame(frame);
-
-  // This should not crash or fail.
-  size_t frames_dropped = 0;
-  frame = RenderAndStep(&tg, &frames_dropped);
-  EXPECT_EQ(tg.interval(1), algorithm_.average_frame_duration());
-
-  // Add a bunch of normal frames and then one with a 3s duration.
-  constexpr base::TimeDelta kLongDuration = base::TimeDelta::FromSeconds(3);
-  for (int i = 1; i < 4; ++i) {
-    frame = CreateFrame(tg.interval(i));
-    frame->metadata()->SetTimeDelta(VideoFrameMetadata::FRAME_DURATION,
-                                    i == 3 ? kLongDuration : tg.interval(1));
-    algorithm_.EnqueueFrame(frame);
-  }
-
-  frame = RenderAndStep(&tg, &frames_dropped);
-  EXPECT_EQ(tg.interval(1), algorithm_.average_frame_duration());
-  EXPECT_EQ(algorithm_.last_frame_end_time(),
-            base::TimeTicks() + kLongDuration + tg.interval(1) * 3);
-}
-
 }  // namespace media
--- a/media/filters/vp9_parser_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/filters/vp9_parser_unittest.cc	2019-05-17 18:53:34.224000000 +0300
@@ -761,7 +761,7 @@
             fhdr.uncompressed_header_size);
 }
 
-INSTANTIATE_TEST_SUITE_P(, Vp9ParserTest, ::testing::ValuesIn(kTestParams));
+INSTANTIATE_TEST_CASE_P(, Vp9ParserTest, ::testing::ValuesIn(kTestParams));
 
 TEST_F(Vp9ParserTest, CheckColorSpace) {
   Vp9FrameHeader fhdr{};
--- a/media/formats/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/BUILD.gn	2019-05-17 18:53:34.224000000 +0300
@@ -14,7 +14,6 @@
     "//media/cdm",
     "//media/filters",
     "//media/muxers",
-    "//media/ffmpeg",
   ]
 
   sources = [
@@ -74,9 +73,9 @@
   ]
 
   deps = [
-    "//media:media_buildflags",
     "//media/base",
     "//skia",
+    "//third_party/libaom:av1_buildflags",
     "//third_party/libwebm",
   ]
 
--- a/media/formats/mp2t/descriptors.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp2t/descriptors.cc	2019-05-17 18:53:34.228000000 +0300
@@ -9,6 +9,7 @@
 #include "base/logging.h"
 #include "media/base/bit_reader.h"
 #include "media/base/encryption_pattern.h"
+#include "media/base/encryption_scheme.h"
 #include "media/formats/mp2t/mp2t_common.h"
 
 namespace media {
@@ -114,7 +115,7 @@
 
 bool Descriptors::HasCADescriptorCenc(int* ca_pid,
                                       int* pssh_pid,
-                                      EncryptionMode* mode) const {
+                                      EncryptionScheme* scheme) const {
   DCHECK(ca_pid);
   DCHECK(pssh_pid);
   int system_id;
@@ -145,8 +146,8 @@
   RCHECK(reader.ReadBits(13, pssh_pid));
   // The pattern is actually set differently for audio and video, so OK not to
   // set it here. Important thing is to set the cipher mode.
-  *mode = EncryptionMode::kCbcs;
-
+  *scheme = EncryptionScheme(EncryptionScheme::CIPHER_MODE_AES_CBC,
+                             EncryptionPattern());
   return true;
 }
 
--- a/media/formats/mp2t/descriptors.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp2t/descriptors.h	2019-05-17 18:53:34.228000000 +0300
@@ -10,11 +10,10 @@
 #include <map>
 #include <string>
 
-#include "media/base/decrypt_config.h"
-
 namespace media {
 
 class BitReader;
+class EncryptionScheme;
 
 namespace mp2t {
 
@@ -49,10 +48,10 @@
   // Indicates whether a CA descriptor is present, and if so, whether it is
   // of the type defined by ISO/IEC 23001-9:2014 (i.e. with a specific
   // system_id value and layout of the private_data). If so, the |ca_pid|,
-  // |pssh_pid| and |mode| are populated with the contents of the descriptor.
+  // |pssh_pid| and |scheme| are populated with the contents of the descriptor.
   bool HasCADescriptorCenc(int* ca_pid,
                            int* pssh_pid,
-                           EncryptionMode* mode) const;
+                           EncryptionScheme* scheme) const;
 
   // Indicates whether a Private Data Indicator descriptor is present with a
   // particular |value|.
--- a/media/formats/mp2t/es_parser_adts.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp2t/es_parser_adts.cc	2019-05-17 18:53:34.228000000 +0300
@@ -15,6 +15,7 @@
 #include "media/base/bit_reader.h"
 #include "media/base/channel_layout.h"
 #include "media/base/encryption_pattern.h"
+#include "media/base/encryption_scheme.h"
 #include "media/base/media_util.h"
 #include "media/base/stream_parser_buffer.h"
 #include "media/base/timestamp_constants.h"
@@ -219,7 +220,7 @@
             std::make_unique<DecryptConfig>(
                 base_decrypt_config->encryption_mode(),
                 base_decrypt_config->key_id(), base_decrypt_config->iv(),
-                subsamples, EncryptionPattern()));
+                subsamples, base_decrypt_config->encryption_pattern()));
       }
     }
 #endif
--- a/media/formats/mp2t/es_parser_h264.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp2t/es_parser_h264.cc	2019-05-17 18:53:34.228000000 +0300
@@ -11,6 +11,7 @@
 #include "base/optional.h"
 #include "media/base/decrypt_config.h"
 #include "media/base/encryption_pattern.h"
+#include "media/base/encryption_scheme.h"
 #include "media/base/media_util.h"
 #include "media/base/stream_parser_buffer.h"
 #include "media/base/timestamp_constants.h"
@@ -484,7 +485,9 @@
             DecryptConfig::CreateCbcsConfig(
                 base_decrypt_config->key_id(), base_decrypt_config->iv(),
                 subsamples,
-                EncryptionPattern(kSampleAESEncryptBlocks,
+                base_decrypt_config->HasPattern()
+                    ? base_decrypt_config->encryption_pattern()
+                    : EncryptionPattern(kSampleAESEncryptBlocks,
                                   kSampleAESSkipBlocks)));
         break;
     }
--- a/media/formats/mp2t/mp2t_stream_parser.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp2t/mp2t_stream_parser.cc	2019-05-17 18:53:34.228000000 +0300
@@ -427,9 +427,9 @@
 bool Mp2tStreamParser::ShouldForceEncryptedParser() {
   // If we expect to handle encrypted data later in the stream, then force the
   // use of the encrypted parser variant so that the initial configuration
-  // reflects the intended encryption mode (even if the initial segment itself
+  // reflects the intended encryption scheme (even if the initial segment itself
   // is not encrypted).
-  return initial_encryption_mode_ != EncryptionMode::kUnencrypted;
+  return initial_scheme_.is_encrypted();
 }
 
 std::unique_ptr<EsParser> Mp2tStreamParser::CreateEncryptedH264Parser(
@@ -829,9 +829,8 @@
 #if BUILDFLAG(ENABLE_HLS_SAMPLE_AES)
 std::unique_ptr<PidState> Mp2tStreamParser::MakeCatPidState() {
   std::unique_ptr<TsSection> cat_section_parser(new TsSectionCat(
-      base::BindRepeating(&Mp2tStreamParser::RegisterCencPids,
-                          base::Unretained(this)),
-      base::BindRepeating(&Mp2tStreamParser::RegisterEncryptionMode,
+      base::Bind(&Mp2tStreamParser::RegisterCencPids, base::Unretained(this)),
+      base::Bind(&Mp2tStreamParser::RegisterEncryptionScheme,
                           base::Unretained(this))));
   std::unique_ptr<PidState> cat_pid_state(new PidState(
       TsSection::kPidCat, PidState::kPidCat, std::move(cat_section_parser)));
@@ -881,10 +880,11 @@
   }
 }
 
-void Mp2tStreamParser::RegisterEncryptionMode(EncryptionMode mode) {
+void Mp2tStreamParser::RegisterEncryptionScheme(
+    const EncryptionScheme& scheme) {
   // We only need to record this for the initial decoder config.
   if (!is_initialized_) {
-    initial_encryption_mode_ = mode;
+    initial_scheme_ = scheme;
   }
   // Reset the DecryptConfig, so that unless and until a CENC-ECM (containing
   // key id and IV) is seen, media data will be considered unencrypted. This is
@@ -895,16 +895,16 @@
 void Mp2tStreamParser::RegisterNewKeyIdAndIv(const std::string& key_id,
                                              const std::string& iv) {
   if (!iv.empty()) {
-    switch (initial_encryption_mode_) {
-      case EncryptionMode::kUnencrypted:
+    switch (initial_scheme_.mode()) {
+      case EncryptionScheme::CIPHER_MODE_UNENCRYPTED:
         decrypt_config_.reset();
         break;
-      case EncryptionMode::kCenc:
+      case EncryptionScheme::CIPHER_MODE_AES_CTR:
         decrypt_config_ = DecryptConfig::CreateCencConfig(key_id, iv, {});
         break;
-      case EncryptionMode::kCbcs:
-        decrypt_config_ =
-            DecryptConfig::CreateCbcsConfig(key_id, iv, {}, base::nullopt);
+      case EncryptionScheme::CIPHER_MODE_AES_CBC:
+        decrypt_config_ = DecryptConfig::CreateCbcsConfig(
+            key_id, iv, {}, initial_scheme_.pattern());
         break;
     }
   }
--- a/media/formats/mp2t/mp2t_stream_parser.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp2t/mp2t_stream_parser.h	2019-05-17 18:53:34.228000000 +0300
@@ -16,6 +16,7 @@
 #include "media/base/audio_decoder_config.h"
 #include "media/base/byte_queue.h"
 #include "media/base/decrypt_config.h"
+#include "media/base/encryption_scheme.h"
 #include "media/base/media_export.h"
 #include "media/base/stream_parser.h"
 #include "media/base/video_decoder_config.h"
@@ -24,6 +25,7 @@
 
 namespace media {
 
+class DecryptConfig;
 class StreamParserBuffer;
 
 namespace mp2t {
@@ -119,10 +121,10 @@
   void RegisterCencPids(int ca_pid, int pssh_pid);
   void UnregisterCencPids();
 
-  // Register a default encryption mode to be used for decoder configs. This
+  // Register a default encryption scheme to be used for decoder configs. This
   // value is only used in the absence of explicit encryption metadata, as might
   // be the case during an unencrypted portion of a live stream.
-  void RegisterEncryptionMode(EncryptionMode mode);
+  void RegisterEncryptionScheme(const EncryptionScheme& scheme);
 
   // Register the new KeyID and IV (parsed from CENC-ECM).
   void RegisterNewKeyIdAndIv(const std::string& key_id, const std::string& iv);
@@ -171,7 +173,7 @@
   TimestampUnroller timestamp_unroller_;
 
 #if BUILDFLAG(ENABLE_HLS_SAMPLE_AES)
-  EncryptionMode initial_encryption_mode_ = EncryptionMode::kUnencrypted;
+  EncryptionScheme initial_scheme_;
 
   // TODO(jrummell): Rather than store the key_id and iv in a DecryptConfig,
   // provide a better way to access the last values seen in a ECM packet.
--- a/media/formats/mp2t/ts_section_cat.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp2t/ts_section_cat.cc	2019-05-17 18:53:34.228000000 +0300
@@ -8,6 +8,7 @@
 
 #include "base/logging.h"
 #include "media/base/bit_reader.h"
+#include "media/base/encryption_scheme.h"
 #include "media/formats/mp2t/descriptors.h"
 #include "media/formats/mp2t/mp2t_common.h"
 
@@ -16,9 +17,9 @@
 
 TsSectionCat::TsSectionCat(
     const RegisterCencPidsCb& register_cenc_ids_cb,
-    const RegisterEncryptionModeCb& register_encryption_mode_cb)
+    const RegisterEncryptionSchemeCb& register_encryption_scheme_cb)
     : register_cenc_ids_cb_(register_cenc_ids_cb),
-      register_encryption_mode_cb_(register_encryption_mode_cb),
+      register_encryption_scheme_cb_(register_encryption_scheme_cb),
       version_number_(-1) {}
 
 TsSectionCat::~TsSectionCat() {}
@@ -58,9 +59,9 @@
 
   Descriptors descriptors;
   int ca_pid, pssh_pid;
-  EncryptionMode mode;
+  EncryptionScheme scheme;
   RCHECK(descriptors.Read(bit_reader, section_length - 4));
-  RCHECK(descriptors.HasCADescriptorCenc(&ca_pid, &pssh_pid, &mode));
+  RCHECK(descriptors.HasCADescriptorCenc(&ca_pid, &pssh_pid, &scheme));
   int crc32;
   RCHECK(bit_reader->ReadBits(32, &crc32));
 
@@ -76,7 +77,7 @@
 
   // Can now register the PIDs and scheme.
   register_cenc_ids_cb_.Run(ca_pid, pssh_pid);
-  register_encryption_mode_cb_.Run(mode);
+  register_encryption_scheme_cb_.Run(scheme);
 
   version_number_ = version_number;
 
--- a/media/formats/mp2t/ts_section_cat.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp2t/ts_section_cat.h	2019-05-17 18:53:34.228000000 +0300
@@ -7,21 +7,23 @@
 
 #include "base/callback.h"
 #include "base/macros.h"
-#include "media/base/decrypt_config.h"
 #include "media/formats/mp2t/ts_section_psi.h"
 
 namespace media {
+
+class EncryptionScheme;
+
 namespace mp2t {
 
 class TsSectionCat : public TsSectionPsi {
  public:
   // RegisterCencPidsCb::Run(int ca_pid, int pssh_pid);
-  using RegisterCencPidsCb = base::RepeatingCallback<void(int, int)>;
-  // RegisterEncryptionMode::Run(EncryptionMode mode);
-  using RegisterEncryptionModeCb =
-      base::RepeatingCallback<void(EncryptionMode)>;
+  using RegisterCencPidsCb = base::Callback<void(int, int)>;
+  // RegisterEncryptionScheme::Run(const EncryptionScheme& scheme);
+  using RegisterEncryptionSchemeCb =
+      base::Callback<void(const EncryptionScheme&)>;
   TsSectionCat(const RegisterCencPidsCb& register_cenc_ids_cb,
-               const RegisterEncryptionModeCb& register_encryption_mode_cb);
+               const RegisterEncryptionSchemeCb& register_encryption_scheme_cb);
   ~TsSectionCat() override;
 
   // TsSectionPsi implementation.
@@ -30,7 +32,7 @@
 
  private:
   RegisterCencPidsCb register_cenc_ids_cb_;
-  RegisterEncryptionModeCb register_encryption_mode_cb_;
+  RegisterEncryptionSchemeCb register_encryption_scheme_cb_;
 
   // Parameters from the CAT.
   int version_number_;
--- a/media/formats/mp4/avc_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp4/avc_unittest.cc	2019-05-17 18:53:34.232000000 +0300
@@ -361,7 +361,7 @@
   EXPECT_EQ(0u, buf.size());
 }
 
-INSTANTIATE_TEST_SUITE_P(AVCConversionTestValues,
+INSTANTIATE_TEST_CASE_P(AVCConversionTestValues,
                          AVCConversionTest,
                          ::testing::Values(1, 2, 4));
 
--- a/media/formats/mp4/bitstream_converter.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp4/bitstream_converter.cc	2019-05-17 18:53:34.232000000 +0300
@@ -7,7 +7,7 @@
 namespace media {
 namespace mp4 {
 
-BitstreamConverter::AnalysisResult::AnalysisResult() {}
+BitstreamConverter::AnalysisResult::AnalysisResult(){};
 
 BitstreamConverter::AnalysisResult::AnalysisResult(const AnalysisResult& other)
     : is_conformant(other.is_conformant), is_keyframe(other.is_keyframe) {}
--- a/media/formats/mp4/box_definitions.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp4/box_definitions.cc	2019-05-17 18:53:34.232000000 +0300
@@ -4,17 +4,14 @@
 
 #include "media/formats/mp4/box_definitions.h"
 
-#include <algorithm>
 #include <memory>
 #include <utility>
 
 #include "base/command_line.h"
 #include "base/logging.h"
-#include "base/metrics/histogram_functions.h"
 #include "base/numerics/safe_math.h"
 #include "base/stl_util.h"
 #include "base/strings/string_number_conversions.h"
-#include "base/strings/string_piece.h"
 #include "build/build_config.h"
 #include "media/base/media_switches.h"
 #include "media/base/media_util.h"
@@ -24,6 +21,7 @@
 #include "media/formats/mp4/es_descriptor.h"
 #include "media/formats/mp4/rcheck.h"
 #include "media/media_buildflags.h"
+#include "third_party/libaom/av1_buildflags.h"
 
 #if BUILDFLAG(USE_PROPRIETARY_CODECS)
 #include "media/formats/mp4/avc.h"
@@ -1381,11 +1379,6 @@
                       "require ISO BMFF moov to contain mvex to indicate that "
                       "Movie Fragments are to be expected.");
 
-  MetadataBox meta;
-  RCHECK(reader->MaybeReadChild(&meta));
-  base::UmaHistogramBoolean("Media.MSE.DetectedShakaPackagerInMp4",
-                            meta.used_shaka_packager);
-
   return reader->MaybeReadChildren(&pssh);
 }
 
@@ -1768,48 +1761,5 @@
   return sample_depends_on_[i];
 }
 
-ID3v2Box::ID3v2Box() = default;
-ID3v2Box::ID3v2Box(const ID3v2Box& other) = default;
-ID3v2Box::~ID3v2Box() = default;
-FourCC ID3v2Box::BoxType() const {
-  return FOURCC_ID32;
-}
-
-bool ID3v2Box::Parse(BoxReader* reader) {
-  // This is reading the ID32 box without regard for what's in it -- there will
-  // likely be binary data in this vector. We don't care though since we're just
-  // going to scan the memory without caring about sentinel values like \0.
-  RCHECK(reader->ReadVec(&id3v2_data,
-                         std::min(static_cast<size_t>(128),
-                                  reader->buffer_size() - reader->pos())));
-  return true;
-}
-
-MetadataBox::MetadataBox() : used_shaka_packager(false) {}
-MetadataBox::MetadataBox(const MetadataBox& other) = default;
-MetadataBox::~MetadataBox() = default;
-FourCC MetadataBox::BoxType() const {
-  return FOURCC_META;
-}
-
-bool MetadataBox::Parse(BoxReader* reader) {
-  RCHECK(reader->ReadFullBoxHeader());
-
-  // This is an optional box, so generate no errors.
-  if (!reader->ScanChildren())
-    return true;
-
-  ID3v2Box id3v2;
-  if (!reader->ReadChild(&id3v2))
-    return true;
-
-  constexpr char kShakaPackager[] = "shaka-packager";
-  used_shaka_packager =
-      base::StringPiece(reinterpret_cast<char*>(id3v2.id3v2_data.data()),
-                        id3v2.id3v2_data.size())
-          .find(kShakaPackager) != base::StringPiece::npos;
-  return true;
-}
-
 }  // namespace mp4
 }  // namespace media
--- a/media/formats/mp4/box_definitions.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp4/box_definitions.h	2019-05-17 18:53:34.232000000 +0300
@@ -39,7 +39,7 @@
   T(const T& other);                      \
   ~T() override;                          \
   bool Parse(BoxReader* reader) override; \
-  FourCC BoxType() const override
+  FourCC BoxType() const override;
 
 struct MEDIA_EXPORT FileType : Box {
   DECLARE_BOX_METHODS(FileType);
@@ -549,18 +549,6 @@
   std::vector<ProtectionSystemSpecificHeader> pssh;
 };
 
-struct MEDIA_EXPORT ID3v2Box : Box {
-  DECLARE_BOX_METHODS(ID3v2Box);
-
-  // Up to a maximum of the first 128 bytes of the ID3v2 box.
-  std::vector<uint8_t> id3v2_data;
-};
-
-struct MEDIA_EXPORT MetadataBox : Box {
-  DECLARE_BOX_METHODS(MetadataBox);
-  bool used_shaka_packager;
-};
-
 #undef DECLARE_BOX
 
 }  // namespace mp4
--- a/media/formats/mp4/fourccs.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp4/fourccs.h	2019-05-17 18:53:34.232000000 +0300
@@ -8,6 +8,7 @@
 #include <string>
 
 #include "media/media_buildflags.h"
+#include "third_party/libaom/av1_buildflags.h"
 
 namespace media {
 namespace mp4 {
@@ -59,7 +60,6 @@
   FOURCC_HVC1 = 0x68766331,
   FOURCC_HVCC = 0x68766343,
 #endif
-  FOURCC_ID32 = 0x49443332,
   FOURCC_IODS = 0x696f6473,
   FOURCC_MDAT = 0x6d646174,
   FOURCC_MDHD = 0x6d646864,
--- a/media/formats/mp4/mp4_stream_parser_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp4/mp4_stream_parser_unittest.cc	2019-05-17 18:53:34.236000000 +0300
@@ -16,7 +16,6 @@
 #include "base/bind_helpers.h"
 #include "base/logging.h"
 #include "base/memory/ref_counted.h"
-#include "base/test/metrics/histogram_tester.h"
 #include "base/test/scoped_feature_list.h"
 #include "base/time/time.h"
 #include "media/base/audio_decoder_config.h"
@@ -259,35 +258,6 @@
   ParseMP4File("bear-1280x720-av_frag.mp4", 512);
 }
 
-constexpr char kShakaPackagerUMA[] = "Media.MSE.DetectedShakaPackagerInMp4";
-
-TEST_F(MP4StreamParserTest, DidNotUseShakaPackager) {
-  // Encrypted files have non-zero duration and are treated as recorded streams.
-  auto params = GetDefaultInitParametersExpectations();
-  params.duration = base::TimeDelta::FromMicroseconds(2736066);
-  params.liveness = DemuxerStream::LIVENESS_RECORDED;
-  params.detected_audio_track_count = 0;
-  InitializeParserWithInitParametersExpectations(params);
-
-  base::HistogramTester tester;
-
-  // Test file has ID32 box, but no shaka player metadata.
-  ParseMP4File("bear-640x360-v_frag-cenc-senc-no-saiz-saio.mp4", 512);
-  tester.ExpectUniqueSample(kShakaPackagerUMA, 0, 1);
-}
-
-TEST_F(MP4StreamParserTest, UsedShakaPackager) {
-  auto params = GetDefaultInitParametersExpectations();
-  params.duration = base::TimeDelta::FromMicroseconds(2736000);
-  params.liveness = DemuxerStream::LIVENESS_RECORDED;
-  params.detected_audio_track_count = 0;
-  InitializeParserWithInitParametersExpectations(params);
-
-  base::HistogramTester tester;
-  ParseMP4File("bear-320x240-v_frag-vp9.mp4", 512);
-  tester.ExpectUniqueSample(kShakaPackagerUMA, 1, 1);
-}
-
 TEST_F(MP4StreamParserTest, BytewiseAppend) {
   // Ensure no incremental errors occur when parsing
   InitializeParser();
@@ -783,7 +753,7 @@
     {1, 1, VIDEO_ROTATION_0},     // Error case
     {5, 5, VIDEO_ROTATION_0},     // Error case
 };
-INSTANTIATE_TEST_SUITE_P(CheckMath,
+INSTANTIATE_TEST_CASE_P(CheckMath,
                          MP4StreamParserRotationMatrixEvaluatorTest,
                          testing::ValuesIn(rotation_test_cases));
 
--- a/media/formats/mp4/track_run_iterator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/mp4/track_run_iterator.cc	2019-05-17 18:53:34.236000000 +0300
@@ -9,7 +9,6 @@
 #include <limits>
 #include <memory>
 
-#include "base/metrics/histogram_macros.h"
 #include "base/numerics/checked_math.h"
 #include "base/numerics/safe_conversions.h"
 #include "base/stl_util.h"
@@ -426,13 +425,6 @@
           base::strict_cast<size_t>(trun.sample_count) <= max_sample_count,
           media_log_, "Metadata overhead exceeds storage limit.");
       tri.samples.resize(trun.sample_count);
-
-      int empty_sample_count = 0;
-      int empty_samples_in_sequence_count = 0;
-
-      UMA_HISTOGRAM_COUNTS_1M("Media.MSE.Mp4TrunSampleCount",
-                              trun.sample_count);
-
       for (size_t k = 0; k < trun.sample_count; k++) {
         if (!PopulateSampleInfo(*trex, traf.header, trun, edit_list_offset, k,
                                 &tri.samples[k], traf.sdtp.sample_depends_on(k),
@@ -440,23 +432,6 @@
           return false;
         }
 
-        UMA_HISTOGRAM_COUNTS_1M("Media.MSE.Mp4SampleSize", tri.samples[k].size);
-
-        if (tri.samples[k].size == 0) {
-          empty_sample_count++;
-          empty_samples_in_sequence_count++;
-        }
-
-        // Report the number of consecutive zero-sized samples seen in a
-        // sequence. Can report counts for 1 or more such sequences within the
-        // same trun, and a sequence can be as short as just 1 empty sample.
-        if (empty_samples_in_sequence_count &&
-            (tri.samples[k].size != 0 || k == trun.sample_count - 1)) {
-          UMA_HISTOGRAM_COUNTS_1M("Media.MSE.Mp4ConsecutiveEmptySamples",
-                                  empty_samples_in_sequence_count);
-          empty_samples_in_sequence_count = 0;
-        }
-
         RCHECK(std::numeric_limits<int64_t>::max() - tri.samples[k].duration >
                run_start_dts);
 
@@ -475,10 +450,6 @@
           RCHECK(GetSampleEncryptionInfoEntry(tri, index));
         is_sample_to_group_valid = sample_to_group_itr.Advance();
       }
-
-      UMA_HISTOGRAM_COUNTS_1M("Media.MSE.Mp4EmptySamplesInTRun",
-                              empty_sample_count);
-
       if (sample_encryption_entries_count > 0) {
         RCHECK(sample_encryption_entries_count >=
                sample_count_sum + trun.sample_count);
--- a/media/formats/webm/webm_cluster_parser.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_cluster_parser.cc	2019-05-17 18:53:34.236000000 +0300
@@ -36,7 +36,7 @@
 };
 
 WebMClusterParser::WebMClusterParser(
-    int64_t timecode_scale_ns,
+    int64_t timecode_scale,
     int audio_track_num,
     base::TimeDelta audio_default_duration,
     int video_track_num,
@@ -47,7 +47,7 @@
     const std::string& video_encryption_key_id,
     const AudioCodec audio_codec,
     MediaLog* media_log)
-    : timecode_multiplier_(timecode_scale_ns / 1000.0),
+    : timecode_multiplier_(timecode_scale / 1000.0),
       ignored_tracks_(ignored_tracks),
       audio_encryption_key_id_(audio_encryption_key_id),
       video_encryption_key_id_(video_encryption_key_id),
--- a/media/formats/webm/webm_cluster_parser.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_cluster_parser.h	2019-05-17 18:53:34.236000000 +0300
@@ -148,7 +148,7 @@
   typedef std::map<int, Track> TextTrackMap;
 
  public:
-  WebMClusterParser(int64_t timecode_scale_ns,
+  WebMClusterParser(int64_t timecode_scale,
                     int audio_track_num,
                     base::TimeDelta audio_default_duration,
                     int video_track_num,
--- a/media/formats/webm/webm_cluster_parser_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_cluster_parser_unittest.cc	2019-05-17 18:53:34.240000000 +0300
@@ -43,7 +43,7 @@
 MATCHER_P(OpusPacketDurationTooHigh, actual_duration_ms, "") {
   return CONTAINS_STRING(
       arg, "Warning, demuxed Opus packet with encoded duration: " +
-               base::NumberToString(static_cast<int64_t>(actual_duration_ms)) +
+               base::IntToString(actual_duration_ms) +
                "ms. Should be no greater than 120ms.");
 }
 
@@ -52,11 +52,9 @@
            opus_duration_ms,
            "") {
   return CONTAINS_STRING(
-      arg, "BlockDuration (" +
-               base::NumberToString(static_cast<int64_t>(block_duration_ms)) +
+      arg, "BlockDuration (" + base::IntToString(block_duration_ms) +
                "ms) differs significantly from encoded duration (" +
-               base::NumberToString(static_cast<int64_t>(opus_duration_ms)) +
-               "ms).");
+               base::IntToString(opus_duration_ms) + "ms).");
 }
 
 namespace {
--- a/media/formats/webm/webm_content_encodings_client_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_content_encodings_client_unittest.cc	2019-05-17 18:53:34.240000000 +0300
@@ -45,7 +45,7 @@
 
 MATCHER_P(UnexpectedContentEncAlgo, algo, "") {
   return CONTAINS_STRING(
-      arg, "Unexpected ContentEncAlgo " + base::NumberToString(algo) + ".");
+      arg, "Unexpected ContentEncAlgo " + base::IntToString(algo) + ".");
 }
 
 class WebMContentEncodingsClientTest : public testing::Test {
--- a/media/formats/webm/webm_info_parser.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_info_parser.cc	2019-05-17 18:53:34.240000000 +0300
@@ -9,16 +9,19 @@
 
 namespace media {
 
-// Default timecode scale, in nanoseconds, if the TimecodeScale element is not
-// specified in the INFO element.
+// Default timecode scale if the TimecodeScale element is
+// not specified in the INFO element.
 static const int kWebMDefaultTimecodeScale = 1000000;
 
-WebMInfoParser::WebMInfoParser() : timecode_scale_ns_(-1), duration_(-1) {}
+WebMInfoParser::WebMInfoParser()
+    : timecode_scale_(-1),
+      duration_(-1) {
+}
 
 WebMInfoParser::~WebMInfoParser() = default;
 
 int WebMInfoParser::Parse(const uint8_t* buf, int size) {
-  timecode_scale_ns_ = -1;
+  timecode_scale_ = -1;
   duration_ = -1;
 
   WebMListParser parser(kWebMIdInfo, this);
@@ -34,10 +37,10 @@
 WebMParserClient* WebMInfoParser::OnListStart(int id) { return this; }
 
 bool WebMInfoParser::OnListEnd(int id) {
-  if (id == kWebMIdInfo && timecode_scale_ns_ == -1) {
+  if (id == kWebMIdInfo && timecode_scale_ == -1) {
     // Set timecode scale to default value if it isn't present in
     // the Info element.
-    timecode_scale_ns_ = kWebMDefaultTimecodeScale;
+    timecode_scale_ = kWebMDefaultTimecodeScale;
   }
   return true;
 }
@@ -51,12 +54,12 @@
     return false;
   }
 
-  if (timecode_scale_ns_ != -1) {
+  if (timecode_scale_ != -1) {
     DVLOG(1) << "Multiple values for id " << std::hex << id << " specified";
     return false;
   }
 
-  timecode_scale_ns_ = val;
+  timecode_scale_ = val;
   return true;
 }
 
--- a/media/formats/webm/webm_info_parser.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_info_parser.h	2019-05-17 18:53:34.240000000 +0300
@@ -28,7 +28,7 @@
   // Returns the number of bytes parsed on success.
   int Parse(const uint8_t* buf, int size);
 
-  int64_t timecode_scale_ns() const { return timecode_scale_ns_; }
+  int64_t timecode_scale() const { return timecode_scale_; }
   double duration() const { return duration_; }
   base::Time date_utc() const { return date_utc_; }
 
@@ -41,7 +41,7 @@
   bool OnBinary(int id, const uint8_t* data, int size) override;
   bool OnString(int id, const std::string& str) override;
 
-  int64_t timecode_scale_ns_;
+  int64_t timecode_scale_;
   double duration_;
   base::Time date_utc_;
 
--- a/media/formats/webm/webm_parser.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_parser.cc	2019-05-17 18:53:34.240000000 +0300
@@ -25,20 +25,12 @@
 
 enum ElementType {
   UNKNOWN,
-  // The following are basic types defined in the Matroska spec.
   LIST,  // Referred to as Master Element in the Matroska spec.
   UINT,
   FLOAT,
   BINARY,
   STRING,
-  // Valid element but we don't care about them right now.
   SKIP,
-  // Aliases of SKIP to help keep type info.
-  SKIP_LIST = SKIP,
-  SKIP_UINT = SKIP,
-  SKIP_FLOAT = SKIP,
-  SKIP_BINARY = SKIP,
-  SKIP_STRING = SKIP,
 };
 
 struct ElementIdInfo {
@@ -53,24 +45,35 @@
   int id_info_count_;
 };
 
-// The following are tables indicating what IDs are valid sub-elements of
-// particular elements. If an element is encountered that doesn't appear in the
-// list, a parsing error is signalled. Elements supported by Matroska but not
-// supported by WebM are marked with SKIP_* types so that they will be skipped
-// but will not fail the parser.
+// The following are tables indicating what IDs are valid sub-elements
+// of particular elements. If an element is encountered that doesn't
+// appear in the list, a parsing error is signalled. Some elements are
+// marked as SKIP because they are valid, but we don't care about them
+// right now.
+//
+// TODO(xhwang): There are many Matroska elements listed here which are not
+// supported by WebM. Since this is a WebM parser, maybe we should not list them
+// here so that the parsing clients doesn't need to handle them.
 
 static const ElementIdInfo kEBMLHeaderIds[] = {
-    {UINT, kWebMIdEBMLVersion},        {UINT, kWebMIdEBMLReadVersion},
-    {UINT, kWebMIdEBMLMaxIDLength},    {UINT, kWebMIdEBMLMaxSizeLength},
-    {STRING, kWebMIdDocType},          {UINT, kWebMIdDocTypeVersion},
+  {UINT, kWebMIdEBMLVersion},
+  {UINT, kWebMIdEBMLReadVersion},
+  {UINT, kWebMIdEBMLMaxIDLength},
+  {UINT, kWebMIdEBMLMaxSizeLength},
+  {STRING, kWebMIdDocType},
+  {UINT, kWebMIdDocTypeVersion},
     {UINT, kWebMIdDocTypeReadVersion},
 };
 
 static const ElementIdInfo kSegmentIds[] = {
-    {LIST, kWebMIdSeekHead}, {LIST, kWebMIdInfo},
-    {LIST, kWebMIdCluster},  {LIST, kWebMIdTracks},
-    {LIST, kWebMIdCues},     {SKIP_LIST, kWebMIdAttachments},
-    {LIST, kWebMIdChapters}, {LIST, kWebMIdTags},
+  {LIST, kWebMIdSeekHead},
+  {LIST, kWebMIdInfo},
+  {LIST, kWebMIdCluster},
+  {LIST, kWebMIdTracks},
+  {LIST, kWebMIdCues},
+  {LIST, kWebMIdAttachments},
+  {LIST, kWebMIdChapters},
+  {LIST, kWebMIdTags},
 };
 
 static const ElementIdInfo kSeekHeadIds[] = {
@@ -83,14 +86,14 @@
 };
 
 static const ElementIdInfo kInfoIds[] = {
-    {SKIP_BINARY, kWebMIdSegmentUID},
-    {SKIP_STRING, kWebMIdSegmentFilename},
-    {SKIP_BINARY, kWebMIdPrevUID},
-    {SKIP_STRING, kWebMIdPrevFilename},
-    {SKIP_BINARY, kWebMIdNextUID},
-    {SKIP_STRING, kWebMIdNextFilename},
-    {SKIP_BINARY, kWebMIdSegmentFamily},
-    {SKIP_LIST, kWebMIdChapterTranslate},
+  {BINARY, kWebMIdSegmentUID},
+  {STRING, kWebMIdSegmentFilename},
+  {BINARY, kWebMIdPrevUID},
+  {STRING, kWebMIdPrevFilename},
+  {BINARY, kWebMIdNextUID},
+  {STRING, kWebMIdNextFilename},
+  {BINARY, kWebMIdSegmentFamily},
+  {LIST, kWebMIdChapterTranslate},
     {UINT, kWebMIdTimecodeScale},
     {FLOAT, kWebMIdDuration},
     {BINARY, kWebMIdDateUTC},
@@ -100,26 +103,33 @@
 };
 
 static const ElementIdInfo kChapterTranslateIds[] = {
-    {SKIP_UINT, kWebMIdChapterTranslateEditionUID},
-    {SKIP_UINT, kWebMIdChapterTranslateCodec},
-    {SKIP_BINARY, kWebMIdChapterTranslateID},
+  {UINT, kWebMIdChapterTranslateEditionUID},
+  {UINT, kWebMIdChapterTranslateCodec},
+  {BINARY, kWebMIdChapterTranslateID},
 };
 
 static const ElementIdInfo kClusterIds[] = {
-    {BINARY, kWebMIdSimpleBlock},     {UINT, kWebMIdTimecode},
-    {SKIP_LIST, kWebMIdSilentTracks}, {SKIP_UINT, kWebMIdPosition},
-    {UINT, kWebMIdPrevSize},          {LIST, kWebMIdBlockGroup},
+  {BINARY, kWebMIdSimpleBlock},
+  {UINT, kWebMIdTimecode},
+  {LIST, kWebMIdSilentTracks},
+  {UINT, kWebMIdPosition},
+  {UINT, kWebMIdPrevSize},
+  {LIST, kWebMIdBlockGroup},
 };
 
 static const ElementIdInfo kSilentTracksIds[] = {
-    {SKIP_UINT, kWebMIdSilentTrackNumber},
+  {UINT, kWebMIdSilentTrackNumber},
 };
 
 static const ElementIdInfo kBlockGroupIds[] = {
-    {BINARY, kWebMIdBlock},          {LIST, kWebMIdBlockAdditions},
-    {UINT, kWebMIdBlockDuration},    {SKIP_UINT, kWebMIdReferencePriority},
-    {BINARY, kWebMIdReferenceBlock}, {SKIP_BINARY, kWebMIdCodecState},
-    {BINARY, kWebMIdDiscardPadding}, {SKIP_LIST, kWebMIdSlices},
+  {BINARY, kWebMIdBlock},
+  {LIST, kWebMIdBlockAdditions},
+  {UINT, kWebMIdBlockDuration},
+  {UINT, kWebMIdReferencePriority},
+  {BINARY, kWebMIdReferenceBlock},
+  {BINARY, kWebMIdCodecState},
+  {BINARY, kWebMIdDiscardPadding},
+  {LIST, kWebMIdSlices},
 };
 
 static const ElementIdInfo kBlockAdditionsIds[] = {
@@ -132,11 +142,11 @@
 };
 
 static const ElementIdInfo kSlicesIds[] = {
-    {SKIP_LIST, kWebMIdTimeSlice},
+  {LIST, kWebMIdTimeSlice},
 };
 
 static const ElementIdInfo kTimeSliceIds[] = {
-    {SKIP_UINT, kWebMIdLaceNumber},
+  {UINT, kWebMIdLaceNumber},
 };
 
 static const ElementIdInfo kTracksIds[] = {
@@ -151,32 +161,32 @@
     {UINT, kWebMIdFlagDefault},
     {UINT, kWebMIdFlagForced},
     {UINT, kWebMIdFlagLacing},
-    {SKIP_UINT, kWebMIdMinCache},
-    {SKIP_UINT, kWebMIdMaxCache},
+  {UINT, kWebMIdMinCache},
+  {UINT, kWebMIdMaxCache},
     {UINT, kWebMIdDefaultDuration},
-    {SKIP_FLOAT, kWebMIdTrackTimecodeScale},
-    {SKIP_UINT, kWebMIdMaxBlockAdditionId},
+  {FLOAT, kWebMIdTrackTimecodeScale},
+  {UINT, kWebMIdMaxBlockAdditionId},
     {STRING, kWebMIdName},
     {STRING, kWebMIdLanguage},
     {STRING, kWebMIdCodecID},
     {BINARY, kWebMIdCodecPrivate},
     {STRING, kWebMIdCodecName},
-    {SKIP_UINT, kWebMIdAttachmentLink},
-    {SKIP_UINT, kWebMIdCodecDecodeAll},
-    {SKIP_UINT, kWebMIdTrackOverlay},
+  {UINT, kWebMIdAttachmentLink},
+  {UINT, kWebMIdCodecDecodeAll},
+  {UINT, kWebMIdTrackOverlay},
     {UINT, kWebMIdCodecDelay},
     {UINT, kWebMIdSeekPreRoll},
-    {SKIP_LIST, kWebMIdTrackTranslate},
+  {LIST, kWebMIdTrackTranslate},
     {LIST, kWebMIdVideo},
     {LIST, kWebMIdAudio},
-    {SKIP_LIST, kWebMIdTrackOperation},
+  {LIST, kWebMIdTrackOperation},
     {LIST, kWebMIdContentEncodings},
 };
 
 static const ElementIdInfo kTrackTranslateIds[] = {
-    {SKIP_UINT, kWebMIdTrackTranslateEditionUID},
-    {SKIP_UINT, kWebMIdTrackTranslateCodec},
-    {SKIP_BINARY, kWebMIdTrackTranslateTrackID},
+  {UINT, kWebMIdTrackTranslateEditionUID},
+  {UINT, kWebMIdTrackTranslateCodec},
+  {BINARY, kWebMIdTrackTranslateTrackID},
 };
 
 static const ElementIdInfo kVideoIds[] = {
@@ -186,8 +196,8 @@
     {UINT, kWebMIdPixelCropTop},    {UINT, kWebMIdPixelCropLeft},
     {UINT, kWebMIdPixelCropRight},  {UINT, kWebMIdDisplayWidth},
     {UINT, kWebMIdDisplayHeight},   {UINT, kWebMIdDisplayUnit},
-    {UINT, kWebMIdAspectRatioType}, {SKIP_BINARY, kWebMIdColorSpace},
-    {SKIP_FLOAT, kWebMIdFrameRate}, {LIST, kWebMIdColour},
+    {UINT, kWebMIdAspectRatioType}, {BINARY, kWebMIdColorSpace},
+    {FLOAT, kWebMIdFrameRate},      {LIST, kWebMIdColour},
 };
 
 static const ElementIdInfo kColourIds[] = {
@@ -228,21 +238,21 @@
 };
 
 static const ElementIdInfo kTrackOperationIds[] = {
-    {SKIP_LIST, kWebMIdTrackCombinePlanes},
-    {SKIP_LIST, kWebMIdJoinBlocks},
+  {LIST, kWebMIdTrackCombinePlanes},
+  {LIST, kWebMIdJoinBlocks},
 };
 
 static const ElementIdInfo kTrackCombinePlanesIds[] = {
-    {SKIP_LIST, kWebMIdTrackPlane},
+  {LIST, kWebMIdTrackPlane},
 };
 
 static const ElementIdInfo kTrackPlaneIds[] = {
-    {SKIP_UINT, kWebMIdTrackPlaneUID},
-    {SKIP_UINT, kWebMIdTrackPlaneType},
+  {UINT, kWebMIdTrackPlaneUID},
+  {UINT, kWebMIdTrackPlaneType},
 };
 
 static const ElementIdInfo kJoinBlocksIds[] = {
-    {SKIP_UINT, kWebMIdTrackJoinUID},
+  {UINT, kWebMIdTrackJoinUID},
 };
 
 static const ElementIdInfo kContentEncodingsIds[] = {
@@ -250,24 +260,26 @@
 };
 
 static const ElementIdInfo kContentEncodingIds[] = {
-    {UINT, kWebMIdContentEncodingOrder}, {UINT, kWebMIdContentEncodingScope},
-    {UINT, kWebMIdContentEncodingType},  {SKIP_LIST, kWebMIdContentCompression},
+  {UINT, kWebMIdContentEncodingOrder},
+  {UINT, kWebMIdContentEncodingScope},
+  {UINT, kWebMIdContentEncodingType},
+  {LIST, kWebMIdContentCompression},
     {LIST, kWebMIdContentEncryption},
 };
 
 static const ElementIdInfo kContentCompressionIds[] = {
-    {SKIP_UINT, kWebMIdContentCompAlgo},
-    {SKIP_BINARY, kWebMIdContentCompSettings},
+  {UINT, kWebMIdContentCompAlgo},
+  {BINARY, kWebMIdContentCompSettings},
 };
 
 static const ElementIdInfo kContentEncryptionIds[] = {
     {LIST, kWebMIdContentEncAESSettings},
     {UINT, kWebMIdContentEncAlgo},
     {BINARY, kWebMIdContentEncKeyID},
-    {SKIP_BINARY, kWebMIdContentSignature},
-    {SKIP_BINARY, kWebMIdContentSigKeyID},
-    {SKIP_UINT, kWebMIdContentSigAlgo},
-    {SKIP_UINT, kWebMIdContentSigHashAlgo},
+  {BINARY, kWebMIdContentSignature},
+  {BINARY, kWebMIdContentSigKeyID},
+  {UINT, kWebMIdContentSigAlgo},
+  {UINT, kWebMIdContentSigHashAlgo},
 };
 
 static const ElementIdInfo kContentEncAESSettingsIds[] = {
@@ -284,23 +296,27 @@
 };
 
 static const ElementIdInfo kCueTrackPositionsIds[] = {
-    {UINT, kWebMIdCueTrack},          {UINT, kWebMIdCueClusterPosition},
-    {UINT, kWebMIdCueBlockNumber},    {SKIP_UINT, kWebMIdCueCodecState},
-    {SKIP_LIST, kWebMIdCueReference},
+  {UINT, kWebMIdCueTrack},
+  {UINT, kWebMIdCueClusterPosition},
+  {UINT, kWebMIdCueBlockNumber},
+  {UINT, kWebMIdCueCodecState},
+  {LIST, kWebMIdCueReference},
 };
 
 static const ElementIdInfo kCueReferenceIds[] = {
-    {SKIP_UINT, kWebMIdCueRefTime},
+  {UINT, kWebMIdCueRefTime},
 };
 
 static const ElementIdInfo kAttachmentsIds[] = {
-    {SKIP_LIST, kWebMIdAttachedFile},
+  {LIST, kWebMIdAttachedFile},
 };
 
 static const ElementIdInfo kAttachedFileIds[] = {
-    {SKIP_STRING, kWebMIdFileDescription}, {SKIP_STRING, kWebMIdFileName},
-    {SKIP_STRING, kWebMIdFileMimeType},    {SKIP_BINARY, kWebMIdFileData},
-    {SKIP_UINT, kWebMIdFileUID},
+  {STRING, kWebMIdFileDescription},
+  {STRING, kWebMIdFileName},
+  {STRING, kWebMIdFileMimeType},
+  {BINARY, kWebMIdFileData},
+  {UINT, kWebMIdFileUID},
 };
 
 static const ElementIdInfo kChaptersIds[] = {
@@ -308,10 +324,10 @@
 };
 
 static const ElementIdInfo kEditionEntryIds[] = {
-    {SKIP_UINT, kWebMIdEditionUID},
-    {SKIP_UINT, kWebMIdEditionFlagHidden},
-    {SKIP_UINT, kWebMIdEditionFlagDefault},
-    {SKIP_UINT, kWebMIdEditionFlagOrdered},
+  {UINT, kWebMIdEditionUID},
+  {UINT, kWebMIdEditionFlagHidden},
+  {UINT, kWebMIdEditionFlagDefault},
+  {UINT, kWebMIdEditionFlagOrdered},
     {LIST, kWebMIdChapterAtom},
 };
 
@@ -319,18 +335,18 @@
     {UINT, kWebMIdChapterUID},
     {UINT, kWebMIdChapterTimeStart},
     {UINT, kWebMIdChapterTimeEnd},
-    {SKIP_UINT, kWebMIdChapterFlagHidden},
-    {SKIP_UINT, kWebMIdChapterFlagEnabled},
-    {SKIP_BINARY, kWebMIdChapterSegmentUID},
-    {SKIP_UINT, kWebMIdChapterSegmentEditionUID},
-    {SKIP_UINT, kWebMIdChapterPhysicalEquiv},
-    {SKIP_LIST, kWebMIdChapterTrack},
+  {UINT, kWebMIdChapterFlagHidden},
+  {UINT, kWebMIdChapterFlagEnabled},
+  {BINARY, kWebMIdChapterSegmentUID},
+  {UINT, kWebMIdChapterSegmentEditionUID},
+  {UINT, kWebMIdChapterPhysicalEquiv},
+  {LIST, kWebMIdChapterTrack},
     {LIST, kWebMIdChapterDisplay},
-    {SKIP_LIST, kWebMIdChapProcess},
+  {LIST, kWebMIdChapProcess},
 };
 
 static const ElementIdInfo kChapterTrackIds[] = {
-    {SKIP_UINT, kWebMIdChapterTrackNumber},
+  {UINT, kWebMIdChapterTrackNumber},
 };
 
 static const ElementIdInfo kChapterDisplayIds[] = {
@@ -340,14 +356,14 @@
 };
 
 static const ElementIdInfo kChapProcessIds[] = {
-    {SKIP_UINT, kWebMIdChapProcessCodecID},
-    {SKIP_BINARY, kWebMIdChapProcessPrivate},
-    {SKIP_LIST, kWebMIdChapProcessCommand},
+  {UINT, kWebMIdChapProcessCodecID},
+  {BINARY, kWebMIdChapProcessPrivate},
+  {LIST, kWebMIdChapProcessCommand},
 };
 
 static const ElementIdInfo kChapProcessCommandIds[] = {
-    {SKIP_UINT, kWebMIdChapProcessTime},
-    {SKIP_BINARY, kWebMIdChapProcessData},
+  {UINT, kWebMIdChapProcessTime},
+  {BINARY, kWebMIdChapProcessData},
 };
 
 static const ElementIdInfo kTagsIds[] = {
@@ -360,14 +376,19 @@
 };
 
 static const ElementIdInfo kTargetsIds[] = {
-    {UINT, kWebMIdTargetTypeValue},    {STRING, kWebMIdTargetType},
-    {UINT, kWebMIdTagTrackUID},        {SKIP_UINT, kWebMIdTagEditionUID},
-    {SKIP_UINT, kWebMIdTagChapterUID}, {SKIP_UINT, kWebMIdTagAttachmentUID},
+  {UINT, kWebMIdTargetTypeValue},
+  {STRING, kWebMIdTargetType},
+  {UINT, kWebMIdTagTrackUID},
+  {UINT, kWebMIdTagEditionUID},
+  {UINT, kWebMIdTagChapterUID},
+  {UINT, kWebMIdTagAttachmentUID},
 };
 
 static const ElementIdInfo kSimpleTagIds[] = {
-    {STRING, kWebMIdTagName},   {STRING, kWebMIdTagLanguage},
-    {UINT, kWebMIdTagDefault},  {STRING, kWebMIdTagString},
+  {STRING, kWebMIdTagName},
+  {STRING, kWebMIdTagLanguage},
+  {UINT, kWebMIdTagDefault},
+  {STRING, kWebMIdTagString},
     {BINARY, kWebMIdTagBinary},
 };
 
--- a/media/formats/webm/webm_stream_parser.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_stream_parser.cc	2019-05-17 18:53:34.240000000 +0300
@@ -202,8 +202,7 @@
 
   bytes_parsed += result;
 
-  int64_t timecode_scale_in_ns = info_parser.timecode_scale_ns();
-  double timecode_scale_in_us = timecode_scale_in_ns / 1000.0;
+  double timecode_scale_in_us = info_parser.timecode_scale() / 1000.0;
   InitParameters params(kInfiniteDuration);
 
   if (info_parser.duration() > 0) {
@@ -238,10 +237,10 @@
   }
 
   cluster_parser_.reset(new WebMClusterParser(
-      timecode_scale_in_ns, tracks_parser.audio_track_num(),
-      tracks_parser.GetAudioDefaultDuration(timecode_scale_in_ns),
+      info_parser.timecode_scale(), tracks_parser.audio_track_num(),
+      tracks_parser.GetAudioDefaultDuration(timecode_scale_in_us),
       tracks_parser.video_track_num(),
-      tracks_parser.GetVideoDefaultDuration(timecode_scale_in_ns),
+      tracks_parser.GetVideoDefaultDuration(timecode_scale_in_us),
       tracks_parser.text_tracks(), tracks_parser.ignored_tracks(),
       tracks_parser.audio_encryption_key_id(),
       tracks_parser.video_encryption_key_id(), audio_config.codec(),
--- a/media/formats/webm/webm_tracks_parser.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_tracks_parser.cc	2019-05-17 18:53:34.240000000 +0300
@@ -30,6 +30,21 @@
   return kTextNone;
 }
 
+static base::TimeDelta PrecisionCappedDefaultDuration(
+    const double timecode_scale_in_us,
+    const int64_t duration_in_ns) {
+  if (duration_in_ns <= 0)
+    return kNoTimestamp;
+
+  int64_t mult = duration_in_ns / 1000;
+  mult /= timecode_scale_in_us;
+  if (mult == 0)
+    return kNoTimestamp;
+
+  mult = static_cast<double>(mult) * timecode_scale_in_us;
+  return base::TimeDelta::FromMicroseconds(mult);
+}
+
 WebMTracksParser::WebMTracksParser(MediaLog* media_log, bool ignore_text_tracks)
     : ignore_text_tracks_(ignore_text_tracks),
       media_log_(media_log),
@@ -40,24 +55,6 @@
 
 WebMTracksParser::~WebMTracksParser() = default;
 
-base::TimeDelta WebMTracksParser::PrecisionCappedDefaultDuration(
-    const int64_t timecode_scale_in_ns,
-    const int64_t duration_in_ns) const {
-  DCHECK_GT(timecode_scale_in_ns, 0);
-  if (duration_in_ns <= 0)
-    return kNoTimestamp;
-
-  // Calculate the (integral) number of complete |timecode_scale_in_ns|
-  // intervals that fit within |duration_in_ns|.
-  int64_t intervals = duration_in_ns / timecode_scale_in_ns;
-
-  int64_t result_us = (intervals * timecode_scale_in_ns) / 1000;
-  if (result_us == 0)
-    return kNoTimestamp;
-
-  return base::TimeDelta::FromMicroseconds(result_us);
-}
-
 void WebMTracksParser::Reset() {
   ResetTrackEntry();
   reset_on_next_parse_ = false;
@@ -106,14 +103,14 @@
 }
 
 base::TimeDelta WebMTracksParser::GetAudioDefaultDuration(
-    const int64_t timecode_scale_in_ns) const {
-  return PrecisionCappedDefaultDuration(timecode_scale_in_ns,
+    const double timecode_scale_in_us) const {
+  return PrecisionCappedDefaultDuration(timecode_scale_in_us,
                                         audio_default_duration_);
 }
 
 base::TimeDelta WebMTracksParser::GetVideoDefaultDuration(
-    const int64_t timecode_scale_in_ns) const {
-  return PrecisionCappedDefaultDuration(timecode_scale_in_ns,
+    const double timecode_scale_in_us) const {
+  return PrecisionCappedDefaultDuration(timecode_scale_in_us,
                                         video_default_duration_);
 }
 
@@ -269,7 +266,7 @@
         MEDIA_LOG(DEBUG, media_log_) << "Ignoring text track " << track_num_;
         ignored_tracks_.insert(track_num_);
       } else {
-        std::string track_num = base::NumberToString(track_num_);
+        std::string track_num = base::Int64ToString(track_num_);
         text_tracks_[track_num_] = TextTrackConfig(
             text_track_kind, track_name_, track_language_, track_num);
       }
--- a/media/formats/webm/webm_tracks_parser.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_tracks_parser.h	2019-05-17 18:53:34.240000000 +0300
@@ -46,12 +46,12 @@
 
   // If TrackEntry DefaultDuration field existed for the associated audio or
   // video track, returns that value converted from ns to base::TimeDelta with
-  // precision not greater than |timecode_scale_in_ns|. Defaults to
+  // precision not greater than |timecode_scale_in_us|. Defaults to
   // kNoTimestamp.
   base::TimeDelta GetAudioDefaultDuration(
-      const int64_t timecode_scale_in_ns) const;
+      const double timecode_scale_in_us) const;
   base::TimeDelta GetVideoDefaultDuration(
-      const int64_t timecode_scale_in_ns) const;
+      const double timecode_scale_in_us) const;
 
   const std::set<int64_t>& ignored_tracks() const { return ignored_tracks_; }
 
@@ -93,19 +93,6 @@
   }
 
  private:
-  // To test PrecisionCappedDefaultDuration.
-  FRIEND_TEST_ALL_PREFIXES(WebMTracksParserTest, PrecisionCapping);
-
-  // Returns the conversion of |duration_in_ns| to a microsecond-granularity
-  // TimeDelta with precision no greater than |timecode_scale_in_ns|.
-  // Returns kNoTimestamp if |duration_in_ns| is <= 0, or the capped precision
-  // of the converted |duration_in_ns| is < 1 microsecond.
-  // Commonly, |timecode_scale_in_ns| is 1000000 (1 millisecond), though the
-  // muxed stream could have used a different time scale.
-  base::TimeDelta PrecisionCappedDefaultDuration(
-      const int64_t timecode_scale_in_ns,
-      const int64_t duration_in_ns) const;
-
   void Reset();
   void ResetTrackEntry();
 
--- a/media/formats/webm/webm_tracks_parser_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_tracks_parser_unittest.cc	2019-05-17 18:53:34.240000000 +0300
@@ -26,8 +26,7 @@
 
 namespace media {
 
-// WebM muxing commonly uses 1 millisecond resolution.
-static const int64_t kOneMsInNs = 1000000;
+static const double kDefaultTimecodeScaleInUs = 1000.0;  // 1 ms resolution
 
 class WebMTracksParserTest : public testing::Test {
  public:
@@ -153,8 +152,10 @@
   EXPECT_LE(0, result);
   EXPECT_EQ(static_cast<int>(buf.size()), result);
 
-  EXPECT_EQ(kNoTimestamp, parser->GetAudioDefaultDuration(kOneMsInNs));
-  EXPECT_EQ(kNoTimestamp, parser->GetVideoDefaultDuration(kOneMsInNs));
+  EXPECT_EQ(kNoTimestamp,
+            parser->GetAudioDefaultDuration(kDefaultTimecodeScaleInUs));
+  EXPECT_EQ(kNoTimestamp,
+            parser->GetVideoDefaultDuration(kDefaultTimecodeScaleInUs));
 
   const VideoDecoderConfig& video_config = parser->video_decoder_config();
   EXPECT_TRUE(video_config.IsValidConfig());
@@ -182,14 +183,14 @@
   EXPECT_EQ(static_cast<int>(buf.size()), result);
 
   EXPECT_EQ(base::TimeDelta::FromMicroseconds(12000),
-            parser->GetAudioDefaultDuration(kOneMsInNs));
+            parser->GetAudioDefaultDuration(kDefaultTimecodeScaleInUs));
   EXPECT_EQ(base::TimeDelta::FromMicroseconds(985000),
-            parser->GetVideoDefaultDuration(5000000));  // 5 ms resolution
-  EXPECT_EQ(kNoTimestamp, parser->GetAudioDefaultDuration(12346000));
+            parser->GetVideoDefaultDuration(5000.0));  // 5 ms resolution
+  EXPECT_EQ(kNoTimestamp, parser->GetAudioDefaultDuration(12346.0));
   EXPECT_EQ(base::TimeDelta::FromMicroseconds(12345),
-            parser->GetAudioDefaultDuration(12345000));
+            parser->GetAudioDefaultDuration(12345.0));
   EXPECT_EQ(base::TimeDelta::FromMicroseconds(12003),
-            parser->GetAudioDefaultDuration(1000300));  // 1.0003 ms resolution
+            parser->GetAudioDefaultDuration(1000.3));  // 1.0003 ms resolution
 }
 
 TEST_F(WebMTracksParserTest, InvalidZeroDefaultDurationSet) {
@@ -250,49 +251,4 @@
   EXPECT_GT(parser->Parse(&buf[0], buf.size()),0);
 }
 
-TEST_F(WebMTracksParserTest, PrecisionCapping) {
-  struct CappingCases {
-    int64_t scale_ns;
-    int64_t duration_ns;
-    base::TimeDelta expected_result;
-  };
-
-  const CappingCases kCappingCases[] = {
-      {kOneMsInNs, -1, kNoTimestamp},
-      {kOneMsInNs, 0, kNoTimestamp},
-      {kOneMsInNs, 1, kNoTimestamp},
-      {kOneMsInNs, 999999, kNoTimestamp},
-      {kOneMsInNs, 1000000, base::TimeDelta::FromMilliseconds(1)},
-      {kOneMsInNs, 1000001, base::TimeDelta::FromMilliseconds(1)},
-      {kOneMsInNs, 1999999, base::TimeDelta::FromMilliseconds(1)},
-      {kOneMsInNs, 2000000, base::TimeDelta::FromMilliseconds(2)},
-      {1, -1, kNoTimestamp},
-      {1, 0, kNoTimestamp},
-
-      // Result < 1us, so kNoTimestamp
-      {1, 1, kNoTimestamp},
-      {1, 999, kNoTimestamp},
-
-      {1, 1000, base::TimeDelta::FromMicroseconds(1)},
-      {1, 1999, base::TimeDelta::FromMicroseconds(1)},
-      {1, 2000, base::TimeDelta::FromMicroseconds(2)},
-
-      {64, 1792, base::TimeDelta::FromMicroseconds(1)},
-  };
-
-  std::unique_ptr<WebMTracksParser> parser(
-      new WebMTracksParser(&media_log_, false));
-
-  for (size_t i = 0; i < base::size(kCappingCases); ++i) {
-    InSequence s;
-    int64_t scale_ns = kCappingCases[i].scale_ns;
-    int64_t duration_ns = kCappingCases[i].duration_ns;
-    base::TimeDelta expected_result = kCappingCases[i].expected_result;
-
-    EXPECT_EQ(parser->PrecisionCappedDefaultDuration(scale_ns, duration_ns),
-              expected_result)
-        << i << ": " << scale_ns << ", " << duration_ns;
-  }
-}
-
 }  // namespace media
--- a/media/formats/webm/webm_video_client.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_video_client.cc	2019-05-17 18:53:34.240000000 +0300
@@ -6,7 +6,7 @@
 
 #include "media/base/video_decoder_config.h"
 #include "media/formats/webm/webm_constants.h"
-#include "media/media_buildflags.h"
+#include "third_party/libaom/av1_buildflags.h"
 
 namespace media {
 
--- a/media/formats/webm/webm_video_client_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/formats/webm/webm_video_client_unittest.cc	2019-05-17 18:53:34.240000000 +0300
@@ -67,7 +67,7 @@
       << expected_config.AsHumanReadableString() << ")";
 }
 
-INSTANTIATE_TEST_SUITE_P(/* No prefix. */,
+INSTANTIATE_TEST_CASE_P(/* No prefix. */,
                          WebMVideoClientTest,
                          ::testing::ValuesIn(kCodecTestParams));
 
--- a/media/gpu/accelerated_video_decoder.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/accelerated_video_decoder.h	2019-05-17 18:53:34.240000000 +0300
@@ -66,13 +66,11 @@
   // we need a new set of them, or when an error occurs.
   virtual DecodeResult Decode() WARN_UNUSED_RESULT = 0;
 
-  // Return dimensions/required number of pictures that client should be ready
-  // to provide for the decoder to function properly (of which up to
-  // GetNumReferenceFrames() might be needed for internal decoding). To be used
-  // after Decode() returns kAllocateNewSurfaces.
+  // Return dimensions/required number of output surfaces that client should
+  // be ready to provide for the decoder to function properly.
+  // To be used after Decode() returns kAllocateNewSurfaces.
   virtual gfx::Size GetPicSize() const = 0;
   virtual size_t GetRequiredNumOfPictures() const = 0;
-  virtual size_t GetNumReferenceFrames() const = 0;
 
   // About 3 secs for 30 fps video. When the new sized keyframe is missed, the
   // decoder cannot decode the frame. The number of frames are skipped until
--- a/media/gpu/android/android_video_decode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/android_video_decode_accelerator.cc	2019-05-17 18:53:34.244000000 +0300
@@ -723,7 +723,7 @@
   // TODO(dwkang): check if there is a way to remove this workaround.
   base::ThreadTaskRunnerHandle::Get()->PostTask(
       FROM_HERE,
-      base::BindOnce(&AndroidVideoDecodeAccelerator::NotifyEndOfBitstreamBuffer,
+      base::Bind(&AndroidVideoDecodeAccelerator::NotifyEndOfBitstreamBuffer,
                      weak_this_factory_.GetWeakPtr(), bitstream_buffer.id()));
   bitstreams_notified_in_advance_.push_back(bitstream_buffer.id());
 
@@ -827,8 +827,7 @@
           picturebuffers_requested_ = true;
           base::ThreadTaskRunnerHandle::Get()->PostTask(
               FROM_HERE,
-              base::BindOnce(
-                  &AndroidVideoDecodeAccelerator::RequestPictureBuffers,
+              base::Bind(&AndroidVideoDecodeAccelerator::RequestPictureBuffers,
                   weak_this_factory_.GetWeakPtr()));
           return false;
         }
@@ -1013,8 +1012,7 @@
   } else {
     base::ThreadTaskRunnerHandle::Get()->PostTask(
         FROM_HERE,
-        base::BindOnce(
-            &AndroidVideoDecodeAccelerator::NotifyEndOfBitstreamBuffer,
+        base::Bind(&AndroidVideoDecodeAccelerator::NotifyEndOfBitstreamBuffer,
             weak_this_factory_.GetWeakPtr(), bitstream_buffer.id()));
   }
 }
@@ -1300,8 +1298,7 @@
     if (bitstream_buffer_id != -1) {
       base::ThreadTaskRunnerHandle::Get()->PostTask(
           FROM_HERE,
-          base::BindOnce(
-              &AndroidVideoDecodeAccelerator::NotifyEndOfBitstreamBuffer,
+          base::Bind(&AndroidVideoDecodeAccelerator::NotifyEndOfBitstreamBuffer,
               weak_this_factory_.GetWeakPtr(), bitstream_buffer_id));
     }
   }
--- a/media/gpu/android/android_video_decode_accelerator_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/android_video_decode_accelerator_unittest.cc	2019-05-17 18:53:34.244000000 +0300
@@ -596,7 +596,7 @@
   return test_profiles;
 }
 
-INSTANTIATE_TEST_SUITE_P(AndroidVideoDecodeAcceleratorTest,
+INSTANTIATE_TEST_CASE_P(AndroidVideoDecodeAcceleratorTest,
                          AndroidVideoDecodeAcceleratorTest,
                          testing::ValuesIn(GetTestList()));
 
--- a/media/gpu/android/android_video_encode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/android_video_encode_accelerator.cc	2019-05-17 18:53:34.244000000 +0300
@@ -208,7 +208,7 @@
       2048;
   base::ThreadTaskRunnerHandle::Get()->PostTask(
       FROM_HERE,
-      base::BindOnce(&VideoEncodeAccelerator::Client::RequireBitstreamBuffers,
+      base::Bind(&VideoEncodeAccelerator::Client::RequireBitstreamBuffers,
                      client_ptr_factory_->GetWeakPtr(), frame_input_count,
                      config.input_visible_size, output_buffer_capacity));
   return true;
@@ -443,8 +443,7 @@
 
   base::ThreadTaskRunnerHandle::Get()->PostTask(
       FROM_HERE,
-      base::BindOnce(
-          &VideoEncodeAccelerator::Client::BitstreamBufferReady,
+      base::Bind(&VideoEncodeAccelerator::Client::BitstreamBufferReady,
           client_ptr_factory_->GetWeakPtr(), bitstream_buffer.id(),
           BitstreamBufferMetadata(size, key_frame, frame_timestamp)));
 }
--- a/media/gpu/android/android_video_encode_accelerator.h	2019-05-17 17:45:41.280000000 +0300
+++ b/media/gpu/android/android_video_encode_accelerator.h	2019-05-17 18:53:34.244000000 +0300
@@ -29,7 +29,7 @@
 
 // Android-specific implementation of VideoEncodeAccelerator, enabling
 // hardware-acceleration of video encoding, based on Android's MediaCodec class
-// (http://developer.8n6r01d.qjz9zk/reference/android/media/MediaCodec.html).  This
+// (http://developer.android.com/reference/android/media/MediaCodec.html).  This
 // class expects to live and be called on a single thread (the GPU process'
 // ChildThread).
 class MEDIA_GPU_EXPORT AndroidVideoEncodeAccelerator
--- a/media/gpu/android/android_video_surface_chooser_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/android_video_surface_chooser_impl_unittest.cc	2019-05-17 18:53:34.244000000 +0300
@@ -436,7 +436,7 @@
 }
 
 // Unless we're promoting aggressively, we should default to TextureOwner.
-INSTANTIATE_TEST_SUITE_P(NoFullscreenUsesTextureOwner,
+INSTANTIATE_TEST_CASE_P(NoFullscreenUsesTextureOwner,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::No),
                                  Values(ShouldBePowerEfficient::Ignored),
@@ -449,7 +449,7 @@
                                  Values(PromoteAggressively::No),
                                  AnyMisc));
 
-INSTANTIATE_TEST_SUITE_P(FullscreenUsesOverlay,
+INSTANTIATE_TEST_CASE_P(FullscreenUsesOverlay,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::Yes),
                                  Values(ShouldBePowerEfficient::Ignored),
@@ -462,7 +462,7 @@
                                  Either(PromoteAggressively),
                                  Values(MiscFlags::None)));
 
-INSTANTIATE_TEST_SUITE_P(RequiredUsesOverlay,
+INSTANTIATE_TEST_CASE_P(RequiredUsesOverlay,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::Yes),
                                  Values(ShouldBePowerEfficient::No),
@@ -479,7 +479,7 @@
 // Secure textures should use an overlay if the compositor will promote them.
 // We don't care about relayout, since it's transient; either behavior is okay
 // if a relayout is epected.  Similarly, hidden frames are fine either way.
-INSTANTIATE_TEST_SUITE_P(SecureUsesOverlayIfPromotable,
+INSTANTIATE_TEST_CASE_P(SecureUsesOverlayIfPromotable,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::Yes),
                                  Values(ShouldBePowerEfficient::No),
@@ -497,7 +497,7 @@
 // so that L3 will fall back to TextureOwner.  Non-dynamic is excluded, since
 // we don't get (or use) compositor feedback before the first frame.  At that
 // point, we've already chosen the output surface and can't switch it.
-INSTANTIATE_TEST_SUITE_P(NotCCPromotableNotRequiredUsesTextureOwner,
+INSTANTIATE_TEST_CASE_P(NotCCPromotableNotRequiredUsesTextureOwner,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::No),
                                  Values(ShouldBePowerEfficient::No),
@@ -512,7 +512,7 @@
 
 // If we're expecting a relayout, then we should never use an overlay unless
 // it's required.
-INSTANTIATE_TEST_SUITE_P(InsecureExpectingRelayoutUsesTextureOwner,
+INSTANTIATE_TEST_CASE_P(InsecureExpectingRelayoutUsesTextureOwner,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::No),
                                  Values(ShouldBePowerEfficient::No),
@@ -526,7 +526,7 @@
                                  AnyMisc));
 
 // "is_fullscreen" should be enough to trigger an overlay pre-M.
-INSTANTIATE_TEST_SUITE_P(NotDynamicInFullscreenUsesOverlay,
+INSTANTIATE_TEST_CASE_P(NotDynamicInFullscreenUsesOverlay,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::Yes),
                                  Values(ShouldBePowerEfficient::No),
@@ -541,7 +541,7 @@
                                         MiscFlags::Persistent)));
 
 // "is_secure" should be enough to trigger an overlay pre-M.
-INSTANTIATE_TEST_SUITE_P(NotDynamicSecureUsesOverlay,
+INSTANTIATE_TEST_CASE_P(NotDynamicSecureUsesOverlay,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::Yes),
                                  Values(ShouldBePowerEfficient::No),
@@ -556,7 +556,7 @@
                                         MiscFlags::Persistent)));
 
 // "is_required" should be enough to trigger an overlay pre-M.
-INSTANTIATE_TEST_SUITE_P(NotDynamicRequiredUsesOverlay,
+INSTANTIATE_TEST_CASE_P(NotDynamicRequiredUsesOverlay,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::Yes),
                                  Values(ShouldBePowerEfficient::No),
@@ -571,7 +571,7 @@
                                         MiscFlags::Persistent)));
 
 // If we're promoting aggressively, then we should request power efficient.
-INSTANTIATE_TEST_SUITE_P(AggressiveOverlayIsPowerEfficient,
+INSTANTIATE_TEST_CASE_P(AggressiveOverlayIsPowerEfficient,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::Yes),
                                  Values(ShouldBePowerEfficient::Yes),
@@ -585,7 +585,7 @@
                                  Values(MiscFlags::None)));
 
 // Rotated video is unsupported for overlays in all cases.
-INSTANTIATE_TEST_SUITE_P(IsVideoRotatedUsesTextureOwner,
+INSTANTIATE_TEST_CASE_P(IsVideoRotatedUsesTextureOwner,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::No),
                                  Either(ShouldBePowerEfficient),
@@ -599,7 +599,7 @@
                                  Values(MiscFlags::Rotated)));
 
 // Persistent, non-required video should not use an overlay.
-INSTANTIATE_TEST_SUITE_P(FullscreenPersistentVideoUsesSurfaceTexture,
+INSTANTIATE_TEST_CASE_P(FullscreenPersistentVideoUsesSurfaceTexture,
                          AndroidVideoSurfaceChooserImplTest,
                          Combine(Values(ShouldUseOverlay::No),
                                  Values(ShouldBePowerEfficient::Ignored),
--- a/media/gpu/android/avda_codec_image.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/avda_codec_image.cc	2019-05-17 18:53:34.244000000 +0300
@@ -36,12 +36,7 @@
   return GL_RGBA;
 }
 
-AVDACodecImage::BindOrCopy AVDACodecImage::ShouldBindOrCopy() {
-  return COPY;
-}
-
 bool AVDACodecImage::BindTexImage(unsigned target) {
-  NOTREACHED();
   return false;
 }
 
--- a/media/gpu/android/avda_codec_image.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/avda_codec_image.h	2019-05-17 18:53:34.244000000 +0300
@@ -31,7 +31,6 @@
   // gl::GLImage implementation
   gfx::Size GetSize() override;
   unsigned GetInternalFormat() override;
-  BindOrCopy ShouldBindOrCopy() override;
   bool BindTexImage(unsigned target) override;
   void ReleaseTexImage(unsigned target) override;
   bool CopyTexImage(unsigned target) override;
--- a/media/gpu/android/avda_picture_buffer_manager.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/avda_picture_buffer_manager.cc	2019-05-17 18:53:34.244000000 +0300
@@ -63,8 +63,7 @@
                                                1,  // depth
                                                0,  // border
                                                GL_RGBA, GL_UNSIGNED_BYTE);
-    texture_owner_ = TextureOwner::Create(
-        std::move(texture), TextureOwner::Mode::kSurfaceTextureInsecure);
+    texture_owner_ = TextureOwner::Create(std::move(texture));
     if (!texture_owner_)
       return false;
 
--- a/media/gpu/android/avda_shared_state.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/avda_shared_state.cc	2019-05-17 18:53:34.244000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/gpu/android/avda_shared_state.h"
 
-#include "base/bind.h"
 #include "base/metrics/histogram_macros.h"
 #include "base/time/time.h"
 #include "media/gpu/android/avda_codec_image.h"
@@ -48,8 +47,6 @@
 
 void AVDASharedState::UpdateTexImage() {
   texture_owner()->UpdateTexImage();
-  if (!texture_owner()->binds_texture_on_update())
-    texture_owner()->EnsureTexImageBound();
   // Helpfully, this is already column major.
   texture_owner()->GetTransformMatrix(gl_matrix_);
 }
--- a/media/gpu/android/avda_surface_bundle.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/avda_surface_bundle.cc	2019-05-17 18:53:34.244000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/gpu/android/avda_surface_bundle.h"
 
-#include "base/bind.h"
 #include "base/threading/sequenced_task_runner_handle.h"
 #include "media/base/android/android_overlay.h"
 
--- a/media/gpu/android/codec_allocator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/codec_allocator.cc	2019-05-17 18:53:34.244000000 +0300
@@ -8,7 +8,6 @@
 
 #include <memory>
 
-#include "base/bind_helpers.h"
 #include "base/logging.h"
 #include "base/single_thread_task_runner.h"
 #include "base/system/sys_info.h"
--- a/media/gpu/android/codec_image.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/codec_image.cc	2019-05-17 18:53:34.244000000 +0300
@@ -20,23 +20,14 @@
 // Makes |texture_owner|'s context current if it isn't already.
 std::unique_ptr<ui::ScopedMakeCurrent> MakeCurrentIfNeeded(
     TextureOwner* texture_owner) {
-  gl::GLContext* context = texture_owner->GetContext();
   // Note: this works for virtual contexts too, because IsCurrent() returns true
   // if their shared platform context is current, regardless of which virtual
   // context is current.
-  if (context->IsCurrent(nullptr))
-    return nullptr;
-
-  auto scoped_current = std::make_unique<ui::ScopedMakeCurrent>(
-      context, texture_owner->GetSurface());
-  // Log an error if ScopedMakeCurrent failed for debugging
-  // https://crbug.com/878042.
-  // TODO(ericrk): Remove this once debugging is completed.
-  if (!context->IsCurrent(nullptr)) {
-    LOG(ERROR) << "Failed to make context current in CodecImage. Subsequent "
-                  "UpdateTexImage may fail.";
-  }
-  return scoped_current;
+  return std::unique_ptr<ui::ScopedMakeCurrent>(
+      texture_owner->GetContext()->IsCurrent(nullptr)
+          ? nullptr
+          : new ui::ScopedMakeCurrent(texture_owner->GetContext(),
+                                      texture_owner->GetSurface()));
 }
 
 }  // namespace
@@ -70,24 +61,18 @@
   return GL_RGBA;
 }
 
-CodecImage::BindOrCopy CodecImage::ShouldBindOrCopy() {
-  // If we're using an overlay, then pretend it's bound.  That way, we'll get
-  // calls to ScheduleOverlayPlane.  Otherwise, CopyTexImage needs to be called.
-  return !texture_owner_ ? BIND : COPY;
-}
-
 bool CodecImage::BindTexImage(unsigned target) {
-  DCHECK_EQ(BIND, ShouldBindOrCopy());
-  return true;
+  // If we're using an overlay, then pretend it's bound.  That way, we'll get
+  // calls to ScheduleOverlayPlane.  Otherwise, fail so that we will be asked
+  // to CopyTexImage.  Note that we could just CopyTexImage here.
+  return !texture_owner_;
 }
 
 void CodecImage::ReleaseTexImage(unsigned target) {}
 
 bool CodecImage::CopyTexImage(unsigned target) {
   TRACE_EVENT0("media", "CodecImage::CopyTexImage");
-  DCHECK_EQ(COPY, ShouldBindOrCopy());
-
-  if (target != GL_TEXTURE_EXTERNAL_OES)
+  if (!texture_owner_ || target != GL_TEXTURE_EXTERNAL_OES)
     return false;
 
   GLint bound_service_id = 0;
@@ -96,7 +81,7 @@
   if (bound_service_id != static_cast<GLint>(texture_owner_->GetTextureId()))
     return false;
 
-  RenderToTextureOwnerFrontBuffer(BindingsMode::kEnsureTexImageBound);
+  RenderToTextureOwnerFrontBuffer(BindingsMode::kDontRestore);
   return true;
 }
 
@@ -155,7 +140,7 @@
   // The matrix is available after we render to the front buffer. If that fails
   // we'll return the matrix from the previous frame, which is more likely to be
   // correct than the identity matrix anyway.
-  RenderToTextureOwnerFrontBuffer(BindingsMode::kDontRestoreIfBound);
+  RenderToTextureOwnerFrontBuffer(BindingsMode::kDontRestore);
   texture_owner_->GetTransformMatrix(matrix);
   YInvertMatrix(matrix);
 }
@@ -176,10 +161,8 @@
 }
 
 bool CodecImage::RenderToFrontBuffer() {
-  // This code is used to trigger early rendering of the image before it is used
-  // for compositing, there is no need to bind the image.
   return texture_owner_
-             ? RenderToTextureOwnerFrontBuffer(BindingsMode::kRestoreIfBound)
+             ? RenderToTextureOwnerFrontBuffer(BindingsMode::kRestore)
              : RenderToOverlay();
 }
 
@@ -206,11 +189,8 @@
 
 bool CodecImage::RenderToTextureOwnerFrontBuffer(BindingsMode bindings_mode) {
   DCHECK(texture_owner_);
-
-  if (phase_ == Phase::kInFrontBuffer) {
-    EnsureBoundIfNeeded(bindings_mode);
+  if (phase_ == Phase::kInFrontBuffer)
     return true;
-  }
   if (phase_ == Phase::kInvalidated)
     return false;
 
@@ -225,31 +205,20 @@
 
   std::unique_ptr<ui::ScopedMakeCurrent> scoped_make_current =
       MakeCurrentIfNeeded(texture_owner_.get());
-  // If updating the image will implicitly update the texture bindings then
-  // restore if requested or the update needed a context switch.
+  // If we have to switch contexts, then we always want to restore the
+  // bindings.
   bool should_restore_bindings =
-      texture_owner_->binds_texture_on_update() &&
-      (bindings_mode == BindingsMode::kRestoreIfBound || !!scoped_make_current);
+      bindings_mode == BindingsMode::kRestore || !!scoped_make_current;
 
   GLint bound_service_id = 0;
   if (should_restore_bindings)
     glGetIntegerv(GL_TEXTURE_BINDING_EXTERNAL_OES, &bound_service_id);
   texture_owner_->UpdateTexImage();
-  EnsureBoundIfNeeded(bindings_mode);
   if (should_restore_bindings)
     glBindTexture(GL_TEXTURE_EXTERNAL_OES, bound_service_id);
   return true;
 }
 
-void CodecImage::EnsureBoundIfNeeded(BindingsMode mode) {
-  DCHECK(texture_owner_);
-
-  if (texture_owner_->binds_texture_on_update() ||
-      mode != BindingsMode::kEnsureTexImageBound)
-    return;
-  texture_owner_->EnsureTexImageBound();
-}
-
 bool CodecImage::RenderToOverlay() {
   if (phase_ == Phase::kInFrontBuffer)
     return true;
@@ -273,7 +242,7 @@
 CodecImage::GetAHardwareBuffer() {
   DCHECK(texture_owner_);
 
-  RenderToTextureOwnerFrontBuffer(BindingsMode::kDontRestoreIfBound);
+  RenderToTextureOwnerFrontBuffer(BindingsMode::kDontRestore);
   return texture_owner_->GetAHardwareBuffer();
 }
 
--- a/media/gpu/android/codec_image_group.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/codec_image_group.cc	2019-05-17 18:53:34.244000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/gpu/android/codec_image_group.h"
 
-#include "base/bind.h"
 #include "base/sequenced_task_runner.h"
 #include "media/gpu/android/avda_surface_bundle.h"
 
--- a/media/gpu/android/codec_image.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/codec_image.h	2019-05-17 18:53:34.244000000 +0300
@@ -42,7 +42,6 @@
   // gl::GLImage implementation
   gfx::Size GetSize() override;
   unsigned GetInternalFormat() override;
-  BindOrCopy ShouldBindOrCopy() override;
   bool BindTexImage(unsigned target) override;
   void ReleaseTexImage(unsigned target) override;
   bool CopyTexImage(unsigned target) override;
@@ -106,22 +105,14 @@
 
   // Renders this image to the texture owner front buffer by first rendering
   // it to the back buffer if it's not already there, and then waiting for the
-  // frame available event before calling UpdateTexImage().
-  enum class BindingsMode {
-    // Ensures that the TextureOwner's texture is bound to the latest image, if
-    // it requires explicit binding.
-    kEnsureTexImageBound,
-
-    // Updates the current image but does not bind it. If updating the image
-    // implicitly binds the texture, the current bindings will be restored.
-    kRestoreIfBound,
-
-    // Updates the current image but does not bind it. If updating the image
-    // implicitly binds the texture, the current bindings will not be restored.
-    kDontRestoreIfBound
-  };
+  // frame available event before calling UpdateTexImage(). Passing
+  // BindingsMode::kDontRestore skips the work of restoring the current texture
+  // bindings if the texture owner's context is already current. Otherwise,
+  // this switches contexts and preserves the texture bindings.
+  // Returns true if the buffer is in the front buffer. Returns false if the
+  // buffer was invalidated.
+  enum class BindingsMode { kRestore, kDontRestore };
   bool RenderToTextureOwnerFrontBuffer(BindingsMode bindings_mode);
-  void EnsureBoundIfNeeded(BindingsMode mode);
 
   // Renders this image to the overlay. Returns true if the buffer is in the
   // overlay front buffer. Returns false if the buffer was invalidated.
--- a/media/gpu/android/codec_image_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/codec_image_unittest.cc	2019-05-17 18:53:34.248000000 +0300
@@ -61,8 +61,8 @@
     // The tests rely on this texture being bound.
     glBindTexture(GL_TEXTURE_EXTERNAL_OES, texture_id_);
 
-    texture_owner_ = new NiceMock<MockTextureOwner>(
-        texture_id_, context_.get(), surface_.get(), BindsTextureOnUpdate());
+    texture_owner_ = new NiceMock<MockTextureOwner>(texture_id_, context_.get(),
+                                                    surface_.get());
   }
 
   void TearDown() override {
@@ -90,8 +90,6 @@
     return image;
   }
 
-  virtual bool BindsTextureOnUpdate() { return true; }
-
   base::test::ScopedTaskEnvironment scoped_task_environment_;
   NiceMock<MockMediaCodecBridge>* codec_;
   std::unique_ptr<CodecWrapper> wrapper_;
@@ -109,10 +107,6 @@
   PromotionHintReceiver promotion_hint_receiver_;
 };
 
-class CodecImageTestExplicitBind : public CodecImageTest {
-  bool BindsTextureOnUpdate() override { return false; }
-};
-
 TEST_F(CodecImageTest, DestructionCbRuns) {
   base::MockCallback<CodecImage::DestructionCb> cb;
   auto i = NewImage(kOverlay, cb.Get());
@@ -127,7 +121,7 @@
 
 TEST_F(CodecImageTest, CopyTexImageIsInvalidForOverlayImages) {
   auto i = NewImage(kOverlay);
-  ASSERT_NE(gl::GLImage::COPY, i->ShouldBindOrCopy());
+  ASSERT_FALSE(i->CopyTexImage(GL_TEXTURE_EXTERNAL_OES));
 }
 
 TEST_F(CodecImageTest, ScheduleOverlayPlaneIsInvalidForTextureOwnerImages) {
@@ -167,18 +161,6 @@
   ASSERT_TRUE(i->was_rendered_to_front_buffer());
 }
 
-TEST_F(CodecImageTestExplicitBind, CopyTexImageTriggersFrontBufferRendering) {
-  auto i = NewImage(kTextureOwner);
-  // Verify that the release comes before the wait.
-  InSequence s;
-  EXPECT_CALL(*codec_, ReleaseOutputBuffer(_, true));
-  EXPECT_CALL(*texture_owner_, WaitForFrameAvailable());
-  EXPECT_CALL(*texture_owner_, UpdateTexImage());
-  EXPECT_CALL(*texture_owner_, EnsureTexImageBound());
-  i->CopyTexImage(GL_TEXTURE_EXTERNAL_OES);
-  ASSERT_TRUE(i->was_rendered_to_front_buffer());
-}
-
 TEST_F(CodecImageTest, GetTextureMatrixTriggersFrontBufferRendering) {
   auto i = NewImage(kTextureOwner);
   InSequence s;
@@ -191,22 +173,6 @@
   ASSERT_TRUE(i->was_rendered_to_front_buffer());
 }
 
-TEST_F(CodecImageTestExplicitBind,
-       GetTextureMatrixTriggersFrontBufferRendering) {
-  // GetTextureMatrix should not bind the image.
-  texture_owner_->expect_update_tex_image = false;
-
-  auto i = NewImage(kTextureOwner);
-  InSequence s;
-  EXPECT_CALL(*codec_, ReleaseOutputBuffer(_, true));
-  EXPECT_CALL(*texture_owner_, WaitForFrameAvailable());
-  EXPECT_CALL(*texture_owner_, UpdateTexImage());
-  EXPECT_CALL(*texture_owner_, GetTransformMatrix(_));
-  float matrix[16];
-  i->GetTextureMatrix(matrix);
-  ASSERT_TRUE(i->was_rendered_to_front_buffer());
-}
-
 TEST_F(CodecImageTest, GetTextureMatrixReturnsIdentityForOverlayImages) {
   auto i = NewImage(kOverlay);
   float matrix[16]{0};
@@ -278,20 +244,6 @@
   GLuint pre_bound_texture = 0;
   glGenTextures(1, &pre_bound_texture);
   glBindTexture(GL_TEXTURE_EXTERNAL_OES, pre_bound_texture);
-  auto i = NewImage(kTextureOwner);
-  EXPECT_CALL(*texture_owner_, UpdateTexImage());
-  i->RenderToFrontBuffer();
-  GLint post_bound_texture = 0;
-  glGetIntegerv(GL_TEXTURE_BINDING_EXTERNAL_OES, &post_bound_texture);
-  ASSERT_EQ(pre_bound_texture, static_cast<GLuint>(post_bound_texture));
-}
-
-TEST_F(CodecImageTestExplicitBind, RenderToFrontBufferDoesNotBindTexture) {
-  texture_owner_->expect_update_tex_image = false;
-
-  GLuint pre_bound_texture = 0;
-  glGenTextures(1, &pre_bound_texture);
-  glBindTexture(GL_TEXTURE_EXTERNAL_OES, pre_bound_texture);
   auto i = NewImage(kTextureOwner);
   EXPECT_CALL(*texture_owner_, UpdateTexImage());
   i->RenderToFrontBuffer();
--- a/media/gpu/android/codec_wrapper.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/codec_wrapper.cc	2019-05-17 18:53:34.248000000 +0300
@@ -10,7 +10,6 @@
 #include <string>
 #include <vector>
 
-#include "base/bind.h"
 #include "base/memory/ptr_util.h"
 #include "base/optional.h"
 #include "base/stl_util.h"
--- a/media/gpu/android/fake_codec_allocator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/fake_codec_allocator.cc	2019-05-17 18:53:34.248000000 +0300
@@ -6,7 +6,6 @@
 
 #include <memory>
 
-#include "base/bind.h"
 #include "base/memory/weak_ptr.h"
 #include "media/base/android/mock_media_codec_bridge.h"
 #include "media/gpu/android/codec_allocator.h"
--- a/media/gpu/android/image_reader_gl_owner.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/image_reader_gl_owner.cc	2019-05-17 18:53:34.248000000 +0300
@@ -19,9 +19,7 @@
 #include "base/threading/thread_task_runner_handle.h"
 #include "gpu/command_buffer/service/abstract_texture.h"
 #include "gpu/ipc/common/android/android_image_reader_utils.h"
-#include "ui/gl/android/android_surface_control_compat.h"
 #include "ui/gl/gl_fence_android_native_fence_sync.h"
-#include "ui/gl/gl_utils.h"
 #include "ui/gl/scoped_binders.h"
 #include "ui/gl/scoped_make_current.h"
 
@@ -60,30 +58,20 @@
       : base::android::ScopedHardwareBufferFenceSync(std::move(handle),
                                                      std::move(fence_fd)),
         texture_owner_(std::move(texture_owner)),
-        image_(image) {
-    DCHECK(image_);
-    texture_owner_->RegisterRefOnImage(image_);
-  }
+        image_(image) {}
   ~ScopedHardwareBufferImpl() override {
-    texture_owner_->ReleaseRefOnImage(image_, std::move(read_fence_));
-  }
-
-  void SetReadFence(base::ScopedFD fence_fd, bool has_context) final {
-    DCHECK(!read_fence_.is_valid());
-    read_fence_ = std::move(fence_fd);
+    texture_owner_->ReleaseRefOnImage(image_);
   }
 
  private:
-  base::ScopedFD read_fence_;
   scoped_refptr<ImageReaderGLOwner> texture_owner_;
   AImage* image_;
 };
 
 ImageReaderGLOwner::ImageReaderGLOwner(
-    std::unique_ptr<gpu::gles2::AbstractTexture> texture,
-    Mode mode)
-    : TextureOwner(false /* binds_texture_on_image_update */,
-                   std::move(texture)),
+    std::unique_ptr<gpu::gles2::AbstractTexture> texture)
+    : TextureOwner(std::move(texture)),
+      current_image_(nullptr),
       loader_(base::android::AndroidImageReader::GetInstance()),
       context_(gl::GLContext::GetCurrent()),
       surface_(gl::GLSurface::GetCurrent()),
@@ -91,20 +79,18 @@
   DCHECK(context_);
   DCHECK(surface_);
 
+  // TODO(khushalsagar): Need plumbing here to select the correct format and
+  // usage for secure media.
+
   // Set the width, height and format to some default value. This parameters
   // are/maybe overriden by the producer sending buffers to this imageReader's
   // Surface.
-  int32_t width = 1, height = 1, max_images = 4;
-  AIMAGE_FORMATS format = mode == Mode::kAImageReaderSecure
-                              ? AIMAGE_FORMAT_PRIVATE
-                              : AIMAGE_FORMAT_YUV_420_888;
+  int32_t width = 1, height = 1, max_images = 3;
+  AIMAGE_FORMATS format = AIMAGE_FORMAT_YUV_420_888;
   AImageReader* reader = nullptr;
   // The usage flag below should be used when the buffer will be read from by
   // the GPU as a texture.
-  uint64_t usage = mode == Mode::kAImageReaderSecure
-                       ? AHARDWAREBUFFER_USAGE_PROTECTED_CONTENT
-                       : AHARDWAREBUFFER_USAGE_GPU_SAMPLED_IMAGE;
-  usage |= gl::SurfaceControl::RequiredUsage();
+  const uint64_t usage = AHARDWAREBUFFER_USAGE_GPU_SAMPLED_IMAGE;
 
   // Create a new reader for images of the desired size and format.
   media_status_t return_code = loader_.AImageReader_newWithUsage(
@@ -137,12 +123,11 @@
 
 ImageReaderGLOwner::~ImageReaderGLOwner() {
   DCHECK_CALLED_ON_VALID_THREAD(thread_checker_);
+  DCHECK_EQ(external_image_refs_.size(), 0u);
 
   // Clear the texture before we return, so that it can OnTextureDestroyed() if
   // it hasn't already.  This will do nothing if it has already been destroyed.
   ClearAbstractTexture();
-
-  DCHECK_EQ(image_refs_.size(), 0u);
 }
 
 void ImageReaderGLOwner::OnTextureDestroyed(gpu::gles2::AbstractTexture*) {
@@ -160,18 +145,13 @@
   // Now we can stop listening to new images.
   loader_.AImageReader_setImageListener(image_reader_, NULL);
 
-  // Delete all images before closing the associated image reader.
-  for (auto& image_ref : image_refs_)
-    loader_.AImage_delete(image_ref.first);
+  // Delete the image before closing the associated image reader.
+  if (current_image_)
+    loader_.AImage_delete(current_image_);
 
   // Delete the image reader.
   loader_.AImageReader_delete(image_reader_);
   image_reader_ = nullptr;
-
-  // Clean up the ImageRefs which should now be a no-op since there is no valid
-  // |image_reader_|.
-  image_refs_.clear();
-  current_image_ref_.reset();
 }
 
 gl::ScopedJavaSurface ImageReaderGLOwner::CreateJavaSurface() const {
@@ -254,65 +234,91 @@
     return;
   }
 
+  // If we have a new Image, delete the previously acquired image.
+  if (!MaybeDeleteCurrentImage())
+    return;
+
   // Make the newly acquired image as current image.
-  current_image_ref_.emplace(this, image, std::move(scoped_acquire_fence_fd));
+  current_image_ = image;
+  current_image_fence_ = std::move(scoped_acquire_fence_fd);
+  current_image_bound_ = false;
+
+  // TODO(khushalsagar): This should be on the public API so that we only bind
+  // the texture if we were going to render it without an overlay.
+  EnsureTexImageBound();
 }
 
 void ImageReaderGLOwner::EnsureTexImageBound() {
-  if (current_image_ref_)
-    current_image_ref_->EnsureBound();
+  if (current_image_bound_)
+    return;
+
+  base::ScopedFD acquire_fence =
+      base::ScopedFD(HANDLE_EINTR(dup(current_image_fence_.get())));
+
+  // Insert an EGL fence and make server wait for image to be available.
+  if (!gpu::InsertEglFenceAndWait(std::move(acquire_fence)))
+    return;
+
+  // Create EGL image from the AImage and bind it to the texture.
+  if (!gpu::CreateAndBindEglImage(current_image_, GetTextureId(), &loader_))
+    return;
+
+  current_image_bound_ = true;
+}
+
+bool ImageReaderGLOwner::MaybeDeleteCurrentImage() {
+  if (!current_image_)
+    return true;
+
+  if (external_image_refs_.count(current_image_) != 0)
+    return true;
+
+  // We should not need a fence if this image was never bound.
+  return gpu::DeleteAImageAsync(current_image_, &loader_);
 }
 
 std::unique_ptr<base::android::ScopedHardwareBufferFenceSync>
 ImageReaderGLOwner::GetAHardwareBuffer() {
-  if (!current_image_ref_)
+  if (!current_image_)
     return nullptr;
 
   AHardwareBuffer* buffer = nullptr;
-  loader_.AImage_getHardwareBuffer(current_image_ref_->image(), &buffer);
+  loader_.AImage_getHardwareBuffer(current_image_, &buffer);
   if (!buffer)
     return nullptr;
 
+  auto fence_fd = base::ScopedFD(HANDLE_EINTR(dup(current_image_fence_.get())));
+
+  // Add a ref that the caller will release.
+  auto it = external_image_refs_.find(current_image_);
+  if (it == external_image_refs_.end())
+    external_image_refs_[current_image_] = 1;
+  else
+    it->second++;
+
   return std::make_unique<ScopedHardwareBufferImpl>(
-      this, current_image_ref_->image(),
+      this, current_image_,
       base::android::ScopedHardwareBufferHandle::Create(buffer),
-      current_image_ref_->GetReadyFence());
+      std::move(fence_fd));
 }
 
-void ImageReaderGLOwner::RegisterRefOnImage(AImage* image) {
-  DCHECK(image_reader_);
-
-  // Add a ref that the caller will release.
-  image_refs_[image].count++;
-}
+void ImageReaderGLOwner::ReleaseRefOnImage(AImage* image) {
+  auto it = external_image_refs_.find(image);
+  DCHECK(it != external_image_refs_.end());
+  DCHECK_GT(it->second, 0u);
+  it->second--;
 
-void ImageReaderGLOwner::ReleaseRefOnImage(AImage* image,
-                                           base::ScopedFD fence_fd) {
-  // During cleanup on losing the texture, all images are synchronously released
-  // and the |image_reader_| is destroyed.
-  if (!image_reader_)
+  if (it->second > 0)
     return;
+  external_image_refs_.erase(it);
 
-  auto it = image_refs_.find(image);
-  DCHECK(it != image_refs_.end());
-
-  auto& image_ref = it->second;
-  DCHECK_GT(image_ref.count, 0u);
-  image_ref.count--;
-  image_ref.release_fence_fd =
-      gl::MergeFDs(std::move(image_ref.release_fence_fd), std::move(fence_fd));
-
-  if (image_ref.count > 0)
+  if (image == current_image_)
     return;
 
-  if (image_ref.release_fence_fd.is_valid()) {
-    loader_.AImage_deleteAsync(image,
-                               std::move(image_ref.release_fence_fd.release()));
-  } else {
+  // No refs on the image. If it is no longer current, delete it. Note that this
+  // can be deleted synchronously here since the caller ensures that any pending
+  // GPU work for the image is finished before marking it for release.
     loader_.AImage_delete(image);
-  }
-
-  image_refs_.erase(it);
 }
 
 void ImageReaderGLOwner::GetTransformMatrix(float mtx[]) {
@@ -368,15 +374,15 @@
   const base::TimeDelta elapsed = call_time - release_time_;
   const base::TimeDelta remaining = max_wait - elapsed;
   release_time_ = base::TimeTicks();
-  bool timed_out = false;
 
   if (remaining <= base::TimeDelta()) {
     if (!frame_available_event_->event.IsSignaled()) {
       DVLOG(1) << "Deferred WaitForFrameAvailable() timed out, elapsed: "
                << elapsed.InMillisecondsF() << "ms";
-      timed_out = true;
     }
-  } else {
+    return;
+  }
+
     DCHECK_LE(remaining, max_wait);
     SCOPED_UMA_HISTOGRAM_TIMER(
         "Media.CodecImage.ImageReaderGLOwner.WaitTimeForFrame");
@@ -386,60 +392,7 @@
                << "ms, additionally waited: " << remaining.InMillisecondsF()
                << "ms, total: " << (elapsed + remaining).InMillisecondsF()
                << "ms";
-      timed_out = true;
-    }
   }
-  UMA_HISTOGRAM_BOOLEAN("Media.CodecImage.ImageReaderGLOwner.FrameTimedOut",
-                        timed_out);
-}
-
-ImageReaderGLOwner::ImageRef::ImageRef() = default;
-ImageReaderGLOwner::ImageRef::~ImageRef() = default;
-ImageReaderGLOwner::ImageRef::ImageRef(ImageRef&& other) = default;
-ImageReaderGLOwner::ImageRef& ImageReaderGLOwner::ImageRef::operator=(
-    ImageRef&& other) = default;
-
-ImageReaderGLOwner::ScopedCurrentImageRef::ScopedCurrentImageRef(
-    ImageReaderGLOwner* texture_owner,
-    AImage* image,
-    base::ScopedFD ready_fence)
-    : texture_owner_(texture_owner),
-      image_(image),
-      ready_fence_(std::move(ready_fence)) {
-  DCHECK(image_);
-  texture_owner_->RegisterRefOnImage(image_);
-}
-
-ImageReaderGLOwner::ScopedCurrentImageRef::~ScopedCurrentImageRef() {
-  base::ScopedFD release_fence;
-  // If there is no |image_reader_|, we are in tear down so no fence is
-  // required.
-  if (image_bound_ && texture_owner_->image_reader_)
-    release_fence = gpu::CreateEglFenceAndExportFd();
-  else
-    release_fence = std::move(ready_fence_);
-  texture_owner_->ReleaseRefOnImage(image_, std::move(release_fence));
-}
-
-base::ScopedFD ImageReaderGLOwner::ScopedCurrentImageRef::GetReadyFence()
-    const {
-  return base::ScopedFD(HANDLE_EINTR(dup(ready_fence_.get())));
-}
-
-void ImageReaderGLOwner::ScopedCurrentImageRef::EnsureBound() {
-  if (image_bound_)
-    return;
-
-  // Insert an EGL fence and make server wait for image to be available.
-  if (!gpu::InsertEglFenceAndWait(GetReadyFence()))
-    return;
-
-  // Create EGL image from the AImage and bind it to the texture.
-  if (!gpu::CreateAndBindEglImage(image_, texture_owner_->GetTextureId(),
-                                  &texture_owner_->loader_))
-    return;
-
-  image_bound_ = true;
 }
 
 }  // namespace media
--- a/media/gpu/android/image_reader_gl_owner.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/image_reader_gl_owner.h	2019-05-17 18:53:34.248000000 +0300
@@ -35,7 +35,6 @@
   gl::GLSurface* GetSurface() const override;
   gl::ScopedJavaSurface CreateJavaSurface() const override;
   void UpdateTexImage() override;
-  void EnsureTexImageBound() override;
   void GetTransformMatrix(float mtx[16]) override;
   void ReleaseBackBuffers() override;
   void SetReleaseTimeToNow() override;
@@ -45,72 +44,41 @@
   std::unique_ptr<base::android::ScopedHardwareBufferFenceSync>
   GetAHardwareBuffer() override;
 
-  const AImageReader* image_reader_for_testing() const { return image_reader_; }
-
  protected:
   void OnTextureDestroyed(gpu::gles2::AbstractTexture*) override;
 
  private:
   friend class TextureOwner;
-  class ScopedHardwareBufferImpl;
-
-  // Manages ownership of the latest image retrieved from AImageReader and
-  // ensuring synchronization of its use in GL using fences.
-  class ScopedCurrentImageRef {
-   public:
-    ScopedCurrentImageRef(ImageReaderGLOwner* texture_owner,
-                          AImage* image,
-                          base::ScopedFD ready_fence);
-    ~ScopedCurrentImageRef();
-    AImage* image() const { return image_; }
-    base::ScopedFD GetReadyFence() const;
-    void EnsureBound();
 
-   private:
-    ImageReaderGLOwner* texture_owner_;
-    AImage* image_;
-    base::ScopedFD ready_fence_;
-
-    // Set to true if the current image is bound to |texture_id_|.
-    bool image_bound_ = false;
-
-    DISALLOW_COPY_AND_ASSIGN(ScopedCurrentImageRef);
-  };
+  class ScopedHardwareBufferImpl;
 
-  ImageReaderGLOwner(std::unique_ptr<gpu::gles2::AbstractTexture> texture,
-                     Mode secure_mode);
+  ImageReaderGLOwner(std::unique_ptr<gpu::gles2::AbstractTexture> texture);
   ~ImageReaderGLOwner() override;
 
-  // Registers and releases a ref on the image. Once the ref-count for an image
-  // goes to 0, it is released back to the AImageReader with an optional release
-  // fence if needed.
-  void RegisterRefOnImage(AImage* image);
-  void ReleaseRefOnImage(AImage* image, base::ScopedFD fence_fd);
+  // Deletes the current image if it has no pending refs. Returns false on
+  // error.
+  bool MaybeDeleteCurrentImage();
+
+  void EnsureTexImageBound();
+  void ReleaseRefOnImage(AImage* image);
 
   // AImageReader instance
   AImageReader* image_reader_;
 
   // Most recently acquired image using image reader. This works like a cached
   // image until next new image is acquired which overwrites this.
-  base::Optional<ScopedCurrentImageRef> current_image_ref_;
+  AImage* current_image_;
+  base::ScopedFD current_image_fence_;
   std::unique_ptr<AImageReader_ImageListener> listener_;
 
-  // A map consisting of pending refs on an AImage. If an image has any refs, it
-  // is automatically released once the ref-count is 0.
-  struct ImageRef {
-    ImageRef();
-    ~ImageRef();
-
-    ImageRef(ImageRef&& other);
-    ImageRef& operator=(ImageRef&& other);
-
-    size_t count = 0u;
-    base::ScopedFD release_fence_fd;
-
-    DISALLOW_COPY_AND_ASSIGN(ImageRef);
-  };
-  using AImageRefMap = base::flat_map<AImage*, ImageRef>;
-  AImageRefMap image_refs_;
+  // Set to true if the current image is bound to |texture_id_|.
+  bool current_image_bound_ = false;
+
+  // A map consisting of pending external refs on an AImage. If an image has any
+  // external refs, it is automatically released once the ref-count is 0 and the
+  // image is no longer current.
+  using AImageRefMap = base::flat_map<AImage*, size_t>;
+  AImageRefMap external_image_refs_;
 
   // reference to the class instance which is used to dynamically
   // load the functions in android libraries at runtime.
--- a/media/gpu/android/image_reader_gl_owner_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/image_reader_gl_owner_unittest.cc	2019-05-17 18:53:34.248000000 +0300
@@ -6,13 +6,11 @@
 
 #include <stdint.h>
 #include <memory>
-#include <utility>
 
 #include "base/message_loop/message_loop.h"
 #include "base/test/scoped_feature_list.h"
 #include "gpu/command_buffer/service/abstract_texture.h"
 #include "media/base/media_switches.h"
-#include "media/gpu/android/image_reader_gl_owner.h"
 #include "media/gpu/android/mock_abstract_texture.h"
 #include "testing/gtest/include/gtest/gtest.h"
 #include "ui/gl/gl_bindings.h"
@@ -29,9 +27,6 @@
 
  protected:
   void SetUp() override {
-    if (!IsImageReaderSupported())
-      return;
-
     scoped_feature_list_.InitAndEnableFeature(media::kAImageReaderVideoOutput);
     gl::init::InitializeGLOneOffImplementation(gl::kGLImplementationEGLGLES2,
                                                false, false, false, true);
@@ -49,11 +44,7 @@
     std::unique_ptr<MockAbstractTexture> texture =
         std::make_unique<MockAbstractTexture>(texture_id_);
     abstract_texture_ = texture->AsWeakPtr();
-    image_reader_ = TextureOwner::Create(std::move(texture), SecureMode());
-  }
-
-  virtual TextureOwner::Mode SecureMode() {
-    return TextureOwner::Mode::kAImageReaderInsecure;
+    image_reader_ = TextureOwner::Create(std::move(texture));
   }
 
   void TearDown() override {
@@ -66,10 +57,6 @@
     gl::init::ShutdownGL(false);
   }
 
-  bool IsImageReaderSupported() const {
-    return base::android::AndroidImageReader::GetInstance().IsSupported();
-  }
-
   base::test::ScopedFeatureList scoped_feature_list_;
   scoped_refptr<TextureOwner> image_reader_;
   GLuint texture_id_ = 0;
@@ -83,16 +70,10 @@
 };
 
 TEST_F(ImageReaderGLOwnerTest, ImageReaderObjectCreation) {
-  if (!IsImageReaderSupported())
-    return;
-
   ASSERT_TRUE(image_reader_);
 }
 
 TEST_F(ImageReaderGLOwnerTest, ScopedJavaSurfaceCreation) {
-  if (!IsImageReaderSupported())
-    return;
-
   gl::ScopedJavaSurface temp = image_reader_->CreateJavaSurface();
   ASSERT_TRUE(temp.IsValid());
 }
@@ -100,9 +81,6 @@
 // Verify that ImageReaderGLOwner creates a bindable GL texture, and deletes
 // it during destruction.
 TEST_F(ImageReaderGLOwnerTest, GLTextureIsCreatedAndDestroyed) {
-  if (!IsImageReaderSupported())
-    return;
-
   // |texture_id| should not work anymore after we delete image_reader_.
   image_reader_ = nullptr;
   EXPECT_FALSE(abstract_texture_);
@@ -110,18 +88,12 @@
 
 // Make sure that image_reader_ remembers the correct context and surface.
 TEST_F(ImageReaderGLOwnerTest, ContextAndSurfaceAreCaptured) {
-  if (!IsImageReaderSupported())
-    return;
-
   ASSERT_EQ(context_, image_reader_->GetContext());
   ASSERT_EQ(surface_, image_reader_->GetSurface());
 }
 
 // Verify that destruction works even if some other context is current.
 TEST_F(ImageReaderGLOwnerTest, DestructionWorksWithWrongContext) {
-  if (!IsImageReaderSupported())
-    return;
-
   scoped_refptr<gl::GLSurface> new_surface(
       new gl::PbufferGLSurfaceEGL(gfx::Size(320, 240)));
   new_surface->Initialize();
@@ -143,24 +115,4 @@
   new_surface = nullptr;
 }
 
-class ImageReaderGLOwnerSecureTest : public ImageReaderGLOwnerTest {
- public:
-  TextureOwner::Mode SecureMode() final {
-    return TextureOwner::Mode::kAImageReaderSecure;
-  }
-};
-
-TEST_F(ImageReaderGLOwnerSecureTest, CreatesSecureAImageReader) {
-  if (!IsImageReaderSupported())
-    return;
-
-  ASSERT_TRUE(image_reader_);
-  auto* a_image_reader = static_cast<ImageReaderGLOwner*>(image_reader_.get())
-                             ->image_reader_for_testing();
-  int32_t format = AIMAGE_FORMAT_YUV_420_888;
-  base::android::AndroidImageReader::GetInstance().AImageReader_getFormat(
-      a_image_reader, &format);
-  EXPECT_EQ(format, AIMAGE_FORMAT_PRIVATE);
-}
-
 }  // namespace media
--- a/media/gpu/android/media_codec_video_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/media_codec_video_decoder.cc	2019-05-17 18:53:34.248000000 +0300
@@ -6,7 +6,6 @@
 
 #include <memory>
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/callback.h"
 #include "base/callback_helpers.h"
@@ -29,7 +28,6 @@
 #include "media/gpu/android/android_video_surface_chooser.h"
 #include "media/gpu/android/codec_allocator.h"
 #include "media/media_buildflags.h"
-#include "media/video/supported_video_decoder_config.h"
 
 #if BUILDFLAG(USE_PROPRIETARY_CODECS)
 #include "media/base/android/extract_sps_and_pps.h"
@@ -122,69 +120,6 @@
 PendingDecode::PendingDecode(PendingDecode&& other) = default;
 PendingDecode::~PendingDecode() = default;
 
-// static
-std::vector<SupportedVideoDecoderConfig>
-MediaCodecVideoDecoder::GetSupportedConfigs() {
-  std::vector<SupportedVideoDecoderConfig> supported_configs;
-
-  if (MediaCodecUtil::IsVp8DecoderAvailable()) {
-    // For unencrypted content, require that the size is at least 360p and that
-    // the MediaCodec implementation is hardware; otherwise fall back to libvpx.
-    if (!MediaCodecUtil::IsKnownUnaccelerated(kCodecVP8,
-                                              MediaCodecDirection::DECODER)) {
-      supported_configs.emplace_back(VP8PROFILE_ANY, VP8PROFILE_ANY,
-                                     gfx::Size(480, 360), gfx::Size(3840, 2160),
-                                     false,   // allow_encrypted
-                                     false);  // require_encrypted
-    }
-
-    // Encrypted content must be decoded by MediaCodec.
-    supported_configs.emplace_back(VP8PROFILE_ANY, VP8PROFILE_ANY,
-                                   gfx::Size(0, 0), gfx::Size(3840, 2160),
-                                   true,   // allow_encrypted
-                                   true);  // require_encrypted
-  }
-
-  if (MediaCodecUtil::IsVp9DecoderAvailable()) {
-    // For unencrypted content, require that the size is at least 360p and that
-    // the MediaCodec implementation is hardware; otherwise fall back to libvpx.
-    if (!MediaCodecUtil::IsKnownUnaccelerated(kCodecVP9,
-                                              MediaCodecDirection::DECODER)) {
-      supported_configs.emplace_back(VP9PROFILE_PROFILE0, VP9PROFILE_PROFILE3,
-                                     gfx::Size(480, 360), gfx::Size(3840, 2160),
-                                     false,   // allow_encrypted
-                                     false);  // require_encrypted
-    }
-
-    // Encrypted content must be decoded by MediaCodec.
-    supported_configs.emplace_back(VP9PROFILE_PROFILE0, VP9PROFILE_PROFILE3,
-                                   gfx::Size(0, 0), gfx::Size(3840, 2160),
-                                   true,   // allow_encrypted
-                                   true);  // require_encrypted
-  }
-
-#if BUILDFLAG(USE_PROPRIETARY_CODECS)
-  // MediaCodec is only guaranteed to support baseline, but some devices may
-  // support others. Advertise support for all H.264 profiles and let the
-  // MediaCodec fail when decoding if it's not actually supported. It's assumed
-  // that there is not software fallback for H.264 on Android.
-  supported_configs.emplace_back(H264PROFILE_BASELINE,
-                                 H264PROFILE_MULTIVIEWHIGH, gfx::Size(0, 0),
-                                 gfx::Size(3840, 2160),
-                                 true,    // allow_encrypted
-                                 false);  // require_encrypted
-
-#if BUILDFLAG(ENABLE_HEVC_DEMUXING)
-  supported_configs.emplace_back(HEVCPROFILE_MAIN, HEVCPROFILE_MAIN10,
-                                 gfx::Size(0, 0), gfx::Size(3840, 2160),
-                                 true,    // allow_encrypted
-                                 false);  // require_encrypted
-#endif
-#endif
-
-  return supported_configs;
-}
-
 MediaCodecVideoDecoder::MediaCodecVideoDecoder(
     const gpu::GpuPreferences& gpu_preferences,
     const gpu::GpuFeatureInfo& gpu_feature_info,
@@ -388,26 +323,19 @@
   lazy_init_pending_ = false;
   codec_allocator_->StartThread(this);
 
+  // SurfaceControl allows TextureOwner to be promoted to an overlay in the
+  // compositing pipeline itself.
+  const bool use_texture_owner_as_overlays = is_surface_control_enabled_;
+
   // Only ask for promotion hints if we can actually switch surfaces, since we
   // wouldn't be able to do anything with them. Also, if threaded texture
-  // mailboxes are enabled, then we turn off overlays anyway.
+  // mailboxes are enabled, then we turn off overlays anyway. And if texture
+  // owner can be used as an overlay, no promotion hints are necessary.
   const bool want_promotion_hints =
       device_info_->IsSetOutputSurfaceSupported() &&
-      !enable_threaded_texture_mailboxes_;
-
-  VideoFrameFactory::OverlayMode overlay_mode =
-      VideoFrameFactory::OverlayMode::kDontRequestPromotionHints;
-  if (is_surface_control_enabled_) {
-    overlay_mode =
-        requires_secure_codec_
-            ? VideoFrameFactory::OverlayMode::kSurfaceControlSecure
-            : VideoFrameFactory::OverlayMode::kSurfaceControlInsecure;
-  } else if (want_promotion_hints) {
-    overlay_mode = VideoFrameFactory::OverlayMode::kRequestPromotionHints;
-  }
-
+      !enable_threaded_texture_mailboxes_ && !use_texture_owner_as_overlays;
   video_frame_factory_->Initialize(
-      overlay_mode,
+      want_promotion_hints, use_texture_owner_as_overlays,
       base::Bind(&MediaCodecVideoDecoder::OnVideoFrameFactoryInitialized,
                  weak_factory_.GetWeakPtr()));
 }
--- a/media/gpu/android/media_codec_video_decoder.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/media_codec_video_decoder.h	2019-05-17 18:53:34.248000000 +0300
@@ -5,13 +5,10 @@
 #ifndef MEDIA_GPU_ANDROID_MEDIA_CODEC_VIDEO_DECODER_H_
 #define MEDIA_GPU_ANDROID_MEDIA_CODEC_VIDEO_DECODER_H_
 
-#include <vector>
-
 #include "base/containers/circular_deque.h"
 #include "base/optional.h"
 #include "base/threading/thread_checker.h"
 #include "base/timer/elapsed_timer.h"
-#include "base/timer/timer.h"
 #include "gpu/config/gpu_feature_info.h"
 #include "gpu/config/gpu_preferences.h"
 #include "media/base/android_overlay_mojo_factory.h"
@@ -28,7 +25,6 @@
 namespace media {
 
 class ScopedAsyncTrace;
-struct SupportedVideoDecoderConfig;
 
 struct PendingDecode {
   static PendingDecode CreateEos();
@@ -58,8 +54,6 @@
 class MEDIA_GPU_EXPORT MediaCodecVideoDecoder : public VideoDecoder,
                                                 public CodecAllocatorClient {
  public:
-  static std::vector<SupportedVideoDecoderConfig> GetSupportedConfigs();
-
   MediaCodecVideoDecoder(
       const gpu::GpuPreferences& gpu_preferences,
       const gpu::GpuFeatureInfo& gpu_feature_info,
--- a/media/gpu/android/media_codec_video_decoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/media_codec_video_decoder_unittest.cc	2019-05-17 18:53:34.248000000 +0300
@@ -58,7 +58,10 @@
 
 class MockVideoFrameFactory : public VideoFrameFactory {
  public:
-  MOCK_METHOD2(Initialize, void(OverlayMode overlay_mode, InitCb init_cb));
+  MOCK_METHOD3(Initialize,
+               void(bool wants_promotion_hint,
+                    bool use_texture_owner_as_overlay,
+                    InitCb init_cb));
   MOCK_METHOD1(MockSetSurfaceBundle, void(scoped_refptr<AVDASurfaceBundle>));
   MOCK_METHOD5(
       MockCreateVideoFrame,
@@ -140,8 +143,10 @@
         std::make_unique<NiceMock<MockVideoFrameFactory>>();
     video_frame_factory_ = video_frame_factory.get();
     // Set up VFF to pass |texture_owner_| via its InitCb.
-    ON_CALL(*video_frame_factory_, Initialize(ExpectedOverlayMode(), _))
-        .WillByDefault(RunCallback<1>(texture_owner));
+    const bool want_promotion_hint =
+        device_info_->IsSetOutputSurfaceSupported();
+    ON_CALL(*video_frame_factory_, Initialize(want_promotion_hint, _, _))
+        .WillByDefault(RunCallback<2>(texture_owner));
 
     auto* observable_mcvd = new DestructionObservableMCVD(
         gpu_preferences_, gpu_feature_info_, device_info_.get(),
@@ -157,14 +162,6 @@
     destruction_observer_->ExpectDestruction();
   }
 
-  VideoFrameFactory::OverlayMode ExpectedOverlayMode() const {
-    const bool want_promotion_hint =
-        device_info_->IsSetOutputSurfaceSupported();
-    return want_promotion_hint
-               ? VideoFrameFactory::OverlayMode::kRequestPromotionHints
-               : VideoFrameFactory::OverlayMode::kDontRequestPromotionHints;
-  }
-
   void CreateCdm(bool has_media_crypto_context,
                  bool require_secure_video_decoder) {
     cdm_ = std::make_unique<MockMediaCryptoContext>(has_media_crypto_context);
@@ -312,8 +309,7 @@
 
 TEST_P(MediaCodecVideoDecoderTest, InitializeDoesntInitSurfaceOrCodec) {
   CreateMcvd();
-  EXPECT_CALL(*video_frame_factory_, Initialize(ExpectedOverlayMode(), _))
-      .Times(0);
+  EXPECT_CALL(*video_frame_factory_, Initialize(_, _, _)).Times(0);
   EXPECT_CALL(*surface_chooser_, MockUpdateState()).Times(0);
   EXPECT_CALL(*codec_allocator_, MockCreateMediaCodecAsync(_, _)).Times(0);
   Initialize(TestVideoConfig::Large(codec_));
@@ -321,7 +317,7 @@
 
 TEST_P(MediaCodecVideoDecoderTest, FirstDecodeTriggersFrameFactoryInit) {
   Initialize(TestVideoConfig::Large(codec_));
-  EXPECT_CALL(*video_frame_factory_, Initialize(ExpectedOverlayMode(), _));
+  EXPECT_CALL(*video_frame_factory_, Initialize(_, _, _));
   mcvd_->Decode(fake_decoder_buffer_, decode_cb_.Get());
 }
 
@@ -375,8 +371,8 @@
 
 TEST_P(MediaCodecVideoDecoderTest, FrameFactoryInitFailureIsAnError) {
   Initialize(TestVideoConfig::Large(codec_));
-  ON_CALL(*video_frame_factory_, Initialize(ExpectedOverlayMode(), _))
-      .WillByDefault(RunCallback<1>(nullptr));
+  ON_CALL(*video_frame_factory_, Initialize(_, _, _))
+      .WillByDefault(RunCallback<2>(nullptr));
   EXPECT_CALL(decode_cb_, Run(DecodeStatus::DECODE_ERROR)).Times(1);
   EXPECT_CALL(*surface_chooser_, MockUpdateState()).Times(0);
   mcvd_->Decode(fake_decoder_buffer_, decode_cb_.Get());
@@ -1031,17 +1027,17 @@
              : std::vector<VideoCodec>();
 }
 
-INSTANTIATE_TEST_SUITE_P(MediaCodecVideoDecoderTest,
+INSTANTIATE_TEST_CASE_P(MediaCodecVideoDecoderTest,
                          MediaCodecVideoDecoderTest,
                          testing::ValuesIn(GetTestList()));
 
 #if BUILDFLAG(USE_PROPRIETARY_CODECS)
-INSTANTIATE_TEST_SUITE_P(MediaCodecVideoDecoderH264Test,
+INSTANTIATE_TEST_CASE_P(MediaCodecVideoDecoderH264Test,
                          MediaCodecVideoDecoderH264Test,
                          testing::ValuesIn(GetH264IfAvailable()));
 #endif
 
-INSTANTIATE_TEST_SUITE_P(MediaCodecVideoDecoderVp8Test,
+INSTANTIATE_TEST_CASE_P(MediaCodecVideoDecoderVp8Test,
                          MediaCodecVideoDecoderVp8Test,
                          testing::ValuesIn(GetVp8IfAvailable()));
 
--- a/media/gpu/android/mock_texture_owner.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/mock_texture_owner.cc	2019-05-17 18:53:34.248000000 +0300
@@ -13,14 +13,11 @@
 
 MockTextureOwner::MockTextureOwner(GLuint fake_texture_id,
                                    gl::GLContext* fake_context,
-                                   gl::GLSurface* fake_surface,
-                                   bool binds_texture_on_update)
-    : TextureOwner(binds_texture_on_update,
-                   std::make_unique<MockAbstractTexture>(fake_texture_id)),
+                                   gl::GLSurface* fake_surface)
+    : TextureOwner(std::make_unique<MockAbstractTexture>(fake_texture_id)),
       fake_context(fake_context),
       fake_surface(fake_surface),
-      expecting_frame_available(false),
-      expect_update_tex_image(!binds_texture_on_update) {
+      expecting_frame_available(false) {
   ON_CALL(*this, GetTextureId()).WillByDefault(Return(fake_texture_id));
   ON_CALL(*this, GetContext()).WillByDefault(Return(fake_context));
   ON_CALL(*this, GetSurface()).WillByDefault(Return(fake_surface));
@@ -34,9 +31,6 @@
   ON_CALL(*this, WaitForFrameAvailable())
       .WillByDefault(
           Invoke(this, &MockTextureOwner::FakeWaitForFrameAvailable));
-  ON_CALL(*this, EnsureTexImageBound()).WillByDefault(Invoke([this] {
-    CHECK(expect_update_tex_image);
-  }));
 }
 
 MockTextureOwner::~MockTextureOwner() {
--- a/media/gpu/android/mock_texture_owner.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/mock_texture_owner.h	2019-05-17 18:53:34.248000000 +0300
@@ -22,15 +22,13 @@
  public:
   MockTextureOwner(GLuint fake_texture_id,
                    gl::GLContext* fake_context,
-                   gl::GLSurface* fake_surface,
-                   bool binds_texture_on_update = false);
+                   gl::GLSurface* fake_surface);
 
   MOCK_CONST_METHOD0(GetTextureId, GLuint());
   MOCK_CONST_METHOD0(GetContext, gl::GLContext*());
   MOCK_CONST_METHOD0(GetSurface, gl::GLSurface*());
   MOCK_CONST_METHOD0(CreateJavaSurface, gl::ScopedJavaSurface());
   MOCK_METHOD0(UpdateTexImage, void());
-  MOCK_METHOD0(EnsureTexImageBound, void());
   MOCK_METHOD1(GetTransformMatrix, void(float mtx[16]));
   MOCK_METHOD0(ReleaseBackBuffers, void());
   MOCK_METHOD0(SetReleaseTimeToNow, void());
@@ -55,7 +53,6 @@
   gl::GLSurface* fake_surface;
   bool expecting_frame_available;
   int get_a_hardware_buffer_count = 0;
-  bool expect_update_tex_image;
 
  protected:
   ~MockTextureOwner();
--- a/media/gpu/android/surface_texture_gl_owner.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/surface_texture_gl_owner.cc	2019-05-17 18:53:34.248000000 +0300
@@ -7,7 +7,6 @@
 #include <memory>
 
 #include "base/android/scoped_hardware_buffer_fence_sync.h"
-#include "base/bind.h"
 #include "base/logging.h"
 #include "base/memory/ptr_util.h"
 #include "base/metrics/histogram_macros.h"
@@ -37,7 +36,7 @@
 
 SurfaceTextureGLOwner::SurfaceTextureGLOwner(
     std::unique_ptr<gpu::gles2::AbstractTexture> texture)
-    : TextureOwner(true /*binds_texture_on_update */, std::move(texture)),
+    : TextureOwner(std::move(texture)),
       surface_texture_(gl::SurfaceTexture::Create(GetTextureId())),
       context_(gl::GLContext::GetCurrent()),
       surface_(gl::GLSurface::GetCurrent()),
@@ -74,10 +73,6 @@
     surface_texture_->UpdateTexImage();
 }
 
-void SurfaceTextureGLOwner::EnsureTexImageBound() {
-  NOTREACHED();
-}
-
 void SurfaceTextureGLOwner::GetTransformMatrix(float mtx[]) {
   DCHECK_CALLED_ON_VALID_THREAD(thread_checker_);
   // If we don't have a SurfaceTexture, then the matrix doesn't matter.  We
@@ -130,15 +125,15 @@
   const base::TimeDelta elapsed = call_time - release_time_;
   const base::TimeDelta remaining = max_wait - elapsed;
   release_time_ = base::TimeTicks();
-  bool timed_out = false;
 
   if (remaining <= base::TimeDelta()) {
     if (!frame_available_event_->event.IsSignaled()) {
       DVLOG(1) << "Deferred WaitForFrameAvailable() timed out, elapsed: "
                << elapsed.InMillisecondsF() << "ms";
-      timed_out = true;
     }
-  } else {
+    return;
+  }
+
     DCHECK_LE(remaining, max_wait);
     SCOPED_UMA_HISTOGRAM_TIMER(
         "Media.CodecImage.SurfaceTextureGLOwner.WaitTimeForFrame");
@@ -148,11 +143,7 @@
                << "ms, additionally waited: " << remaining.InMillisecondsF()
                << "ms, total: " << (elapsed + remaining).InMillisecondsF()
                << "ms";
-      timed_out = true;
-    }
   }
-  UMA_HISTOGRAM_BOOLEAN("Media.CodecImage.SurfaceTextureGLOwner.FrameTimedOut",
-                        timed_out);
 }
 
 std::unique_ptr<base::android::ScopedHardwareBufferFenceSync>
--- a/media/gpu/android/surface_texture_gl_owner.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/surface_texture_gl_owner.h	2019-05-17 18:53:34.248000000 +0300
@@ -34,7 +34,6 @@
   gl::GLSurface* GetSurface() const override;
   gl::ScopedJavaSurface CreateJavaSurface() const override;
   void UpdateTexImage() override;
-  void EnsureTexImageBound() override;
   void GetTransformMatrix(float mtx[16]) override;
   void ReleaseBackBuffers() override;
   void SetReleaseTimeToNow() override;
--- a/media/gpu/android/surface_texture_gl_owner_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/surface_texture_gl_owner_unittest.cc	2019-05-17 18:53:34.248000000 +0300
@@ -50,8 +50,7 @@
     std::unique_ptr<MockAbstractTexture> texture =
         std::make_unique<MockAbstractTexture>(texture_id_);
     abstract_texture_ = texture->AsWeakPtr();
-    surface_texture_ = SurfaceTextureGLOwner::Create(
-        std::move(texture), TextureOwner::Mode::kSurfaceTextureInsecure);
+    surface_texture_ = SurfaceTextureGLOwner::Create(std::move(texture));
     texture_id_ = surface_texture_->GetTextureId();
     EXPECT_TRUE(abstract_texture_);
   }
--- a/media/gpu/android/texture_owner.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/texture_owner.cc	2019-05-17 18:53:34.248000000 +0300
@@ -4,22 +4,21 @@
 
 #include "media/gpu/android/texture_owner.h"
 
-#include "base/bind.h"
+#include "base/android/android_image_reader_compat.h"
 #include "base/feature_list.h"
 #include "base/threading/thread_task_runner_handle.h"
 #include "gpu/command_buffer/service/abstract_texture.h"
 #include "gpu/command_buffer/service/decoder_context.h"
+#include "media/base/media_switches.h"
 #include "media/gpu/android/image_reader_gl_owner.h"
 #include "media/gpu/android/surface_texture_gl_owner.h"
 #include "ui/gl/scoped_binders.h"
 
 namespace media {
 
-TextureOwner::TextureOwner(bool binds_texture_on_update,
-                           std::unique_ptr<gpu::gles2::AbstractTexture> texture)
+TextureOwner::TextureOwner(std::unique_ptr<gpu::gles2::AbstractTexture> texture)
     : base::RefCountedDeleteOnSequence<TextureOwner>(
           base::ThreadTaskRunnerHandle::Get()),
-      binds_texture_on_update_(binds_texture_on_update),
       texture_(std::move(texture)),
       task_runner_(base::ThreadTaskRunnerHandle::Get()) {
   // Notify the subclass when the texture is destroyed.
@@ -36,25 +35,21 @@
 
 // static
 scoped_refptr<TextureOwner> TextureOwner::Create(
-    std::unique_ptr<gpu::gles2::AbstractTexture> texture,
-    Mode mode) {
+    std::unique_ptr<gpu::gles2::AbstractTexture> texture) {
   // Set the parameters on the texture.
   texture->SetParameteri(GL_TEXTURE_MAG_FILTER, GL_LINEAR);
   texture->SetParameteri(GL_TEXTURE_MIN_FILTER, GL_LINEAR);
   texture->SetParameteri(GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
   texture->SetParameteri(GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);
 
-  switch (mode) {
-    case Mode::kAImageReaderSecure:
-      return new ImageReaderGLOwner(std::move(texture), mode);
-    case Mode::kAImageReaderInsecure:
-      return new ImageReaderGLOwner(std::move(texture), mode);
-    case Mode::kSurfaceTextureInsecure:
-      return new SurfaceTextureGLOwner(std::move(texture));
+  // If AImageReader is supported and is enabled by media flag, use it.
+  if (base::android::AndroidImageReader::GetInstance().IsSupported() &&
+      base::FeatureList::IsEnabled(media::kAImageReaderVideoOutput)) {
+    return new ImageReaderGLOwner(std::move(texture));
   }
 
-  NOTREACHED();
-  return nullptr;
+  // If not, fall back to legacy path.
+  return new SurfaceTextureGLOwner(std::move(texture));
 }
 
 // static
--- a/media/gpu/android/texture_owner.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/texture_owner.h	2019-05-17 18:53:34.248000000 +0300
@@ -45,16 +45,8 @@
   // new TextureOwner attached to it. Returns null on failure.
   // |texture| should be either from CreateAbstractTexture() or a mock.  The
   // corresponding GL context must be current.
-  // Mode indicates which framework API to use and whether the video textures
-  // created using this owner should be hardware protected.
-  enum class Mode {
-    kAImageReaderSecure,
-    kAImageReaderInsecure,
-    kSurfaceTextureInsecure
-  };
   static scoped_refptr<TextureOwner> Create(
-      std::unique_ptr<gpu::gles2::AbstractTexture> texture,
-      Mode mode);
+      std::unique_ptr<gpu::gles2::AbstractTexture> texture);
 
   // Create a texture that's appropriate for a TextureOwner.
   static std::unique_ptr<gpu::gles2::AbstractTexture> CreateTexture(
@@ -75,11 +67,6 @@
   // Update the texture image using the latest available image data.
   virtual void UpdateTexImage() = 0;
 
-  // Ensures that the latest texture image is bound to the texture target.
-  // Should only be used if the TextureOwner requires explicit binding of the
-  // image after an update.
-  virtual void EnsureTexImageBound() = 0;
-
   // Transformation matrix if any associated with the texture image.
   virtual void GetTransformMatrix(float mtx[16]) = 0;
   virtual void ReleaseBackBuffers() = 0;
@@ -109,15 +96,12 @@
   virtual std::unique_ptr<base::android::ScopedHardwareBufferFenceSync>
   GetAHardwareBuffer() = 0;
 
-  bool binds_texture_on_update() const { return binds_texture_on_update_; }
-
  protected:
   friend class base::RefCountedDeleteOnSequence<TextureOwner>;
   friend class base::DeleteHelper<TextureOwner>;
 
   // |texture| is the texture that we'll own.
-  TextureOwner(bool binds_texture_on_update,
-               std::unique_ptr<gpu::gles2::AbstractTexture> texture);
+  TextureOwner(std::unique_ptr<gpu::gles2::AbstractTexture> texture);
   virtual ~TextureOwner();
 
   // Drop |texture_| immediately.  Will call OnTextureDestroyed immediately if
@@ -134,10 +118,6 @@
   gpu::gles2::AbstractTexture* texture() const { return texture_.get(); }
 
  private:
-  // Set to true if the updating the image for this owner will automatically
-  // bind it to the texture target.
-  const bool binds_texture_on_update_;
-
   std::unique_ptr<gpu::gles2::AbstractTexture> texture_;
   scoped_refptr<base::SingleThreadTaskRunner> task_runner_;
 
--- a/media/gpu/android/texture_pool.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/texture_pool.cc	2019-05-17 18:53:34.248000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/gpu/android/texture_pool.h"
 
-#include "base/bind.h"
 #include "gpu/command_buffer/service/abstract_texture.h"
 #include "gpu/command_buffer/service/texture_manager.h"
 #include "media/gpu/command_buffer_helper.h"
--- a/media/gpu/android/video_frame_factory.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/video_frame_factory.h	2019-05-17 18:53:34.252000000 +0300
@@ -41,22 +41,13 @@
   // Initializes the factory and runs |init_cb| on the current thread when it's
   // complete. If initialization fails, the returned texture owner will be
   // null.
-  enum class OverlayMode {
-    // When using java overlays, the compositor can provide hints to the media
-    // pipeline to indicate whether the video can be promoted to an overlay.
-    // This indicates whether promotion hints are needed, if framework support
-    // for overlay promotion is available (requires MediaCodec.setOutputSurface
-    // support).
-    kDontRequestPromotionHints,
-    kRequestPromotionHints,
-
-    // When using surface control, the factory should always use a TextureOwner
-    // since it can directly be promoted to an overlay on a frame-by-frame
-    // basis. The bits below indicate whether the media uses a secure codec.
-    kSurfaceControlSecure,
-    kSurfaceControlInsecure
-  };
-  virtual void Initialize(OverlayMode overlay_mode, InitCb init_cb) = 0;
+  // |wants_promotion_hint| tells us whether to mark VideoFrames for compositor
+  // overlay promotion hints or not.
+  // |use_texture_owner_as_overlays| tells us whether TextureOwner can be used
+  // as an overlay, in which case java overlays will never be used.
+  virtual void Initialize(bool wants_promotion_hint,
+                          bool use_texture_owner_as_overlays,
+                          InitCb init_cb) = 0;
 
   // Notify us about the current surface bundle that subsequent video frames
   // should use.
--- a/media/gpu/android/video_frame_factory_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/video_frame_factory_impl.cc	2019-05-17 18:53:34.252000000 +0300
@@ -4,9 +4,7 @@
 
 #include "media/gpu/android/video_frame_factory_impl.h"
 
-#include "base/android/android_image_reader_compat.h"
 #include "base/bind.h"
-#include "base/bind_helpers.h"
 #include "base/callback.h"
 #include "base/memory/ref_counted.h"
 #include "base/single_thread_task_runner.h"
@@ -19,7 +17,6 @@
 #include "gpu/ipc/service/command_buffer_stub.h"
 #include "gpu/ipc/service/gpu_channel.h"
 #include "media/base/bind_to_current_loop.h"
-#include "media/base/media_switches.h"
 #include "media/base/video_frame.h"
 #include "media/gpu/android/codec_image.h"
 #include "media/gpu/android/codec_image_group.h"
@@ -37,30 +34,6 @@
   return stub && stub->decoder_context()->MakeCurrent();
 }
 
-TextureOwner::Mode GetTextureOwnerMode(
-    VideoFrameFactory::OverlayMode overlay_mode) {
-  const bool a_image_reader_supported =
-      base::android::AndroidImageReader::GetInstance().IsSupported();
-
-  switch (overlay_mode) {
-    case VideoFrameFactory::OverlayMode::kDontRequestPromotionHints:
-    case VideoFrameFactory::OverlayMode::kRequestPromotionHints:
-      return a_image_reader_supported && base::FeatureList::IsEnabled(
-                                             media::kAImageReaderVideoOutput)
-                 ? TextureOwner::Mode::kAImageReaderInsecure
-                 : TextureOwner::Mode::kSurfaceTextureInsecure;
-    case VideoFrameFactory::OverlayMode::kSurfaceControlSecure:
-      DCHECK(a_image_reader_supported);
-      return TextureOwner::Mode::kAImageReaderSecure;
-    case VideoFrameFactory::OverlayMode::kSurfaceControlInsecure:
-      DCHECK(a_image_reader_supported);
-      return TextureOwner::Mode::kAImageReaderInsecure;
-  }
-
-  NOTREACHED();
-  return TextureOwner::Mode::kSurfaceTextureInsecure;
-}
-
 }  // namespace
 
 using gpu::gles2::AbstractTexture;
@@ -77,7 +50,8 @@
     gpu_task_runner_->DeleteSoon(FROM_HERE, gpu_video_frame_factory_.release());
 }
 
-void VideoFrameFactoryImpl::Initialize(OverlayMode overlay_mode,
+void VideoFrameFactoryImpl::Initialize(bool wants_promotion_hint,
+                                       bool use_texture_owner_as_overlays,
                                        InitCb init_cb) {
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   DCHECK(!gpu_video_frame_factory_);
@@ -85,7 +59,8 @@
   base::PostTaskAndReplyWithResult(
       gpu_task_runner_.get(), FROM_HERE,
       base::Bind(&GpuVideoFrameFactory::Initialize,
-                 base::Unretained(gpu_video_frame_factory_.get()), overlay_mode,
+                 base::Unretained(gpu_video_frame_factory_.get()),
+                 wants_promotion_hint, use_texture_owner_as_overlays,
                  get_stub_cb_),
       std::move(init_cb));
 }
@@ -156,10 +131,12 @@
 }
 
 scoped_refptr<TextureOwner> GpuVideoFrameFactory::Initialize(
-    VideoFrameFactoryImpl::OverlayMode overlay_mode,
+    bool wants_promotion_hint,
+    bool use_texture_owner_as_overlays,
     VideoFrameFactoryImpl::GetStubCb get_stub_cb) {
   DCHECK_CALLED_ON_VALID_THREAD(thread_checker_);
-  overlay_mode_ = overlay_mode;
+  wants_promotion_hint_ = wants_promotion_hint;
+  use_texture_owner_as_overlays_ = use_texture_owner_as_overlays;
   stub_ = get_stub_cb.Run();
   if (!MakeContextCurrent(stub_))
     return nullptr;
@@ -170,8 +147,7 @@
   decoder_helper_ = GLES2DecoderHelper::Create(stub_->decoder_context());
 
   return TextureOwner::Create(
-      TextureOwner::CreateTexture(stub_->decoder_context()),
-      GetTextureOwnerMode(overlay_mode_));
+      TextureOwner::CreateTexture(stub_->decoder_context()));
 }
 
 void GpuVideoFrameFactory::CreateVideoFrame(
@@ -306,27 +282,21 @@
   if (group->gpu_preferences().enable_threaded_texture_mailboxes)
     frame->metadata()->SetBoolean(VideoFrameMetadata::COPY_REQUIRED, true);
 
-  const bool is_surface_control =
-      overlay_mode_ == VideoFrameFactory::OverlayMode::kSurfaceControlSecure ||
-      overlay_mode_ == VideoFrameFactory::OverlayMode::kSurfaceControlInsecure;
-  const bool wants_promotion_hints =
-      overlay_mode_ == VideoFrameFactory::OverlayMode::kRequestPromotionHints;
-
   bool allow_overlay = false;
-  if (is_surface_control) {
+  if (use_texture_owner_as_overlays_) {
     DCHECK(texture_owner_);
     allow_overlay = true;
   } else {
     // We unconditionally mark the picture as overlayable, even if
     // |!texture_owner_|, if we want to get hints.  It's required, else we won't
     // get hints.
-    allow_overlay = !texture_owner_ || wants_promotion_hints;
+    allow_overlay = !texture_owner_ || wants_promotion_hint_;
   }
 
   frame->metadata()->SetBoolean(VideoFrameMetadata::ALLOW_OVERLAY,
                                 allow_overlay);
   frame->metadata()->SetBoolean(VideoFrameMetadata::WANTS_PROMOTION_HINT,
-                                wants_promotion_hints);
+                                wants_promotion_hint_);
   frame->metadata()->SetBoolean(VideoFrameMetadata::TEXTURE_OWNER,
                                 !!texture_owner_);
 
--- a/media/gpu/android/video_frame_factory_impl.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/android/video_frame_factory_impl.h	2019-05-17 18:53:34.252000000 +0300
@@ -38,7 +38,9 @@
       GetStubCb get_stub_cb);
   ~VideoFrameFactoryImpl() override;
 
-  void Initialize(OverlayMode overlay_mode, InitCb init_cb) override;
+  void Initialize(bool wants_promotion_hint,
+                  bool use_texture_owner_as_overlays,
+                  InitCb init_cb) override;
   void SetSurfaceBundle(
       scoped_refptr<AVDASurfaceBundle> surface_bundle) override;
   void CreateVideoFrame(
@@ -71,7 +73,8 @@
   ~GpuVideoFrameFactory() override;
 
   scoped_refptr<TextureOwner> Initialize(
-      VideoFrameFactory::OverlayMode overlay_mode,
+      bool wants_promotion_hint,
+      bool use_texture_owner_as_overlays,
       VideoFrameFactory::GetStubCb get_stub_cb);
 
   // Creates and returns a VideoFrame with its ReleaseMailboxCB.
@@ -108,13 +111,16 @@
   // Outstanding images that should be considered for early rendering.
   std::vector<CodecImage*> images_;
 
-  gpu::CommandBufferStub* stub_ = nullptr;
+  gpu::CommandBufferStub* stub_;
 
   // Callback to notify us that an image has been destroyed.
   CodecImage::DestructionCb destruction_cb_;
 
-  VideoFrameFactory::OverlayMode overlay_mode_ =
-      VideoFrameFactory::OverlayMode::kDontRequestPromotionHints;
+  // Do we want promotion hints from the compositor?
+  bool wants_promotion_hint_ = false;
+
+  // Indicates whether texture owner can be promoted to an overlay.
+  bool use_texture_owner_as_overlays_ = false;
 
   // A helper for creating textures. Only valid while |stub_| is valid.
   std::unique_ptr<GLES2DecoderHelper> decoder_helper_;
--- a/media/gpu/args.gni	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/args.gni	2019-05-17 18:53:34.252000000 +0300
@@ -10,6 +10,10 @@
   # platforms which have v4l2 hardware encoder / decoder.
   use_v4l2_codec = false
 
+  # Indicates that only definitions available in the mainline linux kernel
+  # will be used.
+  use_linux_v4l2_only = false
+
   # Indicates if VA-API-based hardware acceleration is to be used. This
   # is typically the case on x86-based ChromeOS devices.
   use_vaapi = false
--- a/media/gpu/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/BUILD.gn	2019-05-17 18:53:34.240000000 +0300
@@ -17,6 +17,7 @@
     "USE_VAAPI=$use_vaapi",
     "USE_V4L2_CODEC=$use_v4l2_codec",
     "USE_LIBV4L2=$use_v4lplugin",
+    "USE_LINUX_V4L2=$use_linux_v4l2_only",
   ]
 }
 
@@ -24,7 +25,7 @@
   import("//build/config/mac/mac_sdk.gni")
 }
 
-if (is_chromeos && use_v4lplugin) {
+if (use_v4lplugin) {
   generate_stubs("libv4l2_stubs") {
     extra_header = "v4l2/v4l2_stub_header.fragment"
     sigs = [ "v4l2/v4l2.sig" ]
@@ -217,12 +218,8 @@
       "v4l2/v4l2_h264_accelerator.h",
       "v4l2/v4l2_image_processor.cc",
       "v4l2/v4l2_image_processor.h",
-      "v4l2/v4l2_jpeg_decode_accelerator.cc",
-      "v4l2/v4l2_jpeg_decode_accelerator.h",
       "v4l2/v4l2_jpeg_encode_accelerator.cc",
       "v4l2/v4l2_jpeg_encode_accelerator.h",
-      "v4l2/v4l2_slice_video_decode_accelerator.cc",
-      "v4l2/v4l2_slice_video_decode_accelerator.h",
       "v4l2/v4l2_video_decode_accelerator.cc",
       "v4l2/v4l2_video_decode_accelerator.h",
       "v4l2/v4l2_video_encode_accelerator.cc",
@@ -232,7 +229,14 @@
       "v4l2/v4l2_vp9_accelerator.cc",
       "v4l2/v4l2_vp9_accelerator.h",
     ]
-
+    if (!use_linux_v4l2_only) {
+      sources += [
+        "v4l2_jpeg_decode_accelerator.cc",
+        "v4l2_jpeg_decode_accelerator.h",
+        "v4l2_slice_video_decode_accelerator.cc",
+        "v4l2_slice_video_decode_accelerator.h",
+      ]
+    }
     libs = [
       "EGL",
       "GLESv2",
@@ -345,8 +349,6 @@
       "vp8_picture.h",
       "vp8_reference_frame_vector.cc",
       "vp8_reference_frame_vector.h",
-      "vp9_reference_frame_vector.cc",
-      "vp9_reference_frame_vector.h",
     ]
   }
 
@@ -393,7 +395,6 @@
       ]
       deps += [
         "test:decode_helpers",
-        "test:frame_file_writer",
         "test:frame_validator",
         "//mojo/core/embedder",
         "//ui/display",
@@ -567,7 +568,7 @@
       "//media/test/data/peach_pi-41x23.jpg",
     ]
     if (use_vaapi) {
-      deps += [ "//media/gpu/vaapi:jpeg_decoder_unit_test" ]
+      deps += [ "//media/gpu/vaapi:jpeg_decode_accelerator_unit_test" ]
       data += [ "//media/test/data/pixel-1280x720.jpg" ]
     }
     if (use_x11) {
@@ -641,35 +642,16 @@
     ]
     deps = [
       ":buildflags",
-      "test:frame_file_writer",
       "test:frame_validator",
       "test:video_player",
-      "test:video_player_test_environment",
-      "test:video_player_thumbnail_renderer",
+      "//base/test:test_support",
       "//media:test_support",
       "//mojo/core/embedder",
       "//testing/gtest",
     ]
+    if (use_vaapi) {
+      deps += [ "//media/gpu/vaapi" ]
   }
-}
-
-# TODO(dstaessens@) Make this work on other platforms too.
-if (is_chromeos) {
-  test("video_decode_accelerator_perf_tests") {
-    sources = [
-      "video_decode_accelerator_perf_tests.cc",
-    ]
-    data = [
-      "//media/test/data/",
-    ]
-    deps = [
-      ":buildflags",
-      "test:video_player",
-      "test:video_player_test_environment",
-      "//media:test_support",
-      "//mojo/core/embedder",
-      "//testing/gtest",
-    ]
   }
 }
 
@@ -680,9 +662,7 @@
   deps = [
     ":buildflags",
     ":gpu",
-    "test:frame_validator",
     "test:image_processor",
-    "test:render_helpers",
     "//base/test:test_support",
     "//media:test_support",
     "//mojo/core/embedder",
--- a/media/gpu/command_buffer_helper.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/command_buffer_helper.cc	2019-05-17 18:53:34.252000000 +0300
@@ -28,8 +28,7 @@
     : public CommandBufferHelper,
       public gpu::CommandBufferStub::DestructionObserver {
  public:
-  explicit CommandBufferHelperImpl(gpu::CommandBufferStub* stub)
-      : CommandBufferHelper(stub->channel()->task_runner()), stub_(stub) {
+  explicit CommandBufferHelperImpl(gpu::CommandBufferStub* stub) : stub_(stub) {
     DVLOG(1) << __func__;
     DCHECK(stub_->channel()->task_runner()->BelongsToCurrentThread());
 
@@ -209,11 +208,6 @@
 
 }  // namespace
 
-CommandBufferHelper::CommandBufferHelper(
-    scoped_refptr<base::SequencedTaskRunner> task_runner)
-    : base::RefCountedDeleteOnSequence<CommandBufferHelper>(
-          std::move(task_runner)) {}
-
 // static
 scoped_refptr<CommandBufferHelper> CommandBufferHelper::Create(
     gpu::CommandBufferStub* stub) {
--- a/media/gpu/command_buffer_helper.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/command_buffer_helper.h	2019-05-17 18:53:34.252000000 +0300
@@ -7,10 +7,8 @@
 
 #include "base/callback_forward.h"
 #include "base/macros.h"
-#include "base/memory/ref_counted_delete_on_sequence.h"
+#include "base/memory/ref_counted.h"
 #include "base/memory/scoped_refptr.h"
-#include "base/sequenced_task_runner.h"
-#include "base/sequenced_task_runner_helpers.h"
 #include "gpu/command_buffer/common/mailbox.h"
 #include "gpu/command_buffer/common/sync_token.h"
 #include "media/gpu/media_gpu_export.h"
@@ -32,7 +30,7 @@
 // both hold a ref to the same CommandBufferHelper). Consider making an owned
 // variant.
 class MEDIA_GPU_EXPORT CommandBufferHelper
-    : public base::RefCountedDeleteOnSequence<CommandBufferHelper> {
+    : public base::RefCountedThreadSafe<CommandBufferHelper> {
  public:
   REQUIRE_ADOPTION_FOR_REFCOUNTED_TYPE();
 
@@ -122,8 +120,7 @@
   virtual void SetWillDestroyStubCB(WillDestroyStubCB will_destroy_stub_cb) = 0;
 
  protected:
-  explicit CommandBufferHelper(
-      scoped_refptr<base::SequencedTaskRunner> task_runner);
+  CommandBufferHelper() = default;
 
   // TODO(sandersd): Deleting remaining textures upon destruction requires
   // making the context current, which may be undesireable. Consider adding an
@@ -131,8 +128,7 @@
   virtual ~CommandBufferHelper() = default;
 
  private:
-  friend class base::DeleteHelper<CommandBufferHelper>;
-  friend class base::RefCountedDeleteOnSequence<CommandBufferHelper>;
+  friend class base::RefCountedThreadSafe<CommandBufferHelper>;
 
   DISALLOW_COPY_AND_ASSIGN(CommandBufferHelper);
 };
--- a/media/gpu/DEPS	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/DEPS	2019-05-17 18:53:34.240000000 +0300
@@ -3,7 +3,6 @@
   "+mojo/core/embedder",
   "+services/service_manager/public",
   "+third_party/angle",
-  "+third_party/libsync",
   "+third_party/libyuv",
   "+third_party/v4l-utils",
   "+third_party/webrtc/common_video",
--- a/media/gpu/fake_command_buffer_helper.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/fake_command_buffer_helper.cc	2019-05-17 18:53:34.252000000 +0300
@@ -10,7 +10,7 @@
 
 FakeCommandBufferHelper::FakeCommandBufferHelper(
     scoped_refptr<base::SingleThreadTaskRunner> task_runner)
-    : CommandBufferHelper(task_runner), task_runner_(std::move(task_runner)) {
+    : task_runner_(std::move(task_runner)) {
   DVLOG(1) << __func__;
 }
 
--- a/media/gpu/fake_jpeg_decode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/fake_jpeg_decode_accelerator.cc	2019-05-17 18:53:34.252000000 +0300
@@ -73,7 +73,7 @@
 
   client_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&FakeJpegDecodeAccelerator::OnDecodeDoneOnClientThread,
+      base::Bind(&FakeJpegDecodeAccelerator::OnDecodeDoneOnClientThread,
                      weak_factory_.GetWeakPtr(), bitstream_buffer.id()));
 }
 
@@ -85,7 +85,7 @@
                                             Error error) {
   client_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&FakeJpegDecodeAccelerator::NotifyErrorOnClientThread,
+      base::Bind(&FakeJpegDecodeAccelerator::NotifyErrorOnClientThread,
                      weak_factory_.GetWeakPtr(), bitstream_buffer_id, error));
 }
 
--- a/media/gpu/format_utils.cc	2019-05-17 17:45:41.280000000 +0300
+++ b/media/gpu/format_utils.cc	2019-05-17 18:53:34.252000000 +0300
@@ -20,7 +20,7 @@
     // See ui/ozone drm_util.cc::GetFourCCFormatFromBufferFormat as reference.
     // But here it is only about indicating to not consider the alpha channel.
     // Useful for the compositor to avoid drawing behind as mentioned in
-    // https://chromium-review.9oo91esource.qjz9zk/590772.
+    // https://chromium-review.googlesource.com/590772.
     case gfx::BufferFormat::RGBX_8888:
       return PIXEL_FORMAT_XRGB;
 
--- a/media/gpu/gpu_jpeg_decode_accelerator_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/gpu_jpeg_decode_accelerator_factory.cc	2019-05-17 18:53:34.252000000 +0300
@@ -13,7 +13,8 @@
 #include "media/gpu/buildflags.h"
 #include "media/gpu/fake_jpeg_decode_accelerator.h"
 
-#if BUILDFLAG(USE_V4L2_CODEC) && defined(ARCH_CPU_ARM_FAMILY)
+#if BUILDFLAG(USE_V4L2_CODEC) && defined(ARCH_CPU_ARM_FAMILY) && \
+    !BUILDFLAG(USE_LINUX_V4L2)
 #define USE_V4L2_JDA
 #endif
 
--- a/media/gpu/gpu_video_decode_accelerator_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/gpu_video_decode_accelerator_factory.cc	2019-05-17 18:53:34.252000000 +0300
@@ -24,7 +24,9 @@
 #endif
 #if BUILDFLAG(USE_V4L2_CODEC)
 #include "media/gpu/v4l2/v4l2_device.h"
+#if !BUILDFLAG(USE_LINUX_V4L2)
 #include "media/gpu/v4l2/v4l2_slice_video_decode_accelerator.h"
+#endif
 #include "media/gpu/v4l2/v4l2_video_decode_accelerator.h"
 #include "ui/gl/gl_surface_egl.h"
 #endif
@@ -67,10 +69,12 @@
   vda_profiles = V4L2VideoDecodeAccelerator::GetSupportedProfiles();
   GpuVideoAcceleratorUtil::InsertUniqueDecodeProfiles(
       vda_profiles, &capabilities.supported_profiles);
+#if !BUILDFLAG(USE_LINUX_V4L2)
   vda_profiles = V4L2SliceVideoDecodeAccelerator::GetSupportedProfiles();
   GpuVideoAcceleratorUtil::InsertUniqueDecodeProfiles(
       vda_profiles, &capabilities.supported_profiles);
 #endif
+#endif
 #if BUILDFLAG(USE_VAAPI)
   vda_profiles = VaapiVideoDecodeAccelerator::GetSupportedProfiles();
   GpuVideoAcceleratorUtil::InsertUniqueDecodeProfiles(
@@ -164,8 +168,10 @@
 #endif
 #if BUILDFLAG(USE_V4L2_CODEC)
     &GpuVideoDecodeAcceleratorFactory::CreateV4L2VDA,
+#if !BUILDFLAG(USE_LINUX_V4L2)
     &GpuVideoDecodeAcceleratorFactory::CreateV4L2SVDA,
 #endif
+#endif
 #if BUILDFLAG(USE_VAAPI)
     &GpuVideoDecodeAcceleratorFactory::CreateVaapiVDA,
 #endif
@@ -219,6 +225,7 @@
   return decoder;
 }
 
+#if !BUILDFLAG(USE_LINUX_V4L2)
 std::unique_ptr<VideoDecodeAccelerator>
 GpuVideoDecodeAcceleratorFactory::CreateV4L2SVDA(
     const gpu::GpuDriverBugWorkarounds& workarounds,
@@ -234,6 +241,7 @@
   return decoder;
 }
 #endif
+#endif
 
 #if BUILDFLAG(USE_VAAPI)
 std::unique_ptr<VideoDecodeAccelerator>
--- a/media/gpu/gpu_video_decode_accelerator_factory.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/gpu_video_decode_accelerator_factory.h	2019-05-17 18:53:34.252000000 +0300
@@ -111,11 +111,13 @@
       const gpu::GpuDriverBugWorkarounds& workarounds,
       const gpu::GpuPreferences& gpu_preferences,
       MediaLog* media_log) const;
+#if !BUILDFLAG(USE_LINUX_V4L2)
   std::unique_ptr<VideoDecodeAccelerator> CreateV4L2SVDA(
       const gpu::GpuDriverBugWorkarounds& workarounds,
       const gpu::GpuPreferences& gpu_preferences,
       MediaLog* media_log) const;
 #endif
+#endif
 #if BUILDFLAG(USE_VAAPI)
   std::unique_ptr<VideoDecodeAccelerator> CreateVaapiVDA(
       const gpu::GpuDriverBugWorkarounds& workarounds,
--- a/media/gpu/gpu_video_encode_accelerator_helpers.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/gpu_video_encode_accelerator_helpers.cc	2019-05-17 18:53:34.252000000 +0300
@@ -14,11 +14,6 @@
 // 1080p video.
 constexpr size_t kMaxBitstreamBufferSizeInBytes = 2 * 1024 * 1024;  // 2MB
 
-// The frame size for 1080p (FHD) video in pixels.
-constexpr int k1080PSizeInPixels = 1920 * 1080;
-// The frame size for 1440p (QHD) video in pixels.
-constexpr int k1440PSizeInPixels = 2560 * 1440;
-
 // The mapping from resolution, bitrate, framerate to the bitstream buffer size.
 struct BitstreamBufferSizeInfo {
   int coded_size_area;
@@ -30,26 +25,14 @@
 // The bitstream buffer size for each resolution. The table must be sorted in
 // increasing order by the resolution. The value is decided by measuring the
 // biggest buffer size, and then double the size as margin. (crbug.com/889739)
+// TODO(akahuang): Measure the buffer size when we support 4K encoding.
 constexpr BitstreamBufferSizeInfo kBitstreamBufferSizeTable[] = {
     {320 * 180, 100000, 30, 15000},
     {640 * 360, 500000, 30, 52000},
     {1280 * 720, 1200000, 30, 110000},
     {1920 * 1080, 4000000, 30, 380000},
-    {3840 * 2160, 20000000, 30, 970000},
 };
 
-// Use quadruple size of kMaxBitstreamBufferSizeInBytes when the input frame
-// size is larger than 1440p, double if larger than 1080p. This is chosen
-// empirically for some 4k encoding use cases and Android CTS VideoEncoderTest
-// (crbug.com/927284).
-size_t GetMaxEncodeBitstreamBufferSize(const gfx::Size& size) {
-  if (size.GetArea() > k1440PSizeInPixels)
-    return kMaxBitstreamBufferSizeInBytes * 4;
-  if (size.GetArea() > k1080PSizeInPixels)
-    return kMaxBitstreamBufferSizeInBytes * 2;
-  return kMaxBitstreamBufferSizeInBytes;
-}
-
 }  // namespace
 
 size_t GetEncodeBitstreamBufferSize(const gfx::Size& size,
@@ -65,22 +48,23 @@
           1.0f * (bitrate / framerate) / (data.bitrate_in_bps / data.framerate),
           1.0f);
       return std::min(static_cast<size_t>(data.buffer_size_in_bytes * ratio),
-                      GetMaxEncodeBitstreamBufferSize(size));
+                      kMaxBitstreamBufferSizeInBytes);
     }
   }
-  return GetMaxEncodeBitstreamBufferSize(size);
+  return kMaxBitstreamBufferSizeInBytes;
 }
 
-// Get the maximum output bitstream buffer size. Since we don't change the
-// buffer size when we update bitrate and framerate, we have to calculate the
-// buffer size for the maximum bitrate.
+// Get the maximum output bitstream buffer size. Because we don't change
+// the buffer size when we update bitrate and framerate, we have to calculate
+// the buffer size by the maximum bitrate.
 // However, the maximum bitrate for intel chipset is 40Mbps. The buffer size
 // calculated with this bitrate is always larger than 2MB. Therefore we just
 // return the value.
 // TODO(crbug.com/889739): Deprecate this function after we can update the
 // buffer size while requesting new bitrate and framerate.
-size_t GetEncodeBitstreamBufferSize(const gfx::Size& size) {
-  return GetMaxEncodeBitstreamBufferSize(size);
+size_t GetEncodeBitstreamBufferSize() {
+  // GetEncodeBitstreamBufferSize(size, 40000000, 30)
+  return kMaxBitstreamBufferSizeInBytes;
 }
 
 }  // namespace media
--- a/media/gpu/gpu_video_encode_accelerator_helpers.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/gpu_video_encode_accelerator_helpers.h	2019-05-17 18:53:34.252000000 +0300
@@ -21,8 +21,7 @@
                                                      uint32_t framerate);
 
 // Get the maximum bitstream buffer size for VideoEncodeAccelerator.
-// |size|: the resolution of video stream
-MEDIA_GPU_EXPORT size_t GetEncodeBitstreamBufferSize(const gfx::Size& size);
+MEDIA_GPU_EXPORT size_t GetEncodeBitstreamBufferSize();
 
 }  // namespace media
 
--- a/media/gpu/h264_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/h264_decoder.cc	2019-05-17 18:53:34.252000000 +0300
@@ -1418,13 +1418,7 @@
 }
 
 size_t H264Decoder::GetRequiredNumOfPictures() const {
-  constexpr size_t kPicsInPipeline = limits::kMaxVideoFrames + 1;
-  return GetNumReferenceFrames() + kPicsInPipeline;
-}
-
-size_t H264Decoder::GetNumReferenceFrames() const {
-  // Use the maximum number of pictures in the Decoded Picture Buffer.
-  return dpb_.max_num_pics();
+  return dpb_.max_num_pics() + kPicsInPipeline;
 }
 
 // static
--- a/media/gpu/h264_decoder.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/h264_decoder.h	2019-05-17 18:53:34.252000000 +0300
@@ -160,7 +160,6 @@
   DecodeResult Decode() override WARN_UNUSED_RESULT;
   gfx::Size GetPicSize() const override;
   size_t GetRequiredNumOfPictures() const override;
-  size_t GetNumReferenceFrames() const override;
 
   // Return true if we need to start a new picture.
   static bool IsNewPrimaryCodedPicture(const H264Picture* curr_pic,
@@ -175,6 +174,17 @@
                                              H264Picture* pic);
 
  private:
+  // We need to keep at most kDPBMaxSize pictures in DPB for
+  // reference/to display later and an additional one for the one currently
+  // being decoded. We also ask for some additional ones since VDA needs
+  // to accumulate a few ready-to-output pictures before it actually starts
+  // displaying and giving them back. +2 instead of +1 because of subjective
+  // smoothness improvement during testing.
+  enum {
+    kPicsInPipeline = limits::kMaxVideoFrames + 2,
+    kMaxNumReqPictures = H264DPB::kDPBMaxSize + kPicsInPipeline,
+  };
+
   // Internal state of the decoder.
   enum State {
     // After initialization, need an SPS.
--- a/media/gpu/image_processor.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/image_processor.cc	2019-05-17 18:53:34.256000000 +0300
@@ -33,7 +33,7 @@
 bool ImageProcessor::Process(scoped_refptr<VideoFrame> frame,
                              int output_buffer_index,
                              std::vector<base::ScopedFD> output_dmabuf_fds,
-                             LegacyFrameReadyCB cb) {
+                             FrameReadyCB cb) {
   return ProcessInternal(std::move(frame), output_buffer_index,
                          std::move(output_dmabuf_fds),
                          BindToCurrentLoop(std::move(cb)));
--- a/media/gpu/image_processor.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/image_processor.h	2019-05-17 18:53:34.256000000 +0300
@@ -64,14 +64,6 @@
   // Process() is responsible for making sure this invariant is
   // respected by using media::BindToCurrentLoop().
   using FrameReadyCB = base::OnceCallback<void(scoped_refptr<VideoFrame>)>;
-  // Callback to be used to return a processed image to the client.
-  // Used when calling the "legacy" Process() method with buffers that are
-  // managed by the IP. The first argument is the index of the returned buffer.
-  // FrameReadyCB is guaranteed to be executed on the "client thread".
-  // Process() is responsible for making sure this invariant is
-  // respected by using media::BindToCurrentLoop().
-  using LegacyFrameReadyCB =
-      base::OnceCallback<void(size_t, scoped_refptr<VideoFrame>)>;
 
   // Callback to be used to notify client when ImageProcess encounters error.
   // It should be assigned in subclass' factory method. ErrorCB is guaranteed to
@@ -122,7 +114,7 @@
   bool Process(scoped_refptr<VideoFrame> frame,
                int output_buffer_index,
                std::vector<base::ScopedFD> output_dmabuf_fds,
-               LegacyFrameReadyCB cb);
+               FrameReadyCB cb);
 #endif
 
   // Called by client to process |input_frame| and store in |output_frame|. This
@@ -167,7 +159,7 @@
   virtual bool ProcessInternal(scoped_refptr<VideoFrame> frame,
                                int output_buffer_index,
                                std::vector<base::ScopedFD> output_dmabuf_fds,
-                               LegacyFrameReadyCB cb) = 0;
+                               FrameReadyCB cb) = 0;
 #endif
   virtual bool ProcessInternal(scoped_refptr<VideoFrame> input_frame,
                                scoped_refptr<VideoFrame> output_frame,
--- a/media/gpu/image_processor_test.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/image_processor_test.cc	2019-05-17 18:53:34.256000000 +0300
@@ -2,7 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#include <memory>
 #include <string>
 #include <tuple>
 
@@ -12,13 +11,10 @@
 #include "base/test/test_suite.h"
 #include "build/build_config.h"
 #include "media/base/video_frame.h"
-#include "media/base/video_frame_layout.h"
 #include "media/base/video_types.h"
 #include "media/gpu/image_processor.h"
-#include "media/gpu/test/image.h"
 #include "media/gpu/test/image_processor/image_processor_client.h"
-#include "media/gpu/test/video_frame_helpers.h"
-#include "media/gpu/test/video_frame_validator.h"
+#include "media/gpu/test/video_image_info.h"
 #include "mojo/core/embedder/embedder.h"
 #include "testing/gtest/include/gtest/gtest.h"
 #include "ui/gfx/geometry/size.h"
@@ -26,67 +22,84 @@
 namespace media {
 namespace {
 
-constexpr const base::FilePath::CharType* kI420Image =
-    FILE_PATH_LITERAL("bear_320x192.i420.yuv");
-constexpr const base::FilePath::CharType* kNV12Image =
-    FILE_PATH_LITERAL("bear_320x192.nv12.yuv");
+// I420 formatted 320x192 video frame. (bear)
+// TODO(crbug.com/917951): Dynamically load this info from json file.
+constexpr test::VideoImageInfo kI420Image(
+    FILE_PATH_LITERAL("bear_320x192.i420.yuv"),
+    "962820755c74b28f9385fd67219cc04a",
+    PIXEL_FORMAT_I420,
+    gfx::Size(320, 192));
+
+// NV12 formatted 320x192 video frame. (bear)
+// TODO(crbug.com/917951): Dynamically load this info from json file.
+constexpr test::VideoImageInfo kNV12Image(
+    FILE_PATH_LITERAL("bear_320x192.i420.nv12.yuv"),
+    "ce21986434743d3671056719136d46ff",
+    PIXEL_FORMAT_NV12,
+    gfx::Size(320, 192));
 
 class ImageProcessorSimpleParamTest
     : public ::testing::Test,
       public ::testing::WithParamInterface<
-          std::tuple<base::FilePath, base::FilePath>> {
+          std::tuple<test::VideoImageInfo, test::VideoImageInfo>> {
  public:
   // TODO(crbug.com/917951): Initialize Ozone once.
   void SetUp() override {}
   void TearDown() override {}
 
   std::unique_ptr<test::ImageProcessorClient> CreateImageProcessorClient(
-      const test::Image& input_image,
-      const test::Image& output_image) {
-    auto input_config_layout = test::CreateVideoFrameLayout(
-        input_image.PixelFormat(), input_image.Size());
-    auto output_config_layout = test::CreateVideoFrameLayout(
-        output_image.PixelFormat(), output_image.Size());
+      const test::VideoImageInfo& input_image_info,
+      const test::VideoImageInfo& output_image_info) {
+    // TODO(crbug.com/917951): Pass VideoFrameProcessor.
+    auto input_config_layout = input_image_info.VideoFrameLayout();
+    auto output_config_layout = output_image_info.VideoFrameLayout();
     LOG_ASSERT(input_config_layout);
     LOG_ASSERT(output_config_layout);
     ImageProcessor::PortConfig input_config(*input_config_layout,
-                                            input_image.Size(),
+                                            input_image_info.visible_size,
                                             {VideoFrame::STORAGE_OWNED_MEMORY});
     ImageProcessor::PortConfig output_config(
-        *output_config_layout, output_image.Size(),
+        *output_config_layout, output_image_info.visible_size,
         {VideoFrame::STORAGE_OWNED_MEMORY});
     // TODO(crbug.com/917951): Select more appropriate number of buffers.
     constexpr size_t kNumBuffers = 1;
-    LOG_ASSERT(output_image.IsMetadataLoaded());
-    auto vf_validator = test::VideoFrameValidator::Create(
-        {output_image.Checksum()}, output_image.PixelFormat());
-    std::vector<std::unique_ptr<test::VideoFrameProcessor>> frame_processors;
-    frame_processors.push_back(std::move(vf_validator));
     auto ip_client = test::ImageProcessorClient::Create(
-        input_config, output_config, kNumBuffers, std::move(frame_processors));
+        input_config, output_config, kNumBuffers, true);
     LOG_ASSERT(ip_client) << "Failed to create ImageProcessorClient";
     return ip_client;
   }
 };
 
 TEST_P(ImageProcessorSimpleParamTest, ConvertOneTimeFromMemToMem) {
-  // Load the test input image. We only need the output image's metadata so we
-  // can compare checksums.
-  test::Image input_image(std::get<0>(GetParam()));
-  test::Image output_image(std::get<1>(GetParam()));
-  ASSERT_TRUE(input_image.Load());
-  ASSERT_TRUE(output_image.LoadMetadata());
+  test::VideoImageInfo input_image_info = std::get<0>(GetParam());
+  test::VideoImageInfo output_image_info = std::get<1>(GetParam());
+  auto ip_client =
+      CreateImageProcessorClient(input_image_info, output_image_info);
 
-  auto ip_client = CreateImageProcessorClient(input_image, output_image);
-  ip_client->Process(input_image, output_image);
+  ip_client->Process(input_image_info, output_image_info);
   EXPECT_TRUE(ip_client->WaitUntilNumImageProcessed(1u));
   EXPECT_EQ(ip_client->GetErrorCount(), 0u);
   EXPECT_EQ(ip_client->GetNumOfProcessedImages(), 1u);
-  EXPECT_TRUE(ip_client->WaitForFrameProcessors());
-}
+
+  // TODO(crbug.com/917951): Replace this checker with VideoFrameProcessor
+  // interface and get results by ImageProcessorClient function like
+  // ImageProcessorClient::GetProcessResults().
+  const auto output_frames = ip_client->GetProcessedImages();
+  ASSERT_EQ(output_frames.size(), 1u);
+  auto processed_frame = output_frames[0];
+  ASSERT_TRUE(processed_frame->IsMappable());
+  base::MD5Context context;
+  base::MD5Init(&context);
+  VideoFrame::HashFrameForTesting(&context, processed_frame);
+  base::MD5Digest digest;
+  base::MD5Final(&digest, &context);
+  std::string expected_md5 = output_image_info.md5sum;
+  std::string computed_md5 = MD5DigestToBase16(digest);
+  EXPECT_EQ(expected_md5, computed_md5);
+};
 
 // I420->NV12
-INSTANTIATE_TEST_SUITE_P(ConvertI420ToNV12,
+INSTANTIATE_TEST_CASE_P(ConvertI420ToNV12,
                          ImageProcessorSimpleParamTest,
                          ::testing::Values(std::make_tuple(kI420Image,
                                                            kNV12Image)));
--- a/media/gpu/ipc/client/gpu_video_decode_accelerator_host.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/ipc/client/gpu_video_decode_accelerator_host.cc	2019-05-17 18:53:34.256000000 +0300
@@ -191,8 +191,8 @@
 
   // The gpu::CommandBufferProxyImpl is going away; error out this VDA.
   media_task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&GpuVideoDecodeAcceleratorHost::OnChannelError,
-                                weak_this_));
+      FROM_HERE,
+      base::Bind(&GpuVideoDecodeAcceleratorHost::OnChannelError, weak_this_));
 }
 
 void GpuVideoDecodeAcceleratorHost::PostNotifyError(Error error) {
--- a/media/gpu/ipc/service/gpu_video_decode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/ipc/service/gpu_video_decode_accelerator.cc	2019-05-17 18:53:34.256000000 +0300
@@ -81,7 +81,7 @@
     return nullptr;
   }
 
-  return stub->decoder_context()->GetContextGroup();
+  return stub->context_group().get();
 }
 
 static std::unique_ptr<gpu::gles2::AbstractTexture> CreateAbstractTexture(
@@ -424,7 +424,7 @@
 
   gpu::DecoderContext* decoder_context = stub_->decoder_context();
   gpu::gles2::TextureManager* texture_manager =
-      stub_->decoder_context()->GetContextGroup()->texture_manager();
+      stub_->context_group()->texture_manager();
 
   std::vector<PictureBuffer> buffers;
   std::vector<std::vector<scoped_refptr<gpu::gles2::TextureRef>>> textures;
@@ -555,7 +555,7 @@
   for (auto texture_ref : it->second) {
     GLenum target = texture_ref->texture()->target();
     gpu::gles2::TextureManager* texture_manager =
-        stub_->decoder_context()->GetContextGroup()->texture_manager();
+        stub_->context_group()->texture_manager();
     texture_manager->SetLevelCleared(texture_ref.get(), target, 0, true);
   }
   uncleared_textures_.erase(it);
--- a/media/gpu/ipc/service/media_gpu_channel.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/ipc/service/media_gpu_channel.cc	2019-05-17 18:53:34.256000000 +0300
@@ -108,9 +108,7 @@
   TRACE_EVENT0("gpu", "MediaGpuChannel::OnCreateVideoDecoder");
   gpu::CommandBufferStub* stub =
       channel_->LookupCommandBuffer(command_buffer_route_id);
-  // Only allow stubs that have a ContextGroup, that is, the GLES2 ones. Later
-  // code assumes the ContextGroup is valid.
-  if (!stub || !stub->decoder_context()->GetContextGroup()) {
+  if (!stub) {
     reply_message->set_reply_error();
     Send(reply_message);
     return;
--- a/media/gpu/ipc/service/picture_buffer_manager_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/ipc/service/picture_buffer_manager_unittest.cc	2019-05-17 18:53:34.256000000 +0300
@@ -10,12 +10,34 @@
 #include "base/memory/scoped_refptr.h"
 #include "base/test/mock_callback.h"
 #include "base/test/scoped_task_environment.h"
-#include "media/base/simple_sync_token_client.h"
 #include "media/gpu/fake_command_buffer_helper.h"
 #include "testing/gtest/include/gtest/gtest.h"
 
 namespace media {
 
+namespace {
+
+// TODO(sandersd): Should be part of //media, as it is used by
+// MojoVideoDecoderService (production code) as well.
+class StaticSyncTokenClient : public VideoFrame::SyncTokenClient {
+ public:
+  explicit StaticSyncTokenClient(const gpu::SyncToken& sync_token)
+      : sync_token_(sync_token) {}
+
+  void GenerateSyncToken(gpu::SyncToken* sync_token) final {
+    *sync_token = sync_token_;
+  }
+
+  void WaitSyncToken(const gpu::SyncToken& sync_token) final {}
+
+ private:
+  gpu::SyncToken sync_token_;
+
+  DISALLOW_COPY_AND_ASSIGN(StaticSyncTokenClient);
+};
+
+}  // namespace
+
 class PictureBufferManagerImplTest : public testing::Test {
  public:
   explicit PictureBufferManagerImplTest() {
@@ -66,7 +88,7 @@
     gpu::SyncToken sync_token(gpu::GPU_IO,
                               gpu::CommandBufferId::FromUnsafeValue(1),
                               next_release_count_++);
-    SimpleSyncTokenClient sync_token_client(sync_token);
+    StaticSyncTokenClient sync_token_client(sync_token);
     video_frame->UpdateReleaseSyncToken(&sync_token_client);
     return sync_token;
   }
--- a/media/gpu/ipc/service/vda_video_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/ipc/service/vda_video_decoder.cc	2019-05-17 18:53:34.256000000 +0300
@@ -277,17 +277,11 @@
       false;
 #endif
 
-  // Hardware decoders require ColorSpace to be set beforehand to provide
-  // correct HDR output.
-  const bool is_hdr_color_space_change =
-      config_.profile() == media::VP9PROFILE_PROFILE2 &&
-      config_.color_space_info() != config.color_space_info();
-
   // The configuration is supported.
   config_ = config;
 
   if (reinitializing) {
-    if (is_profile_change || is_hdr_color_space_change) {
+    if (is_profile_change) {
       MEDIA_LOG(INFO, media_log_) << "Reinitializing video decode accelerator "
                                   << "for profile change";
       gpu_task_runner_->PostTask(
--- a/media/gpu/ipc/service/vda_video_decoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/ipc/service/vda_video_decoder_unittest.cc	2019-05-17 18:53:34.260000000 +0300
@@ -6,7 +6,6 @@
 
 #include <stdint.h>
 
-#include "base/bind.h"
 #include "base/memory/ptr_util.h"
 #include "base/memory/scoped_refptr.h"
 #include "base/single_thread_task_runner.h"
@@ -20,7 +19,6 @@
 #include "media/base/decoder_buffer.h"
 #include "media/base/media_util.h"
 #include "media/base/mock_media_log.h"
-#include "media/base/simple_sync_token_client.h"
 #include "media/base/video_codecs.h"
 #include "media/base/video_frame.h"
 #include "media/base/video_rotation.h"
@@ -54,6 +52,25 @@
   return buffer;
 }
 
+// TODO(sandersd): Should be part of //media, as it is used by
+// MojoVideoDecoderService (production code) as well.
+class StaticSyncTokenClient : public VideoFrame::SyncTokenClient {
+ public:
+  explicit StaticSyncTokenClient(const gpu::SyncToken& sync_token)
+      : sync_token_(sync_token) {}
+
+  void GenerateSyncToken(gpu::SyncToken* sync_token) final {
+    *sync_token = sync_token_;
+  }
+
+  void WaitSyncToken(const gpu::SyncToken& sync_token) final {}
+
+ private:
+  gpu::SyncToken sync_token_;
+
+  DISALLOW_COPY_AND_ASSIGN(StaticSyncTokenClient);
+};
+
 VideoDecodeAccelerator::SupportedProfiles GetSupportedProfiles() {
   VideoDecodeAccelerator::SupportedProfiles profiles;
   {
@@ -261,7 +278,7 @@
     gpu::SyncToken sync_token(gpu::GPU_IO,
                               gpu::CommandBufferId::FromUnsafeValue(1),
                               next_release_count_++);
-    SimpleSyncTokenClient sync_token_client(sync_token);
+    StaticSyncTokenClient sync_token_client(sync_token);
     video_frame->UpdateReleaseSyncToken(&sync_token_client);
     return sync_token;
   }
@@ -457,7 +474,7 @@
   NotifyFlushDone();
 }
 
-INSTANTIATE_TEST_SUITE_P(VdaVideoDecoder,
+INSTANTIATE_TEST_CASE_P(VdaVideoDecoder,
                          VdaVideoDecoderTest,
                          ::testing::Values(false, true));
 
--- a/media/gpu/jpeg_encode_accelerator_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/jpeg_encode_accelerator_unittest.cc	2019-05-17 18:53:34.260000000 +0300
@@ -371,9 +371,9 @@
                                      &sw_encoded_size, &elapsed_sw));
 
   g_env->LogToFile("hw_encode_time",
-                   base::NumberToString(elapsed_hw.InMicroseconds()));
+                   base::Int64ToString(elapsed_hw.InMicroseconds()));
   g_env->LogToFile("sw_encode_time",
-                   base::NumberToString(elapsed_sw.InMicroseconds()));
+                   base::Int64ToString(elapsed_sw.InMicroseconds()));
 
   if (g_save_to_file) {
     SaveToFile(test_image, hw_encoded_size, sw_encoded_size);
--- a/media/gpu/libyuv_image_processor.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/libyuv_image_processor.cc	2019-05-17 18:53:34.260000000 +0300
@@ -104,7 +104,7 @@
     scoped_refptr<VideoFrame> frame,
     int output_buffer_index,
     std::vector<base::ScopedFD> output_dmabuf_fds,
-    LegacyFrameReadyCB cb) {
+    FrameReadyCB cb) {
   DCHECK_CALLED_ON_VALID_THREAD(client_thread_checker_);
   NOTIMPLEMENTED();
   return false;
--- a/media/gpu/libyuv_image_processor.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/libyuv_image_processor.h	2019-05-17 18:53:34.260000000 +0300
@@ -61,7 +61,7 @@
   bool ProcessInternal(scoped_refptr<VideoFrame> frame,
                        int output_buffer_index,
                        std::vector<base::ScopedFD> output_dmabuf_fds,
-                       LegacyFrameReadyCB cb) override;
+                       FrameReadyCB cb) override;
 #endif
   bool ProcessInternal(scoped_refptr<VideoFrame> input_frame,
                        scoped_refptr<VideoFrame> output_frame,
--- a/media/gpu/platform_video_frame.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/platform_video_frame.cc	2019-05-17 18:53:34.260000000 +0300
@@ -39,23 +39,34 @@
 
   const size_t num_planes = VideoFrame::NumPlanes(pixel_format);
   std::vector<VideoFrameLayout::Plane> planes(num_planes);
-  std::vector<size_t> buffer_sizes(num_planes);
   for (size_t i = 0; i < num_planes; ++i) {
     planes[i].stride = pixmap->GetDmaBufPitch(i);
     planes[i].offset = pixmap->GetDmaBufOffset(i);
     planes[i].modifier = pixmap->GetDmaBufModifier(i);
-    buffer_sizes[i] = planes[i].offset +
-                      planes[i].stride * VideoFrame::Rows(i, pixel_format,
-                                                          coded_size.height());
   }
 
+  const size_t num_fds = pixmap->GetDmaBufFdCount();
+  std::vector<size_t> buffer_sizes(num_fds, 0u);
+  // If the number of buffer sizes is less than number of planes, the buffer for
+  // plane #i (i > the number of fds) is the last buffer.
+  for (size_t i = 0; i < num_planes; ++i) {
+    size_t buffer_size =
+        planes[i].offset +
+        planes[i].stride *
+            VideoFrame::Rows(i, pixel_format, coded_size.height());
+    if (i < num_fds) {
+      buffer_sizes[i] = buffer_size;
+    } else {
+      buffer_sizes.back() = std::max(buffer_sizes.back(), buffer_size);
+    }
+  }
   auto layout = VideoFrameLayout::CreateWithPlanes(
       pixel_format, coded_size, std::move(planes), std::move(buffer_sizes));
   if (!layout)
     return nullptr;
 
   std::vector<base::ScopedFD> dmabuf_fds;
-  for (size_t i = 0; i < num_planes; ++i) {
+  for (size_t i = 0; i < num_fds; ++i) {
     int duped_fd = HANDLE_EINTR(dup(pixmap->GetDmaBufFd(i)));
     if (duped_fd == -1) {
       DLOG(ERROR) << "Failed duplicating dmabuf fd";
--- a/media/gpu/test/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/BUILD.gn	2019-05-17 18:53:34.260000000 +0300
@@ -27,7 +27,6 @@
   ]
   deps = [
     "//media/gpu",
-    "//third_party/libyuv",
     "//ui/gl/init:init",
   ]
   if (use_ozone) {
@@ -73,20 +72,7 @@
     ":decode_helpers",
     ":helpers",
     "//media/gpu",
-  ]
-}
-
-source_set("frame_file_writer") {
-  testonly = true
-  sources = [
-    "video_frame_file_writer.cc",
-    "video_frame_file_writer.h",
-  ]
-  deps = [
-    ":frame_mapper",
-    ":render_helpers",
-    "//media/gpu",
-    "//ui/gfx/codec:codec",
+    "//third_party/libyuv",
   ]
 }
 
@@ -127,23 +113,6 @@
 
 # TODO(dstaessens@) Make this work on other platforms too.
 if (is_chromeos) {
-  static_library("video_player_thumbnail_renderer") {
-    testonly = true
-    sources = [
-      "video_player/frame_renderer.h",
-      "video_player/frame_renderer_thumbnail.cc",
-      "video_player/frame_renderer_thumbnail.h",
-    ]
-    deps = [
-      ":decode_helpers",
-      "//gpu/command_buffer/common",
-      "//media/gpu",
-      "//ui/gfx/codec:codec",
-      "//ui/gl:gl",
-      "//ui/gl/init:init",
-    ]
-  }
-
   static_library("video_player") {
     testonly = true
     sources = [
@@ -159,47 +128,27 @@
       "video_player/video_player.cc",
       "video_player/video_player.h",
     ]
-    deps = [
-      ":decode_helpers",
-      "//media/gpu",
-    ]
-  }
-
-  static_library("video_player_test_environment") {
-    testonly = true
-    sources = [
-      "video_player/video_player_test_environment.cc",
-      "video_player/video_player_test_environment.h",
-    ]
     data = [
       "//media/test/data/",
     ]
     deps = [
-      ":video_player",
-      "//base/test:test_config",
-      "//base/test:test_support",
+      ":decode_helpers",
       "//media/gpu",
-      "//testing/gtest:gtest",
     ]
     if (use_ozone) {
       deps += [ "//ui/ozone" ]
     }
-    if (use_vaapi) {
-      deps += [ "//media/gpu/vaapi" ]
-    }
   }
 }
 
 static_library("image_processor") {
   testonly = true
   sources = [
-    "image.cc",
-    "image.h",
     "image_processor/image_processor_client.cc",
     "image_processor/image_processor_client.h",
+    "video_image_info.h",
   ]
   deps = [
-    ":render_helpers",
     "//media:test_support",
     "//media/gpu",
     "//testing/gtest",
--- a/media/gpu/test/image_processor/image_processor_client.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/image_processor/image_processor_client.cc	2019-05-17 18:53:34.260000000 +0300
@@ -4,22 +4,22 @@
 
 #include "media/gpu/test/image_processor/image_processor_client.h"
 
-#include <functional>
 #include <utility>
 
-#include "base/bind.h"
 #include "third_party/libyuv/include/libyuv/planar_functions.h"
 
 #include "base/bind_helpers.h"
+#include "base/files/file_util.h"
+#include "base/files/memory_mapped_file.h"
 #include "base/logging.h"
 #include "base/memory/ptr_util.h"
 #include "base/synchronization/waitable_event.h"
 #include "build/build_config.h"
 #include "media/base/bind_to_current_loop.h"
+#include "media/base/test_data_util.h"
 #include "media/base/video_frame.h"
 #include "media/base/video_frame_layout.h"
 #include "media/gpu/image_processor_factory.h"
-#include "media/gpu/test/image.h"
 #include "testing/gtest/include/gtest/gtest.h"
 #include "ui/gfx/geometry/rect.h"
 
@@ -29,6 +29,22 @@
 namespace {
 // TODO(crbug.com/917951): Move these functions to video_frame_helpers.h
 
+// Find the file path for |file_name|.
+base::FilePath GetFilePath(const base::FilePath::CharType* const file_name) {
+  // 1. Try to find |file_name| in the current directory.
+  base::FilePath file_path =
+      base::FilePath(base::FilePath::kCurrentDirectory).Append(file_name);
+  if (base::PathExists(file_path)) {
+    return file_path;
+  }
+
+  // 2. Try media::GetTestDataFilePath(|file_name|), that is,
+  // media/test/data/file_name. This is mainly for Try bot.
+  file_path = media::GetTestDataPath().Append(file_name);
+  LOG_ASSERT(base::PathExists(file_path)) << " Cannot find " << file_name;
+  return file_path;
+}
+
 // Copy |src_frame| into a new VideoFrame with |dst_layout|. The created
 // VideoFrame's content is the same as |src_frame|. Returns nullptr on failure.
 scoped_refptr<VideoFrame> CloneVideoFrameWithLayout(
@@ -62,6 +78,61 @@
   return dst_frame;
 }
 
+// Create VideoFrame from |info| loading |info.file_name|. Return nullptr on
+// failure.
+scoped_refptr<VideoFrame> ReadVideoFrame(const VideoImageInfo& info) {
+  auto path = GetFilePath(info.file_name);
+  // First read file.
+  auto mapped_file = std::make_unique<base::MemoryMappedFile>();
+  if (!mapped_file->Initialize(path)) {
+    LOG(ERROR) << "Failed to read file: " << path;
+    return nullptr;
+  }
+
+  const auto format = info.pixel_format;
+  const auto visible_size = info.visible_size;
+  // Check the file length and md5sum.
+  LOG_ASSERT(mapped_file->length() ==
+             VideoFrame::AllocationSize(format, visible_size));
+  base::MD5Digest digest;
+  base::MD5Sum(mapped_file->data(), mapped_file->length(), &digest);
+  LOG_ASSERT(base::MD5DigestToBase16(digest) == info.md5sum);
+
+  // Create planes for layout. We cannot use WrapExternalData() because it calls
+  // GetDefaultLayout() and it supports only a few pixel formats.
+  const size_t num_planes = VideoFrame::NumPlanes(format);
+  std::vector<VideoFrameLayout::Plane> planes(num_planes);
+  const auto strides = VideoFrame::ComputeStrides(format, visible_size);
+  size_t offset = 0;
+  for (size_t i = 0; i < num_planes; ++i) {
+    planes[i].stride = strides[i];
+    planes[i].offset = offset;
+    offset += VideoFrame::PlaneSize(format, i, visible_size).GetArea();
+  }
+
+  auto layout = VideoFrameLayout::CreateWithPlanes(
+      format, visible_size, std::move(planes), {mapped_file->length()});
+  if (!layout) {
+    LOG(ERROR) << "Failed to create VideoFrameLayout";
+    return nullptr;
+  }
+
+  auto frame = VideoFrame::WrapExternalDataWithLayout(
+      *layout, gfx::Rect(visible_size), visible_size, mapped_file->data(),
+      mapped_file->length(), base::TimeDelta());
+  if (!frame) {
+    LOG(ERROR) << "Failed to create VideoFrame";
+    return nullptr;
+  }
+
+  // Automatically unmap the memory mapped file when the video frame is
+  // destroyed.
+  frame->AddDestructionObserver(base::BindOnce(
+      base::DoNothing::Once<std::unique_ptr<base::MemoryMappedFile>>(),
+      std::move(mapped_file)));
+  return frame;
+}
+
 }  // namespace
 
 // static
@@ -69,9 +140,9 @@
     const ImageProcessor::PortConfig& input_config,
     const ImageProcessor::PortConfig& output_config,
     size_t num_buffers,
-    std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors) {
+    bool store_processed_video_frames) {
   auto ip_client =
-      base::WrapUnique(new ImageProcessorClient(std::move(frame_processors)));
+      base::WrapUnique(new ImageProcessorClient(store_processed_video_frames));
   if (!ip_client->CreateImageProcessor(input_config, output_config,
                                        num_buffers)) {
     LOG(ERROR) << "Failed to create ImageProcessor";
@@ -80,10 +151,9 @@
   return ip_client;
 }
 
-ImageProcessorClient::ImageProcessorClient(
-    std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors)
-    : frame_processors_(std::move(frame_processors)),
-      image_processor_client_thread_("ImageProcessorClientThread"),
+ImageProcessorClient::ImageProcessorClient(bool store_processed_video_frames)
+    : image_processor_client_thread_("ImageProcessorClientThread"),
+      store_processed_video_frames_(store_processed_video_frames),
       output_cv_(&output_lock_),
       num_processed_frames_(0),
       image_processor_error_count_(0) {
@@ -107,13 +177,14 @@
   DCHECK_CALLED_ON_VALID_THREAD(test_main_thread_checker_);
   base::WaitableEvent done(base::WaitableEvent::ResetPolicy::AUTOMATIC,
                            base::WaitableEvent::InitialState::NOT_SIGNALED);
-  // base::Unretained(this) and std::cref() are safe here because |this|,
+  // base::Unretained(this) and base::ConstRef() are safe here because |this|,
   // |input_config| and |output_config| must outlive because this task is
   // blocking.
   image_processor_client_thread_.task_runner()->PostTask(
-      FROM_HERE, base::BindOnce(&ImageProcessorClient::CreateImageProcessorTask,
-                                base::Unretained(this), std::cref(input_config),
-                                std::cref(output_config), num_buffers, &done));
+      FROM_HERE,
+      base::BindOnce(&ImageProcessorClient::CreateImageProcessorTask,
+                     base::Unretained(this), base::ConstRef(input_config),
+                     base::ConstRef(output_config), num_buffers, &done));
   done.Wait();
   if (!image_processor_) {
     LOG(ERROR) << "Failed to create ImageProcessor";
@@ -139,48 +210,15 @@
 }
 
 scoped_refptr<VideoFrame> ImageProcessorClient::CreateInputFrame(
-    const Image& input_image) const {
+    const VideoImageInfo& input_image_info) const {
   DCHECK_CALLED_ON_VALID_THREAD(test_main_thread_checker_);
   LOG_ASSERT(image_processor_);
-  LOG_ASSERT(input_image.IsLoaded());
-  LOG_ASSERT(input_image.DataSize() ==
-             VideoFrame::AllocationSize(input_image.PixelFormat(),
-                                        input_image.Size()));
-
-  const auto format = input_image.PixelFormat();
-  const auto visible_size = input_image.Size();
-
-  // Create planes for layout. We cannot use WrapExternalData() because it
-  // calls GetDefaultLayout() and it supports only a few pixel formats.
-  const size_t num_planes = VideoFrame::NumPlanes(format);
-  std::vector<VideoFrameLayout::Plane> planes(num_planes);
-  const auto strides = VideoFrame::ComputeStrides(format, visible_size);
-  size_t offset = 0;
-  for (size_t i = 0; i < num_planes; ++i) {
-    planes[i].stride = strides[i];
-    planes[i].offset = offset;
-    offset += VideoFrame::PlaneSize(format, i, visible_size).GetArea();
-  }
-
-  auto layout = VideoFrameLayout::CreateWithPlanes(
-      format, visible_size, std::move(planes), {input_image.DataSize()});
-  if (!layout) {
-    LOG(ERROR) << "Failed to create VideoFrameLayout";
-    return nullptr;
-  }
-
-  auto frame = VideoFrame::WrapExternalDataWithLayout(
-      *layout, gfx::Rect(visible_size), visible_size, input_image.Data(),
-      input_image.DataSize(), base::TimeDelta());
-  if (!frame) {
-    LOG(ERROR) << "Failed to create VideoFrame";
-    return nullptr;
-  }
 
+  auto mapped_frame = ReadVideoFrame(input_image_info);
   const auto& input_layout = image_processor_->input_layout();
   if (VideoFrame::IsStorageTypeMappable(
           image_processor_->input_storage_type())) {
-    return CloneVideoFrameWithLayout(frame.get(), input_layout);
+    return CloneVideoFrameWithLayout(mapped_frame.get(), input_layout);
   } else {
 #if defined(OS_CHROMEOS)
     LOG_ASSERT(image_processor_->input_storage_type() ==
@@ -193,17 +231,18 @@
 }
 
 scoped_refptr<VideoFrame> ImageProcessorClient::CreateOutputFrame(
-    const Image& output_image) const {
+    const VideoImageInfo& output_image_info) const {
   DCHECK_CALLED_ON_VALID_THREAD(test_main_thread_checker_);
-  LOG_ASSERT(output_image.IsMetadataLoaded());
   LOG_ASSERT(image_processor_);
 
   const auto& output_layout = image_processor_->output_layout();
   if (VideoFrame::IsStorageTypeMappable(
           image_processor_->input_storage_type())) {
     return VideoFrame::CreateFrameWithLayout(
-        output_layout, gfx::Rect(output_image.Size()), output_image.Size(),
-        base::TimeDelta(), false /* zero_initialize_memory*/);
+        output_layout, gfx::Rect(output_image_info.visible_size),
+        output_image_info.visible_size, base::TimeDelta(),
+        false /* zero_initialize_memory*/
+    );
   } else {
 #if defined(OS_CHROMEOS)
     LOG_ASSERT(image_processor_->input_storage_type() ==
@@ -215,15 +254,12 @@
   }
 }
 
-void ImageProcessorClient::FrameReady(size_t frame_index,
-                                      scoped_refptr<VideoFrame> frame) {
+void ImageProcessorClient::FrameReady(scoped_refptr<VideoFrame> frame) {
   DCHECK_CALLED_ON_VALID_THREAD(image_processor_client_thread_checker_);
 
   base::AutoLock auto_lock_(output_lock_);
-  // VideoFrame should be processed in FIFO order.
-  EXPECT_EQ(frame_index, num_processed_frames_);
-  for (auto& processor : frame_processors_)
-    processor->ProcessVideoFrame(std::move(frame), frame_index);
+  if (store_processed_video_frames_)
+    output_video_frames_.push_back(std::move(frame));
   num_processed_frames_++;
   output_cv_.Signal();
 }
@@ -237,30 +273,28 @@
   // locks again at the end.
   base::AutoLock auto_lock_(output_lock_);
   while (time_waiting < max_wait) {
-    if (num_processed_frames_ >= num_processed)
-      return true;
-
     const base::TimeTicks start_time = base::TimeTicks::Now();
     output_cv_.TimedWait(max_wait);
     time_waiting += base::TimeTicks::Now() - start_time;
-  }
 
+    if (num_processed_frames_ >= num_processed)
+      return true;
+  }
   return false;
 }
 
-bool ImageProcessorClient::WaitForFrameProcessors() {
-  bool success = true;
-  for (auto& processor : frame_processors_)
-    success &= processor->WaitUntilDone();
-
-  return success;
-}
-
 size_t ImageProcessorClient::GetNumOfProcessedImages() const {
   base::AutoLock auto_lock_(output_lock_);
   return num_processed_frames_;
 }
 
+std::vector<scoped_refptr<VideoFrame>>
+ImageProcessorClient::GetProcessedImages() const {
+  LOG_ASSERT(store_processed_video_frames_);
+  base::AutoLock auto_lock_(output_lock_);
+  return output_video_frames_;
+}
+
 size_t ImageProcessorClient::GetErrorCount() const {
   base::AutoLock auto_lock_(output_lock_);
   return image_processor_error_count_;
@@ -272,12 +306,12 @@
   image_processor_error_count_++;
 }
 
-void ImageProcessorClient::Process(const Image& input_image,
-                                   const Image& output_image) {
+void ImageProcessorClient::Process(const VideoImageInfo& input_info,
+                                   const VideoImageInfo& output_info) {
   DCHECK_CALLED_ON_VALID_THREAD(test_main_thread_checker_);
-  auto input_frame = CreateInputFrame(input_image);
+  auto input_frame = CreateInputFrame(input_info);
   ASSERT_TRUE(input_frame);
-  auto output_frame = CreateOutputFrame(output_image);
+  auto output_frame = CreateOutputFrame(input_info);
   ASSERT_TRUE(output_frame);
   image_processor_client_thread_.task_runner()->PostTask(
       FROM_HERE,
@@ -288,14 +322,13 @@
 void ImageProcessorClient::ProcessTask(scoped_refptr<VideoFrame> input_frame,
                                        scoped_refptr<VideoFrame> output_frame) {
   DCHECK_CALLED_ON_VALID_THREAD(image_processor_client_thread_checker_);
-  // base::Unretained(this) and std::cref() for FrameReadyCB is safe here
+  // base::Unretained(this) and base::ConstRef() for FrameReadyCB is safe here
   // because the callback is executed on |image_processor_client_thread_| which
   // is owned by this class.
-  image_processor_->Process(std::move(input_frame), std::move(output_frame),
-                            BindToCurrentLoop(base::BindOnce(
-                                &ImageProcessorClient::FrameReady,
-                                base::Unretained(this), next_frame_index_)));
-  next_frame_index_++;
+  image_processor_->Process(
+      std::move(input_frame), std::move(output_frame),
+      BindToCurrentLoop(base::BindOnce(&ImageProcessorClient::FrameReady,
+                                       base::Unretained(this))));
 }
 
 }  // namespace test
--- a/media/gpu/test/image_processor/image_processor_client.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/image_processor/image_processor_client.h	2019-05-17 18:53:34.260000000 +0300
@@ -17,7 +17,7 @@
 #include "base/threading/thread_checker.h"
 #include "base/time/time.h"
 #include "media/gpu/image_processor.h"
-#include "media/gpu/test/video_frame_helpers.h"
+#include "media/gpu/test/video_image_info.h"
 
 namespace base {
 
@@ -31,38 +31,31 @@
 
 namespace test {
 
-class Image;
-
 // ImageProcessorClient is a client of ImageProcessor for testing purpose.
 // All the public functions must be called on the same thread, usually the test
 // main thread.
 class ImageProcessorClient {
  public:
-  // Create ImageProcessorClient that has ImageProcessor that converts images
-  // from |input_config| to |output_config|. The |num_buffers| parameter
-  // specifies the number of buffers we want to create, but might be ignored by
-  // the image processor. See description in "media/gpu/image_processor.h" for
-  // detail. The |frame_processors| will perform additional processing (e.g.
-  // validation, writing to file) on each video frame produced by the
-  // ImageProcessor.
+  // |store_processed_video_frames| is whether ImageProcessorClient will store
+  // VideoFrame in FrameReady().
+  // TODO(crbug.com/917951): Get VideoFrameProcessor and does not store
+  // VideoFrame.
   static std::unique_ptr<ImageProcessorClient> Create(
       const ImageProcessor::PortConfig& input_config,
       const ImageProcessor::PortConfig& output_config,
       size_t num_buffers,
-      std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors);
+      bool store_processed_video_frames = true);
 
   // Destruct |image_processor_| if it is created.
   ~ImageProcessorClient();
 
   // Process |input_frame| and |output_frame| with |image_processor_|.
-  // Processing is done asynchronously, the WaitUntilNumImageProcessed()
-  // function can be used to wait for the results.
-  void Process(const Image& input_image, const Image& output_image);
+  void Process(const VideoImageInfo& input_info,
+               const VideoImageInfo& output_info);
 
   // TODO(crbug.com/917951): Add Reset() when we test Reset() test case.
 
-  // Wait until |num_processed| frames are processed. Returns false if
-  // |max_wait| is exceeded.
+  // Wait until |num_processed| frames are processed.
   bool WaitUntilNumImageProcessed(
       size_t num_processed,
       base::TimeDelta max_wait = base::TimeDelta::FromSeconds(5));
@@ -70,16 +63,18 @@
   // Get the number of processed VideoFrames.
   size_t GetNumOfProcessedImages() const;
 
-  // Wait until all frame processors have finished processing. Returns whether
-  // processing was successful.
-  bool WaitForFrameProcessors();
+  // Get processed VideoFrames.
+  // This always returns empty unless |store_processed_video_frames_|
+  // TODO(hiroh): Remove this function and instead check md5sum and order
+  // interactively inside of ImageProcessorClient by passing some
+  // VideoFrameProcessor interface class.
+  std::vector<scoped_refptr<VideoFrame>> GetProcessedImages() const;
 
   // Return whether |image_processor_| invokes ImageProcessor::ErrorCB.
   size_t GetErrorCount() const;
 
  private:
-  explicit ImageProcessorClient(
-      std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors);
+  ImageProcessorClient(bool store_processed_video_frames);
 
   // Create ImageProcessor with |input_config|, |output_config| and
   // |num_buffers|.
@@ -98,36 +93,40 @@
                    scoped_refptr<VideoFrame> output_frame);
 
   // FrameReadyCB for ImageProcessor::Process().
-  void FrameReady(size_t frame_index, scoped_refptr<VideoFrame> frame);
+  void FrameReady(scoped_refptr<VideoFrame> frame);
   // ErrorCB for ImageProcessor.
   void NotifyError();
 
   // These are test helper functions to create a VideoFrame from VideoImageInfo,
   // which will be input in Process().
   // Create a VideoFrame using the input layout required by |image_processor_|.
-  scoped_refptr<VideoFrame> CreateInputFrame(const Image& input_image) const;
+  scoped_refptr<VideoFrame> CreateInputFrame(
+      const VideoImageInfo& input_image_info) const;
   // Create a VideoFrame using the output layout required by |image_processor_|.
-  scoped_refptr<VideoFrame> CreateOutputFrame(const Image& output_image) const;
+  scoped_refptr<VideoFrame> CreateOutputFrame(
+      const VideoImageInfo& output_image_info) const;
 
   std::unique_ptr<ImageProcessor> image_processor_;
 
-  // VideoFrameProcessors that will process the video frames produced by
-  // |image_processor_|.
-  std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors_;
-  // Frame index to be assigned to next VideoFrame.
-  size_t next_frame_index_ = 0;
-
   // The thread on which the |image_processor_| is created and destroyed. From
   // the specification of ImageProcessor, ImageProcessor::Process(),
   // FrameReady() and NotifyError() must be run on
   // |image_processor_client_thread_|.
   base::Thread image_processor_client_thread_;
 
+  // If true, ImageProcessorClient stores processed VideoFrame in
+  // |output_video_frames_|.
+  const bool store_processed_video_frames_;
+
   mutable base::Lock output_lock_;
   // This is signaled in FrameReady().
   base::ConditionVariable output_cv_;
   // The number of processed VideoFrame.
   size_t num_processed_frames_ GUARDED_BY(output_lock_);
+  // The collection of processed VideoFrame. It is stored in FrameReady() call
+  // order.
+  std::vector<scoped_refptr<VideoFrame>> output_video_frames_
+      GUARDED_BY(output_lock_);
   // The number of times ImageProcessor::ErrorCB called.
   size_t image_processor_error_count_ GUARDED_BY(output_lock_);
 
--- a/media/gpu/test/rendering_helper.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/rendering_helper.cc	2019-05-17 18:53:34.260000000 +0300
@@ -39,6 +39,38 @@
 
 namespace media {
 
+namespace {
+
+// Helper for Shader creation.
+void CreateShader(GLuint program, GLenum type, const char* source, int size) {
+  GLuint shader = glCreateShader(type);
+  glShaderSource(shader, 1, &source, &size);
+  glCompileShader(shader);
+  int result = GL_FALSE;
+  glGetShaderiv(shader, GL_COMPILE_STATUS, &result);
+  if (!result) {
+    char log[4096];
+    glGetShaderInfoLog(shader, base::size(log), NULL, log);
+    LOG(FATAL) << log;
+  }
+  glAttachShader(program, shader);
+  glDeleteShader(shader);
+  CHECK_EQ(static_cast<int>(glGetError()), GL_NO_ERROR);
+}
+
+void DeleteTexture(uint32_t texture_id) {
+  glDeleteTextures(1, &texture_id);
+  CHECK_EQ(static_cast<int>(glGetError()), GL_NO_ERROR);
+}
+
+// Helper function to set GL viewport.
+void GLSetViewPort(const gfx::Rect& area) {
+  glViewport(area.x(), area.y(), area.width(), area.height());
+  glScissor(area.x(), area.y(), area.width(), area.height());
+}
+
+}  // namespace
+
 bool RenderingHelper::use_gl_ = false;
 
 VideoFrameTexture::VideoFrameTexture(uint32_t texture_target,
@@ -404,39 +436,6 @@
   }
 }
 
-// static
-void RenderingHelper::DeleteTexture(uint32_t texture_id) {
-  glDeleteTextures(1, &texture_id);
-  CHECK_EQ(static_cast<int>(glGetError()), GL_NO_ERROR);
-}
-
-// static
-void RenderingHelper::GLSetViewPort(const gfx::Rect& area) {
-  glViewport(area.x(), area.y(), area.width(), area.height());
-  glScissor(area.x(), area.y(), area.width(), area.height());
-}
-
-// static
-void RenderingHelper::CreateShader(GLuint program,
-                                   GLenum type,
-                                   const char* source,
-                                   int size) {
-  GLuint shader = glCreateShader(type);
-  glShaderSource(shader, 1, &source, &size);
-  glCompileShader(shader);
-  int result = GL_FALSE;
-  glGetShaderiv(shader, GL_COMPILE_STATUS, &result);
-  if (!result) {
-    char log[4096];
-    glGetShaderInfoLog(shader, base::size(log), NULL, log);
-    LOG(FATAL) << log;
-  }
-  glAttachShader(program, shader);
-  glDeleteShader(shader);
-  CHECK_EQ(static_cast<int>(glGetError()), GL_NO_ERROR);
-}
-
-// static
 void RenderingHelper::RenderTexture(uint32_t texture_target,
                                     uint32_t texture_id) {
   // The ExternalOES sampler is bound to GL_TEXTURE1 and the Texture2D sampler
--- a/media/gpu/test/rendering_helper.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/rendering_helper.h	2019-05-17 18:53:34.260000000 +0300
@@ -35,9 +35,6 @@
 class TextureRef;
 }  // namespace test
 
-// TODO(dstaessens@) Most functionality can be removed from this file when the
-// video_decode_accelerator_unittests are deprecated in favor of the new
-// video_decode_accelerator_test.
 class VideoFrameTexture : public base::RefCounted<VideoFrameTexture> {
  public:
   uint32_t texture_id() const { return texture_id_; }
@@ -123,25 +120,9 @@
   void GetThumbnailsAsRGBA(std::vector<unsigned char>* rgba,
                            base::WaitableEvent* done);
 
-  // Delete the texture with specified |texture_id|.
-  static void DeleteTexture(uint32_t texture_id);
-
-  // Set the GL viewport to the specified |area|.
-  static void GLSetViewPort(const gfx::Rect& area);
-
-  // Create a shader with specified |program| id and |type| by compiling the
-  // shader |source| code with length |size|.
-  static void CreateShader(GLuint program,
-                           GLenum type,
-                           const char* source,
-                           int size);
-
-  // Render |texture_id| to the current view port of the screen using target
-  // |texture_target|.
-  static void RenderTexture(uint32_t texture_target, uint32_t texture_id);
-
  private:
   struct RenderedVideo {
+
     // True if there won't be any new video frames comming.
     bool is_flushing = false;
 
@@ -176,6 +157,10 @@
   void DropOneFrameForAllVideos();
   void ScheduleNextRenderContent();
 
+  // Render |texture_id| to the current view port of the screen using target
+  // |texture_target|.
+  void RenderTexture(uint32_t texture_target, uint32_t texture_id);
+
   scoped_refptr<base::SingleThreadTaskRunner> task_runner_;
 
   scoped_refptr<gl::GLContext> gl_context_;
--- a/media/gpu/test/texture_ref.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/texture_ref.cc	2019-05-17 18:53:34.260000000 +0300
@@ -47,8 +47,7 @@
 #if defined(OS_CHROMEOS)
   texture_ref = TextureRef::Create(texture_id, std::move(no_longer_needed_cb));
   LOG_ASSERT(texture_ref);
-  texture_ref->frame_ =
-      CreatePlatformVideoFrame(pixel_format, size, buffer_usage);
+  texture_ref->frame_ = CreateVideoFrame(pixel_format, size, buffer_usage);
 #endif
   return texture_ref;
 }
--- a/media/gpu/test/vaapi_dmabuf_video_frame_mapper.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/vaapi_dmabuf_video_frame_mapper.cc	2019-05-17 18:53:34.260000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/gpu/test/vaapi_dmabuf_video_frame_mapper.h"
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/memory/ptr_util.h"
 #include "build/build_config.h"
--- a/media/gpu/test/video_decode_accelerator_unittest_helpers.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_decode_accelerator_unittest_helpers.cc	2019-05-17 18:53:34.260000000 +0300
@@ -6,7 +6,6 @@
 
 #include <utility>
 
-#include "base/bind.h"
 #include "base/callback_helpers.h"
 #include "base/files/file_util.h"
 #include "base/strings/string_split.h"
@@ -43,8 +42,8 @@
   base::WaitableEvent done(base::WaitableEvent::ResetPolicy::AUTOMATIC,
                            base::WaitableEvent::InitialState::NOT_SIGNALED);
   rendering_thread_.task_runner()->PostTask(
-      FROM_HERE, base::BindOnce(&RenderingHelper::InitializeOneOff,
-                                use_gl_renderer_, &done));
+      FROM_HERE,
+      base::Bind(&RenderingHelper::InitializeOneOff, use_gl_renderer_, &done));
   done.Wait();
 
 #if defined(OS_CHROMEOS)
--- a/media/gpu/test/video_frame_helpers.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_frame_helpers.cc	2019-05-17 18:53:34.260000000 +0300
@@ -9,7 +9,6 @@
 
 #include "base/memory/scoped_refptr.h"
 #include "media/gpu/platform_video_frame.h"
-#include "third_party/libyuv/include/libyuv.h"
 #include "ui/gfx/gpu_memory_buffer.h"
 
 #if defined(OS_CHROMEOS)
@@ -19,142 +18,7 @@
 namespace media {
 namespace test {
 
-namespace {
-
-bool ConvertVideoFrameToI420(const VideoFrame* src_frame,
-                             VideoFrame* dst_frame) {
-  LOG_ASSERT(src_frame->visible_rect() == dst_frame->visible_rect());
-  LOG_ASSERT(dst_frame->format() == PIXEL_FORMAT_I420);
-
-  const auto& visible_rect = src_frame->visible_rect();
-  const int width = visible_rect.width();
-  const int height = visible_rect.height();
-  uint8_t* const dst_y = dst_frame->data(VideoFrame::kYPlane);
-  uint8_t* const dst_u = dst_frame->data(VideoFrame::kUPlane);
-  uint8_t* const dst_v = dst_frame->data(VideoFrame::kVPlane);
-  const int dst_stride_y = dst_frame->stride(VideoFrame::kYPlane);
-  const int dst_stride_u = dst_frame->stride(VideoFrame::kUPlane);
-  const int dst_stride_v = dst_frame->stride(VideoFrame::kVPlane);
-
-  switch (src_frame->format()) {
-    case PIXEL_FORMAT_I420:
-      return libyuv::I420Copy(src_frame->data(VideoFrame::kYPlane),
-                              src_frame->stride(VideoFrame::kYPlane),
-                              src_frame->data(VideoFrame::kUPlane),
-                              src_frame->stride(VideoFrame::kUPlane),
-                              src_frame->data(VideoFrame::kVPlane),
-                              src_frame->stride(VideoFrame::kVPlane), dst_y,
-                              dst_stride_y, dst_u, dst_stride_u, dst_v,
-                              dst_stride_v, width, height) == 0;
-    case PIXEL_FORMAT_NV12:
-      return libyuv::NV12ToI420(src_frame->data(VideoFrame::kYPlane),
-                                src_frame->stride(VideoFrame::kYPlane),
-                                src_frame->data(VideoFrame::kUVPlane),
-                                src_frame->stride(VideoFrame::kUVPlane), dst_y,
-                                dst_stride_y, dst_u, dst_stride_u, dst_v,
-                                dst_stride_v, width, height) == 0;
-    case PIXEL_FORMAT_YV12:
-      // Swap U and V planes.
-      return libyuv::I420Copy(src_frame->data(VideoFrame::kYPlane),
-                              src_frame->stride(VideoFrame::kYPlane),
-                              src_frame->data(VideoFrame::kVPlane),
-                              src_frame->stride(VideoFrame::kVPlane),
-                              src_frame->data(VideoFrame::kUPlane),
-                              src_frame->stride(VideoFrame::kUPlane), dst_y,
-                              dst_stride_y, dst_u, dst_stride_u, dst_v,
-                              dst_stride_v, width, height) == 0;
-    default:
-      LOG(ERROR) << "Unsupported input format: " << src_frame->format();
-      return false;
-  }
-}
-
-bool ConvertVideoFrameToARGB(const VideoFrame* src_frame,
-                             VideoFrame* dst_frame) {
-  LOG_ASSERT(src_frame->visible_rect() == dst_frame->visible_rect());
-  LOG_ASSERT(dst_frame->format() == PIXEL_FORMAT_ARGB);
-
-  const auto& visible_rect = src_frame->visible_rect();
-  const int width = visible_rect.width();
-  const int height = visible_rect.height();
-  uint8_t* const dst_argb = dst_frame->data(VideoFrame::kARGBPlane);
-  const int dst_stride = dst_frame->stride(VideoFrame::kARGBPlane);
-
-  switch (src_frame->format()) {
-    case PIXEL_FORMAT_I420:
-      // Note that we use J420ToARGB instead of I420ToARGB so that the
-      // kYuvJPEGConstants YUV-to-RGB conversion matrix is used.
-      return libyuv::J420ToARGB(src_frame->data(VideoFrame::kYPlane),
-                                src_frame->stride(VideoFrame::kYPlane),
-                                src_frame->data(VideoFrame::kUPlane),
-                                src_frame->stride(VideoFrame::kUPlane),
-                                src_frame->data(VideoFrame::kVPlane),
-                                src_frame->stride(VideoFrame::kVPlane),
-                                dst_argb, dst_stride, width, height) == 0;
-    case PIXEL_FORMAT_NV12:
-      return libyuv::NV12ToARGB(src_frame->data(VideoFrame::kYPlane),
-                                src_frame->stride(VideoFrame::kYPlane),
-                                src_frame->data(VideoFrame::kUVPlane),
-                                src_frame->stride(VideoFrame::kUVPlane),
-                                dst_argb, dst_stride, width, height) == 0;
-    case PIXEL_FORMAT_YV12:
-      // Same as I420, but U and V planes are swapped.
-      return libyuv::J420ToARGB(src_frame->data(VideoFrame::kYPlane),
-                                src_frame->stride(VideoFrame::kYPlane),
-                                src_frame->data(VideoFrame::kVPlane),
-                                src_frame->stride(VideoFrame::kVPlane),
-                                src_frame->data(VideoFrame::kUPlane),
-                                src_frame->stride(VideoFrame::kUPlane),
-                                dst_argb, dst_stride, width, height) == 0;
-      break;
-    default:
-      LOG(ERROR) << "Unsupported input format: " << src_frame->format();
-      return false;
-  }
-}
-
-}  // namespace
-
-bool ConvertVideoFrame(const VideoFrame* src_frame, VideoFrame* dst_frame) {
-  LOG_ASSERT(src_frame->visible_rect() == dst_frame->visible_rect());
-  LOG_ASSERT(src_frame->IsMappable() && dst_frame->IsMappable());
-
-  // Writing into non-owned memory might produce some unexpected side effects.
-  if (dst_frame->storage_type() != VideoFrame::STORAGE_OWNED_MEMORY)
-    LOG(WARNING) << "writing into non-owned memory";
-
-  // Only I420 and ARGB are currently supported as output formats.
-  switch (dst_frame->format()) {
-    case PIXEL_FORMAT_I420:
-      return ConvertVideoFrameToI420(src_frame, dst_frame);
-    case PIXEL_FORMAT_ARGB:
-      return ConvertVideoFrameToARGB(src_frame, dst_frame);
-    default:
-      LOG(ERROR) << "Unsupported output format: " << dst_frame->format();
-      return false;
-  }
-}
-
-scoped_refptr<VideoFrame> ConvertVideoFrame(const VideoFrame* src_frame,
-                                            VideoPixelFormat dst_pixel_format) {
-  gfx::Rect visible_rect = src_frame->visible_rect();
-  auto dst_frame = VideoFrame::CreateFrame(
-      dst_pixel_format, visible_rect.size(), visible_rect, visible_rect.size(),
-      base::TimeDelta());
-  if (!dst_frame) {
-    LOG(ERROR) << "Failed to convert video frame to " << dst_frame->format();
-    return nullptr;
-  }
-  bool conversion_success = ConvertVideoFrame(src_frame, dst_frame.get());
-  if (!conversion_success) {
-    LOG(ERROR) << "Failed to convert video frame to " << dst_frame->format();
-    return nullptr;
-  }
-  return dst_frame;
-}
-
-scoped_refptr<VideoFrame> CreatePlatformVideoFrame(
-    VideoPixelFormat pixel_format,
+scoped_refptr<VideoFrame> CreateVideoFrame(VideoPixelFormat pixel_format,
     const gfx::Size& size,
     gfx::BufferUsage buffer_usage) {
   scoped_refptr<VideoFrame> video_frame;
@@ -191,14 +55,5 @@
   return handle;
 }
 
-base::Optional<VideoFrameLayout> CreateVideoFrameLayout(
-    VideoPixelFormat pixel_format,
-    const gfx::Size& size) {
-  return VideoFrameLayout::CreateWithStrides(
-      pixel_format, size, VideoFrame::ComputeStrides(pixel_format, size),
-      std::vector<size_t>(VideoFrame::NumPlanes(pixel_format),
-                          0) /* buffer_sizes */);
-}
-
 }  // namespace test
 }  // namespace media
--- a/media/gpu/test/video_frame_helpers.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_frame_helpers.h	2019-05-17 18:53:34.260000000 +0300
@@ -6,7 +6,6 @@
 #define MEDIA_GPU_TEST_VIDEO_FRAME_HELPERS_H_
 
 #include "base/memory/scoped_refptr.h"
-#include "media/base/video_frame_layout.h"
 #include "media/base/video_types.h"
 #include "ui/gfx/buffer_types.h"
 #include "ui/gfx/geometry/size.h"
@@ -36,27 +35,10 @@
   // VideoFrameProcessor.
   virtual void ProcessVideoFrame(scoped_refptr<const VideoFrame> video_frame,
                                  size_t frame_index) = 0;
-
-  // Wait until all currently scheduled frames have been processed. Returns
-  // whether processing was successful.
-  virtual bool WaitUntilDone() = 0;
 };
 
-// Convert and copy the |src_frame| to the specified |dst_frame|. Supported
-// input formats are I420, NV12 and YV12. Supported output formats are I420 and
-// ARGB. All mappable output storages types are supported, but writing into
-// non-owned memory might produce unexpected side effects.
-bool ConvertVideoFrame(const VideoFrame* src_frame, VideoFrame* dst_frame);
-
-// Convert and copy the |src_frame| to a new video frame with specified format.
-// Supported input formats are I420, NV12 and YV12. Supported output formats are
-// I420 and ARGB.
-scoped_refptr<VideoFrame> ConvertVideoFrame(const VideoFrame* src_frame,
-                                            VideoPixelFormat dst_pixel_format);
-
-// Create a platform-specific DMA-buffer-backed video frame with specified
-// |pixel_format|, |size| and |buffer_usage|.
-scoped_refptr<VideoFrame> CreatePlatformVideoFrame(
+// Create a video frame with specified |pixel_format| and |size|.
+scoped_refptr<VideoFrame> CreateVideoFrame(
     VideoPixelFormat pixel_format,
     const gfx::Size& size,
     gfx::BufferUsage buffer_usage = gfx::BufferUsage::SCANOUT_VDA_WRITE);
@@ -65,12 +47,6 @@
 gfx::GpuMemoryBufferHandle CreateGpuMemoryBufferHandle(
     scoped_refptr<VideoFrame> video_frame);
 
-// Create a video frame layout for the specified |pixel_format| and |size|. The
-// created layout will have a separate buffer for each plane in the format.
-base::Optional<VideoFrameLayout> CreateVideoFrameLayout(
-    VideoPixelFormat pixel_format,
-    const gfx::Size& size);
-
 }  // namespace test
 }  // namespace media
 
--- a/media/gpu/test/video_frame_validator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_frame_validator.cc	2019-05-17 18:53:34.260000000 +0300
@@ -4,14 +4,15 @@
 
 #include "media/gpu/test/video_frame_validator.h"
 
-#include "base/bind.h"
+#include <libyuv.h>
+
 #include "base/files/file.h"
 #include "base/md5.h"
 #include "base/memory/ptr_util.h"
 #include "base/numerics/safe_conversions.h"
 #include "base/strings/stringprintf.h"
-#include "build/build_config.h"
 #include "media/base/video_frame.h"
+#include "media/gpu/buildflags.h"
 #include "media/gpu/test/video_decode_accelerator_unittest_helpers.h"
 #include "media/gpu/test/video_frame_mapper.h"
 #include "media/gpu/test/video_frame_mapper_factory.h"
@@ -21,20 +22,69 @@
 
 // static
 std::unique_ptr<VideoFrameValidator> VideoFrameValidator::Create(
-    const std::vector<std::string>& expected_frame_checksums,
-    const VideoPixelFormat validation_format) {
-  std::unique_ptr<VideoFrameMapper> video_frame_mapper;
-#if defined(OS_CHROMEOS)
-  video_frame_mapper = VideoFrameMapperFactory::CreateMapper();
+    const std::vector<std::string>& frame_checksums) {
+  auto video_frame_mapper = VideoFrameMapperFactory::CreateMapper();
   if (!video_frame_mapper) {
     LOG(ERROR) << "Failed to create VideoFrameMapper.";
     return nullptr;
   }
-#endif
 
   auto video_frame_validator = base::WrapUnique(new VideoFrameValidator(
-      expected_frame_checksums, std::move(video_frame_mapper),
-      validation_format));
+      VideoFrameValidator::CHECK, base::FilePath(), frame_checksums,
+      base::File(), std::move(video_frame_mapper)));
+  if (!video_frame_validator->Initialize()) {
+    LOG(ERROR) << "Failed to initialize VideoFrameValidator.";
+    return nullptr;
+  }
+
+  return video_frame_validator;
+}
+
+// static
+std::unique_ptr<VideoFrameValidator> VideoFrameValidator::Create(
+    uint32_t flags,
+    const base::FilePath& prefix_output_yuv,
+    const base::FilePath& md5_file_path,
+    bool linear) {
+  if ((flags & VideoFrameValidator::OUTPUTYUV) && prefix_output_yuv.empty()) {
+    LOG(ERROR) << "Prefix of yuv files isn't specified with dump flags.";
+    return nullptr;
+  }
+
+  if ((flags & VideoFrameValidator::GENMD5) &&
+      (flags & VideoFrameValidator::CHECK)) {
+    LOG(ERROR) << "Generating and checking MD5 values at the same time is not "
+               << "supported.";
+  }
+  auto video_frame_mapper = VideoFrameMapperFactory::CreateMapper(linear);
+
+  if (!video_frame_mapper) {
+    LOG(ERROR) << "Failed to create VideoFrameMapper.";
+    return nullptr;
+  }
+
+  std::vector<std::string> md5_of_frames;
+  base::File md5_file;
+  if (flags & VideoFrameValidator::GENMD5) {
+    // Writes out computed md5 values to md5_file_path.
+    md5_file = base::File(md5_file_path, base::File::FLAG_CREATE_ALWAYS |
+                                             base::File::FLAG_WRITE |
+                                             base::File::FLAG_APPEND);
+    if (!md5_file.IsValid()) {
+      LOG(ERROR) << "Failed to create md5 file to write " << md5_file_path;
+      return nullptr;
+    }
+  } else if (flags & VideoFrameValidator::CHECK) {
+    md5_of_frames = ReadGoldenThumbnailMD5s(md5_file_path);
+    if (md5_of_frames.empty()) {
+      LOG(ERROR) << "Failed to read md5 values in " << md5_file_path;
+      return nullptr;
+    }
+  }
+
+  auto video_frame_validator = base::WrapUnique(new VideoFrameValidator(
+      flags, prefix_output_yuv, std::move(md5_of_frames), std::move(md5_file),
+      std::move(video_frame_mapper)));
   if (!video_frame_validator->Initialize()) {
     LOG(ERROR) << "Failed to initialize VideoFrameValidator.";
     return nullptr;
@@ -44,12 +94,16 @@
 }
 
 VideoFrameValidator::VideoFrameValidator(
-    std::vector<std::string> expected_frame_checksums,
-    std::unique_ptr<VideoFrameMapper> video_frame_mapper,
-    VideoPixelFormat validation_format)
-    : expected_frame_checksums_(std::move(expected_frame_checksums)),
+    uint32_t flags,
+    const base::FilePath& prefix_output_yuv,
+    std::vector<std::string> md5_of_frames,
+    base::File md5_file,
+    std::unique_ptr<VideoFrameMapper> video_frame_mapper)
+    : flags_(flags),
+      prefix_output_yuv_(prefix_output_yuv),
+      md5_of_frames_(std::move(md5_of_frames)),
+      md5_file_(std::move(md5_file)),
       video_frame_mapper_(std::move(video_frame_mapper)),
-      validation_format_(validation_format),
       num_frames_validating_(0),
       frame_validator_thread_("FrameValidatorThread"),
       frame_validator_cv_(&frame_validator_lock_) {
@@ -75,10 +129,6 @@
   DCHECK_EQ(0u, num_frames_validating_);
 }
 
-const std::vector<std::string>& VideoFrameValidator::GetFrameChecksums() const {
-  return frame_checksums_;
-}
-
 std::vector<VideoFrameValidator::MismatchedFrameInfo>
 VideoFrameValidator::GetMismatchedFramesInfo() const {
   base::AutoLock auto_lock(frame_validator_lock_);
@@ -90,6 +140,14 @@
   return mismatched_frames_.size();
 }
 
+bool VideoFrameValidator::WaitUntilValidated() const {
+  base::AutoLock auto_lock(frame_validator_lock_);
+  while (num_frames_validating_ > 0) {
+    frame_validator_cv_.Wait();
+  }
+  return mismatched_frames_.size() == 0u;
+}
+
 void VideoFrameValidator::ProcessVideoFrame(
     scoped_refptr<const VideoFrame> video_frame,
     size_t frame_index) {
@@ -106,66 +164,141 @@
                      base::Unretained(this), video_frame, frame_index));
 }
 
-bool VideoFrameValidator::WaitUntilDone() {
-  base::AutoLock auto_lock(frame_validator_lock_);
-  while (num_frames_validating_ > 0) {
-    frame_validator_cv_.Wait();
-  }
-
-  if (mismatched_frames_.size() > 0u) {
-    LOG(ERROR) << mismatched_frames_.size() << " frames failed to validate.";
-    return false;
-  }
-  return true;
-}
-
 void VideoFrameValidator::ProcessVideoFrameTask(
     const scoped_refptr<const VideoFrame> video_frame,
     size_t frame_index) {
   DCHECK_CALLED_ON_VALID_SEQUENCE(validator_thread_sequence_checker_);
 
-  scoped_refptr<const VideoFrame> validated_frame = video_frame;
-  // If this is a DMABuf-backed memory frame we need to map it before accessing.
-#if defined(OS_LINUX)
-  if (validated_frame->storage_type() == VideoFrame::STORAGE_DMABUFS)
-    validated_frame = video_frame_mapper_->Map(std::move(validated_frame));
-#endif
-
-  if (validated_frame->format() != validation_format_) {
-    validated_frame =
-        ConvertVideoFrame(validated_frame.get(), validation_format_);
+  auto standard_frame = CreateStandardizedFrame(video_frame);
+  if (!standard_frame) {
+    return;
+  }
+  std::string computed_md5 = ComputeMD5FromVideoFrame(standard_frame);
+  if (flags_ & Flags::GENMD5) {
+    md5_file_.Write(0, computed_md5.data(), computed_md5.size());
+    md5_file_.Write(0, "\n", 1);
   }
 
-  ASSERT_TRUE(validated_frame);
-  std::string computed_md5 = ComputeMD5FromVideoFrame(validated_frame.get());
-
-  base::AutoLock auto_lock(frame_validator_lock_);
-  frame_checksums_.push_back(computed_md5);
-
-  if (expected_frame_checksums_.size() > 0) {
-    LOG_IF(FATAL, frame_index >= expected_frame_checksums_.size())
+  if (flags_ & Flags::CHECK) {
+    LOG_IF(FATAL, frame_index >= md5_of_frames_.size())
         << "Frame number is over than the number of read md5 values in file.";
-    const auto& expected_md5 = expected_frame_checksums_[frame_index];
+    const auto& expected_md5 = md5_of_frames_[frame_index];
     if (computed_md5 != expected_md5) {
+      base::AutoLock auto_lock(frame_validator_lock_);
       mismatched_frames_.push_back(
           MismatchedFrameInfo{frame_index, computed_md5, expected_md5});
     }
   }
 
+  if (flags_ & Flags::OUTPUTYUV) {
+    LOG_IF(WARNING, !WriteI420ToFile(frame_index, standard_frame.get()))
+        << "Failed to write yuv into file.";
+  }
+
+  base::AutoLock auto_lock(frame_validator_lock_);
   num_frames_validating_--;
   frame_validator_cv_.Signal();
 }
 
+scoped_refptr<VideoFrame> VideoFrameValidator::CreateStandardizedFrame(
+    scoped_refptr<const VideoFrame> video_frame) const {
+  DCHECK_CALLED_ON_VALID_SEQUENCE(validator_thread_sequence_checker_);
+  auto mapped_frame = video_frame_mapper_->Map(std::move(video_frame));
+  if (!mapped_frame) {
+    LOG(FATAL) << "Failed to map decoded picture.";
+    return nullptr;
+  }
+
+  return CreateI420Frame(mapped_frame.get());
+}
+
 std::string VideoFrameValidator::ComputeMD5FromVideoFrame(
-    const VideoFrame* video_frame) const {
+    scoped_refptr<VideoFrame> video_frame) const {
   DCHECK_CALLED_ON_VALID_SEQUENCE(validator_thread_sequence_checker_);
   base::MD5Context context;
   base::MD5Init(&context);
-  VideoFrame::HashFrameForTesting(&context, *video_frame);
+  VideoFrame::HashFrameForTesting(&context, video_frame);
   base::MD5Digest digest;
   base::MD5Final(&digest, &context);
   return MD5DigestToBase16(digest);
 }
 
+scoped_refptr<VideoFrame> VideoFrameValidator::CreateI420Frame(
+    const VideoFrame* const src_frame) const {
+  DCHECK_CALLED_ON_VALID_SEQUENCE(validator_thread_sequence_checker_);
+  const auto& visible_rect = src_frame->visible_rect();
+  const int width = visible_rect.width();
+  const int height = visible_rect.height();
+  auto dst_frame = VideoFrame::CreateFrame(
+      PIXEL_FORMAT_I420, visible_rect.size(), visible_rect, visible_rect.size(),
+      base::TimeDelta());
+  uint8_t* const dst_y = dst_frame->data(VideoFrame::kYPlane);
+  uint8_t* const dst_u = dst_frame->data(VideoFrame::kUPlane);
+  uint8_t* const dst_v = dst_frame->data(VideoFrame::kVPlane);
+  const int dst_stride_y = dst_frame->stride(VideoFrame::kYPlane);
+  const int dst_stride_u = dst_frame->stride(VideoFrame::kUPlane);
+  const int dst_stride_v = dst_frame->stride(VideoFrame::kVPlane);
+  switch (src_frame->format()) {
+    case PIXEL_FORMAT_NV12:
+      libyuv::NV12ToI420(src_frame->data(VideoFrame::kYPlane),
+                         src_frame->stride(VideoFrame::kYPlane),
+                         src_frame->data(VideoFrame::kUVPlane),
+                         src_frame->stride(VideoFrame::kUVPlane), dst_y,
+                         dst_stride_y, dst_u, dst_stride_u, dst_v, dst_stride_v,
+                         width, height);
+      break;
+    case PIXEL_FORMAT_YV12:
+      libyuv::I420Copy(src_frame->data(VideoFrame::kYPlane),
+                       src_frame->stride(VideoFrame::kYPlane),
+                       src_frame->data(VideoFrame::kVPlane),
+                       src_frame->stride(VideoFrame::kVPlane),
+                       src_frame->data(VideoFrame::kUPlane),
+                       src_frame->stride(VideoFrame::kUPlane), dst_y,
+                       dst_stride_y, dst_u, dst_stride_u, dst_v, dst_stride_v,
+                       width, height);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported format: " << src_frame->format();
+      return nullptr;
+  }
+  return dst_frame;
+}
+
+bool VideoFrameValidator::WriteI420ToFile(
+    size_t frame_index,
+    const VideoFrame* const video_frame) const {
+  DCHECK_CALLED_ON_VALID_SEQUENCE(validator_thread_sequence_checker_);
+  if (video_frame->format() != PIXEL_FORMAT_I420) {
+    LOG(ERROR) << "No I420 format frame.";
+    return false;
+  }
+  if (video_frame->storage_type() !=
+      VideoFrame::StorageType::STORAGE_OWNED_MEMORY) {
+    LOG(ERROR) << "Video frame doesn't own memory.";
+    return false;
+  }
+  const int width = video_frame->visible_rect().width();
+  const int height = video_frame->visible_rect().height();
+  base::FilePath::StringType output_yuv_fname;
+  base::SStringPrintf(&output_yuv_fname,
+                      FILE_PATH_LITERAL("%04zu_%dx%d_I420.yuv"), frame_index,
+                      width, height);
+  base::File yuv_file(prefix_output_yuv_.AddExtension(output_yuv_fname),
+                      base::File::FLAG_CREATE_ALWAYS | base::File::FLAG_APPEND);
+  const size_t num_planes = VideoFrame::NumPlanes(video_frame->format());
+  for (size_t i = 0; i < num_planes; i++) {
+    size_t plane_w = VideoFrame::Columns(i, video_frame->format(), width);
+    size_t plane_h = VideoFrame::Rows(i, video_frame->format(), height);
+    int data_size = base::checked_cast<int>(plane_w * plane_h);
+    const uint8_t* data = video_frame->data(i);
+    if (yuv_file.Write(0, reinterpret_cast<const char*>(data), data_size) !=
+        data_size) {
+      LOG(ERROR) << "Fail to write file in plane #" << i;
+      return false;
+    }
+  }
+  return true;
+}
+
 }  // namespace test
 }  // namespace media
--- a/media/gpu/test/video_frame_validator.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_frame_validator.h	2019-05-17 18:53:34.260000000 +0300
@@ -31,31 +31,53 @@
 
 // VideoFrameValidator validates the pixel content of each video frame.
 // It maps a video frame by using VideoFrameMapper, and converts the mapped
-// frame to |validation_format| to resolve pixel format differences on different
-// platforms. Thereafter, it compares md5 values of the mapped and converted
-// buffer with golden md5 values. The golden values are prepared in advance and
-// must be identical on all platforms.
+// frame to I420 format to resolve layout differences due to different pixel
+// layouts/alignments on different platforms.
+// Thereafter, it compares md5 values of the mapped and converted buffer with
+// golden md5 values. The golden values are prepared in advance and must be
+// identical on all platforms.
 // Mapping and verification of a frame is a costly operation and will influence
 // performance measurements.
 class VideoFrameValidator : public VideoFrameProcessor {
  public:
+  enum Flags : uint32_t {
+    // Checks soundness of video frames.
+    CHECK = 1 << 0,
+    // Writes out video frames to files.
+    OUTPUTYUV = 1 << 1,
+    // Writes out md5 values to a file.
+    GENMD5 = 1 << 2,
+  };
+
   struct MismatchedFrameInfo {
     size_t frame_index;
     std::string computed_md5;
     std::string expected_md5;
   };
 
-  // Create an instance of the video frame validator. The calculated checksums
-  // will be compared to the values in |expected_frame_checksums|. If no
-  // checksums are provided only checksum calculation will be done.
+  // Creates an instance of the video frame validator in 'CHECK' mode.
+  // |frame_checksums| should contain the ordered list of md5 frame checksums to
+  // be used by the validator
   static std::unique_ptr<VideoFrameValidator> Create(
-      const std::vector<std::string>& expected_frame_checksums,
-      const VideoPixelFormat validation_format = PIXEL_FORMAT_I420);
+      const std::vector<std::string>& frame_checksums);
 
-  ~VideoFrameValidator() override;
+  // |flags| decides the behavior of created video frame validator. See the
+  // detail in Flags.
+  // |prefix_output_yuv| is the prefix name of saved yuv files.
+  // VideoFrameValidator saves all I420 video frames.
+  // If |prefix_output_yuv_| is not specified, no yuv file will be saved.
+  // |md5_file_path| is the path to the file that contains golden md5 values.
+  // The file contains one md5 value per line, listed in display order.
+  // |linear| represents whether VideoFrame passed on EvaludateVideoFrame() is
+  // linear (i.e non-tiled) or not.
+  // Returns nullptr on failure.
+  static std::unique_ptr<VideoFrameValidator> Create(
+      uint32_t flags,
+      const base::FilePath& prefix_output_yuv,
+      const base::FilePath& md5_file_path,
+      bool linear);
 
-  // Get the ordered list of calculated frame checksums.
-  const std::vector<std::string>& GetFrameChecksums() const;
+  ~VideoFrameValidator() override;
 
   // Returns information of frames that don't match golden md5 values.
   // If there is no mismatched frame, returns an empty vector. This function is
@@ -66,18 +88,21 @@
   // function is thread-safe.
   size_t GetMismatchedFramesCount() const;
 
-  // Interface VideoFrameProcessor
-  void ProcessVideoFrame(scoped_refptr<const VideoFrame> video_frame,
-                         size_t frame_index) override;
   // Wait until all currently scheduled frame validations are done. Returns true
   // if no corrupt frames were found. This function might take a long time to
   // complete, depending on the platform.
-  bool WaitUntilDone() override;
+  bool WaitUntilValidated() const;
+
+  // Interface VideoFrameProcessor
+  void ProcessVideoFrame(scoped_refptr<const VideoFrame> video_frame,
+                         size_t frame_index) override;
 
  private:
-  VideoFrameValidator(std::vector<std::string> expected_frame_checksums,
-                      std::unique_ptr<VideoFrameMapper> video_frame_mapper,
-                      VideoPixelFormat validation_format);
+  VideoFrameValidator(uint32_t flags,
+                      const base::FilePath& prefix_output_yuv,
+                      std::vector<std::string> md5_of_frames,
+                      base::File md5_file,
+                      std::unique_ptr<VideoFrameMapper> video_frame_mapper);
 
   // Start the frame validation thread.
   bool Initialize();
@@ -88,23 +113,41 @@
   void ProcessVideoFrameTask(const scoped_refptr<const VideoFrame> video_frame,
                              size_t frame_index);
 
+  // This maps |video_frame|, converts it to I420 format.
+  // Returns the resulted I420 frame on success, and otherwise return nullptr.
+  // |video_frame| is unchanged in this method.
+  // TODO(dstaessens@) Move frame helper functions to video_frame_helpers.h.
+  scoped_refptr<VideoFrame> CreateStandardizedFrame(
+      scoped_refptr<const VideoFrame> video_frame) const;
+
   // Returns md5 values of video frame represented by |video_frame|.
-  std::string ComputeMD5FromVideoFrame(const VideoFrame* video_frame) const;
+  std::string ComputeMD5FromVideoFrame(
+      scoped_refptr<VideoFrame> video_frame) const;
+
+  // Creates VideoFrame with I420 format from |src_frame|.
+  scoped_refptr<VideoFrame> CreateI420Frame(
+      const VideoFrame* const src_frame) const;
+
+  // Helper function to save I420 yuv image.
+  bool WriteI420ToFile(size_t frame_index,
+                       const VideoFrame* const video_frame) const;
 
   // The results of invalid frame data.
   std::vector<MismatchedFrameInfo> mismatched_frames_
       GUARDED_BY(frame_validator_lock_);
 
-  // The list of calculated MD5 frame checksums.
-  std::vector<std::string> frame_checksums_ GUARDED_BY(frame_validator_lock_);
+  const uint32_t flags_;
 
-  // The list of expected MD5 frame checksums.
-  const std::vector<std::string> expected_frame_checksums_;
+  // Prefix of saved yuv files.
+  const base::FilePath prefix_output_yuv_;
 
-  const std::unique_ptr<VideoFrameMapper> video_frame_mapper_;
+  // Golden MD5 values.
+  std::vector<std::string> md5_of_frames_;
 
-  // VideoPixelFormat the VideoFrame will be converted to for validation.
-  const VideoPixelFormat validation_format_;
+  // File to write md5 values if flags includes GENMD5.
+  base::File md5_file_;
+
+  const std::unique_ptr<VideoFrameMapper> video_frame_mapper_;
 
   // The number of frames currently queued for validation.
   size_t num_frames_validating_ GUARDED_BY(frame_validator_lock_);
--- a/media/gpu/test/video_player/frame_renderer_dummy.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_player/frame_renderer_dummy.cc	2019-05-17 18:53:34.264000000 +0300
@@ -8,6 +8,10 @@
 #include <vector>
 
 #include "base/memory/ptr_util.h"
+#if defined(USE_OZONE)
+#include "ui/ozone/public/ozone_gpu_test_helper.h"
+#include "ui/ozone/public/ozone_platform.h"
+#endif
 
 #define VLOGF(level) VLOG(level) << __func__ << "(): "
 
@@ -18,7 +22,9 @@
   DETACH_FROM_SEQUENCE(sequence_checker_);
 }
 
-FrameRendererDummy::~FrameRendererDummy() {}
+FrameRendererDummy::~FrameRendererDummy() {
+  Destroy();
+}
 
 // static
 std::unique_ptr<FrameRendererDummy> FrameRendererDummy::Create() {
@@ -30,9 +36,37 @@
 }
 
 bool FrameRendererDummy::Initialize() {
+#if defined(USE_OZONE)
+  // Initialize Ozone. This is necessary even though we are not doing any actual
+  // rendering. If not initialized a crash will occur when assigning picture
+  // buffers, even when passing 0 as texture ID.
+  // TODO(@dstaessens):
+  // * Get rid of the Ozone dependency, as it forces us to call 'stop ui' when
+  //   running tests.
+  LOG(INFO) << "Initializing Ozone Platform...\n"
+               "If this hangs indefinitely please call 'stop ui' first!";
+  ui::OzonePlatform::InitParams params = {.single_process = false};
+  ui::OzonePlatform::InitializeForUI(params);
+  ui::OzonePlatform::InitializeForGPU(params);
+  ui::OzonePlatform::GetInstance()->AfterSandboxEntry();
+
+  // Initialize the Ozone GPU helper. If this is not done an error will occur:
+  // "Check failed: drm. No devices available for buffer allocation."
+  // Note: If a task environment is not set up initialization will hang
+  // indefinitely here.
+  gpu_helper_.reset(new ui::OzoneGpuTestHelper());
+  gpu_helper_->Initialize(base::ThreadTaskRunnerHandle::Get());
+#endif
+
   return true;
 }
 
+void FrameRendererDummy::Destroy() {
+#if defined(USE_OZONE)
+  gpu_helper_.reset();
+#endif
+}
+
 void FrameRendererDummy::AcquireGLContext() {
   // As no actual rendering is done we don't have a GLContext to acquire.
 }
@@ -42,18 +76,14 @@
 }
 
 gl::GLContext* FrameRendererDummy::GetGLContext() {
+  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
+
   // As no actual rendering is done we don't have a GLContext.
   return nullptr;
 }
 
-void FrameRendererDummy::RenderFrame(scoped_refptr<VideoFrame> video_frame) {}
-
-scoped_refptr<VideoFrame> FrameRendererDummy::CreateVideoFrame(
-    VideoPixelFormat pixel_format,
-    const gfx::Size& size,
-    uint32_t texture_target,
-    uint32_t* texture_id) {
-  return nullptr;
+void FrameRendererDummy::RenderFrame(scoped_refptr<VideoFrame> video_frame) {
+  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
 }
 
 }  // namespace test
--- a/media/gpu/test/video_player/frame_renderer_dummy.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_player/frame_renderer_dummy.h	2019-05-17 18:53:34.264000000 +0300
@@ -11,6 +11,14 @@
 #include "base/sequence_checker.h"
 #include "media/gpu/test/video_player/frame_renderer.h"
 
+#ifdef USE_OZONE
+namespace ui {
+
+class OzoneGpuTestHelper;
+
+}  // namespace ui
+#endif
+
 namespace media {
 namespace test {
 
@@ -29,16 +37,18 @@
   void ReleaseGLContext() override;
   gl::GLContext* GetGLContext() override;
   void RenderFrame(scoped_refptr<VideoFrame> video_frame) override;
-  scoped_refptr<VideoFrame> CreateVideoFrame(VideoPixelFormat pixel_format,
-                                             const gfx::Size& size,
-                                             uint32_t texture_target,
-                                             uint32_t* texture_id) override;
 
  private:
   FrameRendererDummy();
 
   // Initialize the frame renderer, performs all rendering-related setup.
   bool Initialize();
+  // Destroy the frame renderer.
+  void Destroy();
+
+#ifdef USE_OZONE
+  std::unique_ptr<ui::OzoneGpuTestHelper> gpu_helper_;
+#endif
 
   SEQUENCE_CHECKER(sequence_checker_);
   DISALLOW_COPY_AND_ASSIGN(FrameRendererDummy);
--- a/media/gpu/test/video_player/frame_renderer.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_player/frame_renderer.h	2019-05-17 18:53:34.264000000 +0300
@@ -9,7 +9,6 @@
 
 #include "base/callback_forward.h"
 #include "base/memory/scoped_refptr.h"
-#include "media/base/video_frame.h"
 #include "media/base/video_types.h"
 #include "media/video/picture.h"
 #include "ui/gfx/geometry/size.h"
@@ -43,15 +42,6 @@
   // Render the specified video frame. Once rendering is done the reference to
   // the |video_frame| should be dropped so the video frame can be reused.
   virtual void RenderFrame(scoped_refptr<VideoFrame> video_frame) = 0;
-
-  // Create a texture-backed video frame with specified |pixel_format|, |size|
-  // and |texture_target|. The texture's id will be put in |texture_id|.
-  // TODO(dstaessens@) Remove when allocate mode is removed.
-  virtual scoped_refptr<VideoFrame> CreateVideoFrame(
-      VideoPixelFormat pixel_format,
-      const gfx::Size& size,
-      uint32_t texture_target,
-      uint32_t* texture_id) = 0;
 };
 
 }  // namespace test
--- a/media/gpu/test/video_player/video.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_player/video.cc	2019-05-17 18:53:34.264000000 +0300
@@ -11,6 +11,7 @@
 #include "base/json/json_reader.h"
 #include "base/numerics/safe_conversions.h"
 #include "base/values.h"
+#include "media/gpu/test/video_decode_accelerator_unittest_helpers.h"
 
 #define VLOGF(level) VLOG(level) << __func__ << "(): "
 
@@ -20,6 +21,9 @@
 // Suffix to append to the video file path to get the metadata file path.
 constexpr const base::FilePath::CharType* kMetadataSuffix =
     FILE_PATH_LITERAL(".json");
+// Suffix to append to the video file path to get the checksums file path.
+constexpr const base::FilePath::CharType* kFrameChecksumSuffix =
+    FILE_PATH_LITERAL(".frames.md5");
 
 base::FilePath Video::test_data_path_ = base::FilePath();
 
@@ -62,6 +66,11 @@
     return false;
   }
 
+  if (!LoadFrameChecksums()) {
+    VLOGF(1) << "Failed to load frame checksums";
+    return false;
+  }
+
   return true;
 }
 
@@ -69,18 +78,10 @@
   return data_.size() > 0;
 }
 
-const base::FilePath& Video::FilePath() const {
-  return file_path_;
-}
-
 const std::vector<uint8_t>& Video::Data() const {
   return data_;
 }
 
-VideoCodec Video::Codec() const {
-  return codec_;
-}
-
 VideoCodecProfile Video::Profile() const {
   return profile_;
 }
@@ -89,22 +90,10 @@
   return num_frames_;
 }
 
-uint32_t Video::NumFragments() const {
-  return num_fragments_;
-}
-
-gfx::Size Video::Resolution() const {
-  return resolution_;
-}
-
 const std::vector<std::string>& Video::FrameChecksums() const {
   return frame_checksums_;
 }
 
-const std::vector<std::string>& Video::ThumbnailChecksums() const {
-  return thumbnail_checksums_;
-}
-
 // static
 void Video::SetTestDataPath(const base::FilePath& test_data_path) {
   test_data_path_ = test_data_path;
@@ -131,8 +120,7 @@
   }
 
   base::JSONReader reader;
-  std::unique_ptr<base::Value> metadata(
-      reader.ReadToValueDeprecated(json_data));
+  std::unique_ptr<base::Value> metadata(reader.ReadToValue(json_data));
   if (!metadata) {
     VLOGF(1) << "Failed to parse video metadata: " << json_path << ": "
              << reader.GetErrorMessage();
@@ -146,8 +134,7 @@
     return false;
   }
   profile_ = ConvertStringtoProfile(profile->GetString());
-  codec_ = ConvertProfileToCodec(profile_);
-  if (profile_ == VIDEO_CODEC_PROFILE_UNKNOWN || codec_ == kUnknownVideoCodec) {
+  if (profile_ == VIDEO_CODEC_PROFILE_UNKNOWN) {
     VLOGF(1) << profile->GetString() << " is not supported";
     return false;
   }
@@ -160,56 +147,28 @@
   }
   num_frames_ = static_cast<uint32_t>(num_frames->GetInt());
 
-  const base::Value* num_fragments =
-      metadata->FindKeyOfType("num_fragments", base::Value::Type::INTEGER);
-  if (!num_fragments) {
-    VLOGF(1) << "Key \"num_fragments\" is not found in " << json_path;
-    return false;
-  }
-  num_fragments_ = static_cast<uint32_t>(num_fragments->GetInt());
-
-  const base::Value* width =
-      metadata->FindKeyOfType("width", base::Value::Type::INTEGER);
-  if (!width) {
-    VLOGF(1) << "Key \"width\" is not found in " << json_path;
-    return false;
-  }
-  const base::Value* height =
-      metadata->FindKeyOfType("height", base::Value::Type::INTEGER);
-  if (!height) {
-    VLOGF(1) << "Key \"height\" is not found in " << json_path;
-    return false;
-  }
-  resolution_ = gfx::Size(static_cast<uint32_t>(width->GetInt()),
-                          static_cast<uint32_t>(height->GetInt()));
+  return true;
+}
 
-  const base::Value* md5_checksums =
-      metadata->FindKeyOfType("md5_checksums", base::Value::Type::LIST);
-  if (!md5_checksums) {
-    VLOGF(1) << "Key \"md5_checksums\" is not found in " << json_path;
-    return false;
-  }
-  for (const base::Value& checksum : md5_checksums->GetList()) {
-    frame_checksums_.push_back(checksum.GetString());
-  }
+bool Video::IsMetadataLoaded() const {
+  return profile_ != VIDEO_CODEC_PROFILE_UNKNOWN || num_frames_ != 0;
+}
 
-  const base::Value* thumbnail_checksums =
-      metadata->FindKeyOfType("thumbnail_checksums", base::Value::Type::LIST);
-  if (!thumbnail_checksums) {
-    VLOGF(1) << "Key \"thumbnail_checksums\" is not found in " << json_path;
+bool Video::LoadFrameChecksums() {
+  if (FrameChecksumsLoaded()) {
+    VLOGF(1) << "Frame checksums are already loaded";
     return false;
   }
-  for (const base::Value& checksum : thumbnail_checksums->GetList()) {
-    const std::string& checksum_str = checksum.GetString();
-    if (checksum_str.size() > 0 && checksum_str[0] != '#')
-      thumbnail_checksums_.push_back(checksum_str);
-  }
 
-  return true;
+  frame_checksums_ =
+      ReadGoldenThumbnailMD5s(file_path_.AddExtension(kFrameChecksumSuffix));
+  LOG_ASSERT(frame_checksums_.size() == num_frames_)
+      << "Video frame checksum count does not match number of video frames";
+  return frame_checksums_.size() == num_frames_;
 }
 
-bool Video::IsMetadataLoaded() const {
-  return profile_ != VIDEO_CODEC_PROFILE_UNKNOWN || num_frames_ != 0;
+bool Video::FrameChecksumsLoaded() const {
+  return frame_checksums_.size() == num_frames_;
 }
 
 // static
@@ -223,24 +182,10 @@
   } else if (profile == "VP9PROFILE_PROFILE2") {
     return VP9PROFILE_PROFILE2;
   } else {
-    VLOG(2) << profile << " is not supported";
+    VLOG(2) << profile << " is not supported.";
     return VIDEO_CODEC_PROFILE_UNKNOWN;
   }
 }
 
-// static
-VideoCodec Video::ConvertProfileToCodec(VideoCodecProfile profile) {
-  if (profile >= H264PROFILE_MIN && profile <= H264PROFILE_MAX) {
-    return kCodecH264;
-  } else if (profile >= VP8PROFILE_MIN && profile <= VP8PROFILE_MAX) {
-    return kCodecVP8;
-  } else if (profile >= VP9PROFILE_MIN && profile <= VP9PROFILE_MAX) {
-    return kCodecVP9;
-  } else {
-    VLOG(2) << GetProfileName(profile) << " is not supported";
-    return kUnknownVideoCodec;
-  }
-}
-
 }  // namespace test
 }  // namespace media
--- a/media/gpu/test/video_player/video_decoder_client.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_player/video_decoder_client.cc	2019-05-17 18:53:34.264000000 +0300
@@ -7,7 +7,6 @@
 #include <string>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/memory/ptr_util.h"
 #include "base/threading/thread_task_runner_handle.h"
 #include "media/base/bind_to_current_loop.h"
@@ -25,12 +24,12 @@
 
 VideoDecoderClient::VideoDecoderClient(
     const VideoPlayer::EventCallback& event_cb,
-    std::unique_ptr<FrameRenderer> renderer,
-    std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors,
+    FrameRenderer* renderer,
+    const std::vector<VideoFrameProcessor*>& frame_processors,
     const VideoDecoderClientConfig& config)
     : event_cb_(event_cb),
-      frame_renderer_(std::move(renderer)),
-      frame_processors_(std::move(frame_processors)),
+      frame_renderer_(renderer),
+      frame_processors_(frame_processors),
       decoder_client_thread_("VDAClientDecoderThread"),
       decoder_client_state_(VideoDecoderClientState::kUninitialized),
       decoder_client_config_(config),
@@ -41,30 +40,17 @@
 
 VideoDecoderClient::~VideoDecoderClient() {
   DCHECK_CALLED_ON_VALID_SEQUENCE(video_player_sequence_checker_);
-
-  DestroyDecoder();
-  decoder_client_thread_.Stop();
-
-  // Clear video frames, triggering associated destruction callbacks while we
-  // still have a GLcontext.
-  frame_renderer_->AcquireGLContext();
-  video_frames_.clear();
-  frame_renderer_->ReleaseGLContext();
-
-  // Wait until the frame processors are done, before destroying them. As the
-  // decoder has been destroyed no new frames will be sent to the processors.
-  WaitForFrameProcessors();
+  Destroy();
 }
 
 // static
 std::unique_ptr<VideoDecoderClient> VideoDecoderClient::Create(
     const VideoPlayer::EventCallback& event_cb,
-    std::unique_ptr<FrameRenderer> frame_renderer,
-    std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors,
+    FrameRenderer* frame_renderer,
+    const std::vector<VideoFrameProcessor*>& frame_processors,
     const VideoDecoderClientConfig& config) {
-  auto decoder_client = base::WrapUnique(
-      new VideoDecoderClient(event_cb, std::move(frame_renderer),
-                             std::move(frame_processors), config));
+  auto decoder_client = base::WrapUnique(new VideoDecoderClient(
+      event_cb, frame_renderer, frame_processors, config));
   if (!decoder_client->Initialize()) {
     return nullptr;
   }
@@ -91,6 +77,13 @@
   return true;
 }
 
+void VideoDecoderClient::Destroy() {
+  DCHECK_CALLED_ON_VALID_SEQUENCE(video_player_sequence_checker_);
+
+  DestroyDecoder();
+  decoder_client_thread_.Stop();
+}
+
 void VideoDecoderClient::CreateDecoder(
     const VideoDecodeAccelerator::Config& config,
     const std::vector<uint8_t>& stream) {
@@ -113,17 +106,6 @@
   done.Wait();
 }
 
-bool VideoDecoderClient::WaitForFrameProcessors() {
-  bool success = true;
-  for (auto& frame_processor : frame_processors_)
-    success &= frame_processor->WaitUntilDone();
-  return success;
-}
-
-FrameRenderer* VideoDecoderClient::GetFrameRenderer() const {
-  return frame_renderer_.get();
-}
-
 void VideoDecoderClient::Play() {
   DCHECK_CALLED_ON_VALID_SEQUENCE(video_player_sequence_checker_);
 
@@ -158,48 +140,23 @@
             << " picture buffers with size " << size.height() << "x"
             << size.height();
 
-  // If using import mode, create a set of DMABuf-backed video frames.
-  if (decoder_client_config_.allocation_mode == AllocationMode::kImport) {
+  // Create a set of picture buffers and give them to the decoder.
     std::vector<PictureBuffer> picture_buffers;
-    for (uint32_t i = 0; i < requested_num_of_buffers; ++i) {
+  for (uint32_t i = 0; i < requested_num_of_buffers; ++i)
       picture_buffers.emplace_back(GetNextPictureBufferId(), size);
-    }
     decoder_->AssignPictureBuffers(picture_buffers);
 
     // Create a video frame for each of the picture buffers and provide memory
     // handles to the video frame's data to the decoder.
     for (const PictureBuffer& picture_buffer : picture_buffers) {
       scoped_refptr<VideoFrame> video_frame =
-          CreatePlatformVideoFrame(pixel_format, size);
+        CreateVideoFrame(pixel_format, size);
       LOG_ASSERT(video_frame) << "Failed to create video frame";
       video_frames_.emplace(picture_buffer.id(), video_frame);
       gfx::GpuMemoryBufferHandle handle =
           CreateGpuMemoryBufferHandle(video_frame);
       LOG_ASSERT(!handle.is_null()) << "Failed to create GPU memory handle";
-      decoder_->ImportBufferForPicture(picture_buffer.id(), pixel_format,
-                                       handle);
-    }
-  }
-
-  // If using allocate mode, request a set of texture-backed video frames from
-  // the renderer.
-  if (decoder_client_config_.allocation_mode == AllocationMode::kAllocate) {
-    std::vector<PictureBuffer> picture_buffers;
-    for (uint32_t i = 0; i < requested_num_of_buffers; ++i) {
-      uint32_t texture_id;
-      auto video_frame = frame_renderer_->CreateVideoFrame(
-          pixel_format, size, texture_target, &texture_id);
-      LOG_ASSERT(video_frame) << "Failed to create video frame";
-      int32_t picture_buffer_id = GetNextPictureBufferId();
-      PictureBuffer::TextureIds texture_ids(1, texture_id);
-      picture_buffers.emplace_back(picture_buffer_id, size, texture_ids,
-                                   texture_ids, texture_target, pixel_format);
-      video_frames_.emplace(picture_buffer_id, std::move(video_frame));
-    }
-    // The decoder requires an active GL context to allocate memory.
-    frame_renderer_->AcquireGLContext();
-    decoder_->AssignPictureBuffers(picture_buffers);
-    frame_renderer_->ReleaseGLContext();
+    decoder_->ImportBufferForPicture(picture_buffer.id(), pixel_format, handle);
   }
 }
 
@@ -218,11 +175,10 @@
   LOG_ASSERT(it != video_frames_.end());
   scoped_refptr<VideoFrame> video_frame = it->second;
 
-  // When using import mode, we wrap the video frame in another video frame that
-  // calls ReusePictureBufferTask() upon destruction. When the renderer and
-  // video frame processors are done using the video frame, the associated
-  // picture buffer will automatically be flagged for reuse.
-  if (decoder_client_config_.allocation_mode == AllocationMode::kImport) {
+  // Wrap the video frame in another video frame that calls
+  // ReusePictureBufferTask() upon destruction. When the renderer is done using
+  // the video frame, the associated picture buffer will automatically be
+  // flagged for reuse.
     base::OnceClosure delete_cb = BindToCurrentLoop(
         base::BindOnce(&VideoDecoderClient::ReusePictureBufferTask,
                        base::Unretained(this), picture.picture_buffer_id()));
@@ -234,22 +190,9 @@
 
     frame_renderer_->RenderFrame(wrapped_video_frame);
 
-    for (auto& frame_processor : frame_processors_)
+  for (VideoFrameProcessor* frame_processor : frame_processors_)
       frame_processor->ProcessVideoFrame(wrapped_video_frame,
                                          current_frame_index_);
-  }
-
-  // When using allocate mode, direct texture memory access is not supported.
-  // Since this is required by the video frame processors we can't use these
-  // here. Wrapping a video frame inside another video frame is also not
-  // supported, so we have to render the frame and return the picture buffer
-  // synchronously here. See http://crbug/362521.
-  if (decoder_client_config_.allocation_mode == AllocationMode::kAllocate) {
-    frame_renderer_->RenderFrame(video_frame);
-    ReusePictureBufferTask(picture.picture_buffer_id());
-    return;
-  }
-
   current_frame_index_++;
 }
 
@@ -322,7 +265,7 @@
   if (hasGLContext) {
     decoder_factory_ = GpuVideoDecodeAcceleratorFactory::Create(
         base::BindRepeating(&FrameRenderer::GetGLContext,
-                            base::Unretained(frame_renderer_.get())),
+                            base::Unretained(frame_renderer_)),
         base::BindRepeating([]() { return true; }),
         base::BindRepeating([](uint32_t, uint32_t,
                                const scoped_refptr<gl::GLImage>&,
--- a/media/gpu/test/video_player/video_decoder_client.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_player/video_decoder_client.h	2019-05-17 18:53:34.264000000 +0300
@@ -27,20 +27,11 @@
 class FrameRenderer;
 class VideoFrameProcessor;
 
-// TODO(dstaessens@) Remove allocation mode, temporary added here so we can
-// support the thumbnail test for older platforms that don't support import.
-enum class AllocationMode {
-  kImport,    // Client allocates video frame memory.
-  kAllocate,  // Video decoder allocates video frame memory.
-};
-
 // Video decoder client configuration.
 struct VideoDecoderClientConfig {
   // The maximum number of bitstream buffer decodes that can be requested
   // without waiting for the result of the previous decode requests.
   size_t max_outstanding_decode_requests = 1;
-  // How the pictures buffers should be allocated.
-  AllocationMode allocation_mode = AllocationMode::kImport;
 };
 
 // The video decoder client is responsible for the communication between the
@@ -64,8 +55,8 @@
   // thread-safe.
   static std::unique_ptr<VideoDecoderClient> Create(
       const VideoPlayer::EventCallback& event_cb,
-      std::unique_ptr<FrameRenderer> frame_renderer,
-      std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors,
+      FrameRenderer* frame_renderer,
+      const std::vector<VideoFrameProcessor*>& frame_processors,
       const VideoDecoderClientConfig& config);
 
   // Create a decoder with specified |config| and video |stream|. The video
@@ -76,12 +67,6 @@
   // Destroy the currently active decoder.
   void DestroyDecoder();
 
-  // Wait until all frame processors have finished processing. Returns whether
-  // processing was successful.
-  bool WaitForFrameProcessors();
-  // Get the frame renderer associated with the video decoder client.
-  FrameRenderer* GetFrameRenderer() const;
-
   // Start decoding the video stream, decoder should be idle when this function
   // is called. This function is non-blocking, for each frame decoded a
   // 'kFrameDecoded' event will be thrown.
@@ -102,13 +87,13 @@
     kResetting,
   };
 
-  VideoDecoderClient(
-      const VideoPlayer::EventCallback& event_cb,
-      std::unique_ptr<FrameRenderer> renderer,
-      std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors,
+  VideoDecoderClient(const VideoPlayer::EventCallback& event_cb,
+                     FrameRenderer* renderer,
+                     const std::vector<VideoFrameProcessor*>& frame_processors,
       const VideoDecoderClientConfig& config);
 
   bool Initialize();
+  void Destroy();
 
   // VideoDecodeAccelerator::Client implementation
   void ProvidePictureBuffers(uint32_t requested_num_of_buffers,
@@ -150,8 +135,8 @@
   int32_t GetNextPictureBufferId();
 
   VideoPlayer::EventCallback event_cb_;
-  std::unique_ptr<FrameRenderer> const frame_renderer_;
-  std::vector<std::unique_ptr<VideoFrameProcessor>> const frame_processors_;
+  FrameRenderer* const frame_renderer_;
+  std::vector<VideoFrameProcessor*> const frame_processors_;
 
   std::unique_ptr<GpuVideoDecodeAcceleratorFactory> decoder_factory_;
   std::unique_ptr<VideoDecodeAccelerator> decoder_;
--- a/media/gpu/test/video_player/video.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_player/video.h	2019-05-17 18:53:34.264000000 +0300
@@ -11,7 +11,6 @@
 #include "base/files/file_path.h"
 #include "base/macros.h"
 #include "media/base/video_codecs.h"
-#include "ui/gfx/geometry/size.h"
 
 namespace media {
 namespace test {
@@ -30,43 +29,33 @@
   // Returns true if the video file was loaded.
   bool IsLoaded() const;
 
-  // Get the video file path.
-  const base::FilePath& FilePath() const;
   // Get the video data, will be empty if the video hasn't been loaded yet.
   const std::vector<uint8_t>& Data() const;
 
   // Get the video's codec.
-  VideoCodec Codec() const;
-  // Get the video's codec profile.
   VideoCodecProfile Profile() const;
   // Get the number of frames in the video.
   uint32_t NumFrames() const;
-  // Get the number of fragments in the video.
-  uint32_t NumFragments() const;
-  // Get the video resolution.
-  gfx::Size Resolution() const;
-
   // Get the list of frame checksums.
   const std::vector<std::string>& FrameChecksums() const;
-  // Get the list of thumbnail checksums, used by the "RenderThumbnails" test.
-  // TODO(crbug.com/933632) Remove once the frame validator is supported on all
-  // active platforms.
-  const std::vector<std::string>& ThumbnailChecksums() const;
 
   // Set the default path to the test video data.
   static void SetTestDataPath(const base::FilePath& test_data_path);
 
  private:
-  // Return the profile associated with the |profile| string.
-  static VideoCodecProfile ConvertStringtoProfile(const std::string& profile);
-  // Return the codec associated with the |profile|.
-  static VideoCodec ConvertProfileToCodec(VideoCodecProfile profile);
+  // Return a profile that |codec| represents.
+  static VideoCodecProfile ConvertStringtoProfile(const std::string& codec);
 
   // Load metadata from the JSON file associated with the video file.
   bool LoadMetadata();
   // Return true if video metadata is already loaded.
   bool IsMetadataLoaded() const;
 
+  // Load video frame checksums from the associated checksums file.
+  bool LoadFrameChecksums();
+  // Return true if the video frame checksums are loaded.
+  bool FrameChecksumsLoaded() const;
+
   // The path where all test video files are stored.
   // TODO(dstaessens@) Avoid using a static data path here.
   static base::FilePath test_data_path_;
@@ -78,14 +67,9 @@
 
   // Ordered list of video frame checksums.
   std::vector<std::string> frame_checksums_;
-  // List of thumbnail checksums.
-  std::vector<std::string> thumbnail_checksums_;
 
   VideoCodecProfile profile_ = VIDEO_CODEC_PROFILE_UNKNOWN;
-  VideoCodec codec_ = kUnknownVideoCodec;
   uint32_t num_frames_ = 0;
-  uint32_t num_fragments_ = 0;
-  gfx::Size resolution_;
 
   DISALLOW_COPY_AND_ASSIGN(Video);
 };
--- a/media/gpu/test/video_player/video_player.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_player/video_player.cc	2019-05-17 18:53:34.264000000 +0300
@@ -31,19 +31,18 @@
 // static
 std::unique_ptr<VideoPlayer> VideoPlayer::Create(
     const Video* video,
-    std::unique_ptr<FrameRenderer> frame_renderer,
-    std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors,
+    FrameRenderer* frame_renderer,
+    const std::vector<VideoFrameProcessor*>& frame_processors,
     const VideoDecoderClientConfig& config) {
   auto video_player = base::WrapUnique(new VideoPlayer());
-  video_player->Initialize(video, std::move(frame_renderer),
-                           std::move(frame_processors), config);
+  video_player->Initialize(video, frame_renderer, frame_processors, config);
   return video_player;
 }
 
 void VideoPlayer::Initialize(
     const Video* video,
-    std::unique_ptr<FrameRenderer> frame_renderer,
-    std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors,
+    FrameRenderer* frame_renderer,
+    const std::vector<VideoFrameProcessor*>& frame_processors,
     const VideoDecoderClientConfig& config) {
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   DCHECK_EQ(video_player_state_, VideoPlayerState::kUninitialized);
@@ -53,19 +52,16 @@
   EventCallback event_cb =
       base::BindRepeating(&VideoPlayer::NotifyEvent, base::Unretained(this));
 
-  decoder_client_ = VideoDecoderClient::Create(
-      event_cb, std::move(frame_renderer), std::move(frame_processors), config);
+  decoder_client_ = VideoDecoderClient::Create(event_cb, frame_renderer,
+                                               frame_processors, config);
   CHECK(decoder_client_) << "Failed to create decoder client";
 
-  // Create a decoder for the specified video.
-  // TODO(dstaessens@) Remove support for allocate mode, and always use import
-  // mode. Support for allocate mode is temporary maintained for older platforms
-  // that don't support import mode.
+  // Create a decoder for the specified video. We'll always use import mode as
+  // this is the only mode supported by the media::VideoDecoder interface, which
+  // the video decoders are being migrated to.
   VideoDecodeAccelerator::Config decoder_config(video->Profile());
   decoder_config.output_mode =
-      config.allocation_mode == AllocationMode::kImport
-          ? VideoDecodeAccelerator::Config::OutputMode::IMPORT
-          : VideoDecodeAccelerator::Config::OutputMode::ALLOCATE;
+      VideoDecodeAccelerator::Config::OutputMode::IMPORT;
   decoder_client_->CreateDecoder(decoder_config, video->Data());
 
   video_ = video;
@@ -132,10 +128,6 @@
   return video_player_state_;
 }
 
-FrameRenderer* VideoPlayer::GetFrameRenderer() const {
-  return decoder_client_->GetFrameRenderer();
-}
-
 bool VideoPlayer::WaitForEvent(VideoPlayerEvent event,
                                size_t times,
                                base::TimeDelta max_wait) {
@@ -146,6 +138,10 @@
   base::TimeDelta time_waiting;
   base::AutoLock auto_lock(event_lock_);
   while (true) {
+    const base::TimeTicks start_time = base::TimeTicks::Now();
+    event_cv_.TimedWait(max_wait);
+    time_waiting += base::TimeTicks::Now() - start_time;
+
     // TODO(dstaessens@) Investigate whether we really need to keep the full
     // list of events for more complex testcases.
     // Go through list of events since last wait, looking for the event we're
@@ -162,10 +158,6 @@
     // Check whether we've exceeded the maximum time we're allowed to wait.
     if (time_waiting >= max_wait)
       return false;
-
-    const base::TimeTicks start_time = base::TimeTicks::Now();
-    event_cv_.TimedWait(max_wait);
-    time_waiting += base::TimeTicks::Now() - start_time;
   }
 }
 
@@ -188,10 +180,6 @@
   return video_player_event_counts_[static_cast<size_t>(event)];
 }
 
-bool VideoPlayer::WaitForFrameProcessors() {
-  return !decoder_client_ || decoder_client_->WaitForFrameProcessors();
-}
-
 size_t VideoPlayer::GetFlushDoneCount() const {
   return GetEventCount(VideoPlayerEvent::kFlushDone);
 }
--- a/media/gpu/test/video_player/video_player.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/test/video_player/video_player.h	2019-05-17 18:53:34.264000000 +0300
@@ -15,8 +15,6 @@
 #include "base/synchronization/condition_variable.h"
 #include "base/synchronization/lock.h"
 #include "base/thread_annotations.h"
-#include "media/gpu/test/video_frame_helpers.h"
-#include "media/gpu/test/video_player/frame_renderer.h"
 
 namespace media {
 namespace test {
@@ -25,9 +23,10 @@
 class Video;
 class VideoDecoderClient;
 struct VideoDecoderClientConfig;
+class VideoFrameProcessor;
 
 // Default timeout used when waiting for events.
-constexpr base::TimeDelta kDefaultTimeout = base::TimeDelta::FromSeconds(30);
+constexpr base::TimeDelta kDefaultTimeout = base::TimeDelta::FromSeconds(10);
 
 enum class VideoPlayerState : size_t {
   kUninitialized = 0,
@@ -60,14 +59,10 @@
   // guarantee they outlive the video player.
   static std::unique_ptr<VideoPlayer> Create(
       const Video* video,
-      std::unique_ptr<FrameRenderer> frame_renderer,
-      std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors,
+      FrameRenderer* frame_renderer,
+      const std::vector<VideoFrameProcessor*>& frame_processors,
       const VideoDecoderClientConfig& config);
 
-  // Wait until all frame processors have finished processing. Returns whether
-  // processing was successful.
-  bool WaitForFrameProcessors();
-
   // Play the video asynchronously.
   void Play();
   // Play the video asynchronously. Automatically pause decoding when the
@@ -84,8 +79,6 @@
   size_t GetCurrentFrame() const;
   // Get the current state of the video player.
   VideoPlayerState GetState() const;
-  // Get the frame renderer associated with the video player.
-  FrameRenderer* GetFrameRenderer() const;
 
   // Wait for an event to occur the specified number of times. All events that
   // occurred since last calling this function will be taken into account. All
@@ -113,10 +106,9 @@
  private:
   VideoPlayer();
 
-  void Initialize(
-      const Video* video,
-      std::unique_ptr<FrameRenderer> frame_renderer,
-      std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors,
+  void Initialize(const Video* video,
+                  FrameRenderer* frame_renderer,
+                  const std::vector<VideoFrameProcessor*>& frame_processors,
       const VideoDecoderClientConfig& config);
   void Destroy();
 
--- a/media/gpu/v4l2/generic_v4l2_device.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/generic_v4l2_device.cc	2019-05-17 18:53:34.264000000 +0300
@@ -100,10 +100,20 @@
                               int flags,
                               unsigned int offset) {
   DCHECK(device_fd_.is_valid());
+#if BUILDFLAG(USE_LIBV4L2)
+  if (use_libv4l2_)
+    return v4l2_mmap(addr, len, prot, flags, device_fd_.get(), offset);
+#endif
   return mmap(addr, len, prot, flags, device_fd_.get(), offset);
 }
 
 void GenericV4L2Device::Munmap(void* addr, unsigned int len) {
+#if BUILDFLAG(USE_LIBV4L2)
+  if (use_libv4l2_) {
+    v4l2_munmap(addr, len);
+    return;
+  }
+#endif
   munmap(addr, len);
 }
 
@@ -299,14 +309,8 @@
   gfx::NativePixmapHandle native_pixmap_handle;
 
   std::vector<base::ScopedFD> duped_fds;
-  // The number of file descriptors can be less than the number of planes when
-  // v4l2 pix fmt, |fourcc|, is a single plane format. Duplicating the last
-  // file descriptor should be safely used for the later planes, because they
-  // are on the last buffer.
-  for (size_t i = 0; i < num_planes; ++i) {
-    int fd =
-        i < dmabuf_fds.size() ? dmabuf_fds[i].get() : dmabuf_fds.back().get();
-    duped_fds.emplace_back(HANDLE_EINTR(dup(fd)));
+  for (const auto& fd : dmabuf_fds) {
+    duped_fds.emplace_back(HANDLE_EINTR(dup(fd.get())));
     if (!duped_fds.back().is_valid()) {
       VPLOGF(1) << "Failed duplicating a dmabuf fd";
       return nullptr;
@@ -362,7 +366,7 @@
 
   auto image =
       base::MakeRefCounted<gl::GLImageNativePixmap>(size, buffer_format);
-  bool ret = image->Initialize(std::move(pixmap));
+  bool ret = image->Initialize(pixmap.get());
   DCHECK(ret);
   return image;
 }
@@ -479,7 +483,11 @@
     return false;
 
 #if BUILDFLAG(USE_LIBV4L2)
+#if BUILDFLAG(USE_LINUX_V4L2)
+  if (
+#else
   if (type == Type::kEncoder &&
+#endif
       HANDLE_EINTR(v4l2_fd_open(device_fd_.get(), V4L2_DISABLE_CONVERSION)) !=
           -1) {
     VLOGF(2) << "Using libv4l2 for " << path;
--- a/media/gpu/v4l2/v4l2_device.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_device.cc	2019-05-17 18:53:34.264000000 +0300
@@ -4,8 +4,6 @@
 
 #include "media/gpu/v4l2/v4l2_device.h"
 
-#include <set>
-
 #include <libdrm/drm_fourcc.h>
 #include <linux/videodev2.h>
 #include <string.h>
@@ -156,66 +154,20 @@
   return usage;
 }
 
-// A thread-safe pool of buffer indexes, allowing buffers to be obtained and
-// returned from different threads. All the methods of this class are
-// thread-safe. Users should keep a scoped_refptr to instances of this class
-// in order to ensure the list remains alive as long as they need it.
-class V4L2BuffersList : public base::RefCountedThreadSafe<V4L2BuffersList> {
- public:
-  V4L2BuffersList() = default;
-  // Return a buffer to this list. Also can be called to set the initial pool
-  // of buffers.
-  // Note that it is illegal to return the same buffer twice.
-  void ReturnBuffer(size_t buffer_id);
-  // Get any of the buffers in the list. There is no order guarantee whatsoever.
-  base::Optional<size_t> GetFreeBuffer();
-  // Number of buffers currently in this list.
-  size_t size() const;
-
- private:
-  mutable base::Lock lock_;
-  std::set<size_t> free_buffers_ GUARDED_BY(lock_);
-  DISALLOW_COPY_AND_ASSIGN(V4L2BuffersList);
-};
-
-void V4L2BuffersList::ReturnBuffer(size_t buffer_id) {
-  base::AutoLock auto_lock(lock_);
-
-  auto inserted = free_buffers_.emplace(buffer_id);
-  DCHECK(inserted.second);
-}
-
-base::Optional<size_t> V4L2BuffersList::GetFreeBuffer() {
-  base::AutoLock auto_lock(lock_);
-
-  auto iter = free_buffers_.begin();
-  if (iter == free_buffers_.end()) {
-    DVLOGF(4) << "No free buffer available!";
-    return base::nullopt;
-  }
-
-  size_t buffer_id = *iter;
-  free_buffers_.erase(iter);
-
-  return buffer_id;
-}
-
-size_t V4L2BuffersList::size() const {
-  base::AutoLock auto_lock(lock_);
-
-  return free_buffers_.size();
-}
-
 // Module-private class that let users query/write V4L2 buffer information.
 // It also makes some private V4L2Queue methods available to this module only.
-class V4L2BufferRefBase {
+class V4L2BufferQueueProxy {
  public:
-  V4L2BufferRefBase(const struct v4l2_buffer* v4l2_buffer,
-                    base::WeakPtr<V4L2Queue> queue);
-  ~V4L2BufferRefBase();
+  V4L2BufferQueueProxy(const struct v4l2_buffer* v4l2_buffer,
+                       scoped_refptr<V4L2Queue> queue);
+
+  void ReturnBuffer() { queue_->ReturnBuffer(BufferId()); }
 
   bool QueueBuffer();
-  void* GetPlaneMapping(const size_t plane);
+
+  void* GetPlaneMapping(const size_t plane) {
+    return queue_->buffers_[BufferId()]->GetPlaneMapping(plane);
+  }
 
   // Data from the buffer, that users can query and/or write.
   struct v4l2_buffer v4l2_buffer_;
@@ -227,26 +179,18 @@
  private:
   size_t BufferId() const { return v4l2_buffer_.index; }
 
-  // A weak pointer to the queue this buffer belongs to. Will remain valid as
-  // long as the underlying V4L2 buffer is valid too.
-  // This can only be accessed from the sequence protected by sequence_checker_.
-  // Thread-safe methods (like ~V4L2BufferRefBase) must *never* access this.
-  base::WeakPtr<V4L2Queue> queue_;
-  // Where to return this buffer if it goes out of scope without being queued.
-  scoped_refptr<V4L2BuffersList> return_to_;
-  bool queued = false;
+  // The queue must be kept alive as long as the reference to the buffer exists.
+  scoped_refptr<V4L2Queue> queue_;
 
-  SEQUENCE_CHECKER(sequence_checker_);
-  DISALLOW_COPY_AND_ASSIGN(V4L2BufferRefBase);
+  DISALLOW_COPY_AND_ASSIGN(V4L2BufferQueueProxy);
 };
 
-V4L2BufferRefBase::V4L2BufferRefBase(const struct v4l2_buffer* v4l2_buffer,
-                                     base::WeakPtr<V4L2Queue> queue)
-    : queue_(std::move(queue)), return_to_(queue_->free_buffers_) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
+V4L2BufferQueueProxy::V4L2BufferQueueProxy(
+    const struct v4l2_buffer* v4l2_buffer,
+    scoped_refptr<V4L2Queue> queue)
+    : queue_(std::move(queue)) {
   DCHECK(V4L2_TYPE_IS_MULTIPLANAR(v4l2_buffer->type));
   DCHECK_LE(v4l2_buffer->length, base::size(v4l2_planes_));
-  DCHECK(return_to_);
 
   memcpy(&v4l2_buffer_, v4l2_buffer, sizeof(v4l2_buffer_));
   memcpy(v4l2_planes_, v4l2_buffer->m.planes,
@@ -254,35 +198,17 @@
   v4l2_buffer_.m.planes = v4l2_planes_;
 }
 
-V4L2BufferRefBase::~V4L2BufferRefBase() {
-  // We are the last reference and are only accessing the thread-safe
-  // return_to_, so we are safe to call from any sequence.
-  // If we have been queued, then the queue is our owner so we don't need to
-  // return to the free buffers list.
-  if (!queued)
-    return_to_->ReturnBuffer(BufferId());
-}
-
-bool V4L2BufferRefBase::QueueBuffer() {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
-
-  if (!queue_)
-    return false;
+bool V4L2BufferQueueProxy::QueueBuffer() {
+  bool queued = queue_->QueueBuffer(&v4l2_buffer_);
 
-  queued = queue_->QueueBuffer(&v4l2_buffer_);
+  // If an error occurred during queueing, then the buffer must be made
+  // available again.
+  if (!queued)
+    ReturnBuffer();
 
   return queued;
 }
 
-void* V4L2BufferRefBase::GetPlaneMapping(const size_t plane) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
-
-  if (!queue_)
-    return nullptr;
-
-  return queue_->buffers_[BufferId()]->GetPlaneMapping(plane);
-}
-
 V4L2WritableBufferRef::V4L2WritableBufferRef() {
   // Invalid buffers can be created from any thread.
   DETACH_FROM_SEQUENCE(sequence_checker_);
@@ -290,9 +216,9 @@
 
 V4L2WritableBufferRef::V4L2WritableBufferRef(
     const struct v4l2_buffer* v4l2_buffer,
-    base::WeakPtr<V4L2Queue> queue)
-    : buffer_data_(
-          std::make_unique<V4L2BufferRefBase>(v4l2_buffer, std::move(queue))) {
+    scoped_refptr<V4L2Queue> queue)
+    : buffer_data_(std::make_unique<V4L2BufferQueueProxy>(v4l2_buffer,
+                                                          std::move(queue))) {
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
 }
 
@@ -306,6 +232,7 @@
   // Only valid references should be sequence-checked
   if (buffer_data_) {
     DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
+    buffer_data_->ReturnBuffer();
   }
 }
 
@@ -317,6 +244,8 @@
   if (this == &other)
     return *this;
 
+  if (IsValid())
+    buffer_data_->ReturnBuffer();
   buffer_data_ = std::move(other.buffer_data_);
 
   return *this;
@@ -462,7 +391,7 @@
     return;
   }
 
-  if (bytes_used > GetPlaneSize(plane)) {
+  if (bytes_used >= GetPlaneSize(plane)) {
     VLOGF(1) << "Set bytes used " << bytes_used << " larger than plane size "
              << GetPlaneSize(plane) << ".";
     return;
@@ -491,17 +420,17 @@
 }
 
 V4L2ReadableBuffer::V4L2ReadableBuffer(const struct v4l2_buffer* v4l2_buffer,
-                                       base::WeakPtr<V4L2Queue> queue)
-    : buffer_data_(
-          std::make_unique<V4L2BufferRefBase>(v4l2_buffer, std::move(queue))) {
+                                       scoped_refptr<V4L2Queue> queue)
+    : buffer_data_(std::make_unique<V4L2BufferQueueProxy>(v4l2_buffer,
+                                                          std::move(queue))) {
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
 }
 
 V4L2ReadableBuffer::~V4L2ReadableBuffer() {
-  // This method is thread-safe. Since we are the destructor, we are guaranteed
-  // to be called from the only remaining reference to us. Also, we are just
-  // calling the destructor of buffer_data_, which is also thread-safe.
+  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
   DCHECK(buffer_data_);
+
+  buffer_data_->ReturnBuffer();
 }
 
 bool V4L2ReadableBuffer::IsLast() const {
@@ -550,13 +479,13 @@
  public:
   static V4L2WritableBufferRef CreateWritableRef(
       const struct v4l2_buffer* v4l2_buffer,
-      base::WeakPtr<V4L2Queue> queue) {
+      scoped_refptr<V4L2Queue> queue) {
     return V4L2WritableBufferRef(v4l2_buffer, std::move(queue));
   }
 
   static V4L2ReadableBufferRef CreateReadableRef(
       const struct v4l2_buffer* v4l2_buffer,
-      base::WeakPtr<V4L2Queue> queue) {
+      scoped_refptr<V4L2Queue> queue) {
     return new V4L2ReadableBuffer(v4l2_buffer, std::move(queue));
   }
 };
@@ -564,14 +493,21 @@
 V4L2Queue::V4L2Queue(scoped_refptr<V4L2Device> dev,
                      enum v4l2_buf_type type,
                      base::OnceClosure destroy_cb)
-    : type_(type),
-      device_(dev),
-      destroy_cb_(std::move(destroy_cb)),
-      weak_this_factory_(this) {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
+    : type_(type), device_(dev), destroy_cb_(std::move(destroy_cb)) {
+  // TODO(acourbot): fix clients - the constructor should be called on the same
+  // sequence as the rest.
+  DETACH_FROM_SEQUENCE(sequence_checker_);
 }
 
 V4L2Queue::~V4L2Queue() {
+  // TODO(acourbot): we do this prior to checking the sequence because we
+  // tolerate queues to be destroyed in the wrong thread if they are properly
+  // cleaned up. But ultimately clients should be fixed.
+  if (!is_streaming_ && buffers_.empty()) {
+    std::move(destroy_cb_).Run();
+    return;
+  }
+
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
 
   if (is_streaming_) {
@@ -580,7 +516,7 @@
   }
 
   DCHECK(queued_buffers_.empty());
-  DCHECK(!free_buffers_);
+  DCHECK(free_buffers_.empty());
 
   if (!buffers_.empty()) {
     VLOGF(1) << "Buffers are still allocated, trying to deallocate them...";
@@ -592,7 +528,7 @@
 
 size_t V4L2Queue::AllocateBuffers(size_t count, enum v4l2_memory memory) {
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
-  DCHECK(!free_buffers_);
+  DCHECK_EQ(free_buffers_.size(), 0u);
   DCHECK_EQ(queued_buffers_.size(), 0u);
 
   if (IsStreaming()) {
@@ -638,8 +574,6 @@
 
   memory_ = memory;
 
-  free_buffers_ = new V4L2BuffersList();
-
   // Now query all buffer information.
   for (size_t i = 0; i < reqbufs.count; i++) {
     auto buffer = V4L2Buffer::Create(device_, type_, memory_, planes_count_, i);
@@ -651,11 +585,10 @@
     }
 
     buffers_.emplace_back(std::move(buffer));
-    free_buffers_->ReturnBuffer(i);
+    ReturnBuffer(i);
   }
 
-  DCHECK(free_buffers_);
-  DCHECK_EQ(free_buffers_->size(), buffers_.size());
+  DCHECK_EQ(free_buffers_.size(), buffers_.size());
   DCHECK_EQ(queued_buffers_.size(), 0u);
 
   return buffers_.size();
@@ -669,13 +602,14 @@
     return false;
   }
 
+  if (buffers_.size() != free_buffers_.size()) {
+    VPLOGF(1) << "Trying to deallocate buffers while some are still in use!";
+    return false;
+  }
+
   if (buffers_.size() == 0)
     return true;
 
-  weak_this_factory_.InvalidateWeakPtrs();
-  buffers_.clear();
-  free_buffers_ = nullptr;
-
   // Free all buffers.
   struct v4l2_requestbuffers reqbufs = {};
   reqbufs.count = 0;
@@ -688,7 +622,10 @@
     return false;
   }
 
-  DCHECK(!free_buffers_);
+  buffers_.clear();
+  free_buffers_.clear();
+
+  DCHECK_EQ(free_buffers_.size(), 0u);
   DCHECK_EQ(queued_buffers_.size(), 0u);
 
   return true;
@@ -709,19 +646,25 @@
 
 V4L2WritableBufferRef V4L2Queue::GetFreeBuffer() {
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
+  auto iter = free_buffers_.begin();
 
-  // No buffers allocated at the moment?
-  if (!free_buffers_)
+  if (iter == free_buffers_.end()) {
+    VLOGF(3) << "No free buffer available!";
     return V4L2WritableBufferRef();
+  }
 
-  auto buffer_id = free_buffers_->GetFreeBuffer();
-
-  if (!buffer_id.has_value())
-    return V4L2WritableBufferRef();
+  size_t buffer_id = *iter;
+  free_buffers_.erase(buffer_id);
 
   return V4L2BufferRefFactory::CreateWritableRef(
-      buffers_[buffer_id.value()]->v4l2_buffer(),
-      weak_this_factory_.GetWeakPtr());
+      buffers_[buffer_id]->v4l2_buffer(), this);
+}
+
+void V4L2Queue::ReturnBuffer(size_t buffer_id) {
+  DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
+
+  auto inserted = free_buffers_.emplace(buffer_id);
+  DCHECK_EQ(inserted.second, true);
 }
 
 bool V4L2Queue::QueueBuffer(struct v4l2_buffer* v4l2_buffer) {
@@ -779,10 +722,8 @@
   DCHECK(it != queued_buffers_.end());
   queued_buffers_.erase(*it);
 
-  DCHECK(free_buffers_);
-  return std::make_pair(true,
-                        V4L2BufferRefFactory::CreateReadableRef(
-                            &v4l2_buffer, weak_this_factory_.GetWeakPtr()));
+  return std::make_pair(
+      true, V4L2BufferRefFactory::CreateReadableRef(&v4l2_buffer, this));
 }
 
 bool V4L2Queue::IsStreaming() const {
@@ -823,10 +764,8 @@
     return false;
   }
 
-  for (const auto& buffer_id : queued_buffers_) {
-    DCHECK(free_buffers_);
-    free_buffers_->ReturnBuffer(buffer_id);
-  }
+  for (const auto& buffer_id : queued_buffers_)
+    ReturnBuffer(buffer_id);
 
   queued_buffers_.clear();
 
@@ -844,7 +783,7 @@
 size_t V4L2Queue::FreeBuffersCount() const {
   DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);
 
-  return free_buffers_ ? free_buffers_->size() : 0;
+  return free_buffers_.size();
 }
 
 size_t V4L2Queue::QueuedBuffersCount() const {
@@ -886,7 +825,9 @@
     return scoped_refptr<V4L2Queue>(it->second);
 
   scoped_refptr<V4L2Queue> queue = V4L2QueueFactory::CreateQueue(
-      this, type, base::BindOnce(&V4L2Device::OnQueueDestroyed, this, type));
+      this, type,
+      media::BindToCurrentLoop(
+          base::Bind(&V4L2Device::OnQueueDestroyed, this, type)));
 
   queues_[type] = queue.get();
   return queue;
@@ -979,6 +920,19 @@
 }
 
 // static
+#if BUILDFLAG(USE_LINUX_V4L2)
+uint32_t V4L2Device::VideoCodecProfileToV4L2PixFmt(VideoCodecProfile profile,
+                                                   bool slice_based) {
+  if (profile >= H264PROFILE_MIN && profile <= H264PROFILE_MAX) {
+    return V4L2_PIX_FMT_H264;
+  } else if (profile >= VP8PROFILE_MIN && profile <= VP8PROFILE_MAX) {
+    return V4L2_PIX_FMT_VP8;
+  } else {
+    LOG(FATAL) << "Add more cases as needed";
+    return 0;
+  }
+}
+#else
 uint32_t V4L2Device::VideoCodecProfileToV4L2PixFmt(VideoCodecProfile profile,
                                                    bool slice_based) {
   if (profile >= H264PROFILE_MIN && profile <= H264PROFILE_MAX) {
@@ -1019,6 +973,7 @@
       return VIDEO_CODEC_PROFILE_UNKNOWN;
   }
 }
+#endif
 
 // static
 std::vector<VideoCodecProfile> V4L2Device::V4L2PixFmtToVideoCodecProfiles(
@@ -1029,7 +984,9 @@
 
   switch (pix_fmt) {
     case V4L2_PIX_FMT_H264:
+#if !BUILDFLAG(USE_LINUX_V4L2)
     case V4L2_PIX_FMT_H264_SLICE:
+#endif
       if (is_encoder) {
         // TODO(posciak): need to query the device for supported H.264 profiles,
         // for now choose Main as a sensible default.
@@ -1042,11 +999,14 @@
       break;
 
     case V4L2_PIX_FMT_VP8:
+#if !BUILDFLAG(USE_LINUX_V4L2)
     case V4L2_PIX_FMT_VP8_FRAME:
+#endif
       min_profile = VP8PROFILE_MIN;
       max_profile = VP8PROFILE_MAX;
       break;
 
+#if !BUILDFLAG(USE_LINUX_V4L2)
     case V4L2_PIX_FMT_VP9:
     case V4L2_PIX_FMT_VP9_FRAME: {
       v4l2_queryctrl query_ctrl = {};
@@ -1073,6 +1033,7 @@
       }
       break;
     }
+#endif
 
     default:
       VLOGF(1) << "Unhandled pixelformat " << FourccToString(pix_fmt);
@@ -1103,6 +1064,9 @@
       return DRM_FORMAT_ARGB8888;
 
     case V4L2_PIX_FMT_MT21C:
+#if !BUILDFLAG(USE_LINUX_V4L2)
+    case V4L2_PIX_FMT_MT21:
+#endif
       return DRM_FORMAT_MT21;
 
     default:
--- a/media/gpu/v4l2/v4l2_device.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_device.h	2019-05-17 18:53:34.264000000 +0300
@@ -44,8 +44,7 @@
 namespace media {
 
 class V4L2Queue;
-class V4L2BufferRefBase;
-class V4L2BuffersList;
+class V4L2BufferQueueProxy;
 
 // A unique reference to a buffer for clients to prepare and submit.
 //
@@ -118,10 +117,10 @@
   bool DoQueue() &&;
 
   V4L2WritableBufferRef(const struct v4l2_buffer* v4l2_buffer,
-                        base::WeakPtr<V4L2Queue> queue);
+                        scoped_refptr<V4L2Queue> queue);
   friend class V4L2BufferRefFactory;
 
-  std::unique_ptr<V4L2BufferRefBase> buffer_data_;
+  std::unique_ptr<V4L2BufferQueueProxy> buffer_data_;
 
   SEQUENCE_CHECKER(sequence_checker_);
   DISALLOW_COPY_AND_ASSIGN(V4L2WritableBufferRef);
@@ -132,14 +131,8 @@
 // Clients use this class to query the buffer state and content, and are
 // guaranteed that the buffer will not be reused until all references are
 // destroyed.
-// All methods of this class must be called from the same sequence, but
-// instances of V4L2ReadableBuffer objects can be destroyed from any sequence.
-// They can even outlive the V4L2 buffers they originate from. This flexibility
-// is required because V4L2ReadableBufferRefs can be embedded into VideoFrames,
-// which are then passed to other threads and not necessarily destroyed before
-// the V4L2Queue buffers are freed.
 class MEDIA_GPU_EXPORT V4L2ReadableBuffer
-    : public base::RefCountedThreadSafe<V4L2ReadableBuffer> {
+    : public base::RefCounted<V4L2ReadableBuffer> {
  public:
   // Returns whether the V4L2_BUF_FLAG_LAST flag is set for this buffer.
   bool IsLast() const;
@@ -156,17 +149,16 @@
   size_t BufferId() const;
 
  private:
-  friend class V4L2BufferRefFactory;
-  friend class base::RefCountedThreadSafe<V4L2ReadableBuffer>;
-
   ~V4L2ReadableBuffer();
 
   V4L2ReadableBuffer(const struct v4l2_buffer* v4l2_buffer,
-                     base::WeakPtr<V4L2Queue> queue);
+                     scoped_refptr<V4L2Queue> queue);
+  friend class V4L2BufferRefFactory;
 
-  std::unique_ptr<V4L2BufferRefBase> buffer_data_;
+  std::unique_ptr<V4L2BufferQueueProxy> buffer_data_;
 
   SEQUENCE_CHECKER(sequence_checker_);
+  friend class base::RefCounted<V4L2ReadableBuffer>;
   DISALLOW_COPY_AND_ASSIGN(V4L2ReadableBuffer);
 };
 
@@ -264,6 +256,8 @@
  private:
   ~V4L2Queue();
 
+  // Called when clients lose their reference to a buffer.
+  void ReturnBuffer(size_t buffer_id);
   // Called when clients request a buffer to be queued.
   bool QueueBuffer(struct v4l2_buffer* v4l2_buffer);
 
@@ -276,7 +270,7 @@
 
   // Buffers that are available for client to get and submit.
   // Buffers in this list are not referenced by anyone else than ourselves.
-  scoped_refptr<V4L2BuffersList> free_buffers_;
+  std::set<size_t> free_buffers_;
   // Buffers that have been queued by the client, and not dequeued yet.
   std::set<size_t> queued_buffers_;
 
@@ -284,13 +278,11 @@
   // Callback to call in this queue's destructor.
   base::OnceClosure destroy_cb_;
 
-  base::WeakPtrFactory<V4L2Queue> weak_this_factory_;
-
   V4L2Queue(scoped_refptr<V4L2Device> dev,
             enum v4l2_buf_type type,
             base::OnceClosure destroy_cb);
   friend class V4L2QueueFactory;
-  friend class V4L2BufferRefBase;
+  friend class V4L2BufferQueueProxy;
   friend class base::RefCountedThreadSafe<V4L2Queue>;
 
   SEQUENCE_CHECKER(sequence_checker_);
--- a/media/gpu/v4l2/v4l2_image_processor.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_image_processor.cc	2019-05-17 18:53:34.264000000 +0300
@@ -9,7 +9,6 @@
 #include <sys/eventfd.h>
 #include <sys/ioctl.h>
 #include <sys/mman.h>
-#include <tuple>
 
 #include "base/bind.h"
 #include "base/bind_helpers.h"
@@ -44,6 +43,13 @@
 
 namespace media {
 
+V4L2ImageProcessor::InputRecord::InputRecord() : at_device(false) {}
+
+V4L2ImageProcessor::InputRecord::InputRecord(
+    const V4L2ImageProcessor::InputRecord&) = default;
+
+V4L2ImageProcessor::InputRecord::~InputRecord() {}
+
 V4L2ImageProcessor::OutputRecord::OutputRecord() : at_device(false) {}
 
 V4L2ImageProcessor::OutputRecord::OutputRecord(OutputRecord&&) = default;
@@ -79,19 +85,23 @@
       device_(device),
       device_thread_("V4L2ImageProcessorThread"),
       device_poll_thread_("V4L2ImageProcessorDevicePollThread"),
+      input_streamon_(false),
+      input_buffer_queued_count_(0),
+      output_streamon_(false),
       output_buffer_queued_count_(0),
       num_buffers_(num_buffers),
-      error_cb_(error_cb) {
-  DETACH_FROM_THREAD(device_thread_checker_);
-}
+      error_cb_(error_cb) {}
 
 V4L2ImageProcessor::~V4L2ImageProcessor() {
-  DCHECK_CALLED_ON_VALID_SEQUENCE(client_sequence_checker_);
+  DCHECK_CALLED_ON_VALID_THREAD(client_thread_checker_);
 
   Destroy();
 
   DCHECK(!device_thread_.IsRunning());
   DCHECK(!device_poll_thread_.IsRunning());
+
+  DestroyInputBuffers();
+  DestroyOutputBuffers();
 }
 
 void V4L2ImageProcessor::NotifyError() {
@@ -286,26 +296,18 @@
     return false;
   }
 
-  if (!device_thread_.Start()) {
-    VLOGF(1) << "Initialize(): device thread failed to start";
+  if (!CreateInputBuffers() || !CreateOutputBuffers())
     return false;
-  }
 
-  // Call to AllocateBuffers must be asynchronous.
-  base::WaitableEvent done;
-  bool result;
-  device_thread_.task_runner()->PostTask(
-      FROM_HERE, base::BindOnce(&V4L2ImageProcessor::AllocateBuffersTask,
-                                base::Unretained(this), &result, &done));
-  done.Wait();
-  if (!result) {
+  if (!device_thread_.Start()) {
+    VLOGF(1) << "Initialize(): device thread failed to start";
     return false;
   }
 
   // StartDevicePoll will NotifyError on failure.
   device_thread_.task_runner()->PostTask(
-      FROM_HERE, base::BindOnce(&V4L2ImageProcessor::StartDevicePoll,
-                                base::Unretained(this)));
+      FROM_HERE,
+      base::Bind(&V4L2ImageProcessor::StartDevicePoll, base::Unretained(this)));
 
   VLOGF(2) << "V4L2ImageProcessor initialized for "
            << "input_layout: " << input_layout_
@@ -376,24 +378,20 @@
     scoped_refptr<VideoFrame> frame,
     int output_buffer_index,
     std::vector<base::ScopedFD> output_dmabuf_fds,
-    LegacyFrameReadyCB cb) {
+    FrameReadyCB cb) {
   DVLOGF(4) << "ts=" << frame->timestamp().InMilliseconds();
 
-  auto job_record = std::make_unique<JobRecord>();
-  job_record->input_frame = frame;
-  job_record->output_buffer_index = output_buffer_index;
-  job_record->legacy_ready_cb = std::move(cb);
-
   switch (output_memory_type_) {
     case V4L2_MEMORY_MMAP:
       if (!output_dmabuf_fds.empty()) {
         VLOGF(1) << "output_dmabuf_fds must be empty for MMAP output mode";
         return false;
       }
+      output_dmabuf_fds =
+          DuplicateFDs(output_buffer_map_[output_buffer_index].dmabuf_fds);
       break;
 
     case V4L2_MEMORY_DMABUF:
-      job_record->output_dmabuf_fds = std::move(output_dmabuf_fds);
       break;
 
     default:
@@ -401,6 +399,26 @@
       return false;
   }
 
+  if (output_dmabuf_fds.size() != output_layout_.num_buffers()) {
+    VLOGF(1) << "wrong number of output fds. Expected "
+             << output_layout_.num_buffers() << ", actual "
+             << output_dmabuf_fds.size();
+    return false;
+  }
+
+  std::unique_ptr<JobRecord> job_record(new JobRecord());
+  job_record->input_frame = frame;
+  job_record->output_buffer_index = output_buffer_index;
+  job_record->ready_cb = std::move(cb);
+
+  // Create the output frame
+  job_record->output_frame = VideoFrame::WrapExternalDmabufs(
+      output_layout_, gfx::Rect(output_visible_size_), output_visible_size_,
+      std::move(output_dmabuf_fds), job_record->input_frame->timestamp());
+
+  if (!job_record->output_frame)
+    return false;
+
   // Since device_thread_ is owned by this class. base::Unretained(this) and the
   // raw pointer of that task runner are safe.
   process_task_tracker_.PostTask(
@@ -420,32 +438,16 @@
 void V4L2ImageProcessor::ProcessTask(std::unique_ptr<JobRecord> job_record) {
   DVLOGF(4) << "Reusing output buffer, index="
             << job_record->output_buffer_index;
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
-
-  input_job_queue_.emplace(std::move(job_record));
-  ProcessJobsTask();
-}
-
-void V4L2ImageProcessor::ProcessJobsTask() {
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
+  DCHECK(device_thread_.task_runner()->BelongsToCurrentThread());
 
-  while (!input_job_queue_.empty()) {
-    // The output buffer is already decided, but we need one free input buffer
-    // to schedule the job
-    if (input_queue_->FreeBuffersCount() == 0)
-      break;
-
-    auto job_record = std::move(input_job_queue_.front());
-    input_job_queue_.pop();
-    EnqueueInput(job_record.get());
     EnqueueOutput(job_record.get());
-    running_jobs_.emplace(std::move(job_record));
-  }
+  input_queue_.emplace(std::move(job_record));
+  EnqueueInput();
 }
 
 bool V4L2ImageProcessor::Reset() {
   VLOGF(2);
-  DCHECK_CALLED_ON_VALID_SEQUENCE(client_sequence_checker_);
+  DCHECK_CALLED_ON_VALID_THREAD(client_thread_checker_);
   DCHECK(device_thread_.IsRunning());
 
   process_task_tracker_.TryCancelAll();
@@ -454,7 +456,7 @@
 
 void V4L2ImageProcessor::Destroy() {
   VLOGF(2);
-  DCHECK_CALLED_ON_VALID_SEQUENCE(client_sequence_checker_);
+  DCHECK_CALLED_ON_VALID_THREAD(client_thread_checker_);
 
   // If the device thread is running, destroy using posted task.
   if (device_thread_.IsRunning()) {
@@ -463,9 +465,6 @@
     device_thread_.task_runner()->PostTask(
         FROM_HERE, base::BindOnce(&V4L2ImageProcessor::StopDevicePoll,
                                   base::Unretained(this)));
-    device_thread_.task_runner()->PostTask(
-        FROM_HERE, base::BindOnce(&V4L2ImageProcessor::DestroyBuffersTask,
-                                  base::Unretained(this)));
     // Wait for tasks to finish/early-exit.
     device_thread_.Stop();
   } else {
@@ -476,8 +475,9 @@
 
 bool V4L2ImageProcessor::CreateInputBuffers() {
   VLOGF(2);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
-  DCHECK_EQ(input_queue_, nullptr);
+  DCHECK_CALLED_ON_VALID_THREAD(client_thread_checker_);
+
+  DCHECK(!input_streamon_);
 
   struct v4l2_control control;
   memset(&control, 0, sizeof(control));
@@ -520,27 +520,31 @@
     IOCTL_OR_ERROR_RETURN_FALSE(VIDIOC_S_CROP, &crop);
   }
 
-  input_queue_ = device_->GetQueue(V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE);
-  if (!input_queue_)
+  struct v4l2_requestbuffers reqbufs;
+  memset(&reqbufs, 0, sizeof(reqbufs));
+  reqbufs.count = num_buffers_;
+  reqbufs.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
+  reqbufs.memory = input_memory_type_;
+  IOCTL_OR_ERROR_RETURN_FALSE(VIDIOC_REQBUFS, &reqbufs);
+  if (reqbufs.count != num_buffers_) {
+    VLOGF(1) << "Failed to allocate input buffers. reqbufs.count="
+             << reqbufs.count << ", num_buffers=" << num_buffers_;
     return false;
+  }
 
-  if (input_queue_->AllocateBuffers(num_buffers_, input_memory_type_) == 0u)
-    return false;
+  DCHECK(input_buffer_map_.empty());
+  input_buffer_map_.resize(reqbufs.count);
 
-  if (input_queue_->AllocatedBuffersCount() != num_buffers_) {
-    VLOGF(1) << "Failed to allocate the required number of input buffers. "
-             << "Requested " << num_buffers_ << ", got "
-             << input_queue_->AllocatedBuffersCount() << ".";
-    return false;
-  }
+  for (size_t i = 0; i < input_buffer_map_.size(); ++i)
+    free_input_buffers_.push_back(i);
 
   return true;
 }
 
 bool V4L2ImageProcessor::CreateOutputBuffers() {
   VLOGF(2);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
-  DCHECK_EQ(output_queue_, nullptr);
+  DCHECK_CALLED_ON_VALID_THREAD(client_thread_checker_);
+  DCHECK(!output_streamon_);
 
   struct v4l2_rect visible_rect;
   visible_rect.left = 0;
@@ -549,10 +553,6 @@
   visible_rect.height =
     base::checked_cast<__u32>(output_visible_size_.height());
 
-  output_queue_ = device_->GetQueue(V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE);
-  if (!output_queue_)
-    return false;
-
   struct v4l2_selection selection_arg;
   memset(&selection_arg, 0, sizeof(selection_arg));
   selection_arg.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
@@ -567,18 +567,20 @@
     IOCTL_OR_ERROR_RETURN_FALSE(VIDIOC_S_CROP, &crop);
   }
 
-  if (output_queue_->AllocateBuffers(num_buffers_, output_memory_type_) == 0)
-    return false;
-
-  if (output_queue_->AllocatedBuffersCount() != num_buffers_) {
-    VLOGF(1) << "Failed to allocate output buffers. Allocated number="
-             << output_queue_->AllocatedBuffersCount()
-             << ", Requested number=" << num_buffers_;
+  struct v4l2_requestbuffers reqbufs;
+  memset(&reqbufs, 0, sizeof(reqbufs));
+  reqbufs.count = num_buffers_;
+  reqbufs.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+  reqbufs.memory = output_memory_type_;
+  IOCTL_OR_ERROR_RETURN_FALSE(VIDIOC_REQBUFS, &reqbufs);
+  if (reqbufs.count != num_buffers_) {
+    VLOGF(1) << "Failed to allocate output buffers. reqbufs.count="
+             << reqbufs.count << ", num_buffers=" << num_buffers_;
     return false;
   }
 
   DCHECK(output_buffer_map_.empty());
-  output_buffer_map_.resize(output_queue_->AllocatedBuffersCount());
+  output_buffer_map_.resize(reqbufs.count);
 
   // Get the DMA-BUF FDs for MMAP buffers
   if (output_memory_type_ == V4L2_MEMORY_MMAP) {
@@ -598,24 +600,33 @@
 
 void V4L2ImageProcessor::DestroyInputBuffers() {
   VLOGF(2);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
+  DCHECK_CALLED_ON_VALID_THREAD(client_thread_checker_);
+  DCHECK(!input_streamon_);
+
+  struct v4l2_requestbuffers reqbufs;
+  memset(&reqbufs, 0, sizeof(reqbufs));
+  reqbufs.count = 0;
+  reqbufs.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
+  reqbufs.memory = input_memory_type_;
+  IOCTL_OR_LOG_ERROR(VIDIOC_REQBUFS, &reqbufs);
 
-  // We may be destroyed before we allocate any buffer.
-  if (input_queue_)
-    input_queue_->DeallocateBuffers();
-  input_queue_ = nullptr;
+  input_buffer_map_.clear();
+  free_input_buffers_.clear();
 }
 
 void V4L2ImageProcessor::DestroyOutputBuffers() {
   VLOGF(2);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
+  DCHECK_CALLED_ON_VALID_THREAD(client_thread_checker_);
+  DCHECK(!output_streamon_);
 
   output_buffer_map_.clear();
 
-  // We may be destroyed before we allocate any buffer.
-  if (output_queue_)
-    output_queue_->DeallocateBuffers();
-  output_queue_ = nullptr;
+  struct v4l2_requestbuffers reqbufs;
+  memset(&reqbufs, 0, sizeof(reqbufs));
+  reqbufs.count = 0;
+  reqbufs.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+  reqbufs.memory = output_memory_type_;
+  IOCTL_OR_LOG_ERROR(VIDIOC_REQBUFS, &reqbufs);
 }
 
 void V4L2ImageProcessor::DevicePollTask(bool poll_device) {
@@ -637,7 +648,7 @@
 
 void V4L2ImageProcessor::ServiceDeviceTask() {
   DVLOGF(4);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
+  DCHECK(device_thread_.task_runner()->BelongsToCurrentThread());
   // ServiceDeviceTask() should only ever be scheduled from DevicePollTask(),
   // so either:
   // * device_poll_thread_ is running normally
@@ -646,42 +657,39 @@
   if (!device_poll_thread_.task_runner())
     return;
 
-  DCHECK(input_queue_);
-
   Dequeue();
-  ProcessJobsTask();
+  EnqueueInput();
 
   if (!device_->ClearDevicePollInterrupt()) {
     NotifyError();
     return;
   }
 
-  bool poll_device = (input_queue_->QueuedBuffersCount() > 0 ||
-                      output_buffer_queued_count_ > 0);
+  bool poll_device =
+      (input_buffer_queued_count_ > 0 || output_buffer_queued_count_ > 0);
 
   device_poll_thread_.task_runner()->PostTask(
       FROM_HERE, base::BindOnce(&V4L2ImageProcessor::DevicePollTask,
                                 base::Unretained(this), poll_device));
 
-  DVLOGF(3) << __func__ << ": buffer counts: INPUT[" << input_job_queue_.size()
-            << "] => DEVICE[" << input_queue_->FreeBuffersCount() << "+"
-            << input_queue_->QueuedBuffersCount() << "/"
-            << input_queue_->AllocatedBuffersCount() << "->"
-            << output_buffer_map_.size() - output_buffer_queued_count_ << "+"
-            << output_buffer_queued_count_ << "/" << output_buffer_map_.size()
-            << "]";
+  DVLOGF(3) << __func__ << ": buffer counts: INPUT[" << input_queue_.size()
+            << "] => DEVICE[" << free_input_buffers_.size() << "+"
+            << input_buffer_queued_count_ << "/" << input_buffer_map_.size()
+            << "->" << output_buffer_map_.size() - output_buffer_queued_count_
+            << "+" << output_buffer_queued_count_ << "/"
+            << output_buffer_map_.size() << "]";
 }
 
-void V4L2ImageProcessor::EnqueueInput(const JobRecord* job_record) {
+void V4L2ImageProcessor::EnqueueInput() {
   DVLOGF(4);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
-  DCHECK(input_queue_);
+  DCHECK(device_thread_.task_runner()->BelongsToCurrentThread());
 
-  const size_t old_inputs_queued = input_queue_->QueuedBuffersCount();
-  if (!EnqueueInputRecord(job_record))
+  const int old_inputs_queued = input_buffer_queued_count_;
+  while (!input_queue_.empty() && !free_input_buffers_.empty()) {
+    if (!EnqueueInputRecord())
     return;
-
-  if (old_inputs_queued == 0 && input_queue_->QueuedBuffersCount() != 0) {
+  }
+  if (old_inputs_queued == 0 && input_buffer_queued_count_ != 0) {
     // We started up a previously empty queue.
     // Queue state changed; signal interrupt.
     if (!device_->SetDevicePollInterrupt()) {
@@ -689,15 +697,17 @@
       return;
     }
     // VIDIOC_STREAMON if we haven't yet.
-    if (!input_queue_->Streamon())
-      return;
+    if (!input_streamon_) {
+      __u32 type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
+      IOCTL_OR_ERROR_RETURN(VIDIOC_STREAMON, &type);
+      input_streamon_ = true;
+    }
   }
 }
 
 void V4L2ImageProcessor::EnqueueOutput(const JobRecord* job_record) {
   DVLOGF(4);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
-  DCHECK(output_queue_);
+  DCHECK(device_thread_.task_runner()->BelongsToCurrentThread());
 
   const int old_outputs_queued = output_buffer_queued_count_;
   if (!EnqueueOutputRecord(job_record))
@@ -711,39 +721,50 @@
       return;
     }
     // Start VIDIOC_STREAMON if we haven't yet.
-    if (!output_queue_->Streamon())
-      return;
+    if (!output_streamon_) {
+      __u32 type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+      IOCTL_OR_ERROR_RETURN(VIDIOC_STREAMON, &type);
+      output_streamon_ = true;
+    }
   }
 }
 
 void V4L2ImageProcessor::Dequeue() {
   DVLOGF(4);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
-  DCHECK(input_queue_);
-  DCHECK(output_queue_);
-  DCHECK(input_queue_->IsStreaming());
+  DCHECK(device_thread_.task_runner()->BelongsToCurrentThread());
 
   // Dequeue completed input (VIDEO_OUTPUT) buffers,
   // and recycle to the free list.
   struct v4l2_buffer dqbuf;
   struct v4l2_plane planes[VIDEO_MAX_PLANES];
-  while (input_queue_->QueuedBuffersCount() > 0) {
-    bool res;
-    V4L2ReadableBufferRef buffer;
-    std::tie(res, buffer) = input_queue_->DequeueBuffer();
-    if (!res) {
+  while (input_buffer_queued_count_ > 0) {
+    DCHECK(input_streamon_);
+    memset(&dqbuf, 0, sizeof(dqbuf));
+    memset(&planes, 0, sizeof(planes));
+    dqbuf.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
+    dqbuf.memory = input_memory_type_;
+    dqbuf.m.planes = planes;
+    dqbuf.length = input_layout_.num_buffers();
+    if (device_->Ioctl(VIDIOC_DQBUF, &dqbuf) != 0) {
+      if (errno == EAGAIN) {
+        // EAGAIN if we're just out of buffers to dequeue.
+        break;
+      }
+      VPLOGF(1) << "ioctl() failed: VIDIOC_DQBUF";
       NotifyError();
       return;
     }
-    if (!buffer) {
-      // No error occurred, we are just out of buffers to dequeue.
-      break;
-    }
+    InputRecord& input_record = input_buffer_map_[dqbuf.index];
+    DCHECK(input_record.at_device);
+    input_record.at_device = false;
+    input_record.frame = NULL;
+    free_input_buffers_.push_back(dqbuf.index);
+    input_buffer_queued_count_--;
   }
   // Dequeue completed output (VIDEO_CAPTURE) buffers, recycle to the free list.
   // Return the finished buffer to the client via the job ready callback.
   while (output_buffer_queued_count_ > 0) {
-    DCHECK(output_queue_->IsStreaming());
+    DCHECK(output_streamon_);
     memset(&dqbuf, 0, sizeof(dqbuf));
     memset(&planes, 0, sizeof(planes));
     dqbuf.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
@@ -769,85 +790,75 @@
     std::unique_ptr<JobRecord> job_record = std::move(running_jobs_.front());
     running_jobs_.pop();
 
-    std::vector<base::ScopedFD> output_dmabuf_fds;
-    switch (output_memory_type_) {
-      case V4L2_MEMORY_MMAP:
-        output_dmabuf_fds =
-            DuplicateFDs(output_buffer_map_[dqbuf.index].dmabuf_fds);
-        break;
-
-      case V4L2_MEMORY_DMABUF:
-        output_dmabuf_fds = std::move(job_record->output_dmabuf_fds);
-        break;
-
-      default:
-        NOTREACHED();
-        return;
-    }
-
-    if (output_dmabuf_fds.size() != output_layout_.num_buffers()) {
-      VLOGF(1) << "wrong number of output fds. Expected "
-               << output_layout_.num_buffers() << ", actual "
-               << output_dmabuf_fds.size();
-      return;
-    }
-
-    // Create the output frame
-    auto output_frame = VideoFrame::WrapExternalDmabufs(
-        output_layout_, gfx::Rect(output_visible_size_), output_visible_size_,
-        std::move(output_dmabuf_fds), job_record->input_frame->timestamp());
-
-    if (!output_frame) {
-      DVLOGF(1) << "Error creating output frame!";
-      NotifyError();
-      return;
-    }
-
     DVLOGF(4) << "Processing finished, returning frame, index=" << dqbuf.index;
 
-    if (!job_record->legacy_ready_cb.is_null()) {
-      std::move(job_record->legacy_ready_cb)
-          .Run(dqbuf.index, std::move(output_frame));
-    } else {
-      std::move(job_record->ready_cb).Run(std::move(output_frame));
-    }
+    std::move(job_record->ready_cb).Run(std::move(job_record->output_frame));
   }
 }
 
-bool V4L2ImageProcessor::EnqueueInputRecord(const JobRecord* job_record) {
+bool V4L2ImageProcessor::EnqueueInputRecord() {
   DVLOGF(4);
-  DCHECK(input_queue_);
-  DCHECK_GT(input_queue_->FreeBuffersCount(), 0u);
+  DCHECK(!input_queue_.empty());
+  DCHECK(!free_input_buffers_.empty());
 
   // Enqueue an input (VIDEO_OUTPUT) buffer for an input video frame.
-  V4L2WritableBufferRef buffer(input_queue_->GetFreeBuffer());
-  DCHECK(buffer.IsValid());
+  std::unique_ptr<JobRecord> job_record = std::move(input_queue_.front());
+  input_queue_.pop();
+  const int index = free_input_buffers_.back();
+  InputRecord& input_record = input_buffer_map_[index];
+  DCHECK(!input_record.at_device);
+  input_record.frame = job_record->input_frame;
+  struct v4l2_buffer qbuf;
+  struct v4l2_plane qbuf_planes[VIDEO_MAX_PLANES];
+  memset(&qbuf, 0, sizeof(qbuf));
+  memset(qbuf_planes, 0, sizeof(qbuf_planes));
+  qbuf.index = index;
+  qbuf.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
+  qbuf.memory = input_memory_type_;
+  qbuf.m.planes = qbuf_planes;
+  qbuf.length = input_layout_.num_buffers();
 
-  std::vector<void*> user_ptrs;
+  std::vector<int> fds;
+  if (input_memory_type_ == V4L2_MEMORY_DMABUF) {
+    auto& scoped_fds = input_record.frame->DmabufFds();
+    if (scoped_fds.size() != input_layout_.num_buffers()) {
+      VLOGF(1) << "Invalid number of planes in the frame";
+      return false;
+    }
+    for (auto& fd : scoped_fds)
+      fds.push_back(fd.get());
+  }
   for (size_t i = 0; i < input_layout_.num_buffers(); ++i) {
-    int bytes_used = VideoFrame::PlaneSize(job_record->input_frame->format(), i,
+    qbuf.m.planes[i].bytesused =
+        VideoFrame::PlaneSize(input_record.frame->format(), i,
                                            input_layout_.coded_size())
                          .GetArea();
-    buffer.SetPlaneBytesUsed(i, bytes_used);
-    if (buffer.Memory() == V4L2_MEMORY_USERPTR)
-      user_ptrs.push_back(job_record->input_frame->data(i));
-  }
-
+    qbuf.m.planes[i].length = qbuf.m.planes[i].bytesused;
   switch (input_memory_type_) {
     case V4L2_MEMORY_USERPTR:
-      std::move(buffer).QueueUserPtr(user_ptrs);
+        qbuf.m.planes[i].m.userptr =
+            reinterpret_cast<unsigned long>(input_record.frame->data(i));
       break;
     case V4L2_MEMORY_DMABUF:
-      std::move(buffer).QueueDMABuf(job_record->input_frame->DmabufFds());
+        qbuf.m.planes[i].m.fd = fds[i];
       break;
     default:
       NOTREACHED();
       return false;
   }
+  }
+  DVLOGF(4) << "Calling VIDIOC_QBUF: " << V4L2Device::V4L2BufferToString(qbuf);
+  IOCTL_OR_ERROR_RETURN_FALSE(VIDIOC_QBUF, &qbuf);
+  input_record.at_device = true;
+
   DVLOGF(4) << "enqueued frame ts="
             << job_record->input_frame->timestamp().InMilliseconds()
             << " to device.";
 
+  running_jobs_.emplace(std::move(job_record));
+  free_input_buffers_.pop_back();
+  input_buffer_queued_count_++;
+
   return true;
 }
 
@@ -867,7 +878,7 @@
   qbuf.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
   qbuf.memory = output_memory_type_;
   if (output_memory_type_ == V4L2_MEMORY_DMABUF) {
-    auto& fds = job_record->output_dmabuf_fds;
+    auto& fds = job_record->output_frame->DmabufFds();
     if (fds.size() != output_layout_.num_buffers()) {
       VLOGF(1) << "Invalid number of FDs in output record";
       return false;
@@ -885,26 +896,9 @@
   return true;
 }
 
-void V4L2ImageProcessor::AllocateBuffersTask(bool* result,
-                                             base::WaitableEvent* done) {
-  VLOGF(2);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
-
-  *result = CreateInputBuffers() && CreateOutputBuffers();
-  done->Signal();
-}
-
-void V4L2ImageProcessor::DestroyBuffersTask() {
-  VLOGF(2);
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
-
-  DestroyInputBuffers();
-  DestroyOutputBuffers();
-}
-
 void V4L2ImageProcessor::StartDevicePoll() {
   DVLOGF(3) << "starting device poll";
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
+  DCHECK(device_thread_.task_runner()->BelongsToCurrentThread());
   DCHECK(!device_poll_thread_.IsRunning());
 
   // Start up the device poll thread and schedule its first DevicePollTask().
@@ -922,7 +916,7 @@
 
 void V4L2ImageProcessor::StopDevicePoll() {
   DVLOGF(3) << "stopping device poll";
-  DCHECK_CALLED_ON_VALID_THREAD(device_thread_checker_);
+  DCHECK(device_thread_.task_runner()->BelongsToCurrentThread());
 
   // Signal the DevicePollTask() to stop, and stop the device poll thread.
   bool result = device_->SetDevicePollInterrupt();
@@ -938,19 +932,34 @@
     return;
   }
 
-  if (input_queue_)
-    input_queue_->Streamoff();
+  if (input_streamon_) {
+    __u32 type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
+    IOCTL_OR_ERROR_RETURN(VIDIOC_STREAMOFF, &type);
+  }
+  input_streamon_ = false;
 
-  if (output_queue_)
-    output_queue_->Streamoff();
+  if (output_streamon_) {
+    __u32 type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+    IOCTL_OR_ERROR_RETURN(VIDIOC_STREAMOFF, &type);
+  }
+  output_streamon_ = false;
 
   // Reset all our accounting info.
-  while (!input_job_queue_.empty())
-    input_job_queue_.pop();
+  while (!input_queue_.empty())
+    input_queue_.pop();
 
   while (!running_jobs_.empty())
     running_jobs_.pop();
 
+  free_input_buffers_.clear();
+  for (size_t i = 0; i < input_buffer_map_.size(); ++i) {
+    InputRecord& input_record = input_buffer_map_[i];
+    input_record.at_device = false;
+    input_record.frame = NULL;
+    free_input_buffers_.push_back(i);
+  }
+  input_buffer_queued_count_ = 0;
+
   for (auto& output_buffer : output_buffer_map_)
     output_buffer.at_device = false;
   output_buffer_queued_count_ = 0;
--- a/media/gpu/v4l2/v4l2_image_processor.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_image_processor.h	2019-05-17 18:53:34.264000000 +0300
@@ -18,7 +18,6 @@
 #include "base/files/scoped_file.h"
 #include "base/macros.h"
 #include "base/memory/scoped_refptr.h"
-#include "base/sequence_checker.h"
 #include "base/task/cancelable_task_tracker.h"
 #include "base/threading/thread.h"
 #include "base/threading/thread_checker.h"
@@ -76,6 +75,15 @@
       ErrorCB error_cb);
 
  private:
+  // Record for input buffers.
+  struct InputRecord {
+    InputRecord();
+    InputRecord(const V4L2ImageProcessor::InputRecord&);
+    ~InputRecord();
+    scoped_refptr<VideoFrame> frame;
+    bool at_device;
+  };
+
   // Record for output buffers.
   struct OutputRecord {
     OutputRecord();
@@ -98,9 +106,9 @@
     ~JobRecord();
     scoped_refptr<VideoFrame> input_frame;
     int output_buffer_index;
+    scoped_refptr<VideoFrame> output_frame;
     std::vector<base::ScopedFD> output_dmabuf_fds;
     FrameReadyCB ready_cb;
-    LegacyFrameReadyCB legacy_ready_cb;
   };
 
   V4L2ImageProcessor(scoped_refptr<V4L2Device> device,
@@ -117,10 +125,10 @@
                      ErrorCB error_cb);
 
   bool Initialize();
-  void EnqueueInput(const JobRecord* job_record);
+  void EnqueueInput();
   void EnqueueOutput(const JobRecord* job_record);
   void Dequeue();
-  bool EnqueueInputRecord(const JobRecord* job_record);
+  bool EnqueueInputRecord();
   bool EnqueueOutputRecord(const JobRecord* job_record);
   bool CreateInputBuffers();
   bool CreateOutputBuffers();
@@ -133,19 +141,14 @@
   bool ProcessInternal(scoped_refptr<VideoFrame> frame,
                        int output_buffer_index,
                        std::vector<base::ScopedFD> output_dmabuf_fds,
-                       LegacyFrameReadyCB cb) override;
+                       FrameReadyCB cb) override;
   bool ProcessInternal(scoped_refptr<VideoFrame> input_frame,
                        scoped_refptr<VideoFrame> output_frame,
                        FrameReadyCB cb) override;
 
   void ProcessTask(std::unique_ptr<JobRecord> job_record);
-  void ProcessJobsTask();
   void ServiceDeviceTask();
 
-  // Allocate/Destroy the input/output V4L2 buffers.
-  void AllocateBuffersTask(bool* result, base::WaitableEvent* done);
-  void DestroyBuffersTask();
-
   // Attempt to start/stop device_poll_thread_.
   void StartDevicePoll();
   void StopDevicePoll();
@@ -182,12 +185,20 @@
 
   // All the below members are to be accessed from device_thread_ only
   // (if it's running).
-  base::queue<std::unique_ptr<JobRecord>> input_job_queue_;
+  base::queue<std::unique_ptr<JobRecord>> input_queue_;
   base::queue<std::unique_ptr<JobRecord>> running_jobs_;
 
-  scoped_refptr<V4L2Queue> input_queue_;
-  scoped_refptr<V4L2Queue> output_queue_;
+  // Input queue state.
+  bool input_streamon_;
+  // Number of input buffers enqueued to the device.
+  int input_buffer_queued_count_;
+  // Input buffers ready to use; LIFO since we don't care about ordering.
+  std::vector<int> free_input_buffers_;
+  // Mapping of int index to an input buffer record.
+  std::vector<InputRecord> input_buffer_map_;
 
+  // Output queue state.
+  bool output_streamon_;
   // Number of output buffers enqueued to the device.
   int output_buffer_queued_count_;
   // Mapping of int index to an output buffer record.
@@ -198,10 +209,8 @@
   // Error callback to the client.
   ErrorCB error_cb_;
 
-  // Checker for the sequence that creates this V4L2ImageProcessor.
-  SEQUENCE_CHECKER(client_sequence_checker_);
-  // Checker for the device thread owned by this V4L2ImageProcessor.
-  THREAD_CHECKER(device_thread_checker_);
+  // Checker for the thread that creates this V4L2ImageProcessor.
+  THREAD_CHECKER(client_thread_checker_);
 
   DISALLOW_COPY_AND_ASSIGN(V4L2ImageProcessor);
 };
--- a/media/gpu/v4l2/v4l2.sig	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2.sig	2019-05-17 18:53:34.264000000 +0300
@@ -8,3 +8,5 @@
 LIBV4L_PUBLIC int v4l2_close(int fd);
 LIBV4L_PUBLIC int v4l2_ioctl(int fd, unsigned long int request, ...);
 LIBV4L_PUBLIC int v4l2_fd_open(int fd, int v4l2_flags);
+LIBV4L_PUBLIC void *v4l2_mmap(void *start, size_t length, int prot, int flags, int fd, int64_t offset);
+LIBV4L_PUBLIC int v4l2_munmap(void *_start, size_t length);
--- a/media/gpu/v4l2/v4l2_slice_video_decode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_slice_video_decode_accelerator.cc	2019-05-17 18:53:34.268000000 +0300
@@ -28,7 +28,6 @@
 #include "base/threading/thread_task_runner_handle.h"
 #include "base/time/time.h"
 #include "base/trace_event/memory_dump_manager.h"
-#include "base/trace_event/trace_event.h"
 #include "media/base/bind_to_current_loop.h"
 #include "media/base/media_switches.h"
 #include "media/base/unaligned_shared_memory.h"
@@ -126,9 +125,8 @@
     DVLOGF(5) << "returning input_id: " << input_id;
     client_task_runner->PostTask(
         FROM_HERE,
-        base::BindOnce(
-            &VideoDecodeAccelerator::Client::NotifyEndOfBitstreamBuffer, client,
-            input_id));
+        base::Bind(&VideoDecodeAccelerator::Client::NotifyEndOfBitstreamBuffer,
+                   client, input_id));
   }
 }
 
@@ -316,7 +314,6 @@
   VLOGF(2);
   DCHECK(decoder_thread_task_runner_->BelongsToCurrentThread());
   DCHECK_EQ(state_, kInitialized);
-  TRACE_EVENT0("media,gpu", "V4L2SVDA::InitializeTask");
 
   if (IsDestroyPending())
     return;
@@ -353,7 +350,6 @@
 void V4L2SliceVideoDecodeAccelerator::DestroyTask() {
   DVLOGF(2);
   DCHECK(decoder_thread_task_runner_->BelongsToCurrentThread());
-  TRACE_EVENT0("media,gpu", "V4L2SVDA::DestroyTask");
 
   state_ = kDestroying;
 
@@ -541,7 +537,7 @@
 
   child_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&VideoDecodeAccelerator::Client::ProvidePictureBuffers,
+      base::Bind(&VideoDecodeAccelerator::Client::ProvidePictureBuffers,
                      client_, num_pictures, pixel_format, 1, coded_size_,
                      device_->GetTextureTarget()));
 
@@ -599,7 +595,7 @@
 void V4L2SliceVideoDecodeAccelerator::DevicePollTask(bool poll_device) {
   DVLOGF(3);
   DCHECK(device_poll_thread_.task_runner()->BelongsToCurrentThread());
-  TRACE_EVENT0("media,gpu", "V4L2SVDA::DevicePollTask");
+
   bool event_pending;
   if (!device_->Poll(poll_device, &event_pending)) {
     NOTIFY_ERROR(PLATFORM_FAILURE);
@@ -885,10 +882,6 @@
   DCHECK_NE(output_record.picture_id, -1);
 
   if (output_record.egl_fence) {
-    TRACE_EVENT0("media,gpu",
-                 "V4L2SVDA::EnqueueOutputRecord: "
-                 "GLFenceEGL::ClientWaitWithTimeoutNanos");
-
     // If we have to wait for completion, wait. Note that free_output_buffers_
     // is a FIFO queue, so we always wait on the buffer that has been in the
     // queue the longest. Every 100ms we check whether the decoder is shutting
@@ -1087,9 +1080,6 @@
 
   decoder_input_queue_.push_back(std::move(bitstream_record));
 
-  TRACE_COUNTER_ID1("media,gpu", "V4L2SVDA decoder input BitstreamBuffers",
-                    this, decoder_input_queue_.size());
-
   ScheduleDecodeBufferTaskIfNeeded();
 }
 
@@ -1123,7 +1113,7 @@
   if (state_ == kDecoding) {
     decoder_thread_task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&V4L2SliceVideoDecodeAccelerator::DecodeBufferTask,
+        base::Bind(&V4L2SliceVideoDecodeAccelerator::DecodeBufferTask,
                        base::Unretained(this)));
   }
 }
@@ -1131,7 +1121,6 @@
 void V4L2SliceVideoDecodeAccelerator::DecodeBufferTask() {
   DVLOGF(4);
   DCHECK(decoder_thread_task_runner_->BelongsToCurrentThread());
-  TRACE_EVENT0("media,gpu", "V4L2SVDA::DecodeBufferTask");
 
   if (IsDestroyPending())
     return;
@@ -1142,9 +1131,8 @@
   }
 
   while (true) {
-    TRACE_EVENT_BEGIN0("media,gpu", "V4L2SVDA::DecodeBufferTask AVD::Decode");
-    const AcceleratedVideoDecoder::DecodeResult res = decoder_->Decode();
-    TRACE_EVENT_END0("media,gpu", "V4L2SVDA::DecodeBufferTask AVD::Decode");
+    AcceleratedVideoDecoder::DecodeResult res;
+    res = decoder_->Decode();
     switch (res) {
       case AcceleratedVideoDecoder::kAllocateNewSurfaces:
         VLOGF(2) << "Decoder requesting a new set of surfaces";
@@ -1186,7 +1174,7 @@
   VLOGF(2);
   DCHECK(decoder_thread_task_runner_->BelongsToCurrentThread());
   DCHECK_EQ(state_, kDecoding);
-  TRACE_EVENT_ASYNC_BEGIN0("media,gpu", "V4L2SVDA Resolution Change", this);
+
   DCHECK(!surface_set_change_pending_);
   surface_set_change_pending_ = true;
   NewEventPending();
@@ -1238,7 +1226,6 @@
 
   surface_set_change_pending_ = false;
   VLOGF(2) << "Surface set change finished";
-  TRACE_EVENT_ASYNC_END0("media,gpu", "V4L2SVDA Resolution Change", this);
   return true;
 }
 
@@ -1323,7 +1310,7 @@
 
   decoder_thread_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&V4L2SliceVideoDecodeAccelerator::AssignPictureBuffersTask,
+      base::Bind(&V4L2SliceVideoDecodeAccelerator::AssignPictureBuffersTask,
                      base::Unretained(this), buffers));
 }
 
@@ -1332,8 +1319,6 @@
   VLOGF(2);
   DCHECK(decoder_thread_task_runner_->BelongsToCurrentThread());
   DCHECK_EQ(state_, kAwaitingPictureBuffers);
-  TRACE_EVENT1("media,gpu", "V4L2SVDA::AssignPictureBuffersTask",
-               "buffers_size", buffers.size());
 
   if (IsDestroyPending())
     return;
@@ -1428,8 +1413,6 @@
   DVLOGF(3) << "index=" << buffer_index;
   DCHECK(child_task_runner_->BelongsToCurrentThread());
   DCHECK_NE(texture_id, 0u);
-  TRACE_EVENT1("media,gpu", "V4L2SVDA::CreateGLImageFor", "picture_buffer_id",
-               picture_buffer_id);
 
   if (!make_context_current_cb_) {
     VLOGF(1) << "GL callbacks required for binding to GLImages";
@@ -1509,29 +1492,11 @@
   DVLOGF(3) << "picture_buffer_id=" << picture_buffer_id;
   DCHECK(child_task_runner_->BelongsToCurrentThread());
 
-  std::vector<base::ScopedFD> dmabuf_fds;
+  std::vector<base::ScopedFD> passed_dmabuf_fds;
 #if defined(USE_OZONE)
-  DCHECK_EQ(gpu_memory_buffer_handle.native_pixmap_handle.fds.size(),
-            gpu_memory_buffer_handle.native_pixmap_handle.planes.size());
-  // If the driver does not accept as many fds as we received from the client,
-  // we have to check if the additional fds are actually duplicated fds pointing
-  // to previous planes; if so, we can close the duplicates and keep only the
-  // original fd(s).
-  // Assume that an fd is a duplicate of a previous plane's fd if offset != 0.
-  // Otherwise, if offset == 0, return error as it may be pointing to a new
-  // plane.
-  for (auto& fd : gpu_memory_buffer_handle.native_pixmap_handle.fds) {
-    dmabuf_fds.emplace_back(fd.fd);
-  }
-  for (size_t i = dmabuf_fds.size() - 1; i >= output_planes_count_; i--) {
-    if (gpu_memory_buffer_handle.native_pixmap_handle.planes[i].offset == 0) {
-      VLOGF(1) << "The dmabuf fd points to a new buffer, ";
-      NOTIFY_ERROR(INVALID_ARGUMENT);
-      return;
-    }
-    // Drop safely, because this fd is duplicate dmabuf fd pointing to previous
-    // buffer and the appropriate address can be accessed by associated offset.
-    dmabuf_fds.pop_back();
+  for (const auto& fd : gpu_memory_buffer_handle.native_pixmap_handle.fds) {
+    DCHECK_NE(fd.fd, -1);
+    passed_dmabuf_fds.push_back(base::ScopedFD(fd.fd));
   }
 #endif
 
@@ -1553,7 +1518,8 @@
       FROM_HERE,
       base::BindOnce(
           &V4L2SliceVideoDecodeAccelerator::ImportBufferForPictureTask,
-          base::Unretained(this), picture_buffer_id, std::move(dmabuf_fds)));
+          base::Unretained(this), picture_buffer_id,
+          std::move(passed_dmabuf_fds)));
 }
 
 void V4L2SliceVideoDecodeAccelerator::ImportBufferForPictureTask(
@@ -1633,9 +1599,9 @@
 
   decoder_thread_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&V4L2SliceVideoDecodeAccelerator::ReusePictureBufferTask,
+      base::Bind(&V4L2SliceVideoDecodeAccelerator::ReusePictureBufferTask,
                      base::Unretained(this), picture_buffer_id,
-                     std::move(egl_fence)));
+                 base::Passed(&egl_fence)));
 }
 
 void V4L2SliceVideoDecodeAccelerator::ReusePictureBufferTask(
@@ -1706,7 +1672,6 @@
 void V4L2SliceVideoDecodeAccelerator::InitiateFlush() {
   VLOGF(2);
   DCHECK(decoder_thread_task_runner_->BelongsToCurrentThread());
-  TRACE_EVENT_ASYNC_BEGIN0("media,gpu", "V4L2SVDA Flush", this);
 
   // This will trigger output for all remaining surfaces in the decoder.
   // However, not all of them may be decoded yet (they would be queued
@@ -1754,10 +1719,9 @@
   decoder_flushing_ = false;
   VLOGF(2) << "Flush finished";
 
-  child_task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&Client::NotifyFlushDone, client_));
+  child_task_runner_->PostTask(FROM_HERE,
+                               base::Bind(&Client::NotifyFlushDone, client_));
 
-  TRACE_EVENT_ASYNC_END0("media,gpu", "V4L2SVDA Flush", this);
   return true;
 }
 
@@ -1773,7 +1737,6 @@
 void V4L2SliceVideoDecodeAccelerator::ResetTask() {
   VLOGF(2);
   DCHECK(decoder_thread_task_runner_->BelongsToCurrentThread());
-  TRACE_EVENT_ASYNC_BEGIN0("media,gpu", "V4L2SVDA Reset", this);
 
   if (IsDestroyPending())
     return;
@@ -1831,10 +1794,9 @@
   decoder_resetting_ = false;
   VLOGF(2) << "Reset finished";
 
-  child_task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&Client::NotifyResetDone, client_));
+  child_task_runner_->PostTask(FROM_HERE,
+                               base::Bind(&Client::NotifyResetDone, client_));
 
-  TRACE_EVENT_ASYNC_END0("media,gpu", "V4L2SVDA Reset", this);
   return true;
 }
 
@@ -1969,15 +1931,6 @@
 V4L2SliceVideoDecodeAccelerator::CreateSurface() {
   DCHECK(decoder_thread_task_runner_->BelongsToCurrentThread());
   DCHECK_EQ(state_, kDecoding);
-  TRACE_COUNTER_ID2("media,gpu", "V4L2 input buffers", this, "free",
-                    free_input_buffers_.size(), "in use",
-                    input_buffer_map_.size() - free_input_buffers_.size());
-  TRACE_COUNTER_ID2("media,gpu", "V4L2 output buffers", this, "free",
-                    free_output_buffers_.size(), "in use",
-                    output_buffer_map_.size() - free_output_buffers_.size());
-  TRACE_COUNTER_ID2("media,gpu", "V4L2 output buffers", this, "at client",
-                    GetNumOfOutputRecordsAtClient(), "at device",
-                    GetNumOfOutputRecordsAtDevice());
 
   if (free_input_buffers_.empty() || free_output_buffers_.empty())
     return nullptr;
@@ -1996,7 +1949,7 @@
   scoped_refptr<V4L2DecodeSurface> dec_surface =
       new V4L2ConfigStoreDecodeSurface(
           input, output,
-          base::BindOnce(&V4L2SliceVideoDecodeAccelerator::ReuseOutputBuffer,
+          base::Bind(&V4L2SliceVideoDecodeAccelerator::ReuseOutputBuffer,
                          base::Unretained(this)));
 
   DVLOGF(4) << "Created surface " << input << " -> " << output;
@@ -2019,7 +1972,7 @@
       // all pictures are cleared at the beginning.
       decode_task_runner_->PostTask(
           FROM_HERE,
-          base::BindOnce(&Client::PictureReady, decode_client_, picture));
+          base::Bind(&Client::PictureReady, decode_client_, picture));
       pending_picture_ready_.pop();
     } else if (!cleared || send_now) {
       DVLOGF(4) << "cleared=" << pending_picture_ready_.front().cleared
@@ -2039,7 +1992,7 @@
           FROM_HERE, base::BindOnce(&Client::PictureReady, client_, picture),
           // Unretained is safe. If Client::PictureReady gets to run, |this| is
           // alive. Destroy() will wait the decode thread to finish.
-          base::BindOnce(&V4L2SliceVideoDecodeAccelerator::PictureCleared,
+          base::Bind(&V4L2SliceVideoDecodeAccelerator::PictureCleared,
                          base::Unretained(this)));
       picture_clearing_count_++;
       pending_picture_ready_.pop();
@@ -2079,18 +2032,6 @@
       base::size(supported_input_fourccs_), supported_input_fourccs_);
 }
 
-size_t V4L2SliceVideoDecodeAccelerator::GetNumOfOutputRecordsAtDevice() const {
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-  return std::count_if(output_buffer_map_.begin(), output_buffer_map_.end(),
-                       [](const auto& r) { return r.at_device; });
-}
-
-size_t V4L2SliceVideoDecodeAccelerator::GetNumOfOutputRecordsAtClient() const {
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-  return std::count_if(output_buffer_map_.begin(), output_buffer_map_.end(),
-                       [](const auto& r) { return r.at_client; });
-}
-
 // base::trace_event::MemoryDumpProvider implementation.
 bool V4L2SliceVideoDecodeAccelerator::OnMemoryDump(
     const base::trace_event::MemoryDumpArgs& args,
--- a/media/gpu/v4l2/v4l2_slice_video_decode_accelerator.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_slice_video_decode_accelerator.h	2019-05-17 18:53:34.268000000 +0300
@@ -343,11 +343,6 @@
   // Callback that indicates a picture has been cleared.
   void PictureCleared();
 
-  // Returns the number of OutputRecords at client/device. This is used to
-  // compute values reported for chrome://tracing.
-  size_t GetNumOfOutputRecordsAtClient() const;
-  size_t GetNumOfOutputRecordsAtDevice() const;
-
   size_t input_planes_count_;
   size_t output_planes_count_;
 
--- a/media/gpu/v4l2/v4l2_video_decode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_video_decode_accelerator.cc	2019-05-17 18:53:34.268000000 +0300
@@ -33,10 +33,12 @@
 #include "media/base/video_types.h"
 #include "media/gpu/image_processor_factory.h"
 #include "media/gpu/macros.h"
+#include "media/gpu/buildflags.h"
 #include "media/gpu/v4l2/v4l2_image_processor.h"
 #include "media/video/h264_parser.h"
 #include "ui/gfx/geometry/rect.h"
 #include "ui/gl/gl_context.h"
+#include "ui/gl/gl_fence_egl.h"
 #include "ui/gl/scoped_binders.h"
 
 #define NOTIFY_ERROR(x)                      \
@@ -82,7 +84,10 @@
 
 // static
 const uint32_t V4L2VideoDecodeAccelerator::supported_input_fourccs_[] = {
-    V4L2_PIX_FMT_H264, V4L2_PIX_FMT_VP8, V4L2_PIX_FMT_VP9,
+    V4L2_PIX_FMT_H264, V4L2_PIX_FMT_VP8,
+#if !BUILDFLAG(USE_LINUX_V4L2)
+    V4L2_PIX_FMT_VP9,
+#endif
 };
 
 struct V4L2VideoDecodeAccelerator::BitstreamBufferRef {
@@ -115,12 +120,13 @@
   if (input_id >= 0) {
     client_task_runner->PostTask(
         FROM_HERE,
-        base::BindOnce(&Client::NotifyEndOfBitstreamBuffer, client, input_id));
+        base::Bind(&Client::NotifyEndOfBitstreamBuffer, client, input_id));
   }
 }
 
 V4L2VideoDecodeAccelerator::OutputRecord::OutputRecord()
-    : egl_image(EGL_NO_IMAGE_KHR),
+    : state(kFree),
+      egl_image(EGL_NO_IMAGE_KHR),
       picture_id(-1),
       texture_id(0),
       cleared(false) {}
@@ -148,6 +154,7 @@
       device_(device),
       decoder_delay_bitstream_buffer_id_(-1),
       decoder_decode_buffer_tasks_scheduled_(0),
+      decoder_frames_at_client_(0),
       decoder_flushing_(false),
       decoder_cmd_supported_(false),
       flush_awaiting_last_output_buffer_(false),
@@ -233,71 +240,6 @@
     DVLOGF(2) << "No GL callbacks provided, initializing without GL support";
   }
 
-  decoder_state_ = kInitialized;
-
-  if (!decoder_thread_.Start()) {
-    VLOGF(1) << "decoder thread failed to start";
-    return false;
-  }
-
-  bool result = false;
-  base::WaitableEvent done;
-  decoder_thread_.task_runner()->PostTask(
-      FROM_HERE,
-      base::BindOnce(&V4L2VideoDecodeAccelerator::InitializeTask,
-                     base::Unretained(this), config, &result, &done));
-  done.Wait();
-
-  return result;
-}
-
-void V4L2VideoDecodeAccelerator::InitializeTask(const Config& config,
-                                                bool* result,
-                                                base::WaitableEvent* done) {
-  VLOGF(2);
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-  DCHECK_NE(result, nullptr);
-  DCHECK_NE(done, nullptr);
-  DCHECK_EQ(decoder_state_, kInitialized);
-  TRACE_EVENT0("media,gpu", "V4L2VDA::InitializeTask");
-
-  // The client can keep going as soon as the configuration is checked.
-  // Store the result to the local value to see the result even after |*result|
-  // is released.
-  bool config_result = CheckConfig(config);
-  *result = config_result;
-  done->Signal();
-
-  // No need to keep going is configuration is not supported.
-  if (!config_result)
-    return;
-
-  if (video_profile_ >= H264PROFILE_MIN && video_profile_ <= H264PROFILE_MAX) {
-    decoder_h264_parser_.reset(new H264Parser());
-  }
-
-  base::trace_event::MemoryDumpManager::GetInstance()->RegisterDumpProvider(
-      this, "media::V4l2VideoDecodeAccelerator", decoder_thread_.task_runner());
-
-  // Subscribe to the resolution change event.
-  struct v4l2_event_subscription sub;
-  memset(&sub, 0, sizeof(sub));
-  sub.type = V4L2_EVENT_SOURCE_CHANGE;
-  IOCTL_OR_ERROR_RETURN(VIDIOC_SUBSCRIBE_EVENT, &sub);
-
-  if (!CreateInputBuffers()) {
-    NOTIFY_ERROR(PLATFORM_FAILURE);
-    return;
-  }
-
-  decoder_cmd_supported_ = IsDecoderCmdSupported();
-
-  StartDevicePoll();
-}
-
-bool V4L2VideoDecodeAccelerator::CheckConfig(const Config& config) {
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-
   input_format_fourcc_ =
       V4L2Device::VideoCodecProfileToV4L2PixFmt(video_profile_, false);
 
@@ -330,11 +272,53 @@
   if (!SetupFormats())
     return false;
 
-  // We have confirmed that |config| is supported, tell the good news to the
-  // client.
+  if (video_profile_ >= H264PROFILE_MIN && video_profile_ <= H264PROFILE_MAX) {
+    decoder_h264_parser_.reset(new H264Parser());
+  }
+
+  if (!decoder_thread_.Start()) {
+    VLOGF(1) << "decoder thread failed to start";
+    return false;
+  }
+
+  decoder_state_ = kInitialized;
+
+  base::trace_event::MemoryDumpManager::GetInstance()->RegisterDumpProvider(
+      this, "media::V4l2VideoDecodeAccelerator", decoder_thread_.task_runner());
+
+  // InitializeTask will NOTIFY_ERROR on failure.
+  decoder_thread_.task_runner()->PostTask(
+      FROM_HERE, base::BindOnce(&V4L2VideoDecodeAccelerator::InitializeTask,
+                                base::Unretained(this)));
+
   return true;
 }
 
+void V4L2VideoDecodeAccelerator::InitializeTask() {
+  VLOGF(2);
+  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
+  DCHECK_EQ(decoder_state_, kInitialized);
+
+  if (IsDestroyPending())
+    return;
+
+  // Subscribe to the resolution change event.
+  struct v4l2_event_subscription sub;
+  memset(&sub, 0, sizeof(sub));
+  sub.type = V4L2_EVENT_SOURCE_CHANGE;
+  IOCTL_OR_ERROR_RETURN(VIDIOC_SUBSCRIBE_EVENT, &sub);
+
+  if (!CreateInputBuffers()) {
+    NOTIFY_ERROR(PLATFORM_FAILURE);
+    return;
+  }
+
+  decoder_cmd_supported_ = IsDecoderCmdSupported();
+
+  if (!StartDevicePoll())
+    return;
+}
+
 void V4L2VideoDecodeAccelerator::Decode(
     const BitstreamBuffer& bitstream_buffer) {
   Decode(bitstream_buffer.ToDecoderBuffer(), bitstream_buffer.id());
@@ -366,7 +350,7 @@
 
   decoder_thread_.task_runner()->PostTask(
       FROM_HERE,
-      base::BindOnce(&V4L2VideoDecodeAccelerator::AssignPictureBuffersTask,
+      base::Bind(&V4L2VideoDecodeAccelerator::AssignPictureBuffersTask,
                      base::Unretained(this), buffers));
 }
 
@@ -376,8 +360,6 @@
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
   DCHECK_EQ(decoder_state_, kAwaitingPictureBuffers);
   DCHECK(output_queue_);
-  TRACE_EVENT1("media,gpu", "V4L2VDA::AssignPictureBuffersTask", "buffers_size",
-               buffers.size());
 
   if (IsDestroyPending())
     return;
@@ -425,7 +407,6 @@
     DCHECK(buffer.IsValid());
     int i = buffer.BufferId();
 
-    DCHECK_EQ(output_wait_map_.count(buffers[i].id()), 0u);
     output_wait_map_.emplace(buffers[i].id(), std::move(buffer));
   }
 
@@ -433,7 +414,9 @@
     DCHECK(buffers[i].size() == egl_image_size_);
 
     OutputRecord& output_record = output_buffer_map_[i];
+    DCHECK_EQ(output_record.state, kFree);
     DCHECK_EQ(output_record.egl_image, EGL_NO_IMAGE_KHR);
+    DCHECK(!output_record.egl_fence);
     DCHECK_EQ(output_record.picture_id, -1);
     DCHECK(!output_record.cleared);
     DCHECK(output_record.processor_input_fds.empty());
@@ -443,6 +426,10 @@
                                    ? 0
                                    : buffers[i].service_texture_ids()[0];
 
+    // This will remain kAtClient until ImportBufferForPicture is called, either
+    // by the client, or by ourselves, if we are allocating.
+    output_record.state = kAtClient;
+
     if (image_processor_device_) {
       std::vector<base::ScopedFD> dmabuf_fds = device_->GetDmabufsForV4L2Buffer(
           i, output_planes_count_, V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE);
@@ -516,7 +503,7 @@
   }
 
   decoder_thread_.task_runner()->PostTask(
-      FROM_HERE, base::BindOnce(&V4L2VideoDecodeAccelerator::AssignEGLImage,
+      FROM_HERE, base::Bind(&V4L2VideoDecodeAccelerator::AssignEGLImage,
                                 base::Unretained(this), buffer_index,
                                 picture_buffer_id, egl_image));
 }
@@ -549,13 +536,13 @@
 
   OutputRecord& output_record = output_buffer_map_[buffer_index];
   DCHECK_EQ(output_record.egl_image, EGL_NO_IMAGE_KHR);
+  DCHECK(!output_record.egl_fence);
 
   output_record.egl_image = egl_image;
 
   // Make ourselves available if CreateEGLImageFor has been called from
   // ImportBufferForPictureTask.
   if (!image_processor_) {
-    DCHECK_EQ(output_wait_map_.count(picture_buffer_id), 1u);
     output_wait_map_.erase(picture_buffer_id);
     if (decoder_state_ != kChangingResolution) {
       Enqueue();
@@ -570,39 +557,13 @@
     const gfx::GpuMemoryBufferHandle& gpu_memory_buffer_handle) {
   DVLOGF(3) << "picture_buffer_id=" << picture_buffer_id;
   DCHECK(child_task_runner_->BelongsToCurrentThread());
+
   if (output_mode_ != Config::OutputMode::IMPORT) {
     VLOGF(1) << "Cannot import in non-import mode";
     NOTIFY_ERROR(INVALID_ARGUMENT);
     return;
   }
 
-  std::vector<base::ScopedFD> dmabuf_fds;
-  std::vector<gfx::NativePixmapPlane> planes;
-#if defined(USE_OZONE)
-  DCHECK_EQ(gpu_memory_buffer_handle.native_pixmap_handle.fds.size(),
-            gpu_memory_buffer_handle.native_pixmap_handle.planes.size());
-
-  for (auto& fd : gpu_memory_buffer_handle.native_pixmap_handle.fds) {
-    dmabuf_fds.emplace_back(fd.fd);
-  }
-
-  planes = gpu_memory_buffer_handle.native_pixmap_handle.planes;
-#endif
-
-  decoder_thread_.task_runner()->PostTask(
-      FROM_HERE,
-      base::BindOnce(
-          &V4L2VideoDecodeAccelerator::ImportBufferForPictureForImportTask,
-          base::Unretained(this), picture_buffer_id, pixel_format,
-          std::move(dmabuf_fds), std::move(planes)));
-}
-
-void V4L2VideoDecodeAccelerator::ImportBufferForPictureForImportTask(
-    int32_t picture_buffer_id,
-    VideoPixelFormat pixel_format,
-    std::vector<base::ScopedFD> dmabuf_fds,
-    std::vector<gfx::NativePixmapPlane> planes) {
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
   // |output_format_fourcc_| is the output format of the decoder. It is not
   // the final output format from the image processor (if exists).
   // Use |egl_image_format_fourcc_|, it will be the final output format.
@@ -613,30 +574,25 @@
     return;
   }
 
-  // If the driver does not accept as many fds as we received from the client,
-  // we have to check if the additional fds are actually duplicated fds pointing
-  // to previous planes; if so, we can close the duplicates and keep only the
-  // original fd(s).
-  // Assume that an fd is a duplicate of a previous plane's fd if offset != 0.
-  // Otherwise, if offset == 0, return error as it may be pointing to a new
-  // plane.
-  for (size_t i = dmabuf_fds.size() - 1; i >= egl_image_planes_count_; i--) {
-    if (planes[i].offset == 0) {
-      VLOGF(1) << "The dmabuf fd points to a new buffer, ";
-      NOTIFY_ERROR(INVALID_ARGUMENT);
-      return;
-    }
-    // Drop safely, because this fd is duplicate dmabuf fd pointing to previous
-    // buffer and the appropriate address can be accessed by associated offset.
-    dmabuf_fds.pop_back();
-  }
-
-  for (const auto& plane : planes) {
+  std::vector<base::ScopedFD> dmabuf_fds;
+  int32_t stride = 0;
+#if defined(USE_OZONE)
+  for (const auto& fd : gpu_memory_buffer_handle.native_pixmap_handle.fds) {
+    DCHECK_NE(fd.fd, -1);
+    dmabuf_fds.push_back(base::ScopedFD(fd.fd));
+  }
+  stride = gpu_memory_buffer_handle.native_pixmap_handle.planes[0].stride;
+  for (const auto& plane :
+       gpu_memory_buffer_handle.native_pixmap_handle.planes) {
     DVLOGF(3) << ": offset=" << plane.offset << ", stride=" << plane.stride;
   }
+#endif
 
-  ImportBufferForPictureTask(picture_buffer_id, std::move(dmabuf_fds),
-                             planes[0].stride);
+  decoder_thread_.task_runner()->PostTask(
+      FROM_HERE,
+      base::Bind(&V4L2VideoDecodeAccelerator::ImportBufferForPictureTask,
+                 base::Unretained(this), picture_buffer_id,
+                 base::Passed(&dmabuf_fds), stride));
 }
 
 void V4L2VideoDecodeAccelerator::ImportBufferForPictureTask(
@@ -647,9 +603,6 @@
             << ", dmabuf_fds.size()=" << dmabuf_fds.size()
             << ", stride=" << stride;
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-  TRACE_EVENT2("media,gpu", "V4L2VDA::ImportBufferForPictureTask",
-               "picture_buffer_id", picture_buffer_id, "dmabuf_fds_size",
-               dmabuf_fds.size());
 
   if (IsDestroyPending())
     return;
@@ -669,6 +622,12 @@
     return;
   }
 
+  if (iter->state != kAtClient) {
+    VLOGF(1) << "Cannot import buffer not owned by client";
+    NOTIFY_ERROR(INVALID_ARGUMENT);
+    return;
+  }
+
   int plane_horiz_bits_per_pixel = VideoFrame::PlaneHorizontalBitsPerPixel(
       V4L2Device::V4L2PixFmtToVideoPixelFormat(egl_image_format_fourcc_), 0);
   if (plane_horiz_bits_per_pixel == 0 ||
@@ -709,12 +668,13 @@
     iter->output_fds = DuplicateFDs(dmabuf_fds);
   }
 
+  iter->state = kFree;
   if (iter->texture_id != 0) {
     if (iter->egl_image != EGL_NO_IMAGE_KHR) {
       child_task_runner_->PostTask(
           FROM_HERE,
-          base::BindOnce(base::IgnoreResult(&V4L2Device::DestroyEGLImage),
-                         device_, egl_display_, iter->egl_image));
+          base::Bind(base::IgnoreResult(&V4L2Device::DestroyEGLImage), device_,
+                     egl_display_, iter->egl_image));
     }
 
     size_t index = iter - output_buffer_map_.begin();
@@ -727,7 +687,7 @@
           FROM_HERE,
           base::BindOnce(&V4L2VideoDecodeAccelerator::CreateEGLImageFor,
                          weak_this_, index, picture_buffer_id,
-                         std::move(dmabuf_fds), iter->texture_id,
+                         base::Passed(&dmabuf_fds), iter->texture_id,
                          egl_image_size_, egl_image_format_fourcc_));
 
       // Early return, AssignEGLImage will make the buffer available for
@@ -737,7 +697,6 @@
   }
 
   // The buffer can now be used for decoding
-  DCHECK_EQ(output_wait_map_.count(picture_buffer_id), 1u);
   output_wait_map_.erase(picture_buffer_id);
   if (decoder_state_ != kChangingResolution) {
     Enqueue();
@@ -774,7 +733,7 @@
       FROM_HERE,
       base::BindOnce(&V4L2VideoDecodeAccelerator::ReusePictureBufferTask,
                      base::Unretained(this), picture_buffer_id,
-                     std::move(egl_fence)));
+                     base::Passed(&egl_fence)));
 }
 
 void V4L2VideoDecodeAccelerator::Flush() {
@@ -812,6 +771,9 @@
                                   base::Unretained(this)));
     // DestroyTask() will cause the decoder_thread_ to flush all tasks.
     decoder_thread_.Stop();
+  } else {
+    // Otherwise, call the destroy task directly.
+    DestroyTask();
   }
 
   delete this;
@@ -843,6 +805,7 @@
   DVLOGF(4) << "input_id=" << bitstream_id;
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
   DCHECK_NE(decoder_state_, kUninitialized);
+  TRACE_EVENT1("media,gpu", "V4L2VDA::DecodeTask", "input_id", bitstream_id);
 
   if (IsDestroyPending())
     return;
@@ -990,8 +953,6 @@
 bool V4L2VideoDecodeAccelerator::AdvanceFrameFragment(const uint8_t* data,
                                                       size_t size,
                                                       size_t* endpos) {
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-
   if (video_profile_ >= H264PROFILE_MIN && video_profile_ <= H264PROFILE_MAX) {
     // For H264, we need to feed HW one frame at a time.  This is going to take
     // some parsing of our input stream.
@@ -1236,9 +1197,6 @@
   // Enqueue once since there's new available input for it.
   Enqueue();
 
-  TRACE_COUNTER_ID1("media,gpu", "V4L2VDA input ready buffers", this,
-                    input_ready_queue_.size());
-
   return (decoder_state_ != kError);
 }
 
@@ -1246,14 +1204,13 @@
   DVLOGF(4);
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
   DCHECK_NE(decoder_state_, kUninitialized);
+  DCHECK(input_queue_);
+  DCHECK(output_queue_);
   TRACE_EVENT0("media,gpu", "V4L2VDA::ServiceDeviceTask");
 
   if (IsDestroyPending())
     return;
 
-  DCHECK(input_queue_);
-  DCHECK(output_queue_);
-
   if (decoder_state_ == kResetting) {
     DVLOGF(3) << "early out: kResetting state";
     return;
@@ -1323,54 +1280,21 @@
             << output_queue_->FreeBuffersCount() << "+"
             << output_queue_->QueuedBuffersCount() << "/"
             << output_buffer_map_.size() << "] => PROCESSOR["
-            << buffers_at_ip_.size() << "] => CLIENT["
-            << buffers_at_client_.size() << "]";
+            << image_processor_bitstream_buffer_ids_.size() << "] => CLIENT["
+            << decoder_frames_at_client_ << "]";
 
   ScheduleDecodeBufferTaskIfNeeded();
   if (resolution_change_pending)
     StartResolutionChange();
 }
 
-void V4L2VideoDecodeAccelerator::CheckGLFences() {
-  DVLOGF(4);
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-
-  while (!buffers_awaiting_fence_.empty()) {
-    if (buffers_awaiting_fence_.front().first->HasCompleted()) {
-      // Buffer at the front of the queue goes back to V4L2Queue's free list
-      // and can be reused.
-      buffers_awaiting_fence_.pop();
-    } else {
-      // If we have no free buffers available, then preemptively schedule a
-      // call to Enqueue() in a short time, otherwise we may starve out of
-      // buffers. The delay chosen roughly corresponds to the time a frame is
-      // displayed, which should be optimal in most cases.
-      if (output_queue_->FreeBuffersCount() == 0) {
-        constexpr int64_t resched_delay = 17;
-
-        decoder_thread_.task_runner()->PostDelayedTask(
-            FROM_HERE,
-            base::BindOnce(&V4L2VideoDecodeAccelerator::Enqueue,
-                           base::Unretained(this)),
-            base::TimeDelta::FromMilliseconds(resched_delay));
-      }
-      break;
-    }
-  }
-}
-
 void V4L2VideoDecodeAccelerator::Enqueue() {
   DVLOGF(4);
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
   DCHECK_NE(decoder_state_, kUninitialized);
-
-  // Early return if we are running after DestroyTask() or a resolution change.
-  // This can happen due to the PostDelayedTask() in CheckGLFences().
-  if (IsDestroyPending() || decoder_state_ == kChangingResolution)
-    return;
-
   DCHECK(input_queue_);
   DCHECK(output_queue_);
+  TRACE_EVENT0("media,gpu", "V4L2VDA::Enqueue");
 
   // Drain the pipe of completed decode buffers.
   const int old_inputs_queued = input_queue_->QueuedBuffersCount();
@@ -1438,8 +1362,6 @@
 
   // Enqueue all the outputs we can.
   const int old_outputs_queued = output_queue_->QueuedBuffersCount();
-  // Release output buffers which GL fences have been signaled.
-  CheckGLFences();
   while (output_queue_->FreeBuffersCount() > 0) {
     if (!EnqueueOutputRecord())
       return;
@@ -1487,6 +1409,7 @@
   DCHECK_NE(decoder_state_, kUninitialized);
   DCHECK(input_queue_);
   DCHECK(output_queue_);
+  TRACE_EVENT0("media,gpu", "V4L2VDA::Dequeue");
 
   while (input_queue_->QueuedBuffersCount() > 0) {
     if (!DequeueInputBuffer())
@@ -1536,25 +1459,28 @@
     return false;
   }
 
-  V4L2ReadableBufferRef buf(std::move(ret.second));
+  V4L2ReadableBufferRef buf = ret.second;
 
   DCHECK_LT(buf->BufferId(), output_buffer_map_.size());
   OutputRecord& output_record = output_buffer_map_[buf->BufferId()];
+  DCHECK_EQ(output_record.state, kAtDevice);
   DCHECK_NE(output_record.picture_id, -1);
-  // Zero-bytes buffers are returned as part of a flush and can be dismissed.
-  if (buf->GetPlaneBytesUsed(0) > 0) {
+  if (buf->GetPlaneBytesUsed(0) == 0) {
+    // This is an empty output buffer returned as part of a flush.
+    output_record.state = kFree;
+  } else {
     int32_t bitstream_buffer_id = buf->GetTimeStamp().tv_sec;
     DCHECK_GE(bitstream_buffer_id, 0);
     DVLOGF(4) << "Dequeue output buffer: dqbuf index=" << buf->BufferId()
               << " bitstream input_id=" << bitstream_buffer_id;
     if (image_processor_device_) {
-      if (!ProcessFrame(bitstream_buffer_id, buf)) {
+      if (!ProcessFrame(bitstream_buffer_id, buf->BufferId())) {
         VLOGF(1) << "Processing frame failed";
         NOTIFY_ERROR(PLATFORM_FAILURE);
         return false;
       }
     } else {
-      SendBufferToClient(buf->BufferId(), bitstream_buffer_id, buf);
+      SendBufferToClient(buf->BufferId(), bitstream_buffer_id);
     }
   }
   if (buf->IsLast()) {
@@ -1569,6 +1495,12 @@
     }
   }
 
+  if (buf->GetPlaneBytesUsed(0) > 0) {
+    // Keep a reference to this buffer until the client returns it
+    DCHECK_EQ(buffers_at_client_.count(output_record.picture_id), 0u);
+    buffers_at_client_.emplace(output_record.picture_id, std::move(buf));
+  }
+
   return true;
 }
 
@@ -1595,7 +1527,36 @@
   DCHECK(buffer.IsValid());
 
   OutputRecord& output_record = output_buffer_map_[buffer.BufferId()];
+  DCHECK_EQ(output_record.state, kFree);
   DCHECK_NE(output_record.picture_id, -1);
+  if (output_record.egl_fence) {
+    TRACE_EVENT0(
+        "media,gpu",
+        "V4L2VDA::EnqueueOutputRecord: GLFenceEGL::ClientWaitWithTimeoutNanos");
+    // If we have to wait for completion, wait. Note that free_output_buffers_
+    // is a FIFO queue, so we always wait on the buffer that has been in the
+    // queue the longest. Every 100ms we check whether the decoder is shutting
+    // down, or we might get stuck waiting on a fence that will never come.
+    while (!IsDestroyPending()) {
+      const EGLTimeKHR wait_ns =
+          base::TimeDelta::FromMilliseconds(100).InNanoseconds();
+      EGLint result =
+          output_record.egl_fence->ClientWaitWithTimeoutNanos(wait_ns);
+      if (result == EGL_CONDITION_SATISFIED_KHR) {
+        break;
+      } else if (result == EGL_FALSE) {
+        // This will cause tearing, but is safe otherwise.
+        DVLOGF(1) << "GLFenceEGL::ClientWaitWithTimeoutNanos failed!";
+        break;
+      }
+      DCHECK_EQ(result, EGL_TIMEOUT_EXPIRED_KHR);
+    }
+
+    if (IsDestroyPending())
+      return false;
+
+    output_record.egl_fence.reset();
+  }
 
   bool ret = false;
   switch (buffer.Memory()) {
@@ -1615,6 +1576,7 @@
     return false;
   }
 
+  output_record.state = kAtDevice;
   return true;
 }
 
@@ -1623,6 +1585,7 @@
     std::unique_ptr<gl::GLFenceEGL> egl_fence) {
   DVLOGF(4) << "picture_buffer_id=" << picture_buffer_id;
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
+  TRACE_EVENT0("media,gpu", "V4L2VDA::ReusePictureBufferTask");
 
   if (IsDestroyPending())
     return;
@@ -1649,31 +1612,30 @@
               << " not in use (anymore?).";
     return;
   }
+  V4L2ReadableBufferRef buffer = std::move(iter->second);
+  buffers_at_client_.erase(iter);
 
-  // Take ownership of the EGL fence and keep the buffer out of the game until
-  // the fence signals.
-  if (egl_fence)
-    buffers_awaiting_fence_.emplace(
-        std::make_pair(std::move(egl_fence), std::move(iter->second)));
+  OutputRecord& output_record = output_buffer_map_[buffer->BufferId()];
+  if (output_record.state != kAtClient) {
+    VLOGF(1) << "picture_buffer_id not reusable";
+    NOTIFY_ERROR(INVALID_ARGUMENT);
+    return;
+  }
 
-  buffers_at_client_.erase(iter);
+  DCHECK(!output_record.egl_fence);
+  output_record.state = kFree;
+  decoder_frames_at_client_--;
+  // Take ownership of the EGL fence.
+  output_record.egl_fence = std::move(egl_fence);
 
   // We got a buffer back, so enqueue it back.
   Enqueue();
-
-  TRACE_COUNTER_ID2(
-      "media,gpu", "V4L2 output buffers", this, "in client",
-      buffers_at_client_.size(), "in vda",
-      output_buffer_map_.size() - buffers_at_client_.size());
-  TRACE_COUNTER_ID2(
-      "media,gpu", "V4L2 output buffers in vda", this, "free",
-      output_queue_->FreeBuffersCount(), "in device or IP",
-      output_queue_->QueuedBuffersCount() + buffers_at_ip_.size());
 }
 
 void V4L2VideoDecodeAccelerator::FlushTask() {
   VLOGF(2);
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
+  TRACE_EVENT0("media,gpu", "V4L2VDA::FlushTask");
 
   if (IsDestroyPending())
     return;
@@ -1683,8 +1645,6 @@
     return;
   }
 
-  TRACE_EVENT_ASYNC_BEGIN0("media,gpu", "V4L2VDA::FlushTask", this);
-
   // We don't support stacked flushing.
   DCHECK(!decoder_flushing_);
 
@@ -1724,7 +1684,7 @@
     DVLOGF(3) << "Some input buffers are not dequeued.";
     return;
   }
-  if (!buffers_at_ip_.empty()) {
+  if (image_processor_bitstream_buffer_ids_.size() != 0) {
     DVLOGF(3) << "Waiting for image processor to complete.";
     return;
   }
@@ -1754,12 +1714,11 @@
 }
 
 void V4L2VideoDecodeAccelerator::NofityFlushDone() {
-  TRACE_EVENT_ASYNC_END0("media,gpu", "V4L2VDA::FlushTask", this);
   decoder_delay_bitstream_buffer_id_ = -1;
   decoder_flushing_ = false;
   VLOGF(2) << "returning flush";
-  child_task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&Client::NotifyFlushDone, client_));
+  child_task_runner_->PostTask(FROM_HERE,
+                               base::Bind(&Client::NotifyFlushDone, client_));
 }
 
 bool V4L2VideoDecodeAccelerator::IsDecoderCmdSupported() {
@@ -1794,6 +1753,7 @@
 void V4L2VideoDecodeAccelerator::ResetTask() {
   VLOGF(2);
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
+  TRACE_EVENT0("media,gpu", "V4L2VDA::ResetTask");
 
   if (IsDestroyPending())
     return;
@@ -1802,9 +1762,6 @@
     VLOGF(2) << "early out: kError state";
     return;
   }
-
-  TRACE_EVENT_ASYNC_BEGIN0("media,gpu", "V4L2VDA::ResetTask", this);
-
   decoder_current_bitstream_buffer_.reset();
   while (!decoder_input_queue_.empty())
     decoder_input_queue_.pop_front();
@@ -1870,6 +1827,7 @@
 void V4L2VideoDecodeAccelerator::ResetDoneTask() {
   VLOGF(2);
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
+  TRACE_EVENT0("media,gpu", "V4L2VDA::ResetDoneTask");
 
   if (IsDestroyPending())
     return;
@@ -1879,8 +1837,6 @@
     return;
   }
 
-  TRACE_EVENT_ASYNC_END0("media,gpu", "V4L2VDA::ResetTask", this);
-
   // Start poll thread if NotifyFlushDoneIfNeeded has not already.
   if (!device_poll_thread_.IsRunning()) {
     if (!StartDevicePoll())
@@ -1898,8 +1854,8 @@
 
   decoder_partial_frame_pending_ = false;
   decoder_delay_bitstream_buffer_id_ = -1;
-  child_task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&Client::NotifyResetDone, client_));
+  child_task_runner_->PostTask(FROM_HERE,
+                               base::Bind(&Client::NotifyResetDone, client_));
 
   // While we were resetting, we early-outed DecodeBufferTask()s.
   ScheduleDecodeBufferTaskIfNeeded();
@@ -1907,7 +1863,6 @@
 
 void V4L2VideoDecodeAccelerator::DestroyTask() {
   VLOGF(2);
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
   TRACE_EVENT0("media,gpu", "V4L2VDA::DestroyTask");
 
   // DestroyTask() should run regardless of decoder_state_.
@@ -1921,27 +1876,24 @@
   decoder_current_bitstream_buffer_.reset();
   current_input_buffer_ = V4L2WritableBufferRef();
   decoder_decode_buffer_tasks_scheduled_ = 0;
+  decoder_frames_at_client_ = 0;
   while (!decoder_input_queue_.empty())
     decoder_input_queue_.pop_front();
   decoder_flushing_ = false;
 
-  // First liberate all the frames held by the client.
-  buffers_at_client_.clear();
-
   image_processor_ = nullptr;
-  while (!buffers_at_ip_.empty())
-    buffers_at_ip_.pop();
 
   DestroyInputBuffers();
   DestroyOutputBuffers();
 
-  input_queue_ = nullptr;
-  output_queue_ = nullptr;
-
-  decoder_h264_parser_ = nullptr;
-
+  if (decoder_thread_.IsRunning()) {
+    DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
+    // DestroyTask can be executed on not only decoder_thread but also child
+    // thread. When decoder thread is Stop(), |this| is not registered in
+    // MemoryDumpManager. So
   base::trace_event::MemoryDumpManager::GetInstance()->UnregisterDumpProvider(
       this);
+  }
 }
 
 bool V4L2VideoDecodeAccelerator::StartDevicePoll() {
@@ -1964,11 +1916,13 @@
 
 bool V4L2VideoDecodeAccelerator::StopDevicePoll() {
   DVLOGF(3);
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
 
   if (!device_poll_thread_.IsRunning())
     return true;
 
+  if (decoder_thread_.IsRunning())
+    DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
+
   // Signal the DevicePollTask() to stop, and stop the device poll thread.
   if (!device_->SetDevicePollInterrupt()) {
     VPLOGF(1) << "SetDevicePollInterrupt(): failed";
@@ -1987,8 +1941,6 @@
 
 bool V4L2VideoDecodeAccelerator::StopOutputStream() {
   VLOGF(2);
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-
   if (!output_queue_ || !output_queue_->IsStreaming())
     return true;
 
@@ -2000,13 +1952,21 @@
   // Output stream is stopped. No need to wait for the buffer anymore.
   flush_awaiting_last_output_buffer_ = false;
 
+  for (size_t i = 0; i < output_buffer_map_.size(); ++i) {
+    // After streamoff, the device drops ownership of all buffers, even if we
+    // don't dequeue them explicitly. Some of them may still be owned by the
+    // client however. Reuse only those that aren't.
+    OutputRecord& output_record = output_buffer_map_[i];
+    if (output_record.state == kAtDevice) {
+      output_record.state = kFree;
+      DCHECK(!output_record.egl_fence);
+    }
+  }
   return true;
 }
 
 bool V4L2VideoDecodeAccelerator::StopInputStream() {
   VLOGF(2);
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-
   if (!input_queue_ || !input_queue_->IsStreaming())
     return true;
 
@@ -2035,13 +1995,11 @@
   decoder_state_ = kChangingResolution;
   SendPictureReady();  // Send all pending PictureReady.
 
-  if (!buffers_at_ip_.empty()) {
+  if (!image_processor_bitstream_buffer_ids_.empty()) {
     VLOGF(2) << "Wait image processor to finish before destroying buffers.";
     return;
   }
 
-  buffers_at_client_.clear();
-
   image_processor_ = nullptr;
 
   if (!DestroyOutputBuffers()) {
@@ -2272,10 +2230,12 @@
 }
 
 bool V4L2VideoDecodeAccelerator::SetupFormats() {
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
-  DCHECK_EQ(decoder_state_, kInitialized);
-  DCHECK(!input_queue_->IsStreaming());
-  DCHECK(!output_queue_->IsStreaming());
+  // We always run this as we prepare to initialize.
+  DCHECK(child_task_runner_->BelongsToCurrentThread());
+  DCHECK_EQ(decoder_state_, kUninitialized);
+  // TODO(acourbot@) this is running in the wrong thread!
+  // DCHECK(!input_queue_->IsStreaming());
+  // DCHECK(!output_queue_->IsStreaming());
 
   size_t input_size;
   gfx::Size max_resolution, min_resolution;
@@ -2422,9 +2382,16 @@
 
   if (!image_processor_->Reset())
     return false;
-
-  while (!buffers_at_ip_.empty())
-    buffers_at_ip_.pop();
+  for (size_t i = 0; i < output_buffer_map_.size(); ++i) {
+    OutputRecord& output_record = output_buffer_map_[i];
+    if (output_record.state == kAtProcessor) {
+      DCHECK_EQ(buffers_at_client_.count(output_record.picture_id), 1u);
+      buffers_at_client_.erase(output_record.picture_id);
+      output_record.state = kFree;
+    }
+  }
+  while (!image_processor_bitstream_buffer_ids_.empty())
+    image_processor_bitstream_buffer_ids_.pop();
 
   return true;
 }
@@ -2495,17 +2462,21 @@
 }
 
 bool V4L2VideoDecodeAccelerator::ProcessFrame(int32_t bitstream_buffer_id,
-                                              V4L2ReadableBufferRef buf) {
+                                              int output_buffer_index) {
   DVLOGF(4);
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
 
+  OutputRecord& output_record = output_buffer_map_[output_buffer_index];
+  DCHECK_EQ(output_record.state, kAtDevice);
+  output_record.state = kAtProcessor;
+  image_processor_bitstream_buffer_ids_.push(bitstream_buffer_id);
+
   auto layout = VideoFrameLayout::Create(
       V4L2Device::V4L2PixFmtToVideoPixelFormat(output_format_fourcc_),
       coded_size_);
   if (!layout) {
     return false;
   }
-  OutputRecord& output_record = output_buffer_map_[buf->BufferId()];
   scoped_refptr<VideoFrame> input_frame = VideoFrame::WrapExternalDmabufs(
       *layout, gfx::Rect(visible_size_), visible_size_,
       DuplicateFDs(output_record.processor_input_fds), base::TimeDelta());
@@ -2521,17 +2492,14 @@
     if (output_fds.empty())
       return false;
   }
-
-  // Keep reference to the IP input until the frame is processed
-  buffers_at_ip_.push(std::make_pair(bitstream_buffer_id, buf));
-
   // Unretained(this) is safe for FrameReadyCB because |decoder_thread_| is
   // owned by this V4L2VideoDecodeAccelerator and |this| must be valid when
   // FrameReadyCB is executed.
   image_processor_->Process(
-      input_frame, buf->BufferId(), std::move(output_fds),
+      input_frame, output_buffer_index, std::move(output_fds),
       base::BindOnce(&V4L2VideoDecodeAccelerator::FrameProcessed,
-                     base::Unretained(this), bitstream_buffer_id));
+                     base::Unretained(this), bitstream_buffer_id,
+                     output_buffer_index));
   return true;
 }
 
@@ -2586,7 +2554,8 @@
 
 void V4L2VideoDecodeAccelerator::DestroyInputBuffers() {
   VLOGF(2);
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
+  DCHECK(!decoder_thread_.IsRunning() ||
+         decoder_thread_.task_runner()->BelongsToCurrentThread());
 
   if (!input_queue_)
     return;
@@ -2596,7 +2565,8 @@
 
 bool V4L2VideoDecodeAccelerator::DestroyOutputBuffers() {
   VLOGF(2);
-  DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
+  DCHECK(!decoder_thread_.IsRunning() ||
+         decoder_thread_.task_runner()->BelongsToCurrentThread());
   DCHECK(!output_queue_ || !output_queue_->IsStreaming());
   bool success = true;
 
@@ -2612,18 +2582,21 @@
     if (output_record.egl_image != EGL_NO_IMAGE_KHR) {
       child_task_runner_->PostTask(
           FROM_HERE,
-          base::BindOnce(base::IgnoreResult(&V4L2Device::DestroyEGLImage),
-                         device_, egl_display_, output_record.egl_image));
+          base::Bind(base::IgnoreResult(&V4L2Device::DestroyEGLImage), device_,
+                     egl_display_, output_record.egl_image));
     }
 
+    output_record.egl_fence.reset();
+
     DVLOGF(3) << "dismissing PictureBuffer id=" << output_record.picture_id;
     child_task_runner_->PostTask(
         FROM_HERE, base::BindOnce(&Client::DismissPictureBuffer, client_,
                                   output_record.picture_id));
   }
 
-  while (!buffers_awaiting_fence_.empty())
-    buffers_awaiting_fence_.pop();
+  // TODO(acourbot@) the client should properly drop all references to the
+  // frames it holds instead!
+  buffers_at_client_.clear();
 
   if (!output_queue_->DeallocateBuffers()) {
     NOTIFY_ERROR(PLATFORM_FAILURE);
@@ -2631,26 +2604,22 @@
   }
 
   output_buffer_map_.clear();
+  // The client may still hold some buffers. The texture holds a reference to
+  // the buffer. It is OK to free the buffer and destroy EGLImage here.
+  decoder_frames_at_client_ = 0;
 
   return success;
 }
 
 void V4L2VideoDecodeAccelerator::SendBufferToClient(
-    size_t output_buffer_index,
-    int32_t bitstream_buffer_id,
-    V4L2ReadableBufferRef vda_buffer,
-    scoped_refptr<VideoFrame> frame) {
+    size_t buffer_index,
+    int32_t bitstream_buffer_id) {
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
   DCHECK_GE(bitstream_buffer_id, 0);
-  OutputRecord& output_record = output_buffer_map_[output_buffer_index];
+  OutputRecord& output_record = output_buffer_map_[buffer_index];
 
-  DCHECK_EQ(buffers_at_client_.count(output_record.picture_id), 0u);
-  // We need to keep the VDA buffer for now, as the IP still needs to be told
-  // which buffer to use so we cannot use this buffer index before the client
-  // has returned the corresponding IP buffer.
-  buffers_at_client_.emplace(
-      output_record.picture_id,
-      std::make_pair(std::move(vda_buffer), std::move(frame)));
+  output_record.state = kAtClient;
+  decoder_frames_at_client_++;
   // TODO(hubbe): Insert correct color space. http://crbug.com/647725
   const Picture picture(output_record.picture_id, bitstream_buffer_id,
                         gfx::Rect(visible_size_), gfx::ColorSpace(), false);
@@ -2674,7 +2643,7 @@
       // all pictures are cleared at the beginning.
       decode_task_runner_->PostTask(
           FROM_HERE,
-          base::BindOnce(&Client::PictureReady, decode_client_, picture));
+          base::Bind(&Client::PictureReady, decode_client_, picture));
       pending_picture_ready_.pop();
     } else if (!cleared || send_now) {
       DVLOGF(4) << "cleared=" << pending_picture_ready_.front().cleared
@@ -2690,7 +2659,7 @@
           FROM_HERE, base::BindOnce(&Client::PictureReady, client_, picture),
           // Unretained is safe. If Client::PictureReady gets to run, |this| is
           // alive. Destroy() will wait the decode thread to finish.
-          base::BindOnce(&V4L2VideoDecodeAccelerator::PictureCleared,
+          base::Bind(&V4L2VideoDecodeAccelerator::PictureCleared,
                          base::Unretained(this)));
       picture_clearing_count_++;
       pending_picture_ready_.pop();
@@ -2713,22 +2682,22 @@
 
 void V4L2VideoDecodeAccelerator::FrameProcessed(
     int32_t bitstream_buffer_id,
-    size_t ip_buffer_index,
+    int output_buffer_index,
     scoped_refptr<VideoFrame> frame) {
-  DVLOGF(4) << "ip_buffer_index=" << ip_buffer_index
+  DVLOGF(4) << "output_buffer_index=" << output_buffer_index
             << ", bitstream_buffer_id=" << bitstream_buffer_id;
   DCHECK(decoder_thread_.task_runner()->BelongsToCurrentThread());
   // TODO(crbug.com/921825): Remove this workaround once reset callback is
   // implemented.
-  if (buffers_at_ip_.empty() ||
-      buffers_at_ip_.front().first != bitstream_buffer_id ||
+  if (image_processor_bitstream_buffer_ids_.empty() ||
+      image_processor_bitstream_buffer_ids_.front() != bitstream_buffer_id ||
       output_buffer_map_.empty()) {
     // This can happen if image processor is reset.
     // V4L2VideoDecodeAccelerator::Reset() makes
-    // |buffers_at_ip_| empty.
+    // |image_processor_bitstream_buffer_ids| empty.
     // During ImageProcessor::Reset(), some FrameProcessed() can have been
     // posted to |decoder_thread|. |bitsream_buffer_id| is pushed to
-    // |buffers_at_ip_| in ProcessFrame(). Although we
+    // |image_processor_bitstream_buffer_ids_| in ProcessFrame(). Although we
     // are not sure a new bitstream buffer id is pushed after Reset() and before
     // FrameProcessed(), We should skip the case of mismatch of bitstream buffer
     // id for safety.
@@ -2739,14 +2708,13 @@
               << bitstream_buffer_id;
     return;
   }
-  DCHECK_GE(ip_buffer_index, 0u);
-  DCHECK_LT(ip_buffer_index, output_buffer_map_.size());
+  DCHECK_GE(output_buffer_index, 0);
+  DCHECK_LT(output_buffer_index, static_cast<int>(output_buffer_map_.size()));
 
-  // This is the output record for the buffer received from the IP, which index
-  // may differ from the buffer used by the VDA.
-  OutputRecord& ip_output_record = output_buffer_map_[ip_buffer_index];
-  DVLOGF(4) << "picture_id=" << ip_output_record.picture_id;
-  DCHECK_NE(ip_output_record.picture_id, -1);
+  OutputRecord& output_record = output_buffer_map_[output_buffer_index];
+  DVLOGF(4) << "picture_id=" << output_record.picture_id;
+  DCHECK_EQ(output_record.state, kAtProcessor);
+  DCHECK_NE(output_record.picture_id, -1);
 
   // If the picture has not been cleared yet, this means it is the first time
   // we are seeing this buffer from the image processor. Schedule a call to
@@ -2754,28 +2722,21 @@
   // guaranteed that CreateEGLImageFor will complete before the picture is sent
   // to the client as both events happen on the child thread due to the picture
   // uncleared status.
-  if (ip_output_record.texture_id != 0 && !ip_output_record.cleared) {
+  if (output_record.texture_id != 0 && !output_record.cleared) {
     DCHECK(frame->HasDmaBufs());
     child_task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&V4L2VideoDecodeAccelerator::CreateEGLImageFor,
-                       weak_this_, ip_buffer_index, ip_output_record.picture_id,
-                       media::DuplicateFDs(frame->DmabufFds()),
-                       ip_output_record.texture_id, egl_image_size_,
-                       egl_image_format_fourcc_));
+        base::BindOnce(
+            &V4L2VideoDecodeAccelerator::CreateEGLImageFor, weak_this_,
+            output_buffer_index, output_record.picture_id,
+            media::DuplicateFDs(frame->DmabufFds()), output_record.texture_id,
+            egl_image_size_, egl_image_format_fourcc_));
   }
 
-  // Remove our job from the IP jobs queue
-  DCHECK_GT(buffers_at_ip_.size(), 0u);
-  DCHECK(buffers_at_ip_.front().first == bitstream_buffer_id);
-  // This is the VDA buffer used as input of the IP.
-  V4L2ReadableBufferRef vda_buffer = std::move(buffers_at_ip_.front().second);
-  buffers_at_ip_.pop();
-
-  SendBufferToClient(ip_buffer_index, bitstream_buffer_id,
-                     std::move(vda_buffer), std::move(frame));
+  SendBufferToClient(output_buffer_index, bitstream_buffer_id);
   // Flush or resolution change may be waiting image processor to finish.
-  if (buffers_at_ip_.empty()) {
+  image_processor_bitstream_buffer_ids_.pop();
+  if (image_processor_bitstream_buffer_ids_.empty()) {
     NotifyFlushDoneIfNeeded();
     if (decoder_state_ == kChangingResolution)
       StartResolutionChange();
--- a/media/gpu/v4l2/v4l2_video_decode_accelerator.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_video_decode_accelerator.h	2019-05-17 18:53:34.268000000 +0300
@@ -13,10 +13,7 @@
 #include <stdint.h>
 
 #include <list>
-#include <map>
 #include <memory>
-#include <queue>
-#include <utility>
 #include <vector>
 
 #include "base/callback_forward.h"
@@ -36,7 +33,6 @@
 #include "media/video/video_decode_accelerator.h"
 #include "ui/gfx/geometry/size.h"
 #include "ui/gl/gl_bindings.h"
-#include "ui/gl/gl_fence_egl.h"
 
 namespace gl {
 
@@ -167,6 +163,13 @@
     kDestroying,  // Destroying state, when shutting down the decoder.
   };
 
+  enum OutputRecordState {
+    kFree,         // Ready to be queued to the device.
+    kAtDevice,     // Held by device.
+    kAtProcessor,  // Held by image processor.
+    kAtClient,     // Held by client of V4L2VideoDecodeAccelerator.
+  };
+
   enum BufferId {
     kFlushBufferId = -2  // Buffer id for flush buffer, queued by FlushTask().
   };
@@ -188,7 +191,10 @@
     OutputRecord();
     OutputRecord(OutputRecord&&);
     ~OutputRecord();
+    OutputRecordState state;
     EGLImageKHR egl_image;  // EGLImageKHR for the output buffer.
+    std::unique_ptr<gl::GLFenceEGL> egl_fence;  // sync the compositor's use of
+                                                // the EGLImage.
     int32_t picture_id;     // picture buffer id as returned to PictureReady().
     GLuint texture_id;
     bool cleared;           // Whether the texture is cleared and safe to render
@@ -204,10 +210,7 @@
   //
 
   // Task to finish initialization on decoder_thread_.
-  void InitializeTask(const Config& config,
-                      bool* result,
-                      base::WaitableEvent* done);
-  bool CheckConfig(const Config& config);
+  void InitializeTask();
 
   // Enqueue a buffer to decode.  This will enqueue a buffer to the
   // decoder_input_queue_, then queue a DecodeBufferTask() to actually decode
@@ -246,14 +249,6 @@
                                   std::vector<base::ScopedFD> dmabuf_fds,
                                   int32_t stride);
 
-  // Check |planes| and |dmabuf_fds| are valid in import mode, besides
-  // ImportBufferForPicture.
-  void ImportBufferForPictureForImportTask(
-      int32_t picture_buffer_id,
-      VideoPixelFormat pixel_format,
-      std::vector<base::ScopedFD> dmabuf_fds,
-      std::vector<gfx::NativePixmapPlane> planes);
-
   // Create an EGLImage for the buffer associated with V4L2 |buffer_index| and
   // for |picture_buffer_id|, backed by dmabuf file descriptors in
   // |passed_dmabuf_fds|, taking ownership of them.
@@ -276,9 +271,6 @@
   // DevicePollTask().  If |event_pending| is true, one or more events
   // on file descriptor are pending.
   void ServiceDeviceTask(bool event_pending);
-
-  // Release buffers awaiting for their fence to be signaled.
-  void CheckGLFences();
   // Handle the various device queues.
   void Enqueue();
   void Dequeue();
@@ -404,17 +396,9 @@
   bool CreateImageProcessor();
   // Send a frame to the image processor to process. The index of decoder
   // output buffer is |output_buffer_index| and its id is |bitstream_buffer_id|.
-  bool ProcessFrame(int32_t bitstream_buffer_id, V4L2ReadableBufferRef buf);
+  bool ProcessFrame(int32_t bitstream_buffer_id, int output_buffer_index);
 
-  // Send a buffer to the client.
-  // |buffer_index| is the output buffer index of the buffer to be sent.
-  // |bitstream_buffer_id| is the bitstream ID from which the buffer results.
-  // |vda_buffer| is the output VDA buffer containing the decoded frame.
-  // |frame| is the IP frame that will be sent to the client, if IP is used.
-  void SendBufferToClient(size_t buffer_index,
-                          int32_t bitstream_buffer_id,
-                          V4L2ReadableBufferRef vda_buffer,
-                          scoped_refptr<VideoFrame> frame = nullptr);
+  void SendBufferToClient(size_t buffer_index, int32_t bitstream_buffer_id);
 
   //
   // Methods run on child thread.
@@ -430,7 +414,7 @@
   // |bitstream_buffer_id| and stored in |output_buffer_index| buffer of
   // image processor.
   void FrameProcessed(int32_t bitstream_buffer_id,
-                      size_t output_buffer_index,
+                      int output_buffer_index,
                       scoped_refptr<VideoFrame> frame);
 
   // Image processor notifies an error.
@@ -488,6 +472,8 @@
   // task execution should complete one buffer.  If we fall behind (due to
   // resource backpressure, etc.), we'll have to schedule more to catch up.
   int decoder_decode_buffer_tasks_scheduled_;
+  // Picture buffers held by the client.
+  int decoder_frames_at_client_;
 
   // Are we flushing?
   bool decoder_flushing_;
@@ -533,18 +519,8 @@
   // Buffers that have been allocated but are awaiting an ImportBuffer
   // or AssignEGLImage event.
   std::map<int32_t, V4L2WritableBufferRef> output_wait_map_;
-  // Bitstream IDs and VDA buffers currently being processed by the IP.
-  std::queue<std::pair<int32_t, V4L2ReadableBufferRef>> buffers_at_ip_;
   // Keeps decoded buffers out of the free list until the client returns them.
-  // First element is the VDA buffer, second is the (optional) IP buffer.
-  std::map<int32_t, std::pair<V4L2ReadableBufferRef, scoped_refptr<VideoFrame>>>
-      buffers_at_client_;
-  // Queue of buffers that have been returned by the client, but which fence
-  // hasn't been signaled yet. Keeps both the VDA and (optional) IP buffer.
-  std::queue<
-      std::pair<std::unique_ptr<gl::GLFenceEGL>,
-                std::pair<V4L2ReadableBufferRef, scoped_refptr<VideoFrame>>>>
-      buffers_awaiting_fence_;
+  std::map<int32_t, V4L2ReadableBufferRef> buffers_at_client_;
 
   // Mapping of int index to output buffer record.
   std::vector<OutputRecord> output_buffer_map_;
@@ -606,6 +582,11 @@
   // Number of planes for EGLImage.
   size_t egl_image_planes_count_;
 
+  // IDs of bitstream buffers sent to image processor to process. After a
+  // buffer is processed, it will sent to render if the id is in this
+  // queue. If the id is not in this queue, the buffer will be dropped.
+  base::queue<int> image_processor_bitstream_buffer_ids_;
+
   // Input format V4L2 fourccs this class supports.
   static const uint32_t supported_input_fourccs_[];
 
--- a/media/gpu/v4l2/v4l2_video_encode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_video_encode_accelerator.cc	2019-05-17 18:53:34.268000000 +0300
@@ -15,7 +15,6 @@
 #include <numeric>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/callback.h"
 #include "base/command_line.h"
 #include "base/numerics/safe_conversions.h"
@@ -184,7 +183,8 @@
   if (!is_flush_supported_)
     VLOGF(2) << "V4L2_ENC_CMD_STOP is not supported.";
 
-  struct v4l2_capability caps{};
+  struct v4l2_capability caps;
+  memset(&caps, 0, sizeof(caps));
   const __u32 kCapsRequired = V4L2_CAP_VIDEO_M2M_MPLANE | V4L2_CAP_STREAMING;
   IOCTL_OR_ERROR_RETURN_FALSE(VIDIOC_QUERYCAP, &caps);
   if ((caps.capabilities & kCapsRequired) != kCapsRequired) {
@@ -293,9 +293,9 @@
 
   child_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(
-          &Client::RequireBitstreamBuffers, client_, kInputBufferCount,
-          image_processor_.get() ? image_processor_->input_layout().coded_size()
+      base::Bind(&Client::RequireBitstreamBuffers, client_, kInputBufferCount,
+                 image_processor_.get()
+                     ? image_processor_->input_layout().coded_size()
                                  : input_allocated_size_,
           output_buffer_byte_size_));
   return true;
@@ -399,8 +399,8 @@
         if (!image_processor_->Process(
                 frame, output_buffer_index, std::vector<base::ScopedFD>(),
                 base::BindOnce(&V4L2VideoEncodeAccelerator::FrameProcessed,
-                               weak_this_, force_keyframe,
-                               frame->timestamp()))) {
+                               weak_this_, force_keyframe, frame->timestamp(),
+                               output_buffer_index))) {
           NOTIFY_ERROR(kPlatformFailureError);
         }
       }
@@ -436,8 +436,8 @@
       new BitstreamBufferRef(buffer.id(), std::move(shm)));
   encoder_thread_.task_runner()->PostTask(
       FROM_HERE,
-      base::BindOnce(&V4L2VideoEncodeAccelerator::UseOutputBitstreamBufferTask,
-                     base::Unretained(this), std::move(buffer_ref)));
+      base::Bind(&V4L2VideoEncodeAccelerator::UseOutputBitstreamBufferTask,
+                 base::Unretained(this), base::Passed(&buffer_ref)));
 }
 
 void V4L2VideoEncodeAccelerator::RequestEncodingParametersChange(
@@ -448,7 +448,7 @@
 
   encoder_thread_.task_runner()->PostTask(
       FROM_HERE,
-      base::BindOnce(
+      base::Bind(
           &V4L2VideoEncodeAccelerator::RequestEncodingParametersChangeTask,
           base::Unretained(this), bitrate, framerate));
 }
@@ -493,7 +493,7 @@
   encoder_thread_.task_runner()->PostTask(
       FROM_HERE,
       base::BindOnce(&V4L2VideoEncodeAccelerator::FlushTask,
-                     base::Unretained(this), std::move(flush_callback)));
+                     base::Unretained(this), base::Passed(&flush_callback)));
 }
 
 void V4L2VideoEncodeAccelerator::FlushTask(FlushCallback flush_callback) {
@@ -528,18 +528,18 @@
 void V4L2VideoEncodeAccelerator::FrameProcessed(
     bool force_keyframe,
     base::TimeDelta timestamp,
-    size_t output_buffer_index,
+    int output_buffer_index,
     scoped_refptr<VideoFrame> frame) {
   DCHECK(child_task_runner_->BelongsToCurrentThread());
   DVLOGF(4) << "force_keyframe=" << force_keyframe
             << ", output_buffer_index=" << output_buffer_index;
-  DCHECK_GE(output_buffer_index, 0u);
+  DCHECK_GE(output_buffer_index, 0);
   DCHECK(encoder_thread_.IsRunning());
   DCHECK(!weak_this_.WasInvalidated());
 
-  frame->AddDestructionObserver(BindToCurrentLoop(base::BindOnce(
-      &V4L2VideoEncodeAccelerator::ReuseImageProcessorOutputBuffer, weak_this_,
-      output_buffer_index)));
+  frame->AddDestructionObserver(BindToCurrentLoop(
+      base::Bind(&V4L2VideoEncodeAccelerator::ReuseImageProcessorOutputBuffer,
+                 weak_this_, output_buffer_index)));
 
   encoder_thread_.task_runner()->PostTask(
       FROM_HERE, base::BindOnce(&V4L2VideoEncodeAccelerator::EncodeTask,
@@ -547,7 +547,7 @@
 }
 
 void V4L2VideoEncodeAccelerator::ReuseImageProcessorOutputBuffer(
-    size_t output_buffer_index) {
+    int output_buffer_index) {
   DCHECK(child_task_runner_->BelongsToCurrentThread());
   DVLOGF(4) << "output_buffer_index=" << output_buffer_index;
   free_image_processor_output_buffer_indices_.push_back(output_buffer_index);
@@ -742,7 +742,8 @@
             FROM_HERE, base::BindOnce(std::move(flush_callback_), true));
         return;
       }
-      struct v4l2_encoder_cmd cmd{};
+      struct v4l2_encoder_cmd cmd;
+      memset(&cmd, 0, sizeof(cmd));
       cmd.cmd = V4L2_ENC_CMD_STOP;
       if (device_->Ioctl(VIDIOC_ENCODER_CMD, &cmd) != 0) {
         VPLOGF(1) << "ioctl() failed: VIDIOC_ENCODER_CMD";
@@ -863,8 +864,7 @@
 
     child_task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&Client::BitstreamBufferReady, client_,
-                       bitstream_buffer_id,
+        base::Bind(&Client::BitstreamBufferReady, client_, bitstream_buffer_id,
                        BitstreamBufferMetadata(
                            output_data_size, key_frame,
                            base::TimeDelta::FromMicroseconds(
@@ -879,7 +879,8 @@
       child_task_runner_->PostTask(
           FROM_HERE, base::BindOnce(std::move(flush_callback_), true));
       // Start the encoder again.
-      struct v4l2_encoder_cmd cmd{};
+      struct v4l2_encoder_cmd cmd;
+      memset(&cmd, 0, sizeof(cmd));
       cmd.cmd = V4L2_ENC_CMD_START;
       IOCTL_OR_ERROR_RETURN(VIDIOC_ENCODER_CMD, &cmd);
     }
@@ -899,7 +900,8 @@
   InputFrameInfo frame_info = encoder_input_queue_.front();
   if (frame_info.force_keyframe) {
     std::vector<struct v4l2_ext_control> ctrls;
-    struct v4l2_ext_control ctrl{};
+    struct v4l2_ext_control ctrl;
+    memset(&ctrl, 0, sizeof(ctrl));
     ctrl.id = V4L2_CID_MPEG_VIDEO_FORCE_KEY_FRAME;
     ctrls.push_back(ctrl);
     if (!SetExtCtrls(ctrls)) {
@@ -913,8 +915,10 @@
   const int index = free_input_buffers_.back();
   InputRecord& input_record = input_buffer_map_[index];
   DCHECK(!input_record.at_device);
-  struct v4l2_buffer qbuf{};
-  struct v4l2_plane qbuf_planes[VIDEO_MAX_PLANES] = {};
+  struct v4l2_buffer qbuf;
+  struct v4l2_plane qbuf_planes[VIDEO_MAX_PLANES];
+  memset(&qbuf, 0, sizeof(qbuf));
+  memset(qbuf_planes, 0, sizeof(qbuf_planes));
   qbuf.index = index;
   qbuf.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
   qbuf.m.planes = qbuf_planes;
@@ -996,8 +1000,10 @@
   OutputRecord& output_record = output_buffer_map_[index];
   DCHECK(!output_record.at_device);
   DCHECK(!output_record.buffer_ref);
-  struct v4l2_buffer qbuf{};
-  struct v4l2_plane qbuf_planes[1] = {};
+  struct v4l2_buffer qbuf;
+  struct v4l2_plane qbuf_planes[1];
+  memset(&qbuf, 0, sizeof(qbuf));
+  memset(qbuf_planes, 0, sizeof(qbuf_planes));
   qbuf.index = index;
   qbuf.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
   qbuf.memory = V4L2_MEMORY_MMAP;
@@ -1121,8 +1127,8 @@
   scoped_refptr<base::SingleThreadTaskRunner> task_runner =
       encoder_thread_.task_runner();
   if (task_runner && !task_runner->BelongsToCurrentThread()) {
-    task_runner->PostTask(
-        FROM_HERE, base::BindOnce(&V4L2VideoEncodeAccelerator::SetErrorState,
+    task_runner->PostTask(FROM_HERE,
+                          base::Bind(&V4L2VideoEncodeAccelerator::SetErrorState,
                                   base::Unretained(this), error));
     return;
   }
@@ -1145,7 +1151,8 @@
   DCHECK_GT(framerate, 0u);
 
   std::vector<struct v4l2_ext_control> ctrls;
-  struct v4l2_ext_control ctrl{};
+  struct v4l2_ext_control ctrl;
+  memset(&ctrl, 0, sizeof(ctrl));
   ctrl.id = V4L2_CID_MPEG_VIDEO_BITRATE;
   ctrl.value = bitrate;
   ctrls.push_back(ctrl);
@@ -1155,7 +1162,8 @@
     return;
   }
 
-  struct v4l2_streamparm parms{};
+  struct v4l2_streamparm parms;
+  memset(&parms, 0, sizeof(parms));
   parms.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
   // Note that we are provided "frames per second" but V4L2 expects "time per
   // frame"; hence we provide the reciprocal of the framerate here.
@@ -1170,10 +1178,10 @@
   DCHECK(!input_streamon_);
   DCHECK(!output_streamon_);
 
-  DCHECK(!visible_size_.IsEmpty());
-  output_buffer_byte_size_ = GetEncodeBitstreamBufferSize(visible_size_);
+  output_buffer_byte_size_ = GetEncodeBitstreamBufferSize();
 
-  struct v4l2_format format{};
+  struct v4l2_format format;
+  memset(&format, 0, sizeof(format));
   format.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
   format.fmt.pix_mp.width = visible_size_.width();
   format.fmt.pix_mp.height = visible_size_.height();
@@ -1216,7 +1224,8 @@
     DCHECK_LE(planes_count, static_cast<size_t>(VIDEO_MAX_PLANES));
     VLOGF(2) << "Trying S_FMT with " << FourccToString(pix_fmt) << " ("
              << trying_format << ").";
-    struct v4l2_format format{};
+    struct v4l2_format format;
+    memset(&format, 0, sizeof(format));
     format.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
     format.fmt.pix_mp.width = visible_size_.width();
     format.fmt.pix_mp.height = visible_size_.height();
@@ -1267,7 +1276,8 @@
   visible_rect.width = visible_size_.width();
   visible_rect.height = visible_size_.height();
 
-  struct v4l2_selection selection_arg{};
+  struct v4l2_selection selection_arg;
+  memset(&selection_arg, 0, sizeof(selection_arg));
   selection_arg.type = V4L2_BUF_TYPE_VIDEO_OUTPUT;
   selection_arg.target = V4L2_SEL_TGT_CROP;
   selection_arg.r = visible_rect;
@@ -1279,7 +1289,8 @@
     visible_rect = selection_arg.r;
   } else {
     VLOGF(2) << "Fallback to VIDIOC_S/G_CROP";
-    struct v4l2_crop crop{};
+    struct v4l2_crop crop;
+    memset(&crop, 0, sizeof(crop));
     crop.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
     crop.c = visible_rect;
     IOCTL_OR_ERROR_RETURN_FALSE(VIDIOC_S_CROP, &crop);
@@ -1295,7 +1306,8 @@
 }
 
 bool V4L2VideoEncodeAccelerator::IsCtrlExposed(uint32_t ctrl_id) {
-  struct v4l2_queryctrl query_ctrl{};
+  struct v4l2_queryctrl query_ctrl;
+  memset(&query_ctrl, 0, sizeof(query_ctrl));
   query_ctrl.id = ctrl_id;
 
   return device_->Ioctl(VIDIOC_QUERYCTRL, &query_ctrl) == 0;
@@ -1303,7 +1315,8 @@
 
 bool V4L2VideoEncodeAccelerator::SetExtCtrls(
     std::vector<struct v4l2_ext_control> ctrls) {
-  struct v4l2_ext_controls ext_ctrls{};
+  struct v4l2_ext_controls ext_ctrls;
+  memset(&ext_ctrls, 0, sizeof(ext_ctrls));
   ext_ctrls.ctrl_class = V4L2_CTRL_CLASS_MPEG;
   ext_ctrls.count = ctrls.size();
   ext_ctrls.controls = &ctrls[0];
@@ -1312,9 +1325,10 @@
 
 bool V4L2VideoEncodeAccelerator::InitControls(const Config& config) {
   std::vector<struct v4l2_ext_control> ctrls;
-  struct v4l2_ext_control ctrl{};
+  struct v4l2_ext_control ctrl;
 
   // Enable frame-level bitrate control. This is the only mandatory control.
+  memset(&ctrl, 0, sizeof(ctrl));
   ctrl.id = V4L2_CID_MPEG_VIDEO_FRAME_RC_ENABLE;
   ctrl.value = 1;
   ctrls.push_back(ctrl);
@@ -1437,7 +1451,8 @@
   DCHECK(encoder_thread_.task_runner()->BelongsToCurrentThread());
   DCHECK(!input_streamon_);
 
-  struct v4l2_requestbuffers reqbufs{};
+  struct v4l2_requestbuffers reqbufs;
+  memset(&reqbufs, 0, sizeof(reqbufs));
   // Driver will modify to the appropriate number of buffers.
   reqbufs.count = kInputBufferCount;
   reqbufs.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
@@ -1457,7 +1472,8 @@
   DCHECK(child_task_runner_->BelongsToCurrentThread());
   DCHECK(!output_streamon_);
 
-  struct v4l2_requestbuffers reqbufs{};
+  struct v4l2_requestbuffers reqbufs;
+  memset(&reqbufs, 0, sizeof(reqbufs));
   reqbufs.count = kOutputBufferCount;
   reqbufs.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
   reqbufs.memory = V4L2_MEMORY_MMAP;
@@ -1466,8 +1482,10 @@
   DCHECK(output_buffer_map_.empty());
   output_buffer_map_ = std::vector<OutputRecord>(reqbufs.count);
   for (size_t i = 0; i < output_buffer_map_.size(); ++i) {
-    struct v4l2_plane planes[1] = {};
-    struct v4l2_buffer buffer{};
+    struct v4l2_plane planes[1];
+    struct v4l2_buffer buffer;
+    memset(&buffer, 0, sizeof(buffer));
+    memset(planes, 0, sizeof(planes));
     buffer.index = i;
     buffer.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
     buffer.memory = V4L2_MEMORY_MMAP;
@@ -1501,7 +1519,8 @@
   if (input_buffer_map_.empty())
     return;
 
-  struct v4l2_requestbuffers reqbufs{};
+  struct v4l2_requestbuffers reqbufs;
+  memset(&reqbufs, 0, sizeof(reqbufs));
   reqbufs.count = 0;
   reqbufs.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
   reqbufs.memory = input_memory_type_;
@@ -1526,7 +1545,8 @@
                       output_buffer_map_[i].length);
   }
 
-  struct v4l2_requestbuffers reqbufs{};
+  struct v4l2_requestbuffers reqbufs;
+  memset(&reqbufs, 0, sizeof(reqbufs));
   reqbufs.count = 0;
   reqbufs.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
   reqbufs.memory = V4L2_MEMORY_MMAP;
--- a/media/gpu/v4l2/v4l2_video_encode_accelerator.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/v4l2/v4l2_video_encode_accelerator.h	2019-05-17 18:53:34.268000000 +0300
@@ -117,7 +117,7 @@
   // encode.
   void FrameProcessed(bool force_keyframe,
                       base::TimeDelta timestamp,
-                      size_t output_buffer_index,
+                      int output_buffer_index,
                       scoped_refptr<VideoFrame> frame);
 
   // Error callback for handling image processor errors.
@@ -218,7 +218,7 @@
   bool AllocateImageProcessorOutputBuffers();
 
   // Recycle output buffer of image processor with |output_buffer_index|.
-  void ReuseImageProcessorOutputBuffer(size_t output_buffer_index);
+  void ReuseImageProcessorOutputBuffer(int output_buffer_index);
 
   // Copy encoded stream data from an output V4L2 buffer at |bitstream_data|
   // of size |bitstream_size| into a BitstreamBuffer referenced by |buffer_ref|,
--- a/media/gpu/vaapi/accelerated_video_encoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/accelerated_video_encoder.cc	2019-05-17 18:53:34.272000000 +0300
@@ -56,7 +56,7 @@
 }
 
 size_t AcceleratedVideoEncoder::GetBitstreamBufferSize() const {
-  return GetEncodeBitstreamBufferSize(GetCodedSize());
+  return GetEncodeBitstreamBufferSize();
 }
 
 }  // namespace media
--- a/media/gpu/vaapi/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/BUILD.gn	2019-05-17 18:53:34.272000000 +0300
@@ -41,8 +41,6 @@
     "vaapi_h264_accelerator.h",
     "vaapi_jpeg_decode_accelerator.cc",
     "vaapi_jpeg_decode_accelerator.h",
-    "vaapi_jpeg_decoder.cc",
-    "vaapi_jpeg_decoder.h",
     "vaapi_jpeg_encode_accelerator.cc",
     "vaapi_jpeg_encode_accelerator.h",
     "vaapi_jpeg_encoder.cc",
@@ -65,20 +63,16 @@
     "vaapi_wrapper.h",
     "vp8_encoder.cc",
     "vp8_encoder.h",
-    "vp9_encoder.cc",
-    "vp9_encoder.h",
   ]
 
   configs += [ "//third_party/libyuv:libyuv_config" ]
 
   deps = [
     ":libva_stubs",
-    "//base",
     "//gpu/ipc/service",
     "//media",
     "//media/gpu:common",
     "//third_party/libyuv",
-    "//ui/gfx/geometry",
   ]
 
   if (is_linux) {
@@ -133,18 +127,19 @@
   ]
 }
 
-source_set("jpeg_decoder_unit_test") {
+source_set("jpeg_decode_accelerator_unit_test") {
   testonly = true
   sources = [
-    "vaapi_jpeg_decoder_unittest.cc",
+    "vaapi_jpeg_decode_accelerator_unittest.cc",
   ]
   deps = [
     ":vaapi",
-    "//base",
     "//base/test:test_support",
+    "//gpu:test_support",
     "//media:test_support",
+    "//media/gpu:common",
+    "//testing/gmock",
     "//testing/gtest",
-    "//third_party/libyuv:libyuv",
-    "//ui/gfx/geometry",
+    "//ui/gfx:test_support",
   ]
 }
--- a/media/gpu/vaapi/h264_encoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/h264_encoder.cc	2019-05-17 18:53:34.272000000 +0300
@@ -172,8 +172,6 @@
   }
 
   if (pic->type == H264SliceHeader::kISlice) {
-    // We always generate SPS and PPS with I(DR) frame. This will help for Seek
-    // operation on the generated stream.
     if (!accelerator_->SubmitPackedHeaders(encode_job, packed_sps_,
                                            packed_pps_)) {
       DVLOGF(1) << "Failed submitting keyframe headers";
@@ -215,19 +213,7 @@
   curr_params_.cpb_size_bits =
       curr_params_.bitrate_bps * curr_params_.cpb_window_size_ms / 1000;
 
-  bool previous_encoding_parameters_changed = encoding_parameters_changed_;
-
   UpdateSPS();
-
-  // If SPS parameters are updated, it is required to send the SPS with IDR
-  // frame. However, as a special case, we do not generate IDR frame if only
-  // bitrate and framerate parameters are updated. This is safe because these
-  // will not make a difference on decoder processing. The updated SPS will be
-  // sent a next periodic or requested I(DR) frame. On the other hand, bitrate
-  // and framerate parameter
-  // changes must be affected for encoding. UpdateSPS()+SubmitFrameParameters()
-  // shall apply them to an encoder properly.
-  encoding_parameters_changed_ = previous_encoding_parameters_changed;
   return true;
 }
 
--- a/media/gpu/vaapi/h264_encoder.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/h264_encoder.h	2019-05-17 18:53:34.272000000 +0300
@@ -147,8 +147,8 @@
   // idr_pic_id (spec section 7.4.3) to be used for the next frame.
   unsigned int idr_pic_id_ = 0;
 
-  // True if encoding parameters have changed that affect decoder process, then
-  // we need to submit a keyframe with updated parameters.
+  // True if encoding parameters have changed and we need to submit a keyframe
+  // with updated parameters.
   bool encoding_parameters_changed_ = false;
 
   // Currently active reference frames.
--- a/media/gpu/vaapi/vaapi_jpeg_decode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_jpeg_decode_accelerator.cc	2019-05-17 18:53:34.272000000 +0300
@@ -5,29 +5,27 @@
 #include "media/gpu/vaapi/vaapi_jpeg_decode_accelerator.h"
 
 #include <stddef.h>
+#include <string.h>
 
+#include <memory>
 #include <utility>
 
 #include <va/va.h>
 
 #include "base/bind.h"
-#include "base/containers/span.h"
-#include "base/location.h"
 #include "base/logging.h"
 #include "base/metrics/histogram_macros.h"
-#include "base/numerics/safe_conversions.h"
-#include "base/single_thread_task_runner.h"
 #include "base/threading/thread_task_runner_handle.h"
 #include "base/trace_event/trace_event.h"
 #include "media/base/bitstream_buffer.h"
 #include "media/base/unaligned_shared_memory.h"
 #include "media/base/video_frame.h"
-#include "media/base/video_types.h"
+#include "media/filters/jpeg_parser.h"
 #include "media/gpu/macros.h"
+#include "media/gpu/vaapi/vaapi_picture.h"
 #include "media/gpu/vaapi/vaapi_utils.h"
 #include "media/gpu/vaapi/vaapi_wrapper.h"
 #include "third_party/libyuv/include/libyuv.h"
-#include "ui/gfx/geometry/size.h"
 
 namespace media {
 
@@ -44,33 +41,233 @@
                             VAJDA_DECODER_FAILURES_MAX + 1);
 }
 
-static JpegDecodeAccelerator::Error VaapiJpegDecodeStatusToError(
-    VaapiJpegDecodeStatus status) {
-  switch (status) {
-    case VaapiJpegDecodeStatus::kSuccess:
-      return JpegDecodeAccelerator::Error::NO_ERRORS;
-    case VaapiJpegDecodeStatus::kParseJpegFailed:
-      return JpegDecodeAccelerator::Error::PARSE_JPEG_FAILED;
-    case VaapiJpegDecodeStatus::kUnsupportedSubsampling:
-      return JpegDecodeAccelerator::Error::UNSUPPORTED_JPEG;
+// Check the value of VA_FOURCC_YUYV, as we don't have access to the VA_FOURCC
+// macro in the header file without pulling in the entire <va/va.h>.
+static_assert(VA_FOURCC_YUYV == VA_FOURCC('Y', 'U', 'Y', 'V'),
+              "VA_FOURCC_YUYV must be equal to VA_FOURCC('Y', 'U', 'Y', 'V')");
+constexpr VAImageFormat kImageFormatI420 = {.fourcc = VA_FOURCC_I420,
+                                            .byte_order = VA_LSB_FIRST,
+                                            .bits_per_pixel = 12};
+constexpr VAImageFormat kImageFormatYUYV = {.fourcc = VA_FOURCC_YUYV,
+                                            .byte_order = VA_LSB_FIRST,
+                                            .bits_per_pixel = 16};
+
+// Convert the specified surface format to the associated output image format.
+bool VaSurfaceFormatToImageFormat(uint32_t va_rt_format,
+                                  VAImageFormat* va_image_format) {
+  switch (va_rt_format) {
+    case VA_RT_FORMAT_YUV420:
+      *va_image_format = kImageFormatI420;
+      return true;
+    case VA_RT_FORMAT_YUV422:
+      *va_image_format = kImageFormatYUYV;
+      return true;
     default:
-      return JpegDecodeAccelerator::Error::PLATFORM_FAILURE;
+      return false;
   }
 }
 
-static bool VerifyDataSize(const VAImage* image) {
-  const gfx::Size dimensions(base::strict_cast<int>(image->width),
-                             base::strict_cast<int>(image->height));
-  size_t min_size = 0;
-  if (image->format.fourcc == VA_FOURCC_I420) {
-    min_size = VideoFrame::AllocationSize(PIXEL_FORMAT_I420, dimensions);
-  } else if (image->format.fourcc == VA_FOURCC_YUY2 ||
-             image->format.fourcc == VA_FOURCC('Y', 'U', 'Y', 'V')) {
-    min_size = VideoFrame::AllocationSize(PIXEL_FORMAT_YUY2, dimensions);
-  } else {
+static unsigned int VaSurfaceFormatForJpeg(
+    const JpegFrameHeader& frame_header) {
+  // The range of sampling factor is [1, 4]. Pack them into integer to make the
+  // matching code simpler. For example, 0x211 means the sampling factor are 2,
+  // 1, 1 for 3 components.
+  unsigned int h = 0, v = 0;
+  for (int i = 0; i < frame_header.num_components; i++) {
+    DCHECK_LE(frame_header.components[i].horizontal_sampling_factor, 4);
+    DCHECK_LE(frame_header.components[i].vertical_sampling_factor, 4);
+    h = h << 4 | frame_header.components[i].horizontal_sampling_factor;
+    v = v << 4 | frame_header.components[i].vertical_sampling_factor;
+  }
+
+  switch (frame_header.num_components) {
+    case 1:  // Grey image
+      return VA_RT_FORMAT_YUV400;
+
+    case 3:  // Y Cb Cr color image
+      // See https://en.wikipedia.org/wiki/Chroma_subsampling for the
+      // definition of these numbers.
+      if (h == 0x211 && v == 0x211)
+        return VA_RT_FORMAT_YUV420;
+
+      if (h == 0x211 && v == 0x111)
+        return VA_RT_FORMAT_YUV422;
+
+      if (h == 0x111 && v == 0x111)
+        return VA_RT_FORMAT_YUV444;
+
+      if (h == 0x411 && v == 0x111)
+        return VA_RT_FORMAT_YUV411;
+  }
+  VLOGF(1) << "Unsupported sampling factor: num_components="
+           << frame_header.num_components << ", h=" << std::hex << h
+           << ", v=" << v;
+
+  return 0;
+}
+
+// VAAPI only supports a subset of JPEG profiles. This function determines
+// whether a given parsed JPEG result is supported or not.
+static bool IsVaapiSupportedJpeg(const JpegParseResult& jpeg) {
+  if (jpeg.frame_header.visible_width < 1 ||
+      jpeg.frame_header.visible_height < 1) {
+    DLOG(ERROR) << "width(" << jpeg.frame_header.visible_width
+                << ") and height(" << jpeg.frame_header.visible_height
+                << ") should be at least 1";
+    return false;
+  }
+
+  // Size 64k*64k is the maximum in the JPEG standard. VAAPI doesn't support
+  // resolutions larger than 16k*16k.
+  const int kMaxDimension = 16384;
+  if (jpeg.frame_header.coded_width > kMaxDimension ||
+      jpeg.frame_header.coded_height > kMaxDimension) {
+    DLOG(ERROR) << "VAAPI doesn't support size("
+                << jpeg.frame_header.coded_width << "*"
+                << jpeg.frame_header.coded_height << ") larger than "
+                << kMaxDimension << "*" << kMaxDimension;
+    return false;
+  }
+
+  if (jpeg.frame_header.num_components != 3) {
+    DLOG(ERROR) << "VAAPI doesn't support num_components("
+                << static_cast<int>(jpeg.frame_header.num_components)
+                << ") != 3";
     return false;
   }
-  return base::strict_cast<size_t>(image->data_size) >= min_size;
+
+  if (jpeg.frame_header.components[0].horizontal_sampling_factor <
+          jpeg.frame_header.components[1].horizontal_sampling_factor ||
+      jpeg.frame_header.components[0].horizontal_sampling_factor <
+          jpeg.frame_header.components[2].horizontal_sampling_factor) {
+    DLOG(ERROR) << "VAAPI doesn't supports horizontal sampling factor of Y"
+                << " smaller than Cb and Cr";
+    return false;
+  }
+
+  if (jpeg.frame_header.components[0].vertical_sampling_factor <
+          jpeg.frame_header.components[1].vertical_sampling_factor ||
+      jpeg.frame_header.components[0].vertical_sampling_factor <
+          jpeg.frame_header.components[2].vertical_sampling_factor) {
+    DLOG(ERROR) << "VAAPI doesn't supports vertical sampling factor of Y"
+                << " smaller than Cb and Cr";
+    return false;
+  }
+
+  return true;
+}
+
+static void FillPictureParameters(
+    const JpegFrameHeader& frame_header,
+    VAPictureParameterBufferJPEGBaseline* pic_param) {
+  memset(pic_param, 0, sizeof(*pic_param));
+  pic_param->picture_width = frame_header.coded_width;
+  pic_param->picture_height = frame_header.coded_height;
+  pic_param->num_components = frame_header.num_components;
+
+  for (int i = 0; i < pic_param->num_components; i++) {
+    pic_param->components[i].component_id = frame_header.components[i].id;
+    pic_param->components[i].h_sampling_factor =
+        frame_header.components[i].horizontal_sampling_factor;
+    pic_param->components[i].v_sampling_factor =
+        frame_header.components[i].vertical_sampling_factor;
+    pic_param->components[i].quantiser_table_selector =
+        frame_header.components[i].quantization_table_selector;
+  }
+}
+
+static void FillIQMatrix(const JpegQuantizationTable* q_table,
+                         VAIQMatrixBufferJPEGBaseline* iq_matrix) {
+  memset(iq_matrix, 0, sizeof(*iq_matrix));
+  static_assert(kJpegMaxQuantizationTableNum ==
+                    std::extent<decltype(iq_matrix->load_quantiser_table)>(),
+                "max number of quantization table mismatched");
+  static_assert(
+      sizeof(iq_matrix->quantiser_table[0]) == sizeof(q_table[0].value),
+      "number of quantization entries mismatched");
+  for (size_t i = 0; i < kJpegMaxQuantizationTableNum; i++) {
+    if (!q_table[i].valid)
+      continue;
+    iq_matrix->load_quantiser_table[i] = 1;
+    for (size_t j = 0; j < base::size(q_table[i].value); j++)
+      iq_matrix->quantiser_table[i][j] = q_table[i].value[j];
+  }
+}
+
+static void FillHuffmanTable(const JpegHuffmanTable* dc_table,
+                             const JpegHuffmanTable* ac_table,
+                             VAHuffmanTableBufferJPEGBaseline* huffman_table) {
+  memset(huffman_table, 0, sizeof(*huffman_table));
+  // Use default huffman tables if not specified in header.
+  bool has_huffman_table = false;
+  for (size_t i = 0; i < kJpegMaxHuffmanTableNumBaseline; i++) {
+    if (dc_table[i].valid || ac_table[i].valid) {
+      has_huffman_table = true;
+      break;
+    }
+  }
+  if (!has_huffman_table) {
+    dc_table = kDefaultDcTable;
+    ac_table = kDefaultAcTable;
+  }
+
+  static_assert(kJpegMaxHuffmanTableNumBaseline ==
+                    std::extent<decltype(huffman_table->load_huffman_table)>(),
+                "max number of huffman table mismatched");
+  static_assert(sizeof(huffman_table->huffman_table[0].num_dc_codes) ==
+                    sizeof(dc_table[0].code_length),
+                "size of huffman table code length mismatch");
+  static_assert(sizeof(huffman_table->huffman_table[0].dc_values[0]) ==
+                    sizeof(dc_table[0].code_value[0]),
+                "size of huffman table code value mismatch");
+  for (size_t i = 0; i < kJpegMaxHuffmanTableNumBaseline; i++) {
+    if (!dc_table[i].valid || !ac_table[i].valid)
+      continue;
+    huffman_table->load_huffman_table[i] = 1;
+
+    memcpy(huffman_table->huffman_table[i].num_dc_codes,
+           dc_table[i].code_length,
+           sizeof(huffman_table->huffman_table[i].num_dc_codes));
+    memcpy(huffman_table->huffman_table[i].dc_values, dc_table[i].code_value,
+           sizeof(huffman_table->huffman_table[i].dc_values));
+    memcpy(huffman_table->huffman_table[i].num_ac_codes,
+           ac_table[i].code_length,
+           sizeof(huffman_table->huffman_table[i].num_ac_codes));
+    memcpy(huffman_table->huffman_table[i].ac_values, ac_table[i].code_value,
+           sizeof(huffman_table->huffman_table[i].ac_values));
+  }
+}
+
+static void FillSliceParameters(
+    const JpegParseResult& parse_result,
+    VASliceParameterBufferJPEGBaseline* slice_param) {
+  memset(slice_param, 0, sizeof(*slice_param));
+  slice_param->slice_data_size = parse_result.data_size;
+  slice_param->slice_data_offset = 0;
+  slice_param->slice_data_flag = VA_SLICE_DATA_FLAG_ALL;
+  slice_param->slice_horizontal_position = 0;
+  slice_param->slice_vertical_position = 0;
+  slice_param->num_components = parse_result.scan.num_components;
+  for (int i = 0; i < slice_param->num_components; i++) {
+    slice_param->components[i].component_selector =
+        parse_result.scan.components[i].component_selector;
+    slice_param->components[i].dc_table_selector =
+        parse_result.scan.components[i].dc_selector;
+    slice_param->components[i].ac_table_selector =
+        parse_result.scan.components[i].ac_selector;
+  }
+  slice_param->restart_interval = parse_result.restart_interval;
+
+  // Cast to int to prevent overflow.
+  int max_h_factor =
+      parse_result.frame_header.components[0].horizontal_sampling_factor;
+  int max_v_factor =
+      parse_result.frame_header.components[0].vertical_sampling_factor;
+  int mcu_cols = parse_result.frame_header.coded_width / (max_h_factor * 8);
+  DCHECK_GT(mcu_cols, 0);
+  int mcu_rows = parse_result.frame_header.coded_height / (max_v_factor * 8);
+  DCHECK_GT(mcu_rows, 0);
+  slice_param->num_mcus = mcu_rows * mcu_cols;
 }
 
 }  // namespace
@@ -100,6 +297,8 @@
       io_task_runner_(io_task_runner),
       client_(nullptr),
       decoder_thread_("VaapiJpegDecoderThread"),
+      va_surface_id_(VA_INVALID_SURFACE),
+      va_rt_format_(0),
       weak_this_factory_(this) {}
 
 VaapiJpegDecodeAccelerator::~VaapiJpegDecodeAccelerator() {
@@ -116,8 +315,14 @@
 
   client_ = client;
 
-  if (!decoder_.Initialize(base::BindRepeating(&ReportToUMA, VAAPI_ERROR)))
+  vaapi_wrapper_ =
+      VaapiWrapper::Create(VaapiWrapper::kDecode, VAProfileJPEGBaseline,
+                           base::Bind(&ReportToUMA, VAAPI_ERROR));
+
+  if (!vaapi_wrapper_.get()) {
+    VLOGF(1) << "Failed initializing VAAPI";
     return false;
+  }
 
   if (!decoder_thread_.Start()) {
     VLOGF(1) << "Failed to start decoding thread.";
@@ -128,25 +333,46 @@
   return true;
 }
 
-bool VaapiJpegDecodeAccelerator::OutputPictureOnTaskRunner(
-    std::unique_ptr<ScopedVAImage> scoped_image,
+bool VaapiJpegDecodeAccelerator::OutputPicture(
+    VASurfaceID va_surface_id,
+    uint32_t va_surface_format,
     int32_t input_buffer_id,
     const scoped_refptr<VideoFrame>& video_frame) {
   DCHECK(decoder_task_runner_->BelongsToCurrentThread());
 
-  TRACE_EVENT1("jpeg", "VaapiJpegDecodeAccelerator::OutputPictureOnTaskRunner",
+  TRACE_EVENT1("jpeg", "VaapiJpegDecodeAccelerator::OutputPicture",
                "input_buffer_id", input_buffer_id);
 
+  DVLOGF(4) << "Outputting VASurface " << va_surface_id
+            << " into video_frame associated with input buffer id "
+            << input_buffer_id;
+
+  // Specify which image format we will request from the VAAPI. As the expected
+  // output format is I420, we will first try this format. If converting to I420
+  // is not supported by the decoder, we will request the image in its original
+  // chroma sampling format.
+  VAImageFormat va_image_format = kImageFormatI420;
+  if (!VaapiWrapper::IsImageFormatSupported(va_image_format)) {
+    if (!VaSurfaceFormatToImageFormat(va_surface_format, &va_image_format)) {
+      VLOGF(1) << "Unsupported surface format";
+      return false;
+    }
+  }
+
+  const gfx::Size coded_size = video_frame->coded_size();
+  auto scoped_image = vaapi_wrapper_->CreateVaImage(
+      va_surface_id, &va_image_format, coded_size);
+  if (!scoped_image) {
+    VLOGF(1) << "Cannot get VAImage";
+    return false;
+  }
+  const VAImage* image = scoped_image->image();
+  auto* mem = static_cast<uint8_t*>(scoped_image->va_buffer()->data());
+
   // Copy image content from VAImage to VideoFrame. If the image is not in the
   // I420 format we'll have to convert it.
-  DCHECK(scoped_image);
-  auto* mem = static_cast<uint8_t*>(scoped_image->va_buffer()->data());
-  const VAImage* image = scoped_image->image();
-  DCHECK_GE(base::strict_cast<int>(image->width),
-            video_frame->coded_size().width());
-  DCHECK_GE(base::strict_cast<int>(image->height),
-            video_frame->coded_size().height());
-  DCHECK(VerifyDataSize(image));
+  DCHECK_GE(image->width, coded_size.width());
+  DCHECK_GE(image->height, coded_size.height());
   uint8_t* dst_y = video_frame->data(VideoFrame::kYPlane);
   uint8_t* dst_u = video_frame->data(VideoFrame::kUPlane);
   uint8_t* dst_v = video_frame->data(VideoFrame::kVPlane);
@@ -154,7 +380,7 @@
   size_t dst_u_stride = video_frame->stride(VideoFrame::kUPlane);
   size_t dst_v_stride = video_frame->stride(VideoFrame::kVPlane);
 
-  switch (image->format.fourcc) {
+  switch (va_image_format.fourcc) {
     case VA_FOURCC_I420: {
       DCHECK_EQ(image->num_planes, 3u);
       const uint8_t* src_y = mem + image->offsets[0];
@@ -166,30 +392,28 @@
       if (libyuv::I420Copy(src_y, src_y_stride, src_u, src_u_stride, src_v,
                            src_v_stride, dst_y, dst_y_stride, dst_u,
                            dst_u_stride, dst_v, dst_v_stride,
-                           video_frame->coded_size().width(),
-                           video_frame->coded_size().height())) {
+                           coded_size.width(), coded_size.height())) {
         VLOGF(1) << "I420Copy failed";
         return false;
       }
       break;
     }
     case VA_FOURCC_YUY2:
-    case VA_FOURCC('Y', 'U', 'Y', 'V'): {
+    case VA_FOURCC_YUYV: {
       DCHECK_EQ(image->num_planes, 1u);
       const uint8_t* src_yuy2 = mem + image->offsets[0];
       const size_t src_yuy2_stride = image->pitches[0];
       if (libyuv::YUY2ToI420(src_yuy2, src_yuy2_stride, dst_y, dst_y_stride,
                              dst_u, dst_u_stride, dst_v, dst_v_stride,
-                             video_frame->coded_size().width(),
-                             video_frame->coded_size().height())) {
+                             coded_size.width(), coded_size.height())) {
         VLOGF(1) << "YUY2ToI420 failed";
         return false;
       }
       break;
     }
     default:
-      VLOGF(1) << "Can't convert image to I420: unsupported format "
-               << FourccToString(image->format.fourcc);
+      VLOGF(1) << "Can't convert image to I420: unsupported format 0x"
+               << std::hex << va_image_format.fourcc;
       return false;
   }
 
@@ -209,20 +433,53 @@
   DCHECK(decoder_task_runner_->BelongsToCurrentThread());
   TRACE_EVENT0("jpeg", "DecodeTask");
 
-  VaapiJpegDecodeStatus status;
-  std::unique_ptr<ScopedVAImage> image = decoder_.DoDecode(
-      base::make_span<const uint8_t>(static_cast<const uint8_t*>(shm->memory()),
-                                     shm->size()),
-      &status);
-  if (status != VaapiJpegDecodeStatus::kSuccess) {
-    NotifyError(bitstream_buffer_id, VaapiJpegDecodeStatusToError(status));
+  JpegParseResult parse_result;
+  if (!ParseJpegPicture(static_cast<const uint8_t*>(shm->memory()), shm->size(),
+                        &parse_result)) {
+    VLOGF(1) << "ParseJpegPicture failed";
+    NotifyError(bitstream_buffer_id, PARSE_JPEG_FAILED);
     return;
   }
 
-  if (!OutputPictureOnTaskRunner(std::move(image), bitstream_buffer_id,
+  const uint32_t picture_va_rt_format =
+      VaSurfaceFormatForJpeg(parse_result.frame_header);
+  if (!picture_va_rt_format) {
+    VLOGF(1) << "Unsupported subsampling";
+    NotifyError(bitstream_buffer_id, UNSUPPORTED_JPEG);
+    return;
+  }
+
+  // Reuse VASurface if size doesn't change.
+  gfx::Size new_coded_size(parse_result.frame_header.coded_width,
+                           parse_result.frame_header.coded_height);
+  if (new_coded_size != coded_size_ || va_surface_id_ == VA_INVALID_SURFACE ||
+      picture_va_rt_format != va_rt_format_) {
+    vaapi_wrapper_->DestroyContextAndSurfaces();
+    va_surface_id_ = VA_INVALID_SURFACE;
+    va_rt_format_ = picture_va_rt_format;
+
+    std::vector<VASurfaceID> va_surfaces;
+    if (!vaapi_wrapper_->CreateContextAndSurfaces(va_rt_format_, new_coded_size,
+                                                  1, &va_surfaces)) {
+      VLOGF(1) << "Create VA surface failed";
+      NotifyError(bitstream_buffer_id, PLATFORM_FAILURE);
+      return;
+    }
+    va_surface_id_ = va_surfaces[0];
+    coded_size_ = new_coded_size;
+  }
+
+  if (!DoDecode(vaapi_wrapper_.get(), parse_result, va_surface_id_)) {
+    VLOGF(1) << "Decode JPEG failed";
+    NotifyError(bitstream_buffer_id, PLATFORM_FAILURE);
+    return;
+  }
+
+  if (!OutputPicture(va_surface_id_, picture_va_rt_format, bitstream_buffer_id,
                                  video_frame)) {
     VLOGF(1) << "Output picture failed";
     NotifyError(bitstream_buffer_id, PLATFORM_FAILURE);
+    return;
   }
 }
 
@@ -263,4 +520,51 @@
   return VaapiWrapper::IsJpegDecodeSupported();
 }
 
+// static
+bool VaapiJpegDecodeAccelerator::DoDecode(VaapiWrapper* vaapi_wrapper,
+                                          const JpegParseResult& parse_result,
+                                          VASurfaceID va_surface) {
+  DCHECK_NE(va_surface, VA_INVALID_SURFACE);
+  if (!IsVaapiSupportedJpeg(parse_result))
+    return false;
+
+  // Set picture parameters.
+  VAPictureParameterBufferJPEGBaseline pic_param;
+  FillPictureParameters(parse_result.frame_header, &pic_param);
+  if (!vaapi_wrapper->SubmitBuffer(VAPictureParameterBufferType, &pic_param)) {
+    return false;
+  }
+
+  // Set quantization table.
+  VAIQMatrixBufferJPEGBaseline iq_matrix;
+  FillIQMatrix(parse_result.q_table, &iq_matrix);
+  if (!vaapi_wrapper->SubmitBuffer(VAIQMatrixBufferType, &iq_matrix)) {
+    return false;
+  }
+
+  // Set huffman table.
+  VAHuffmanTableBufferJPEGBaseline huffman_table;
+  FillHuffmanTable(parse_result.dc_table, parse_result.ac_table,
+                   &huffman_table);
+  if (!vaapi_wrapper->SubmitBuffer(VAHuffmanTableBufferType, &huffman_table)) {
+    return false;
+  }
+
+  // Set slice parameters.
+  VASliceParameterBufferJPEGBaseline slice_param;
+  FillSliceParameters(parse_result, &slice_param);
+  if (!vaapi_wrapper->SubmitBuffer(VASliceParameterBufferType, &slice_param)) {
+    return false;
+  }
+
+  // Set scan data.
+  if (!vaapi_wrapper->SubmitBuffer(VASliceDataBufferType,
+                                   parse_result.data_size,
+                                   const_cast<char*>(parse_result.data))) {
+    return false;
+  }
+
+  return vaapi_wrapper->ExecuteAndDestroyPendingBuffers(va_surface);
+}
+
 }  // namespace media
--- a/media/gpu/vaapi/vaapi_jpeg_decode_accelerator.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_jpeg_decode_accelerator.h	2019-05-17 18:53:34.272000000 +0300
@@ -10,23 +10,27 @@
 #include <memory>
 
 #include "base/macros.h"
-#include "base/memory/scoped_refptr.h"
 #include "base/memory/weak_ptr.h"
+#include "base/single_thread_task_runner.h"
 #include "base/threading/thread.h"
 #include "media/gpu/media_gpu_export.h"
-#include "media/gpu/vaapi/vaapi_jpeg_decoder.h"
 #include "media/video/jpeg_decode_accelerator.h"
 
-namespace base {
-class SingleThreadTaskRunner;
-}
+// These data types are defined in va/va.h using typedef, reproduced here.
+typedef struct _VAImageFormat VAImageFormat;
+typedef unsigned int VASurfaceID;
 
 namespace media {
 
 class BitstreamBuffer;
-class ScopedVAImage;
+struct JpegParseResult;
 class UnalignedSharedMemory;
-class VideoFrame;
+class VaapiWrapper;
+class VaapiJpegDecodeAcceleratorTest;
+
+// Alternative notation for the VA_FOURCC_YUY2 format, <va/va.h> doesn't provide
+// this specific packing/ordering.
+constexpr uint32_t VA_FOURCC_YUYV = 0x56595559;
 
 // Class to provide JPEG decode acceleration for Intel systems with hardware
 // support for it, and on which libva is available.
@@ -50,6 +54,8 @@
   bool IsSupported() override;
 
  private:
+  friend class VaapiJpegDecodeAcceleratorTest;
+
   // Notifies the client that an error has occurred and decoding cannot
   // continue. The client is notified on the |task_runner_|, i.e., the thread in
   // which |*this| was created.
@@ -64,12 +70,24 @@
                   std::unique_ptr<UnalignedSharedMemory> shm,
                   scoped_refptr<VideoFrame> video_frame);
 
-  // Puts contents of |image| into given |video_frame| and passes the
-  // |input_buffer_id| of the resulting picture to client for output.
-  bool OutputPictureOnTaskRunner(std::unique_ptr<ScopedVAImage> image,
+  // Puts contents of |va_surface| into given |video_frame|, releases the
+  // surface and passes the |input_buffer_id| of the resulting picture to
+  // client for output.
+  bool OutputPicture(VASurfaceID va_surface_id,
+                     uint32_t va_surface_format,
                                  int32_t input_buffer_id,
                                  const scoped_refptr<VideoFrame>& video_frame);
 
+  // Decodes a JPEG picture. It will fill VA-API parameters and call
+  // corresponding VA-API methods according to the JPEG |parse_result|. Decoded
+  // data will be outputted to the given |va_surface|. Returns false on failure.
+  // |vaapi_wrapper| should be initialized in kDecode mode with
+  // VAProfileJPEGBaseline profile. |va_surface| should be created with size at
+  // least as large as the picture size.
+  static bool DoDecode(VaapiWrapper* vaapi_wrapper,
+                       const JpegParseResult& parse_result,
+                       VASurfaceID va_surface);
+
   // ChildThread's task runner.
   const scoped_refptr<base::SingleThreadTaskRunner> task_runner_;
 
@@ -79,16 +97,23 @@
   // The client of this class.
   Client* client_;
 
-  VaapiJpegDecoder decoder_;
+  scoped_refptr<VaapiWrapper> vaapi_wrapper_;
 
-  // Comes after |decoder_| to ensure its destructor is executed before
-  // |decoder_| is destroyed.
+  // Comes after vaapi_wrapper_ to ensure its destructor is executed before
+  // |vaapi_wrapper_| is destroyed.
   base::Thread decoder_thread_;
   // Use this to post tasks to |decoder_thread_| instead of
   // |decoder_thread_.task_runner()| because the latter will be NULL once
   // |decoder_thread_.Stop()| returns.
   scoped_refptr<base::SingleThreadTaskRunner> decoder_task_runner_;
 
+  // The current VA surface for decoding.
+  VASurfaceID va_surface_id_;
+  // The coded size associated with |va_surface_id_|.
+  gfx::Size coded_size_;
+  // The VA RT format associated with |va_surface_id_|.
+  unsigned int va_rt_format_;
+
   // WeakPtr factory for use in posting tasks from |decoder_task_runner_| back
   // to |task_runner_|.  Since |decoder_thread_| is a fully owned member of
   // this class, tasks posted to it may use base::Unretained(this), and tasks
--- a/media/gpu/vaapi/vaapi_jpeg_encode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_jpeg_encode_accelerator.cc	2019-05-17 18:53:34.272000000 +0300
@@ -322,7 +322,7 @@
     VLOGF(1) << "Failed to map output buffer";
     task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&VaapiJpegEncodeAccelerator::NotifyError, weak_this_,
+        base::Bind(&VaapiJpegEncodeAccelerator::NotifyError, weak_this_,
                        buffer_id, INACCESSIBLE_OUTPUT_BUFFER));
     return;
   }
@@ -333,8 +333,8 @@
 
   encoder_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&VaapiJpegEncodeAccelerator::Encoder::EncodeTask,
-                     base::Unretained(encoder_.get()), std::move(request)));
+      base::Bind(&VaapiJpegEncodeAccelerator::Encoder::EncodeTask,
+                 base::Unretained(encoder_.get()), base::Passed(&request)));
 }
 
 }  // namespace media
--- a/media/gpu/vaapi/vaapi_picture_native_pixmap_ozone.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_picture_native_pixmap_ozone.cc	2019-05-17 18:53:34.272000000 +0300
@@ -72,7 +72,7 @@
   const gfx::BufferFormat format = pixmap_->GetBufferFormat();
 
   auto image = base::MakeRefCounted<gl::GLImageNativePixmap>(size_, format);
-  if (!image->Initialize(pixmap_)) {
+  if (!image->Initialize(pixmap_.get())) {
     LOG(ERROR) << "Failed to create GLImage";
     return false;
   }
--- a/media/gpu/vaapi/vaapi_video_decode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_video_decode_accelerator.cc	2019-05-17 18:53:34.276000000 +0300
@@ -16,7 +16,6 @@
 #include "base/logging.h"
 #include "base/macros.h"
 #include "base/metrics/histogram_macros.h"
-#include "base/numerics/ranges.h"
 #include "base/stl_util.h"
 #include "base/strings/string_util.h"
 #include "base/synchronization/waitable_event.h"
@@ -74,8 +73,8 @@
 }
 #endif
 
-// Returns true if the CPU is an Intel Kaby/Gemini/Sky Lake or later.
-// Cpu platform id's are referenced from the following file in kernel source
+// Returns true if the CPU is an Intel Kaby Lake or later.
+// cpu platform id's are referenced from the following file in kernel source
 // arch/x86/include/asm/intel-family.h
 bool IsKabyLakeOrLater() {
   constexpr int kPentiumAndLaterFamily = 0x06;
@@ -96,6 +96,16 @@
   return is_geminilake_or_later;
 }
 
+// Decides if the current platform and profile may decode using the client's
+// PictureBuffers, or engage the Vpp to adapt VaApi's and the client's format.
+bool ShouldDecodeOnclientPictureBuffers(
+    VideoDecodeAccelerator::Config::OutputMode output_mode,
+    VideoCodecProfile profile) {
+  return output_mode == VideoDecodeAccelerator::Config::OutputMode::ALLOCATE &&
+         (IsKabyLakeOrLater() || IsGeminiLakeOrLater()) &&
+         profile == VP9PROFILE_PROFILE0;
+}
+
 }  // namespace
 
 #define RETURN_AND_NOTIFY_ON_FAILURE(result, log, error_code, ret) \
@@ -138,8 +148,8 @@
 void VaapiVideoDecodeAccelerator::NotifyError(Error error) {
   if (!task_runner_->BelongsToCurrentThread()) {
     DCHECK(decoder_thread_task_runner_->BelongsToCurrentThread());
-    task_runner_->PostTask(
-        FROM_HERE, base::BindOnce(&VaapiVideoDecodeAccelerator::NotifyError,
+    task_runner_->PostTask(FROM_HERE,
+                           base::Bind(&VaapiVideoDecodeAccelerator::NotifyError,
                                   weak_this_, error));
     return;
   }
@@ -162,15 +172,13 @@
     : state_(kUninitialized),
       input_ready_(&lock_),
       vaapi_picture_factory_(new VaapiPictureFactory()),
-      buffer_allocation_mode_(BufferAllocationMode::kNormal),
       surfaces_available_(&lock_),
+      decode_using_client_picture_buffers_(false),
       task_runner_(base::ThreadTaskRunnerHandle::Get()),
       decoder_thread_("VaapiDecoderThread"),
       finish_flush_pending_(false),
       awaiting_va_surfaces_recycle_(false),
       requested_num_pics_(0),
-      requested_num_reference_frames_(0),
-      previously_requested_num_reference_frames_(0),
       profile_(VIDEO_CODEC_PROFILE_UNKNOWN),
       make_context_current_cb_(make_context_current_cb),
       bind_image_cb_(bind_image_cb),
@@ -240,8 +248,8 @@
   state_ = kIdle;
   profile_ = profile;
   output_mode_ = config.output_mode;
-  buffer_allocation_mode_ = DecideBufferAllocationMode();
-  previously_requested_num_reference_frames_ = 0;
+  decode_using_client_picture_buffers_ =
+      ShouldDecodeOnclientPictureBuffers(output_mode_, profile_);
   return true;
 }
 
@@ -258,7 +266,7 @@
   {
     base::AutoLock auto_lock(lock_);
     int32_t picture_buffer_id = available_picture_buffers_.front();
-    if (buffer_allocation_mode_ == BufferAllocationMode::kNone) {
+    if (decode_using_client_picture_buffers_) {
       // Find the |pictures_| entry matching |va_surface_id|.
       for (const auto& id_and_picture : pictures_) {
         if (id_and_picture.second->va_surface_id() == va_surface_id) {
@@ -278,7 +286,7 @@
   DVLOGF(4) << "Outputting VASurface " << va_surface->id()
             << " into pixmap bound to picture buffer id " << output_id;
 
-  if (buffer_allocation_mode_ != BufferAllocationMode::kNone) {
+  if (!decode_using_client_picture_buffers_) {
     TRACE_EVENT2("media,gpu", "VAVDA::DownloadFromSurface", "input_id",
                  input_id, "output_id", output_id);
     RETURN_AND_NOTIFY_ON_FAILURE(picture->DownloadFromSurface(va_surface),
@@ -468,10 +476,9 @@
         VLOGF(2) << "Decoder requesting a new set of surfaces";
         task_runner_->PostTask(
             FROM_HERE,
-            base::BindOnce(
-                &VaapiVideoDecodeAccelerator::InitiateSurfaceSetChange,
+            base::Bind(&VaapiVideoDecodeAccelerator::InitiateSurfaceSetChange,
                 weak_this_, decoder_->GetRequiredNumOfPictures(),
-                decoder_->GetPicSize(), decoder_->GetNumReferenceFrames()));
+                       decoder_->GetPicSize()));
         // We'll get rescheduled once ProvidePictureBuffers() finishes.
         return;
 
@@ -508,41 +515,22 @@
   }
 }
 
-void VaapiVideoDecodeAccelerator::InitiateSurfaceSetChange(
-    size_t num_pics,
-    gfx::Size size,
-    size_t num_reference_frames) {
+void VaapiVideoDecodeAccelerator::InitiateSurfaceSetChange(size_t num_pics,
+                                                           gfx::Size size) {
   DCHECK(task_runner_->BelongsToCurrentThread());
   DCHECK(!awaiting_va_surfaces_recycle_);
-  DCHECK_GT(num_pics, num_reference_frames);
 
   // At this point decoder has stopped running and has already posted onto our
   // loop any remaining output request callbacks, which executed before we got
-  // here. Some of them might have been pended though, because we might not have
-  // had enough PictureBuffers to output surfaces to. Initiate a wait cycle,
+  // here. Some of them might have been pended though, because we might not
+  // have had enough TFPictures to output surfaces to. Initiate a wait cycle,
   // which will wait for client to return enough PictureBuffers to us, so that
   // we can finish all pending output callbacks, releasing associated surfaces.
+  VLOGF(2) << "Initiating surface set change";
   awaiting_va_surfaces_recycle_ = true;
 
-  requested_pic_size_ = size;
-
-  if (buffer_allocation_mode_ == BufferAllocationMode::kSuperReduced) {
-    // Add one to the reference frames for the one being currently egressed.
-    requested_num_reference_frames_ = num_reference_frames + 1;
-    requested_num_pics_ = num_pics - num_reference_frames;
-  } else if (buffer_allocation_mode_ == BufferAllocationMode::kReduced) {
-    // Add one to the reference frames for the one being currently egressed,
-    // and an extra allocation for both |client_| and |decoder_|.
-    requested_num_reference_frames_ = num_reference_frames + 2;
-    requested_num_pics_ = num_pics - num_reference_frames + 1;
-  } else {
-    requested_num_reference_frames_ = 0;
     requested_num_pics_ = num_pics;
-  }
-
-  VLOGF(2) << " |requested_num_pics_| = " << requested_num_pics_
-           << "; |requested_num_reference_frames_| = "
-           << requested_num_reference_frames_;
+  requested_pic_size_ = size;
 
   TryFinishSurfaceSetChange();
 }
@@ -554,28 +542,22 @@
     return;
 
   base::AutoLock auto_lock(lock_);
-  const size_t expected_max_available_va_surfaces =
-      IsBufferAllocationModeReducedOrSuperReduced()
-          ? previously_requested_num_reference_frames_
-          : pictures_.size();
   if (!pending_output_cbs_.empty() ||
-      expected_max_available_va_surfaces != available_va_surfaces_.size()) {
-    // If we're here the stream resolution has changed; we need to wait until:
-    // - all |pending_output_cbs_| have been executed
-    // - all VASurfaces are back to |available_va_surfaces_|; we can't use
-    //   |requested_num_reference_frames_| for comparison, since it might have
-    //   changed in the previous call to InitiateSurfaceSetChange(), so we use
-    //   |previously_requested_num_reference_frames_| instead.
+      pictures_.size() != available_va_surfaces_.size()) {
+    // Either:
+    // 1. Not all pending pending output callbacks have been executed yet.
+    // Wait for the client to return enough pictures and retry later.
+    // 2. The above happened and all surface release callbacks have been posted
+    // as the result, but not all have executed yet. Post ourselves after them
+    // to let them release surfaces.
     DVLOGF(2) << "Awaiting pending output/surface release callbacks to finish";
     task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&VaapiVideoDecodeAccelerator::TryFinishSurfaceSetChange,
+        base::Bind(&VaapiVideoDecodeAccelerator::TryFinishSurfaceSetChange,
                        weak_this_));
     return;
   }
 
-  previously_requested_num_reference_frames_ = requested_num_reference_frames_;
-
   // All surfaces released, destroy them and dismiss all PictureBuffers.
   awaiting_va_surfaces_recycle_ = false;
   available_va_surfaces_.clear();
@@ -599,7 +581,6 @@
       base::BindOnce(&Client::ProvidePictureBuffers, client_,
                      requested_num_pics_, format, 1, requested_pic_size_,
                      vaapi_picture_factory_->GetGLTextureTarget()));
-  // |client_| may respond via AssignPictureBuffers().
 }
 
 void VaapiVideoDecodeAccelerator::Decode(
@@ -644,11 +625,10 @@
   const unsigned int va_format = GetVaFormatForVideoCodecProfile(profile_);
   std::vector<VASurfaceID> va_surface_ids;
 
-  // If we aren't in BufferAllocationMode::kNone, we have to allocate a
+  // If we can't |decode_using_client_picture_buffers_|, we have to allocate a
   // |vpp_vaapi_wrapper_| for VaapiPicture to DownloadFromSurface() the VA's
   // internal decoded frame.
-  if (buffer_allocation_mode_ != BufferAllocationMode::kNone &&
-      !vpp_vaapi_wrapper_) {
+  if (!decode_using_client_picture_buffers_ && !vpp_vaapi_wrapper_) {
     vpp_vaapi_wrapper_ = VaapiWrapper::Create(
         VaapiWrapper::kVideoProcess, VAProfileNone,
         base::BindRepeating(&ReportToUMA, VAAPI_VPP_ERROR));
@@ -661,12 +641,11 @@
   for (size_t i = 0; i < buffers.size(); ++i) {
     DCHECK(requested_pic_size_ == buffers[i].size());
 
-    // If we aren't in BufferAllocationMode::kNone, this |picture| is
+    // If |decode_using_client_picture_buffers_| is false, this |picture| is
     // only used as a copy destination. Therefore, the VaapiWrapper used and
     // owned by |picture| is |vpp_vaapi_wrapper_|.
     std::unique_ptr<VaapiPicture> picture = vaapi_picture_factory_->Create(
-        (buffer_allocation_mode_ == BufferAllocationMode::kNone)
-            ? vaapi_wrapper_
+        decode_using_client_picture_buffers_ ? vaapi_wrapper_
             : vpp_vaapi_wrapper_,
         make_context_current_cb_, bind_image_cb_, buffers[i]);
     RETURN_AND_NOTIFY_ON_FAILURE(picture, "Failed creating a VaapiPicture",
@@ -689,27 +668,21 @@
     surfaces_available_.Signal();
   }
 
-  // If we aren't in BufferAllocationMode::kNone, we use |va_surface_ids| for
+  // If |decode_using_client_picture_buffers_|, we use |va_surface_ids| for
   // decode, otherwise ask |vaapi_wrapper_| to allocate them for us.
-  if (buffer_allocation_mode_ == BufferAllocationMode::kNone) {
+  if (decode_using_client_picture_buffers_) {
     DCHECK(!va_surface_ids.empty());
     RETURN_AND_NOTIFY_ON_FAILURE(
         vaapi_wrapper_->CreateContext(va_format, requested_pic_size_),
         "Failed creating VA Context", PLATFORM_FAILURE, );
-    DCHECK_EQ(va_surface_ids.size(), buffers.size());
   } else {
-    const size_t requested_num_surfaces =
-        IsBufferAllocationModeReducedOrSuperReduced()
-            ? requested_num_reference_frames_
-            : pictures_.size();
-    CHECK_NE(requested_num_surfaces, 0u);
     va_surface_ids.clear();
-    RETURN_AND_NOTIFY_ON_FAILURE(vaapi_wrapper_->CreateContextAndSurfaces(
-                                     va_format, requested_pic_size_,
-                                     requested_num_surfaces, &va_surface_ids),
-                                 "Failed creating VA Surfaces",
-                                 PLATFORM_FAILURE, );
+    RETURN_AND_NOTIFY_ON_FAILURE(
+        vaapi_wrapper_->CreateContextAndSurfaces(
+            va_format, requested_pic_size_, buffers.size(), &va_surface_ids),
+        "Failed creating VA Surfaces", PLATFORM_FAILURE, );
   }
+  DCHECK_EQ(va_surface_ids.size(), buffers.size());
 
   available_va_surfaces_.assign(va_surface_ids.begin(), va_surface_ids.end());
 
@@ -813,7 +786,7 @@
 
   task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&VaapiVideoDecodeAccelerator::FinishFlush, weak_this_));
+      base::Bind(&VaapiVideoDecodeAccelerator::FinishFlush, weak_this_));
 }
 
 void VaapiVideoDecodeAccelerator::Flush() {
@@ -852,7 +825,7 @@
   }
 
   task_runner_->PostTask(FROM_HERE,
-                         base::BindOnce(&Client::NotifyFlushDone, client_));
+                         base::Bind(&Client::NotifyFlushDone, client_));
 }
 
 void VaapiVideoDecodeAccelerator::ResetTask() {
@@ -873,7 +846,7 @@
   // And let client know that we are done with reset.
   task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&VaapiVideoDecodeAccelerator::FinishReset, weak_this_));
+      base::Bind(&VaapiVideoDecodeAccelerator::FinishReset, weak_this_));
 }
 
 void VaapiVideoDecodeAccelerator::Reset() {
@@ -918,14 +891,14 @@
     // Let the surface set change finish first before resetting.
     task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&VaapiVideoDecodeAccelerator::FinishReset, weak_this_));
+        base::Bind(&VaapiVideoDecodeAccelerator::FinishReset, weak_this_));
     return;
   }
 
   state_ = kIdle;
 
   task_runner_->PostTask(FROM_HERE,
-                         base::BindOnce(&Client::NotifyResetDone, client_));
+                         base::Bind(&Client::NotifyResetDone, client_));
 
   // The client might have given us new buffers via Decode() while we were
   // resetting and might be waiting for our move, and not call Decode() anymore
@@ -990,7 +963,7 @@
   if (!task_runner_->BelongsToCurrentThread()) {
     task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&VaapiVideoDecodeAccelerator::SurfaceReady, weak_this_,
+        base::Bind(&VaapiVideoDecodeAccelerator::SurfaceReady, weak_this_,
                        dec_surface, bitstream_id, visible_rect, color_space));
     return;
   }
@@ -1018,15 +991,12 @@
     return nullptr;
 
   DCHECK(!awaiting_va_surfaces_recycle_);
-  if (buffer_allocation_mode_ != BufferAllocationMode::kNone) {
+  if (!decode_using_client_picture_buffers_) {
     const VASurfaceID id = available_va_surfaces_.front();
     available_va_surfaces_.pop_front();
 
     TRACE_COUNTER_ID2("media,gpu", "Vaapi VASurfaceIDs", this, "used",
-                      (IsBufferAllocationModeReducedOrSuperReduced()
-                           ? requested_num_reference_frames_
-                           : pictures_.size()) -
-                          available_va_surfaces_.size(),
+                      pictures_.size() - available_va_surfaces_.size(),
                       "available", available_va_surfaces_.size());
 
     return new VASurface(id, requested_pic_size_,
@@ -1057,16 +1027,12 @@
 void VaapiVideoDecodeAccelerator::RecycleVASurfaceID(
     VASurfaceID va_surface_id) {
   DCHECK(task_runner_->BelongsToCurrentThread());
-
   {
     base::AutoLock auto_lock(lock_);
     available_va_surfaces_.push_back(va_surface_id);
-    if (buffer_allocation_mode_ != BufferAllocationMode::kNone) {
+    if (!decode_using_client_picture_buffers_) {
       TRACE_COUNTER_ID2("media,gpu", "Vaapi VASurfaceIDs", this, "used",
-                        (IsBufferAllocationModeReducedOrSuperReduced()
-                             ? requested_num_reference_frames_
-                             : pictures_.size()) -
-                            available_va_surfaces_.size(),
+                        pictures_.size() - available_va_surfaces_.size(),
                         "available", available_va_surfaces_.size());
     }
     surfaces_available_.Signal();
@@ -1074,60 +1040,13 @@
 
   TryOutputPicture();
 }
-
-// static
-VideoDecodeAccelerator::SupportedProfiles
-VaapiVideoDecodeAccelerator::GetSupportedProfiles() {
-  return VaapiWrapper::GetSupportedDecodeProfiles();
-}
-
-VaapiVideoDecodeAccelerator::BufferAllocationMode
-VaapiVideoDecodeAccelerator::DecideBufferAllocationMode() const {
-  // TODO(crbug.com/912295): Enable a better BufferAllocationMode for IMPORT
-  // |output_mode_| as well.
-  if (output_mode_ == VideoDecodeAccelerator::Config::OutputMode::IMPORT)
-    return BufferAllocationMode::kNormal;
-
-  // On KabyLake, GeminiLake and later we can pass to libva the client's
-  // PictureBuffers to decode onto, which skips the use of the Vpp unit and its
-  // associated format reconciliation copy, avoiding all internal buffer
-  // allocations.
-  // TODO(crbug.com/822346,crbug.com/910986): Enable other codecs/platforms.
-  if ((IsKabyLakeOrLater() || IsGeminiLakeOrLater()) &&
-      profile_ == VP9PROFILE_PROFILE0) {
-    return BufferAllocationMode::kNone;
-  }
-
-  // If we're here, we have to use the Vpp unit and allocate buffers for
-  // |decoder_|; usually we'd have to allocate the |decoder_|s
-  // GetRequiredNumOfPictures() internally, we can allocate just |decoder_|s
-  // GetNumReferenceFrames() + 1. Moreover, we also request the |client_| to
-  // allocate less than the usual |decoder_|s GetRequiredNumOfPictures().
-
-  // Another +1 is experimentally needed for high-to-high resolution changes.
-  // TODO(mcasas): Figure out why and why only H264, see crbug.com/912295 and
-  // http://crrev.com/c/1363807/9/media/gpu/h264_decoder.cc#1449.
-  if (profile_ >= H264PROFILE_MIN && profile_ <= H264PROFILE_MAX)
-    return BufferAllocationMode::kReduced;
-
-  return BufferAllocationMode::kSuperReduced;
-}
-
-bool VaapiVideoDecodeAccelerator::IsBufferAllocationModeReducedOrSuperReduced()
-    const {
-  return buffer_allocation_mode_ == BufferAllocationMode::kSuperReduced ||
-         buffer_allocation_mode_ == BufferAllocationMode::kReduced;
-}
-
 bool VaapiVideoDecodeAccelerator::OnMemoryDump(
     const base::trace_event::MemoryDumpArgs& args,
     base::trace_event::ProcessMemoryDump* pmd) {
   using base::trace_event::MemoryAllocatorDump;
   base::AutoLock auto_lock(lock_);
-  if (buffer_allocation_mode_ == BufferAllocationMode::kNone ||
-      !requested_num_reference_frames_) {
+  if (decode_using_client_picture_buffers_ || pictures_.empty())
     return false;
-  }
 
   auto dump_name = base::StringPrintf("gpu/vaapi/decoder/0x%" PRIxPTR,
                                       reinterpret_cast<uintptr_t>(this));
@@ -1141,23 +1060,24 @@
   const float va_surface_bytes_per_pixel =
       va_surface_format == VA_RT_FORMAT_YUV420 ? kNumBytesPerPixelYUV420
                                                : kNumBytesPerPixelYUV420_10bpp;
-  // Report |requested_num_surfaces| and the associated memory size. The
-  // calculated size is an estimation since we don't know the internal VA
+  // Report |pictures_.size()| and the associated memory size.
+  // The calculated size is an estimation since we don't know the internal VA
   // strides, texture compression, headers, etc, but is a good lower boundary.
-  const size_t requested_num_surfaces =
-      IsBufferAllocationModeReducedOrSuperReduced()
-          ? requested_num_reference_frames_
-          : pictures_.size();
-  dump->AddScalar(MemoryAllocatorDump::kNameSize,
-                  MemoryAllocatorDump::kUnitsBytes,
-                  static_cast<uint64_t>(requested_num_surfaces *
-                                        requested_pic_size_.GetArea() *
+  dump->AddScalar(
+      MemoryAllocatorDump::kNameSize, MemoryAllocatorDump::kUnitsBytes,
+      static_cast<uint64_t>(pictures_.size() * requested_pic_size_.GetArea() *
                                         va_surface_bytes_per_pixel));
   dump->AddScalar(MemoryAllocatorDump::kNameObjectCount,
                   MemoryAllocatorDump::kUnitsObjects,
-                  static_cast<uint64_t>(requested_num_surfaces));
+                  static_cast<uint64_t>(pictures_.size()));
 
   return true;
 }
 
+// static
+VideoDecodeAccelerator::SupportedProfiles
+VaapiVideoDecodeAccelerator::GetSupportedProfiles() {
+  return VaapiWrapper::GetSupportedDecodeProfiles();
+}
+
 }  // namespace media
--- a/media/gpu/vaapi/vaapi_video_decode_accelerator.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_video_decode_accelerator.h	2019-05-17 18:53:34.276000000 +0300
@@ -178,38 +178,13 @@
   // |available_va_surfaces_|
   void RecycleVASurfaceID(VASurfaceID va_surface_id);
 
-  // Request a new set of |num_pics| PictureBuffers to be allocated by
-  // |client_|. Up to |num_reference_frames| out of |num_pics_| might be needed
-  // by |decoder_|.
-  void InitiateSurfaceSetChange(size_t num_pics,
-                                gfx::Size size,
-                                size_t num_reference_frames);
+  // Initiate wait cycle for surfaces to be released before we release them
+  // and allocate new ones, as requested by the decoder.
+  void InitiateSurfaceSetChange(size_t num_pics, gfx::Size size);
 
   // Check if the surfaces have been released or post ourselves for later.
   void TryFinishSurfaceSetChange();
 
-  // Different modes of internal buffer allocations.
-  enum class BufferAllocationMode {
-    // Only using |client_|s provided PictureBuffers, none internal.
-    kNone,
-    // Using a reduced amount of |client_|s provided PictureBuffers and
-    // |decoder_|s GetNumReferenceFrames() internallly.
-    kSuperReduced,
-    // Similar to kSuperReduced, but we have to increase slightly the amount of
-    // PictureBuffers allocated for the |client_|.
-    kReduced,
-
-    // VaapiVideoDecodeAccelerator can work with this mode on all platforms.
-    // Using |client_|s provided PictureBuffers and as many internally
-    // allocated.
-    kNormal,
-  };
-
-  // Decides the concrete buffer allocation mode, depending on the hardware
-  // platform and other parameters.
-  BufferAllocationMode DecideBufferAllocationMode() const;
-  bool IsBufferAllocationModeReducedOrSuperReduced() const;
-
   // VAVDA state.
   enum State {
     // Initialize() not called yet or failed.
@@ -246,9 +221,6 @@
   // Only used on |decoder_thread_task_runner_|.
   std::unique_ptr<AcceleratedVideoDecoder> decoder_;
 
-  // Filled in during Initialize().
-  BufferAllocationMode buffer_allocation_mode_;
-
   // VaapiWrapper for VPP (Video Post Processing). This is used for copying
   // from a decoded surface to a surface bound to client's PictureBuffer.
   scoped_refptr<VaapiWrapper> vpp_vaapi_wrapper_;
@@ -280,6 +252,10 @@
   // Only used on |task_runner_|.
   base::queue<base::OnceClosure> pending_output_cbs_;
 
+  // Under some circumstances, we can pass to libva our own VASurfaceIDs to
+  // decode onto, which skips one copy. Only used on |task_runner_|.
+  bool decode_using_client_picture_buffers_;
+
   // WeakPtr<> pointing to |this| for use in posting tasks from the decoder
   // thread back to the ChildThread.  Because the decoder thread is a member of
   // this class, any task running on the decoder thread is guaranteed that this
@@ -313,14 +289,10 @@
   // to be returned before we can free them. Only used on |task_runner_|.
   bool awaiting_va_surfaces_recycle_;
 
-  // Last requested number/resolution of output PictureBuffers.
+  // Last requested number/resolution of output picture buffers and their
+  // format.
   size_t requested_num_pics_;
   gfx::Size requested_pic_size_;
-  // Max number of reference frames needed by |decoder_|. Only used on
-  // |task_runner_| and when in BufferAllocationMode::kNone.
-  size_t requested_num_reference_frames_;
-  size_t previously_requested_num_reference_frames_;
-
   VideoCodecProfile profile_;
 
   // Callback to make GL context current.
--- a/media/gpu/vaapi/vaapi_video_decode_accelerator_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_video_decode_accelerator_unittest.cc	2019-05-17 18:53:34.276000000 +0300
@@ -40,7 +40,7 @@
 constexpr int32_t kBitstreamId = 123;
 constexpr size_t kInputSize = 256;
 
-constexpr size_t kNumPictures = 4;
+constexpr size_t kNumPictures = 2;
 const gfx::Size kPictureSize(64, 48);
 
 constexpr size_t kNewNumPictures = 3;
@@ -61,7 +61,6 @@
   MOCK_METHOD0(Decode, DecodeResult());
   MOCK_CONST_METHOD0(GetPicSize, gfx::Size());
   MOCK_CONST_METHOD0(GetRequiredNumOfPictures, size_t());
-  MOCK_CONST_METHOD0(GetNumReferenceFrames, size_t());
 };
 
 class MockVaapiWrapper : public VaapiWrapper {
@@ -154,8 +153,7 @@
     decoder_thread_.Start();
 
     // Don't want to go through a vda_->Initialize() because it binds too many
-    // items of the environment. Instead, do all the necessary steps here.
-
+    // items of the environment. Instead, just start the decoder thread.
     vda_.decoder_thread_task_runner_ = decoder_thread_.task_runner();
 
     // Plug in all the mocks and ourselves as the |client_|.
@@ -165,13 +163,10 @@
     vda_.vpp_vaapi_wrapper_ = mock_vpp_vaapi_wrapper_;
     vda_.vaapi_picture_factory_.reset(mock_vaapi_picture_factory_);
 
-    // TODO(crbug.com/917999): add IMPORT mode to test variations.
     vda_.output_mode_ = VideoDecodeAccelerator::Config::OutputMode::ALLOCATE;
 
-    vda_.buffer_allocation_mode_ =
-        GetParam().decode_using_client_picture_buffers
-            ? VaapiVideoDecodeAccelerator::BufferAllocationMode::kNone
-            : VaapiVideoDecodeAccelerator::BufferAllocationMode::kSuperReduced;
+    vda_.decode_using_client_picture_buffers_ =
+        GetParam().decode_using_client_picture_buffers;
 
     vda_.state_ = VaapiVideoDecodeAccelerator::kIdle;
   }
@@ -226,9 +221,6 @@
     EXPECT_CALL(*mock_decoder_, GetRequiredNumOfPictures())
         .WillOnce(Return(num_pictures));
     EXPECT_CALL(*mock_decoder_, GetPicSize()).WillOnce(Return(picture_size));
-    const size_t kNumReferenceFrames = num_pictures / 2;
-    EXPECT_CALL(*mock_decoder_, GetNumReferenceFrames())
-        .WillOnce(Return(kNumReferenceFrames));
     EXPECT_CALL(*mock_vaapi_wrapper_, DestroyContextAndSurfaces());
 
     if (expect_dismiss_picture_buffers) {
@@ -236,15 +228,8 @@
           .Times(num_picture_buffers_to_dismiss);
     }
 
-    const size_t expected_num_picture_buffers_requested =
-        vda_.buffer_allocation_mode_ ==
-                VaapiVideoDecodeAccelerator::BufferAllocationMode::kSuperReduced
-            ? num_pictures - kNumReferenceFrames
-            : num_pictures;
-
     EXPECT_CALL(*this,
-                ProvidePictureBuffers(expected_num_picture_buffers_requested, _,
-                                      1, picture_size, _))
+                ProvidePictureBuffers(num_pictures, _, 1, picture_size, _))
         .WillOnce(RunClosure(quit_closure));
 
     base::SharedMemoryHandle handle;
@@ -278,21 +263,17 @@
           MockCreateVaapiPicture(mock_vaapi_wrapper_.get(), picture_size))
           .Times(num_pictures);
     } else {
-      EXPECT_EQ(
-          vda_.buffer_allocation_mode_,
-          VaapiVideoDecodeAccelerator::BufferAllocationMode::kSuperReduced);
-      const size_t kNumReferenceFrames = 1 + num_pictures / 2;
-      EXPECT_CALL(
-          *mock_vaapi_wrapper_,
-          CreateContextAndSurfaces(_, picture_size, kNumReferenceFrames, _))
+      EXPECT_CALL(*mock_vaapi_wrapper_,
+                  CreateContextAndSurfaces(_, picture_size, num_pictures, _))
           .WillOnce(DoAll(
-              WithArg<3>(Invoke([kNumReferenceFrames](
-                                    std::vector<VASurfaceID>* va_surface_ids) {
-                va_surface_ids->resize(kNumReferenceFrames);
+              WithArg<3>(Invoke(
+                  [num_pictures](std::vector<VASurfaceID>* va_surface_ids) {
+                    va_surface_ids->resize(num_pictures);
               })),
               Return(true)));
-      EXPECT_CALL(*mock_vaapi_picture_factory_,
-                  MockCreateVaapiPicture(_, picture_size))
+      EXPECT_CALL(
+          *mock_vaapi_picture_factory_,
+          MockCreateVaapiPicture(mock_vpp_vaapi_wrapper_.get(), picture_size))
           .Times(num_pictures);
     }
 
@@ -466,7 +447,7 @@
     {VP9PROFILE_MIN, false /* decode_using_client_picture_buffers */},
     {VP9PROFILE_MIN, true /* decode_using_client_picture_buffers */}};
 
-INSTANTIATE_TEST_SUITE_P(/* No prefix. */,
+INSTANTIATE_TEST_CASE_P(/* No prefix. */,
                          VaapiVideoDecodeAcceleratorTest,
                          ValuesIn(kTestCases));
 
--- a/media/gpu/vaapi/vaapi_video_encode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_video_encode_accelerator.cc	2019-05-17 18:53:34.276000000 +0300
@@ -16,7 +16,6 @@
 #include <va/va_enc_vp8.h>
 
 #include "base/bind.h"
-#include "base/bind_helpers.h"
 #include "base/callback.h"
 #include "base/macros.h"
 #include "base/memory/ptr_util.h"
@@ -37,9 +36,7 @@
 #include "media/gpu/vaapi/vaapi_common.h"
 #include "media/gpu/vaapi/vaapi_picture_factory.h"
 #include "media/gpu/vaapi/vp8_encoder.h"
-#include "media/gpu/vaapi/vp9_encoder.h"
 #include "media/gpu/vp8_reference_frame_vector.h"
-#include "media/gpu/vp9_reference_frame_vector.h"
 
 #if defined(OS_POSIX)
 #include "media/gpu/vaapi/vaapi_picture_native_pixmap.h"
@@ -196,27 +193,6 @@
   VaapiVideoEncodeAccelerator* const vea_;
 };
 
-class VaapiVideoEncodeAccelerator::VP9Accelerator
-    : public VP9Encoder::Accelerator {
- public:
-  explicit VP9Accelerator(VaapiVideoEncodeAccelerator* vea) : vea_(vea) {}
-
-  ~VP9Accelerator() override = default;
-
-  // VP9Encoder::Accelerator implementation.
-  scoped_refptr<VP9Picture> GetPicture(
-      AcceleratedVideoEncoder::EncodeJob* job) override;
-
-  bool SubmitFrameParameters(
-      AcceleratedVideoEncoder::EncodeJob* job,
-      const VP9Encoder::EncodeParams& encode_params,
-      scoped_refptr<VP9Picture> pic,
-      const Vp9ReferenceFrameVector& ref_frames) override;
-
- private:
-  VaapiVideoEncodeAccelerator* const vea_;
-};
-
 VaapiVideoEncodeAccelerator::VaapiVideoEncodeAccelerator()
     : codec_(kUnknownVideoCodec),
       output_buffer_byte_size_(0),
@@ -245,7 +221,7 @@
   client_ = client_ptr_factory_->GetWeakPtr();
 
   codec_ = VideoCodecProfileToVideoCodec(config.output_profile);
-  if (codec_ != kCodecH264 && codec_ != kCodecVP8 && codec_ != kCodecVP9) {
+  if (codec_ != kCodecH264 && codec_ != kCodecVP8) {
     VLOGF(1) << "Unsupported profile: "
              << GetProfileName(config.output_profile);
     return false;
@@ -342,11 +318,6 @@
           std::make_unique<VP8Encoder>(std::make_unique<VP8Accelerator>(this));
       break;
 
-    case kCodecVP9:
-      encoder_ =
-          std::make_unique<VP9Encoder>(std::make_unique<VP9Accelerator>(this));
-      break;
-
     default:
       NOTREACHED() << "Unsupported codec type " << GetCodecName(codec_);
       return;
@@ -644,8 +615,8 @@
 
   encoder_thread_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&VaapiVideoEncodeAccelerator::UseOutputBitstreamBufferTask,
-                     base::Unretained(this), std::move(buffer_ref)));
+      base::Bind(&VaapiVideoEncodeAccelerator::UseOutputBitstreamBufferTask,
+                 base::Unretained(this), base::Passed(&buffer_ref)));
 }
 
 void VaapiVideoEncodeAccelerator::UseOutputBitstreamBufferTask(
@@ -672,7 +643,7 @@
   allocation.SetBitrate(0, 0, bitrate);
   encoder_thread_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(
+      base::Bind(
           &VaapiVideoEncodeAccelerator::RequestEncodingParametersChangeTask,
           base::Unretained(this), allocation, framerate));
 }
@@ -686,7 +657,7 @@
 
   encoder_thread_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(
+      base::Bind(
           &VaapiVideoEncodeAccelerator::RequestEncodingParametersChangeTask,
           base::Unretained(this), bitrate_allocation, framerate));
 }
@@ -1189,138 +1160,6 @@
 
   job->AddSetupCallback(base::BindOnce(
       &VaapiVideoEncodeAccelerator::SubmitVAEncMiscParamBuffer,
-      base::Unretained(vea_), VAEncMiscParameterTypeRateControl,
-      MakeRefCountedBytes(&rate_control_param, sizeof(rate_control_param))));
-
-  job->AddSetupCallback(base::BindOnce(
-      &VaapiVideoEncodeAccelerator::SubmitVAEncMiscParamBuffer,
-      base::Unretained(vea_), VAEncMiscParameterTypeFrameRate,
-      MakeRefCountedBytes(&framerate_param, sizeof(framerate_param))));
-
-  job->AddSetupCallback(
-      base::BindOnce(&VaapiVideoEncodeAccelerator::SubmitVAEncMiscParamBuffer,
-                     base::Unretained(vea_), VAEncMiscParameterTypeHRD,
-                     MakeRefCountedBytes(&hrd_param, sizeof(hrd_param))));
-
-  return true;
-}
-
-scoped_refptr<VP9Picture>
-VaapiVideoEncodeAccelerator::VP9Accelerator::GetPicture(
-    AcceleratedVideoEncoder::EncodeJob* job) {
-  return base::MakeRefCounted<VaapiVP9Picture>(
-      job->AsVaapiEncodeJob()->reconstructed_surface());
-}
-
-bool VaapiVideoEncodeAccelerator::VP9Accelerator::SubmitFrameParameters(
-    AcceleratedVideoEncoder::EncodeJob* job,
-    const VP9Encoder::EncodeParams& encode_params,
-    scoped_refptr<VP9Picture> pic,
-    const Vp9ReferenceFrameVector& ref_frames) {
-  VAEncSequenceParameterBufferVP9 seq_param = {};
-
-  const auto& frame_header = pic->frame_hdr;
-  // TODO(crbug.com/811912): Double check whether the
-  // max_frame_width or max_frame_height affects any of the memory
-  // allocation and tighten these values based on that.
-  constexpr gfx::Size kMaxFrameSize(4096, 4096);
-  seq_param.max_frame_width = kMaxFrameSize.height();
-  seq_param.max_frame_height = kMaxFrameSize.width();
-  seq_param.bits_per_second = encode_params.bitrate_allocation.GetSumBps();
-  seq_param.intra_period = encode_params.kf_period_frames;
-
-  VAEncPictureParameterBufferVP9 pic_param = {};
-
-  pic_param.frame_width_src = frame_header->frame_width;
-  pic_param.frame_height_src = frame_header->frame_height;
-  pic_param.frame_width_dst = frame_header->render_width;
-  pic_param.frame_height_dst = frame_header->render_height;
-
-  pic_param.reconstructed_frame = pic->AsVaapiVP9Picture()->GetVASurfaceID();
-  DCHECK_NE(pic_param.reconstructed_frame, VA_INVALID_ID);
-
-  for (size_t i = 0; i < kVp9NumRefFrames; i++) {
-    auto ref_pic = ref_frames.GetFrame(i);
-    pic_param.reference_frames[i] =
-        ref_pic ? ref_pic->AsVaapiVP9Picture()->GetVASurfaceID()
-                : VA_INVALID_ID;
-  }
-
-  pic_param.coded_buf = job->AsVaapiEncodeJob()->coded_buffer_id();
-  DCHECK_NE(pic_param.coded_buf, VA_INVALID_ID);
-
-  if (frame_header->IsKeyframe()) {
-    pic_param.ref_flags.bits.force_kf = true;
-  } else {
-    // use golden, altref and last for prediction
-    pic_param.ref_flags.bits.ref_frame_ctrl_l0 = 0x07;
-    pic_param.ref_flags.bits.ref_last_idx = frame_header->ref_frame_idx[0];
-    pic_param.ref_flags.bits.ref_gf_idx = frame_header->ref_frame_idx[1];
-    pic_param.ref_flags.bits.ref_arf_idx = frame_header->ref_frame_idx[2];
-  }
-
-  pic_param.pic_flags.bits.frame_type = frame_header->frame_type;
-  pic_param.pic_flags.bits.show_frame = frame_header->show_frame;
-  pic_param.pic_flags.bits.error_resilient_mode =
-      frame_header->error_resilient_mode;
-  pic_param.pic_flags.bits.intra_only = frame_header->intra_only;
-  pic_param.pic_flags.bits.allow_high_precision_mv =
-      frame_header->allow_high_precision_mv;
-  pic_param.pic_flags.bits.mcomp_filter_type =
-      frame_header->interpolation_filter;
-  pic_param.pic_flags.bits.frame_parallel_decoding_mode =
-      frame_header->frame_parallel_decoding_mode;
-  pic_param.pic_flags.bits.reset_frame_context =
-      frame_header->reset_frame_context;
-  pic_param.pic_flags.bits.refresh_frame_context =
-      frame_header->refresh_frame_context;
-  pic_param.pic_flags.bits.frame_context_idx = frame_header->frame_context_idx;
-
-  pic_param.refresh_frame_flags = frame_header->refresh_frame_flags;
-
-  pic_param.luma_ac_qindex = frame_header->quant_params.base_q_idx;
-  pic_param.luma_dc_qindex_delta = frame_header->quant_params.delta_q_y_dc;
-  pic_param.chroma_ac_qindex_delta = frame_header->quant_params.delta_q_uv_ac;
-  pic_param.chroma_dc_qindex_delta = frame_header->quant_params.delta_q_uv_dc;
-
-  // TODO(crbug.com/924786): Unlike the current vp8 implementation,
-  // SegmentationParams and LoopFilterParams are the part of Parser structure
-  // rather than included them in FrameHeader. So, for now, we are not taking
-  // segmentation and loopfilter related parameter from frame_hdr. But since the
-  // filter level may affect on quality at lower bitrates, we set a constant
-  // value (== 10) which is what other VA-API implementations like libyami and
-  // gstreamer-vaapi are using.
-  pic_param.filter_level = 10;
-  pic_param.log2_tile_rows = frame_header->tile_rows_log2;
-  pic_param.log2_tile_columns = frame_header->tile_cols_log2;
-
-  VAEncMiscParameterRateControl rate_control_param = {};
-  rate_control_param.bits_per_second =
-      encode_params.bitrate_allocation.GetSumBps();
-  rate_control_param.target_percentage = kTargetBitratePercentage;
-  rate_control_param.window_size = encode_params.cpb_window_size_ms;
-  rate_control_param.initial_qp = encode_params.initial_qp;
-  rate_control_param.rc_flags.bits.disable_frame_skip = true;
-
-  VAEncMiscParameterFrameRate framerate_param = {};
-  framerate_param.framerate = encode_params.framerate;
-
-  VAEncMiscParameterHRD hrd_param = {};
-  hrd_param.buffer_size = encode_params.cpb_size_bits;
-  hrd_param.initial_buffer_fullness = hrd_param.buffer_size / 2;
-
-  job->AddSetupCallback(
-      base::BindOnce(&VaapiVideoEncodeAccelerator::SubmitBuffer,
-                     base::Unretained(vea_), VAEncSequenceParameterBufferType,
-                     MakeRefCountedBytes(&seq_param, sizeof(seq_param))));
-
-  job->AddSetupCallback(
-      base::BindOnce(&VaapiVideoEncodeAccelerator::SubmitBuffer,
-                     base::Unretained(vea_), VAEncPictureParameterBufferType,
-                     MakeRefCountedBytes(&pic_param, sizeof(pic_param))));
-
-  job->AddSetupCallback(base::BindOnce(
-      &VaapiVideoEncodeAccelerator::SubmitVAEncMiscParamBuffer,
       base::Unretained(vea_), VAEncMiscParameterTypeRateControl,
       MakeRefCountedBytes(&rate_control_param, sizeof(rate_control_param))));
 
--- a/media/gpu/vaapi/vaapi_video_encode_accelerator.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_video_encode_accelerator.h	2019-05-17 18:53:34.276000000 +0300
@@ -52,7 +52,6 @@
  private:
   class H264Accelerator;
   class VP8Accelerator;
-  class VP9Accelerator;
 
   // Encoder state.
   enum State {
--- a/media/gpu/vaapi/vaapi_wrapper.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_wrapper.cc	2019-05-17 18:53:34.276000000 +0300
@@ -185,14 +185,6 @@
     return true;
   }
 
-  // TODO(crbug.com/811912): Remove once VP9 encoding is to be enabled by
-  // default.
-  if (mode == VaapiWrapper::CodecMode::kEncode &&
-      va_profile == VAProfileVP9Profile0 &&
-      !base::FeatureList::IsEnabled(kVaapiVP9Encoder)) {
-    return true;
-  }
-
   return false;
 }
 
@@ -402,8 +394,7 @@
     required_attribs.push_back({VAConfigAttribRateControl, VA_RC_CBR});
 
   // VAConfigAttribEncPackedHeaders is H.264 specific.
-  if ((profile >= VAProfileH264Baseline && profile <= VAProfileH264High) ||
-      (profile == VAProfileH264ConstrainedBaseline)) {
+  if (profile >= VAProfileH264Baseline && profile <= VAProfileH264High) {
     required_attribs.push_back(
         {VAConfigAttribEncPackedHeaders,
          VA_ENC_PACKED_HEADER_SEQUENCE | VA_ENC_PACKED_HEADER_PICTURE});
@@ -1019,9 +1010,15 @@
   va_attrib_extbuf.width = size.width();
   va_attrib_extbuf.height = size.height();
 
+  size_t num_fds = pixmap->GetDmaBufFdCount();
   size_t num_planes =
       gfx::NumberOfPlanesForBufferFormat(pixmap->GetBufferFormat());
-  std::vector<uintptr_t> fds(num_planes);
+  if (num_fds == 0 || num_fds > num_planes) {
+    LOG(ERROR) << "Invalid number of dmabuf fds: " << num_fds
+               << " , planes: " << num_planes;
+    return nullptr;
+  }
+
   for (size_t i = 0; i < num_planes; ++i) {
     va_attrib_extbuf.pitches[i] = pixmap->GetDmaBufPitch(i);
     va_attrib_extbuf.offsets[i] = pixmap->GetDmaBufOffset(i);
@@ -1030,15 +1027,17 @@
   }
   va_attrib_extbuf.num_planes = num_planes;
 
-  if (pixmap->GetDmaBufFd(0) < 0) {
+  std::vector<unsigned long> fds(num_fds);
+  for (size_t i = 0; i < num_fds; ++i) {
+    int dmabuf_fd = pixmap->GetDmaBufFd(i);
+    if (dmabuf_fd < 0) {
     LOG(ERROR) << "Failed to get dmabuf from an Ozone NativePixmap";
     return nullptr;
   }
-  // We only have to pass the first file descriptor to a driver. A VA-API driver
-  // shall create a VASurface from the single fd correctly.
-  uintptr_t fd = base::checked_cast<uintptr_t>(pixmap->GetDmaBufFd(0));
-  va_attrib_extbuf.buffers = &fd;
-  va_attrib_extbuf.num_buffers = 1u;
+    fds[i] = dmabuf_fd;
+  }
+  va_attrib_extbuf.buffers = fds.data();
+  va_attrib_extbuf.num_buffers = fds.size();
 
   va_attrib_extbuf.flags = 0;
   va_attrib_extbuf.private_data = NULL;
--- a/media/gpu/vaapi/vaapi_wrapper.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_wrapper.h	2019-05-17 18:53:34.276000000 +0300
@@ -223,7 +223,7 @@
 
  private:
   friend class base::RefCountedThreadSafe<VaapiWrapper>;
-  friend class VaapiJpegDecoderTest;
+  friend class VaapiJpegDecodeAcceleratorTest;
 
   bool Initialize(CodecMode mode, VAProfile va_profile);
   void Deinitialize();
--- a/media/gpu/vaapi/vp8_encoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vaapi/vp8_encoder.cc	2019-05-17 18:53:34.276000000 +0300
@@ -11,15 +11,15 @@
 
 namespace {
 // Keyframe period.
-constexpr size_t kKFPeriod = 3000;
+const size_t kKFPeriod = 3000;
 
 // Arbitrarily chosen bitrate window size for rate control, in ms.
-constexpr int kCPBWindowSizeMs = 1500;
+const int kCPBWindowSizeMs = 1500;
 
 // Based on WebRTC's defaults.
-constexpr int kMinQP = 4;
-constexpr int kMaxQP = 112;
-constexpr int kDefaultQP = (3 * kMinQP + kMaxQP) / 4;
+const int kMinQP = 4;
+const int kMaxQP = 112;
+const int kDefaultQP = (3 * kMinQP + kMaxQP) / 4;
 }  // namespace
 
 VP8Encoder::EncodeParams::EncodeParams()
@@ -147,11 +147,7 @@
   DCHECK(!visible_size_.IsEmpty());
   current_frame_hdr_.width = visible_size_.width();
   current_frame_hdr_.height = visible_size_.height();
-  // Since initial_qp is always kDefaultQP (=31), y_ac_qi should be 27
-  // (the table index for kDefaultQP, see rfc 14.1. table ac_qlookup)
-  DCHECK_EQ(current_params_.initial_qp, kDefaultQP);
-  constexpr uint8_t kDefaultQPACQIndex = 27;
-  current_frame_hdr_.quantization_hdr.y_ac_qi = kDefaultQPACQIndex;
+  current_frame_hdr_.quantization_hdr.y_ac_qi = current_params_.initial_qp;
   current_frame_hdr_.show_frame = true;
   // TODO(sprang): Make this dynamic. Value based on reference implementation
   // in libyami (https://github.com/intel/libyami).
--- a/media/gpu/video_decode_accelerator_tests.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/video_decode_accelerator_tests.cc	2019-05-17 18:53:34.276000000 +0300
@@ -2,71 +2,91 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
+#include "base/at_exit.h"
 #include "base/command_line.h"
+#include "base/test/scoped_task_environment.h"
+#include "base/test/test_timeouts.h"
 #include "media/base/test_data_util.h"
-#include "media/gpu/test/video_frame_file_writer.h"
+#include "media/gpu/buildflags.h"
 #include "media/gpu/test/video_frame_validator.h"
 #include "media/gpu/test/video_player/frame_renderer_dummy.h"
-#include "media/gpu/test/video_player/frame_renderer_thumbnail.h"
 #include "media/gpu/test/video_player/video.h"
 #include "media/gpu/test/video_player/video_collection.h"
 #include "media/gpu/test/video_player/video_decoder_client.h"
 #include "media/gpu/test/video_player/video_player.h"
-#include "media/gpu/test/video_player/video_player_test_environment.h"
 #include "mojo/core/embedder/embedder.h"
 #include "testing/gtest/include/gtest/gtest.h"
 
+#if BUILDFLAG(USE_VAAPI)
+#include "media/gpu/vaapi/vaapi_wrapper.h"
+#endif
+
 namespace media {
 namespace test {
 
 namespace {
-// Video decoder tests usage message.
-constexpr const char* usage_msg =
-    "usage: video_decode_accelerator_tests [--help] [--disable_validator]\n"
-    "                                      [--output_frames] [<video>]\n";
-// Video decoder tests help message.
-constexpr const char* help_msg =
-    "Run the video decode accelerator tests on the specified video. If no\n"
-    "video is specified the default \"test-25fps.h264\" video will be used.\n"
-    "\nThe following arguments are supported:\n"
-    "  --disable_validator  disable frame validation, useful on old\n"
-    "                       platforms that don't support import mode.\n"
-    "  --output_frames      write all decoded video frames to the\n"
-    "                       \"video_frames\" folder.\n"
-    "  --help               display this help and exit.\n";
+// Test environment for video decode tests. Performs setup and teardown once for
+// the entire test run.
+class VideoDecoderTestEnvironment : public ::testing::Environment {
+ public:
+  explicit VideoDecoderTestEnvironment(const Video* video) : video_(video) {}
+  virtual ~VideoDecoderTestEnvironment() {}
+
+  // Set up the video decode test environment, only called once.
+  void SetUp() override;
+  // Tear down the video decode test environment, only called once.
+  void TearDown() override;
+
+  std::unique_ptr<base::test::ScopedTaskEnvironment> task_environment_;
+  std::unique_ptr<FrameRendererDummy> dummy_frame_renderer_;
+  const Video* const video_;
+
+  // An exit manager is required to run callbacks on shutdown.
+  base::AtExitManager at_exit_manager;
+};
+
+void VideoDecoderTestEnvironment::SetUp() {
+  // Setting up a task environment will create a task runner for the current
+  // thread and allow posting tasks to other threads. This is required for the
+  // test video player to function correctly.
+  TestTimeouts::Initialize();
+  task_environment_ = std::make_unique<base::test::ScopedTaskEnvironment>(
+      base::test::ScopedTaskEnvironment::MainThreadType::UI);
+
+  // Set the default test data path.
+  media::test::Video::SetTestDataPath(media::GetTestDataPath());
+
+  // Perform all static initialization that is required when running video
+  // decoders in a test environment.
+#if BUILDFLAG(USE_VAAPI)
+  media::VaapiWrapper::PreSandboxInitialization();
+#endif
+
+  dummy_frame_renderer_ = FrameRendererDummy::Create();
+  ASSERT_NE(dummy_frame_renderer_, nullptr);
+}
+
+void VideoDecoderTestEnvironment::TearDown() {
+  dummy_frame_renderer_.reset();
+  task_environment_.reset();
+}
 
-media::test::VideoPlayerTestEnvironment* g_env;
+media::test::VideoDecoderTestEnvironment* g_env;
 
 // Video decode test class. Performs setup and teardown for each single test.
 class VideoDecoderTest : public ::testing::Test {
  public:
   std::unique_ptr<VideoPlayer> CreateVideoPlayer(
       const Video* video,
-      const VideoDecoderClientConfig& config = VideoDecoderClientConfig(),
-      std::unique_ptr<FrameRenderer> frame_renderer =
-          FrameRendererDummy::Create()) {
-    LOG_ASSERT(video);
-    std::vector<std::unique_ptr<VideoFrameProcessor>> frame_processors;
-
-    // Validate decoded video frames.
-    if (g_env->enable_validator_) {
-      frame_processors.push_back(
-          media::test::VideoFrameValidator::Create(video->FrameChecksums()));
+      const VideoDecoderClientConfig& config = VideoDecoderClientConfig()) {
+    frame_validator_ =
+        media::test::VideoFrameValidator::Create(video->FrameChecksums());
+    return VideoPlayer::Create(video, g_env->dummy_frame_renderer_.get(),
+                               {frame_validator_.get()}, config);
     }
 
-    // Write decoded video frames to the 'video_frames/<test_name/>' folder.
-    if (g_env->output_frames_) {
-      const ::testing::TestInfo* const test_info =
-          ::testing::UnitTest::GetInstance()->current_test_info();
-      base::FilePath output_folder =
-          base::FilePath("video_frames")
-              .Append(base::FilePath(test_info->name()));
-      frame_processors.push_back(VideoFrameFileWriter::Create(output_folder));
-    }
-
-    return VideoPlayer::Create(video, std::move(frame_renderer),
-                               std::move(frame_processors), config);
-  }
+ protected:
+  std::unique_ptr<VideoFrameValidator> frame_validator_;
 };
 
 }  // namespace
@@ -81,7 +101,7 @@
 
   EXPECT_EQ(tvp->GetFlushDoneCount(), 1u);
   EXPECT_EQ(tvp->GetFrameDecodedCount(), g_env->video_->NumFrames());
-  EXPECT_TRUE(tvp->WaitForFrameProcessors());
+  EXPECT_TRUE(frame_validator_->WaitUntilValidated());
 }
 
 // Flush the decoder immediately after initialization.
@@ -95,7 +115,29 @@
 
   EXPECT_EQ(tvp->GetFlushDoneCount(), 2u);
   EXPECT_EQ(tvp->GetFrameDecodedCount(), g_env->video_->NumFrames());
-  EXPECT_TRUE(tvp->WaitForFrameProcessors());
+  EXPECT_TRUE(frame_validator_->WaitUntilValidated());
+}
+
+// Flush the decoder immediately after doing a mid-stream reset, without waiting
+// for a kResetDone event.
+TEST_F(VideoDecoderTest, FlushBeforeResetDone) {
+  auto tvp = CreateVideoPlayer(g_env->video_);
+
+  tvp->Play();
+  EXPECT_TRUE(tvp->WaitForFrameDecoded(g_env->video_->NumFrames() / 2));
+  tvp->Reset();
+  tvp->Flush();
+  EXPECT_TRUE(tvp->WaitForResetDone());
+  EXPECT_TRUE(tvp->WaitForFlushDone());
+
+  // As flush doesn't cancel reset, we should have received a single kResetDone
+  // and kFlushDone event. We didn't decode the entire video, but more frames
+  // might be decoded by the time we called reset, so we can only check whether
+  // the decoded frame count is <= the total number of frames.
+  EXPECT_EQ(tvp->GetResetDoneCount(), 1u);
+  EXPECT_EQ(tvp->GetFlushDoneCount(), 1u);
+  EXPECT_LE(tvp->GetFrameDecodedCount(), g_env->video_->NumFrames());
+  EXPECT_TRUE(frame_validator_->WaitUntilValidated());
 }
 
 // Reset the decoder immediately after initialization.
@@ -110,7 +152,7 @@
   EXPECT_EQ(tvp->GetResetDoneCount(), 1u);
   EXPECT_EQ(tvp->GetFlushDoneCount(), 1u);
   EXPECT_EQ(tvp->GetFrameDecodedCount(), g_env->video_->NumFrames());
-  EXPECT_TRUE(tvp->WaitForFrameProcessors());
+  EXPECT_TRUE(frame_validator_->WaitUntilValidated());
 }
 
 // Reset the decoder when the middle of the stream is reached.
@@ -129,7 +171,7 @@
   EXPECT_EQ(tvp->GetFlushDoneCount(), 1u);
   EXPECT_EQ(tvp->GetFrameDecodedCount(),
             numFramesDecoded + g_env->video_->NumFrames());
-  EXPECT_TRUE(tvp->WaitForFrameProcessors());
+  EXPECT_TRUE(frame_validator_->WaitUntilValidated());
 }
 
 // Reset the decoder when the end of the stream is reached.
@@ -147,7 +189,7 @@
   EXPECT_EQ(tvp->GetResetDoneCount(), 1u);
   EXPECT_EQ(tvp->GetFlushDoneCount(), 2u);
   EXPECT_EQ(tvp->GetFrameDecodedCount(), g_env->video_->NumFrames() * 2);
-  EXPECT_TRUE(tvp->WaitForFrameProcessors());
+  EXPECT_TRUE(frame_validator_->WaitUntilValidated());
 }
 
 // Reset the decoder immediately when the end-of-stream flush starts, without
@@ -169,7 +211,22 @@
   EXPECT_LE(tvp->GetFlushDoneCount(), 1u);
   EXPECT_EQ(tvp->GetResetDoneCount(), 1u);
   EXPECT_LE(tvp->GetFrameDecodedCount(), g_env->video_->NumFrames());
-  EXPECT_TRUE(tvp->WaitForFrameProcessors());
+  EXPECT_TRUE(frame_validator_->WaitUntilValidated());
+}
+
+// Play video from start to end. Multiple buffer decodes will be queued in the
+// decoder, without waiting for the result of the previous decode requests.
+TEST_F(VideoDecoderTest, FlushAtEndOfStream_MultipleOutstandingDecodes) {
+  VideoDecoderClientConfig config;
+  config.max_outstanding_decode_requests = 5;
+  auto tvp = CreateVideoPlayer(g_env->video_, config);
+
+  tvp->Play();
+  EXPECT_TRUE(tvp->WaitForFlushDone());
+
+  EXPECT_EQ(tvp->GetFlushDoneCount(), 1u);
+  EXPECT_EQ(tvp->GetFrameDecodedCount(), g_env->video_->NumFrames());
+  EXPECT_TRUE(frame_validator_->WaitUntilValidated());
 }
 
 // Reset the decoder immediately when encountering the first config info in a
@@ -195,87 +252,15 @@
   EXPECT_EQ(tvp->GetFrameDecodedCount(),
             numFramesDecoded + g_env->video_->NumFrames());
   EXPECT_GE(tvp->GetEventCount(VideoPlayerEvent::kConfigInfo), 1u);
-  EXPECT_TRUE(tvp->WaitForFrameProcessors());
-}
-
-// Play video from start to end. Multiple buffer decodes will be queued in the
-// decoder, without waiting for the result of the previous decode requests.
-TEST_F(VideoDecoderTest, FlushAtEndOfStream_MultipleOutstandingDecodes) {
-  VideoDecoderClientConfig config;
-  config.max_outstanding_decode_requests = 5;
-  auto tvp = CreateVideoPlayer(g_env->video_, config);
-
-  tvp->Play();
-  EXPECT_TRUE(tvp->WaitForFlushDone());
-
-  EXPECT_EQ(tvp->GetFlushDoneCount(), 1u);
-  EXPECT_EQ(tvp->GetFrameDecodedCount(), g_env->video_->NumFrames());
-  EXPECT_TRUE(tvp->WaitForFrameProcessors());
-}
-
-// Play multiple videos simultaneously from start to finish.
-TEST_F(VideoDecoderTest, FlushAtEndOfStream_MultipleConcurrentDecodes) {
-  // The minimal number of concurrent decoders we expect to be supported.
-  constexpr size_t kMinSupportedConcurrentDecoders = 3;
-
-  std::vector<std::unique_ptr<VideoPlayer>> tvps(
-      kMinSupportedConcurrentDecoders);
-  for (size_t i = 0; i < kMinSupportedConcurrentDecoders; ++i)
-    tvps[i] = CreateVideoPlayer(g_env->video_);
-
-  for (size_t i = 0; i < kMinSupportedConcurrentDecoders; ++i)
-    tvps[i]->Play();
-
-  for (size_t i = 0; i < kMinSupportedConcurrentDecoders; ++i) {
-    EXPECT_TRUE(tvps[i]->WaitForFlushDone());
-    EXPECT_EQ(tvps[i]->GetFlushDoneCount(), 1u);
-    EXPECT_EQ(tvps[i]->GetFrameDecodedCount(), g_env->video_->NumFrames());
-    EXPECT_TRUE(tvps[i]->WaitForFrameProcessors());
-  }
-}
-
-// Play a video from start to finish. Thumbnails of the decoded frames will be
-// rendered into a image, whose checksum is compared to a golden value. This
-// test is only needed on older platforms that don't support the video frame
-// validator, which requires direct access to the video frame's memory. This
-// test is only ran when --disable_validator is specified, and will be
-// deprecated in the future.
-TEST_F(VideoDecoderTest, FlushAtEndOfStream_RenderThumbnails) {
-  if (g_env->enable_validator_)
-    GTEST_SKIP();
-
-  VideoDecoderClientConfig config;
-  config.allocation_mode = AllocationMode::kAllocate;
-  auto tvp = CreateVideoPlayer(
-      g_env->video_, config,
-      FrameRendererThumbnail::Create(g_env->video_->ThumbnailChecksums()));
-
-  tvp->Play();
-  EXPECT_TRUE(tvp->WaitForFlushDone());
-
-  EXPECT_EQ(tvp->GetFlushDoneCount(), 1u);
-  EXPECT_EQ(tvp->GetFrameDecodedCount(), g_env->video_->NumFrames());
-  EXPECT_TRUE(tvp->WaitForFrameProcessors());
-  EXPECT_TRUE(static_cast<FrameRendererThumbnail*>(tvp->GetFrameRenderer())
-                  ->ValidateThumbnail());
+  EXPECT_EQ(0u, frame_validator_->GetMismatchedFramesCount());
 }
 
 }  // namespace test
 }  // namespace media
 
 int main(int argc, char** argv) {
-  base::CommandLine::Init(argc, argv);
-  const base::CommandLine* cmd_line = base::CommandLine::ForCurrentProcess();
-
-  // Print the help message if requested. This needs to be done before
-  // initializing gtest, to overwrite the default gtest help message.
-  LOG_ASSERT(cmd_line);
-  if (cmd_line->HasSwitch("help")) {
-    std::cout << media::test::usage_msg << media::test::help_msg;
-    return 0;
-  }
-
   testing::InitGoogleTest(&argc, argv);
+  base::CommandLine::Init(argc, argv);
 
   // Using shared memory requires mojo to be initialized (crbug.com/849207).
   mojo::core::Init();
@@ -285,44 +270,12 @@
   settings.logging_dest = logging::LOG_TO_SYSTEM_DEBUG_LOG;
   LOG_ASSERT(logging::InitLogging(settings));
 
-  // Set the default test data path.
-  media::test::Video::SetTestDataPath(media::GetTestDataPath());
-
-  // Check if a video was specified on the command line.
-  std::unique_ptr<media::test::Video> video;
-  base::CommandLine::StringVector args = cmd_line->GetArgs();
-  if (args.size() >= 1) {
-    video = std::make_unique<media::test::Video>(base::FilePath(args[0]));
-    if (!video->Load()) {
-      LOG(ERROR) << "Failed to load " << args[0];
-      return 0;
-    }
-  }
-
-  // Set up our test environment.
-  media::test::g_env = static_cast<media::test::VideoPlayerTestEnvironment*>(
+  // Set up our test environment
+  const media::test::Video* video =
+      &media::test::kDefaultTestVideoCollection[0];
+  media::test::g_env = static_cast<media::test::VideoDecoderTestEnvironment*>(
       testing::AddGlobalTestEnvironment(
-          new media::test::VideoPlayerTestEnvironment(
-              video ? video.get()
-                    : &media::test::kDefaultTestVideoCollection[0])));
-
-  // Parse command line arguments.
-  base::CommandLine::SwitchMap switches = cmd_line->GetSwitches();
-  for (base::CommandLine::SwitchMap::const_iterator it = switches.begin();
-       it != switches.end(); ++it) {
-    if (it->first == "disable_validator") {
-      media::test::g_env->enable_validator_ = false;
-    } else if (it->first == "output_frames") {
-      media::test::g_env->output_frames_ = true;
-    } else if (it->first.find("gtest_") == 0 || it->first == "v" ||
-               it->first == "vmodule") {
-      // Ignore
-    } else {
-      std::cout << "unknown option: --" << it->first << "\n"
-                << media::test::usage_msg;
-      return 0;
-    }
-  }
+          new media::test::VideoDecoderTestEnvironment(video)));
 
   return RUN_ALL_TESTS();
 }
--- a/media/gpu/video_decode_accelerator_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/video_decode_accelerator_unittest.cc	2019-05-17 18:53:34.280000000 +0300
@@ -68,7 +68,6 @@
 #include "media/gpu/test/texture_ref.h"
 #include "media/gpu/test/video_accelerator_unittest_helpers.h"
 #include "media/gpu/test/video_decode_accelerator_unittest_helpers.h"
-#include "media/gpu/test/video_frame_file_writer.h"
 #include "media/gpu/test/video_frame_validator.h"
 #include "media/video/h264_parser.h"
 #include "mojo/core/embedder/embedder.h"
@@ -135,12 +134,9 @@
 // requesting the VDA itself to allocate buffers.
 bool g_test_import = false;
 
-// Validate decoded frames using frame validator.
-bool g_validate_frames = false;
-// Calculate decoded frame checksums using frame validator;
-bool g_calculate_checksums = false;
-// Write decoded frames to YUV files.
-bool g_output_frames = false;
+// VideoFrameValidator flags.
+// If this is set to non-zero, g_test_import becomes true.
+uint32_t g_frame_validator_flags = 0;
 
 // This is the location of the test files. If empty, they're in the current
 // working directory.
@@ -296,7 +292,6 @@
       std::string encoded_data,
       RenderingHelper* rendering_helper,
       std::unique_ptr<media::test::VideoFrameValidator> video_frame_validator,
-      std::unique_ptr<media::test::VideoFrameFileWriter> video_frame_writer,
       media::test::ClientStateNotification<ClientState>* note);
   ~GLRenderingVDAClient() override;
   void CreateAndStartDecoder();
@@ -399,7 +394,6 @@
   const std::unique_ptr<media::test::EncodedDataHelper> encoded_data_helper_;
   const std::unique_ptr<media::test::VideoFrameValidator>
       video_frame_validator_;
-  const std::unique_ptr<media::test::VideoFrameFileWriter> video_frame_writer_;
 
   base::WeakPtr<GLRenderingVDAClient> weak_this_;
   base::WeakPtrFactory<GLRenderingVDAClient> weak_this_factory_;
@@ -419,7 +413,6 @@
     std::string encoded_data,
     RenderingHelper* rendering_helper,
     std::unique_ptr<media::test::VideoFrameValidator> video_frame_validator,
-    std::unique_ptr<media::test::VideoFrameFileWriter> video_frame_writer,
     media::test::ClientStateNotification<ClientState>* note)
     : config_(std::move(config)),
       rendering_helper_(rendering_helper),
@@ -441,7 +434,6 @@
           std::move(encoded_data),
           config_.profile)),
       video_frame_validator_(std::move(video_frame_validator)),
-      video_frame_writer_(std::move(video_frame_writer)),
       weak_this_factory_(this) {
   DCHECK_NE(config.profile, VIDEO_CODEC_PROFILE_UNKNOWN);
   LOG_ASSERT(config_.num_in_flight_decodes > 0);
@@ -615,16 +607,9 @@
     ASSERT_NE(video_frame.get(), nullptr);
     video_frame_validator_->ProcessVideoFrame(std::move(video_frame),
                                               frame_index_);
-    video_frame_validator_->WaitUntilDone();
-  }
-  if (video_frame_writer_) {
-    auto video_frame = texture_it->second->ExportVideoFrame(visible_rect);
-    ASSERT_NE(video_frame.get(), nullptr);
-    video_frame_writer_->ProcessVideoFrame(std::move(video_frame),
-                                           frame_index_);
-    video_frame_writer_->WaitUntilDone();
-  }
+    video_frame_validator_->WaitUntilValidated();
   frame_index_++;
+  }
   rendering_helper_->ConsumeVideoFrame(config_.window_id,
                                        std::move(video_frame_texture));
 }
@@ -893,7 +878,7 @@
   if (config_.decode_calls_per_second > 0) {
     base::ThreadTaskRunnerHandle::Get()->PostDelayedTask(
         FROM_HERE,
-        base::BindOnce(&GLRenderingVDAClient::DecodeNextFragment, AsWeakPtr()),
+        base::Bind(&GLRenderingVDAClient::DecodeNextFragment, AsWeakPtr()),
         base::TimeDelta::FromSeconds(1) / config_.decode_calls_per_second);
   } else {
     // Unless DecodeNextFragment() is posted from the above PostDelayedTask(),
@@ -986,16 +971,17 @@
 void VideoDecodeAcceleratorTest::TearDown() {
   // |clients_| must be deleted first because |clients_| use |notes_|.
   g_env->GetRenderingTaskRunner()->PostTask(
-      FROM_HERE, base::BindOnce(&Delete<ClientsVector>, std::move(clients_)));
+      FROM_HERE,
+      base::BindOnce(&Delete<ClientsVector>, base::Passed(&clients_)));
 
   g_env->GetRenderingTaskRunner()->PostTask(
-      FROM_HERE, base::BindOnce(&Delete<NotesVector>, std::move(notes_)));
+      FROM_HERE, base::BindOnce(&Delete<NotesVector>, base::Passed(&notes_)));
 
   WaitUntilIdle();
 
   g_env->GetRenderingTaskRunner()->PostTask(
       FROM_HERE,
-      base::BindOnce(&Delete<TestFilesVector>, std::move(test_video_files_)));
+      base::Bind(&Delete<TestFilesVector>, base::Passed(&test_video_files_)));
 
   base::WaitableEvent done(base::WaitableEvent::ResetPolicy::AUTOMATIC,
                            base::WaitableEvent::InitialState::NOT_SIGNALED);
@@ -1082,9 +1068,9 @@
   base::WaitableEvent done(base::WaitableEvent::ResetPolicy::AUTOMATIC,
                            base::WaitableEvent::InitialState::NOT_SIGNALED);
   g_env->GetRenderingTaskRunner()->PostTask(
-      FROM_HERE, base::BindOnce(&RenderingHelper::Initialize,
-                                base::Unretained(&rendering_helper_),
-                                helper_params, &done));
+      FROM_HERE,
+      base::Bind(&RenderingHelper::Initialize,
+                 base::Unretained(&rendering_helper_), helper_params, &done));
   done.Wait();
 }
 
@@ -1113,7 +1099,7 @@
                            base::WaitableEvent::InitialState::NOT_SIGNALED);
   g_env->GetRenderingTaskRunner()->PostTask(
       FROM_HERE,
-      base::BindOnce(&base::WaitableEvent::Signal, base::Unretained(&done)));
+      base::Bind(&base::WaitableEvent::Signal, base::Unretained(&done)));
   done.Wait();
 }
 
@@ -1165,41 +1151,25 @@
 std::unique_ptr<media::test::VideoFrameValidator>
 CreateAndInitializeVideoFrameValidator(
     const base::FilePath::StringType& video_file) {
-  DCHECK(g_validate_frames || g_calculate_checksums);
-
-  // Read md5 frame checksums.
-  base::FilePath filepath(video_file);
-  std::vector<std::string> frame_checksums;
-  if (g_validate_frames) {
-    base::FilePath md5_file_path =
-        filepath.AddExtension(FILE_PATH_LITERAL(".frames.md5"));
-    frame_checksums = test::ReadGoldenThumbnailMD5s(md5_file_path);
-    if (frame_checksums.empty()) {
-      LOG(ERROR) << "Failed to read md5 values in " << md5_file_path;
-      return nullptr;
-    }
-  }
-
-  return media::test::VideoFrameValidator::Create(frame_checksums);
-}
-
-std::unique_ptr<media::test::VideoFrameFileWriter>
-CreateAndInitializeVideoFrameWriter(
-    const base::FilePath::StringType& video_file) {
-  DCHECK(g_output_frames);
-
   // Initialize prefix of yuv files.
   base::FilePath prefix_output_yuv;
   base::FilePath filepath(video_file);
+  if (g_frame_validator_flags & test::VideoFrameValidator::OUTPUTYUV) {
   if (!g_thumbnail_output_dir.empty() &&
       base::DirectoryExists(g_thumbnail_output_dir)) {
     prefix_output_yuv = g_thumbnail_output_dir.Append(filepath.BaseName());
   } else {
-    prefix_output_yuv =
-        GetTestDataFile(filepath).AddExtension(FILE_PATH_LITERAL(".frames"));
+      prefix_output_yuv = GetTestDataFile(filepath);
   }
-  return media::test::VideoFrameFileWriter::Create(
-      prefix_output_yuv, media::test::VideoFrameFileWriter::OutputFormat::kYUV);
+  }
+#if BUILDFLAG(USE_VAAPI)
+  bool linear = false;
+#else
+  bool linear = true;
+#endif
+  return media::test::VideoFrameValidator::Create(
+      g_frame_validator_flags, prefix_output_yuv,
+      filepath.AddExtension(FILE_PATH_LITERAL(".frames.md5")), linear);
 }
 
 // Fails on Win only. crbug.com/849368
@@ -1238,7 +1208,8 @@
   notes_.resize(num_concurrent_decoders);
   clients_.resize(num_concurrent_decoders);
 
-  if (g_validate_frames) {
+  bool use_video_frame_validator = g_frame_validator_flags != 0;
+  if (use_video_frame_validator) {
     LOG(INFO) << "Using Frame Validator..";
 #if !defined(OS_CHROMEOS)
     LOG(FATAL) << "FrameValidator (g_frame_validator) cannot be used on "
@@ -1275,20 +1246,14 @@
     config.num_frames = video_file->num_frames;
 
     std::unique_ptr<media::test::VideoFrameValidator> video_frame_validator;
-    if (g_validate_frames) {
+    if (use_video_frame_validator) {
       video_frame_validator =
           CreateAndInitializeVideoFrameValidator(video_file->file_name);
       ASSERT_NE(video_frame_validator.get(), nullptr);
     }
-    std::unique_ptr<media::test::VideoFrameFileWriter> video_frame_writer;
-    if (g_output_frames) {
-      video_frame_writer =
-          CreateAndInitializeVideoFrameWriter(video_file->file_name);
-    }
     clients_[index] = std::make_unique<GLRenderingVDAClient>(
         std::move(config), video_file->data_str, &rendering_helper_,
-        std::move(video_frame_validator), std::move(video_frame_writer),
-        notes_[index].get());
+        std::move(video_frame_validator), notes_[index].get());
   }
 
   RenderingHelperParams helper_params;
@@ -1389,7 +1354,7 @@
                              base::WaitableEvent::InitialState::NOT_SIGNALED);
     g_env->GetRenderingTaskRunner()->PostTask(
         FROM_HERE,
-        base::BindOnce(&RenderingHelper::GetThumbnailsAsRGBA,
+        base::Bind(&RenderingHelper::GetThumbnailsAsRGBA,
                        base::Unretained(&rendering_helper_), &rgba, &done));
     done.Wait();
 
@@ -1463,18 +1428,17 @@
       clients_[i]->OutputFrameDeliveryTimes(&output_file);
     }
   }
-}
+};
 
 // Test that replay after EOS works fine.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     ReplayAfterEOS,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(
         std::make_tuple(1, 1, 4, END_OF_STREAM_RESET, CS_RESET, false, false)));
 
 // Test that Reset() before the first Decode() works fine.
-INSTANTIATE_TEST_SUITE_P(
-    ResetBeforeDecode,
+INSTANTIATE_TEST_CASE_P(ResetBeforeDecode,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(std::make_tuple(1,
                                       1,
@@ -1485,7 +1449,7 @@
                                       false)));
 
 // Test Reset() immediately after Decode() containing config info.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     ResetAfterFirstConfigInfo,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(std::make_tuple(1,
@@ -1497,7 +1461,7 @@
                                       false)));
 
 // Test Reset() immediately after Flush() and before NotifyFlushDone().
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     ResetBeforeNotifyFlushDone,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(std::make_tuple(1,
@@ -1510,13 +1474,13 @@
 
 // Test that Reset() mid-stream works fine and doesn't affect decoding even when
 // Decode() calls are made during the reset.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     MidStreamReset,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(
         std::make_tuple(1, 1, 1, MID_STREAM_RESET, CS_RESET, false, false)));
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     SlowRendering,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(
@@ -1524,7 +1488,7 @@
 
 // Test that Destroy() mid-stream works fine (primarily this is testing that no
 // crashes occur).
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     TearDownTiming,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(
@@ -1580,7 +1544,7 @@
                         false)));
 
 // Test that decoding various variation works with multiple in-flight decodes.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     DecodeVariations,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(
@@ -1597,7 +1561,7 @@
 
 // Find out how many concurrent decoders can go before we exhaust system
 // resources.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     ResourceExhaustion,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(std::make_tuple(kMinSupportedNumConcurrentDecoders,
@@ -1616,8 +1580,8 @@
                                       false)));
 
 // Allow MAYBE macro substitution.
-#define WRAPPED_INSTANTIATE_TEST_SUITE_P(a, b, c) \
-  INSTANTIATE_TEST_SUITE_P(a, b, c)
+#define WRAPPED_INSTANTIATE_TEST_CASE_P(a, b, c) \
+  INSTANTIATE_TEST_CASE_P(a, b, c)
 
 #if defined(OS_WIN)
 // There are no reference images for windows.
@@ -1626,7 +1590,7 @@
 #define MAYBE_Thumbnail Thumbnail
 #endif
 // Thumbnailing test
-WRAPPED_INSTANTIATE_TEST_SUITE_P(
+WRAPPED_INSTANTIATE_TEST_CASE_P(
     MAYBE_Thumbnail,
     VideoDecodeAcceleratorParamTest,
     ::testing::Values(
@@ -1649,7 +1613,7 @@
 
   clients_.push_back(std::make_unique<GLRenderingVDAClient>(
       std::move(config), video_file->data_str, &rendering_helper_, nullptr,
-      nullptr, notes_[0].get()));
+      notes_[0].get()));
   RenderingHelperParams helper_params;
   helper_params.num_windows = 1;
   InitializeRenderingHelper(helper_params);
@@ -1684,7 +1648,7 @@
 
   clients_.push_back(std::make_unique<GLRenderingVDAClient>(
       std::move(config), video_file->data_str, &rendering_helper_, nullptr,
-      nullptr, notes_[0].get()));
+      notes_[0].get()));
   RenderingHelperParams helper_params;
   helper_params.num_windows = 1;
   InitializeRenderingHelper(helper_params);
@@ -1698,8 +1662,7 @@
 // --gtest_filter=VideoDecodeAcceleratorTest.DISABLED_GenMD5 and
 // --gtest_also_run_disabled_tests
 TEST_F(VideoDecodeAcceleratorTest, DISABLED_GenMD5) {
-  g_validate_frames = false;
-  g_calculate_checksums = true;
+  g_frame_validator_flags = test::VideoFrameValidator::GENMD5;
   g_test_import = true;
 
   ASSERT_EQ(test_video_files_.size(), 1u);
@@ -1713,11 +1676,9 @@
   config.num_frames = video_file->num_frames;
   auto video_frame_validator =
       CreateAndInitializeVideoFrameValidator(video_file->file_name);
-  media::test::VideoFrameValidator* frame_validator =
-      video_frame_validator.get();
   clients_.push_back(std::make_unique<GLRenderingVDAClient>(
       std::move(config), video_file->data_str, &rendering_helper_,
-      std::move(video_frame_validator), nullptr, notes_[0].get()));
+      std::move(video_frame_validator), notes_[0].get()));
   RenderingHelperParams helper_params;
   helper_params.num_windows = 1;
   InitializeRenderingHelper(helper_params);
@@ -1725,24 +1686,8 @@
   ClientState last_state = WaitUntilDecodeFinish(notes_[0].get());
   EXPECT_NE(CS_ERROR, last_state);
 
-  // Write out computed md5 values.
-  frame_validator->WaitUntilDone();
-  const std::vector<std::string>& frame_checksums =
-      frame_validator->GetFrameChecksums();
-  base::FilePath md5_file_path(video_file->file_name);
-  md5_file_path = md5_file_path.AddExtension(FILE_PATH_LITERAL(".frames.md5"));
-  base::File md5_file(md5_file_path, base::File::FLAG_CREATE_ALWAYS |
-                                         base::File::FLAG_WRITE |
-                                         base::File::FLAG_APPEND);
-  if (!md5_file.IsValid())
-    LOG(ERROR) << "Failed to create md5 file to write " << md5_file_path;
-
-  for (const std::string& frame_checksum : frame_checksums) {
-    md5_file.Write(0, frame_checksum.data(), frame_checksum.size());
-    md5_file.Write(0, "\n", 1);
-  }
-
   g_test_import = false;
+  g_frame_validator_flags = 0;
 }
 #endif
 
@@ -1862,9 +1807,11 @@
                                      base::SPLIT_WANT_NONEMPTY);
       for (auto& f : flags) {
         if (f == "check") {
-          media::g_validate_frames = true;
+          media::g_frame_validator_flags |=
+              media::test::VideoFrameValidator::CHECK;
         } else if (f == "dump") {
-          media::g_output_frames = true;
+          media::g_frame_validator_flags |=
+              media::test::VideoFrameValidator::OUTPUTYUV;
         } else {
           LOG(FATAL) << "Unknown flag: " << f;
         }
--- a/media/gpu/video_encode_accelerator_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/video_encode_accelerator_unittest.cc	2019-05-17 18:53:34.280000000 +0300
@@ -31,12 +31,10 @@
 #include "base/strings/stringprintf.h"
 #include "base/strings/utf_string_conversions.h"
 #include "base/test/launcher/unit_test_launcher.h"
-#include "base/test/scoped_feature_list.h"
 #include "base/test/scoped_task_environment.h"
 #include "base/test/test_suite.h"
 #include "base/threading/thread.h"
 #include "base/threading/thread_checker.h"
-#include "base/threading/thread_task_runner_handle.h"
 #include "base/time/time.h"
 #include "base/timer/timer.h"
 #include "build/build_config.h"
@@ -44,7 +42,6 @@
 #include "media/base/bitstream_buffer.h"
 #include "media/base/cdm_context.h"
 #include "media/base/decoder_buffer.h"
-#include "media/base/media_switches.h"
 #include "media/base/media_util.h"
 #include "media/base/test_data_util.h"
 #include "media/base/video_decoder.h"
@@ -52,7 +49,6 @@
 #include "media/filters/ffmpeg_video_decoder.h"
 #include "media/filters/ivf_parser.h"
 #include "media/filters/vp8_parser.h"
-#include "media/filters/vp9_parser.h"
 #include "media/gpu/buildflags.h"
 #include "media/gpu/gpu_video_encode_accelerator_factory.h"
 #include "media/gpu/h264_decoder.h"
@@ -60,7 +56,6 @@
 #include "media/gpu/test/video_accelerator_unittest_helpers.h"
 #include "media/gpu/test/video_encode_accelerator_unittest_helpers.h"
 #include "media/video/fake_video_encode_accelerator.h"
-#include "media/video/h264_level_limits.h"
 #include "media/video/h264_parser.h"
 #include "media/video/video_encode_accelerator.h"
 #include "mojo/core/embedder/embedder.h"
@@ -106,8 +101,6 @@
 const double kBitrateTolerance = 0.1;
 // Minimum required FPS throughput for the basic performance test.
 const uint32_t kMinPerfFPS = 30;
-// The frame size for 2160p (UHD 4K) video in pixels.
-const int k2160PSizeInPixels = 3840 * 2160;
 // Minimum (arbitrary) number of frames required to enforce bitrate requirements
 // over. Streams shorter than this may be too short to realistically require
 // an encoder to be able to converge to the requested bitrate over.
@@ -127,7 +120,7 @@
 // The syntax of each test stream is:
 // "in_filename:width:height:profile:out_filename:requested_bitrate
 //  :requested_framerate:requested_subsequent_bitrate
-//  :requested_subsequent_framerate:pixel_format:requested_level"
+//  :requested_subsequent_framerate:pixel_format"
 // Instead of ":", "," can be used as a seperator as well. Note that ":" does
 // not work on Windows as it interferes with file paths.
 // - |in_filename| is YUV raw stream. Its format must be |pixel_format|
@@ -135,18 +128,16 @@
 // - |width| and |height| are in pixels.
 // - |profile| to encode into (values of VideoCodecProfile).
 // - |out_filename| filename to save the encoded stream to (optional). The
-//   format for H264 is Annex-B byte stream. The format for VP8 and VP9 is IVF.
-//   Output stream is saved for the simple encode test only. H264 raw stream and
-//   IVF can be used as input of VDA unittest. H264 raw stream can be played by
+//   format for H264 is Annex-B byte stream. The format for VP8 is IVF. Output
+//   stream is saved for the simple encode test only. H264 raw stream and IVF
+//   can be used as input of VDA unittest. H264 raw stream can be played by
 //   "mplayer -fps 25 out.h264" and IVF can be played by mplayer directly.
 //   Helpful description: http://wiki.multimedia.cx/index.php?title=IVF
 // Further parameters are optional (need to provide preceding positional
 // parameters if a specific subsequent parameter is required):
-// - |requested_bitrate| requested bitrate in bits per second, use
-//                       kDefaultBitrate if not provided.
+// - |requested_bitrate| requested bitrate in bits per second.
 //   Bitrate is only forced for tests that test bitrate.
-// - |requested_framerate| requested initial framerate, use kDefaultFramerate
-//                         if not provided.
+// - |requested_framerate| requested initial framerate.
 // - |requested_subsequent_bitrate| bitrate to switch to in the middle of the
 //                                  stream.
 // - |requested_subsequent_framerate| framerate to switch to in the middle
@@ -154,9 +145,6 @@
 // - |pixel_format| is the VideoPixelFormat of |in_filename|. Users needs to
 //   set the value corresponding to the desired format. If it is not specified,
 //   this would be PIXEL_FORMAT_I420.
-// - |requested_level| requested output level. Currently only for H264 codec and
-//                     the value should be assigned as H264LevelIDC enum in
-//                     h264_parser.h. Use kDefaultH264Level if not provided.
 
 #if defined(OS_CHROMEOS) || defined(OS_LINUX)
 const char* g_default_in_filename = "bear_320x192_40frames.yuv";
@@ -191,13 +179,6 @@
 
 bool g_fake_encoder = false;
 
-// Enable/Disable ForceLevel test. Since currently not all devices support level
-// configuration, test could be disabled by the command line
-// "--force_level=false" (or set "true" to enable).
-// TODO(johnylin): enable ForceLevel after supporting dynamically query drivers
-//                 level capability. https://crbug.com/878674.
-bool g_force_level = false;
-
 // This identifies the storage type of inputting VideoFrame on Encode().
 // If |native_input| is true, inputting VideoFrame on Encode() is DmaBuf-backed
 // VideoFrame. Otherwise, it is MEM-backed VideoFrame.
@@ -262,8 +243,8 @@
   TestStream()
       : num_frames(0),
         aligned_buffer_size(0),
-        requested_bitrate(kDefaultBitrate),
-        requested_framerate(kDefaultFramerate),
+        requested_bitrate(0),
+        requested_framerate(0),
         requested_subsequent_bitrate(0),
         requested_subsequent_framerate(0) {}
   ~TestStream() {}
@@ -294,7 +275,6 @@
   unsigned int requested_framerate;
   unsigned int requested_subsequent_bitrate;
   unsigned int requested_subsequent_framerate;
-  base::Optional<uint8_t> requested_level;
 };
 
 // Return the |percentile| from a sorted vector.
@@ -318,10 +298,6 @@
   return profile >= VP8PROFILE_MIN && profile <= VP8PROFILE_MAX;
 }
 
-static bool IsVP9(VideoCodecProfile profile) {
-  return profile >= VP9PROFILE_MIN && profile <= VP9PROFILE_MAX;
-}
-
 // Helper functions to do string conversions.
 static base::FilePath::StringType StringToFilePathStringType(
     const std::string& str) {
@@ -459,7 +435,7 @@
                                  base::TRIM_WHITESPACE, base::SPLIT_WANT_ALL);
     }
     LOG_ASSERT(fields.size() >= 4U) << data;
-    LOG_ASSERT(fields.size() <= 11U) << data;
+    LOG_ASSERT(fields.size() <= 10U) << data;
     auto test_stream = std::make_unique<TestStream>();
 
     test_stream->in_filename = FilePathStringTypeToString(fields[0]);
@@ -504,41 +480,10 @@
       LOG_ASSERT(base::StringToUint(fields[9], &format));
       test_stream->pixel_format = static_cast<VideoPixelFormat>(format);
     }
-
-    if (fields.size() >= 11 && !fields[10].empty()) {
-      unsigned int requested_level = 0;
-      LOG_ASSERT(base::StringToUint(fields[10], &requested_level));
-      test_stream->requested_level =
-          base::checked_cast<uint8_t>(requested_level);
-    }
     test_streams->push_back(std::move(test_stream));
   }
 }
 
-// Check if the parameter set in |test_stream| are valid for initializing
-// encoder.
-static bool CheckH264InitConfigValidity(const TestStream* test_stream) {
-  if (!test_stream->requested_level)
-    return true;  // regard as valid if level is not assigned.
-
-  constexpr size_t kH264MacroblockSizeInPixels = 16;
-  gfx::Size coded_size =
-      gfx::Size(base::bits::Align(test_stream->visible_size.width(),
-                                  kH264MacroblockSizeInPixels),
-                base::bits::Align(test_stream->visible_size.height(),
-                                  kH264MacroblockSizeInPixels));
-  uint32_t framesize_in_mbs =
-      coded_size.GetArea() /
-      (kH264MacroblockSizeInPixels * kH264MacroblockSizeInPixels);
-
-  // Check if frame size, initial bitrate and macroblock processing rate are
-  // valid from the limit of profile and level.
-  return CheckH264LevelLimits(
-      test_stream->requested_profile, test_stream->requested_level.value(),
-      test_stream->requested_bitrate, test_stream->requested_framerate,
-      framesize_in_mbs);
-}
-
 // Basic test environment shared across multiple test cases. We only need to
 // setup it once for all test cases.
 // It helps
@@ -688,11 +633,9 @@
 
   virtual ~StreamValidator() {}
 
-  // Provide a StreamValidator instance for the given |profile| and |level|.
-  // |level| is optional and should specified only if codec is h264.
+  // Provide a StreamValidator instance for the given |profile|.
   static std::unique_ptr<StreamValidator> Create(
       VideoCodecProfile profile,
-      base::Optional<uint8_t> level,
       const FrameFoundCallback& frame_cb);
 
   // Process and verify contents of a bitstream buffer.
@@ -708,10 +651,8 @@
 
 class H264Validator : public StreamValidator {
  public:
-  H264Validator(base::Optional<uint8_t> level,
-                const FrameFoundCallback& frame_cb)
+  explicit H264Validator(const FrameFoundCallback& frame_cb)
       : StreamValidator(frame_cb),
-        target_level_(level),
         seen_sps_(false),
         seen_pps_(false),
         seen_idr_(false),
@@ -723,10 +664,6 @@
   bool IsNewPicture(const H264SliceHeader& slice_hdr);
   bool UpdateCurrentPicture(const H264SliceHeader& slice_hdr);
 
-  // H264Validator will check output level with the value of |target_level_|
-  // unless it is base::nullopt.
-  base::Optional<uint8_t> target_level_;
-
   // Set to true when encoder provides us with the corresponding NALU type.
   bool seen_sps_;
   bool seen_pps_;
@@ -782,16 +719,6 @@
             h264_parser_.GetSPS(sps_id)->GetVisibleRect().value_or(gfx::Rect());
         ASSERT_FALSE(visible_size.IsEmpty());
         visible_size_ = visible_size.size();
-        // Check the output level is equal to target level.
-        if (target_level_) {
-          LOG(INFO) << "Target level: "
-                    << static_cast<int>(target_level_.value())
-                    << ", output level: "
-                    << static_cast<int>(
-                           h264_parser_.GetSPS(sps_id)->GetIndicatedLevel());
-          ASSERT_EQ(h264_parser_.GetSPS(sps_id)->GetIndicatedLevel(),
-                    target_level_.value());
-        }
         seen_sps_ = true;
         break;
       }
@@ -870,49 +797,16 @@
   frame_cb_.Run(header.IsKeyframe(), visible_size_);
 }
 
-class VP9Validator : public StreamValidator {
- public:
-  explicit VP9Validator(const FrameFoundCallback& frame_cb)
-      : StreamValidator(frame_cb), parser_(false), seen_keyframe_(false) {}
-
-  void ProcessStreamBuffer(const uint8_t* stream, size_t size) override;
-
- private:
-  Vp9Parser parser_;
-  // Have we already got a keyframe in the stream?
-  bool seen_keyframe_;
-};
-
-void VP9Validator::ProcessStreamBuffer(const uint8_t* stream, size_t size) {
-  // TODO(posciak): We could be getting more frames in the buffer, but there is
-  // no simple way to detect this. We'd need to parse the frames and go through
-  // partition numbers/sizes. For now assume one frame per buffer.
-  Vp9FrameHeader header;
-  parser_.SetStream(stream, size, nullptr);
-  EXPECT_TRUE(Vp9Parser::kInvalidStream !=
-              parser_.ParseNextFrame(&header, nullptr));
-  if (header.IsKeyframe()) {
-    seen_keyframe_ = true;
-    visible_size_.SetSize(header.render_width, header.render_height);
-  }
-
-  EXPECT_TRUE(seen_keyframe_);
-  ASSERT_FALSE(visible_size_.IsEmpty());
-  frame_cb_.Run(header.IsKeyframe(), visible_size_);
-}
 // static
 std::unique_ptr<StreamValidator> StreamValidator::Create(
     VideoCodecProfile profile,
-    base::Optional<uint8_t> level,
     const FrameFoundCallback& frame_cb) {
   std::unique_ptr<StreamValidator> validator;
 
   if (IsH264(profile)) {
-    validator.reset(new H264Validator(level, frame_cb));
+    validator.reset(new H264Validator(frame_cb));
   } else if (IsVP8(profile)) {
     validator.reset(new VP8Validator(frame_cb));
-  } else if (IsVP9(profile)) {
-    validator.reset(new VP9Validator(frame_cb));
   } else {
     LOG(FATAL) << "Unsupported profile: " << GetProfileName(profile);
   }
@@ -990,8 +884,6 @@
       decoder_state_(UNINITIALIZED) {
   // Allow decoding of individual NALU. Entire frames are required by default.
   decoder_->set_decode_nalus(true);
-
-  DETACH_FROM_THREAD(thread_checker_);
 }
 
 void VideoFrameQualityValidator::Initialize(const gfx::Size& coded_size,
@@ -1001,24 +893,18 @@
   gfx::Size natural_size(visible_size.size());
   // The default output format of ffmpeg video decoder is YV12.
   VideoDecoderConfig config;
-  if (IsVP8(profile_)) {
+  if (IsVP8(profile_))
     config.Initialize(kCodecVP8, VP8PROFILE_ANY, pixel_format_,
                       VideoColorSpace(), VIDEO_ROTATION_0, coded_size,
                       visible_size, natural_size, EmptyExtraData(),
                       Unencrypted());
-  } else if (IsVP9(profile_)) {
-    config.Initialize(kCodecVP9, VP9PROFILE_PROFILE0, pixel_format_,
-                      VideoColorSpace(), VIDEO_ROTATION_0, coded_size,
-                      visible_size, natural_size, EmptyExtraData(),
-                      Unencrypted());
-  } else if (IsH264(profile_)) {
+  else if (IsH264(profile_))
     config.Initialize(kCodecH264, H264PROFILE_MAIN, pixel_format_,
                       VideoColorSpace(), VIDEO_ROTATION_0, coded_size,
                       visible_size, natural_size, EmptyExtraData(),
                       Unencrypted());
-  } else {
+  else
     LOG_ASSERT(0) << "Invalid profile " << GetProfileName(profile_);
-  }
 
   decoder_->Initialize(
       config, false, nullptr,
@@ -1369,8 +1255,7 @@
             bool mid_stream_bitrate_switch,
             bool mid_stream_framerate_switch,
             bool verify_output,
-            bool verify_output_timestamp,
-            bool force_level);
+            bool verify_output_timestamp);
   void CreateEncoder();
   void DestroyEncoder();
 
@@ -1432,7 +1317,7 @@
   void LogPerf();
 
   // Write IVF file header to test_stream_->out_filename.
-  void WriteIvfFileHeader(uint32_t fourcc);
+  void WriteIvfFileHeader();
 
   // Write an IVF frame header to test_stream_->out_filename.
   void WriteIvfFrameHeader(int frame_index, size_t frame_size);
@@ -1580,8 +1465,7 @@
                      bool mid_stream_bitrate_switch,
                      bool mid_stream_framerate_switch,
                      bool verify_output,
-                     bool verify_output_timestamp,
-                     bool force_level)
+                     bool verify_output_timestamp)
     : VEAClientBase(note),
       state_(CS_CREATED),
       test_stream_(test_stream),
@@ -1609,15 +1493,10 @@
   if (keyframe_period_)
     LOG_ASSERT(kMaxKeyframeDelay < keyframe_period_);
 
-  // Only check target level against requested level if |force_level| is true.
-  base::Optional<uint8_t> target_level;
-  if (force_level)
-    target_level = test_stream_->requested_level;
-
   // Fake encoder produces an invalid stream, so skip validating it.
   if (!g_fake_encoder) {
     stream_validator_ = StreamValidator::Create(
-        test_stream_->requested_profile, target_level,
+        test_stream_->requested_profile,
         base::BindRepeating(&VEAClient::HandleEncodedFrame,
                             base::Unretained(this)));
     CHECK(stream_validator_);
@@ -1666,6 +1545,8 @@
       return encoder;
     return nullptr;
   } else {
+    // TODO(johnylin): add level testing in video_encode_accelerator_unittest
+    //                 crbug.com/863327
     return GpuVideoEncodeAcceleratorFactory::CreateVEA(config, client,
                                                        gpu_preferences);
   }
@@ -1682,7 +1563,7 @@
   const VideoEncodeAccelerator::Config config(
       test_stream_->pixel_format, test_stream_->visible_size,
       test_stream_->requested_profile, requested_bitrate_, requested_framerate_,
-      test_stream_->requested_level, storage_type);
+      base::nullopt, storage_type);
   encoder_ = CreateVideoEncodeAccelerator(config, this, gpu::GpuPreferences());
   if (!encoder_) {
     LOG(ERROR) << "Failed creating a VideoEncodeAccelerator.";
@@ -1715,7 +1596,15 @@
 
 void VEAClient::UpdateTestStreamData(bool mid_stream_bitrate_switch,
                                      bool mid_stream_framerate_switch) {
+  // Use defaults for bitrate/framerate if they are not provided.
+  if (test_stream_->requested_bitrate == 0)
+    requested_bitrate_ = kDefaultBitrate;
+  else
   requested_bitrate_ = test_stream_->requested_bitrate;
+
+  if (test_stream_->requested_framerate == 0)
+    requested_framerate_ = kDefaultFramerate;
+  else
   requested_framerate_ = test_stream_->requested_framerate;
 
   // If bitrate/framerate switch is requested, use the subsequent values if
@@ -1762,8 +1651,6 @@
   DCHECK(thread_checker_.CalledOnValidThread());
   ASSERT_EQ(CS_INITIALIZED, state_);
   SetState(CS_ENCODING);
-  constexpr uint32_t kVp8Fourcc = 0x30385056;
-  constexpr uint32_t kVp9Fourcc = 0x30395056;
 
   if (quality_validator_)
     quality_validator_->Initialize(input_coded_size,
@@ -1783,13 +1670,8 @@
              << kMinFramesForBitrateTests << " frames";
     num_frames_to_encode_ = kMinFramesForBitrateTests;
   }
-  if (save_to_file_) {
-    if (IsVP8(test_stream_->requested_profile)) {
-      WriteIvfFileHeader(kVp8Fourcc);
-    } else if (IsVP9(test_stream_->requested_profile)) {
-      WriteIvfFileHeader(kVp9Fourcc);
-    }
-  }
+  if (save_to_file_ && IsVP8(test_stream_->requested_profile))
+    WriteIvfFileHeader();
 
   input_coded_size_ = input_coded_size;
   num_required_input_buffers_ = input_count;
@@ -1877,8 +1759,7 @@
     }
 
     if (save_to_file_) {
-      if (IsVP8(test_stream_->requested_profile) ||
-          IsVP9(test_stream_->requested_profile))
+      if (IsVP8(test_stream_->requested_profile))
         WriteIvfFrameHeader(num_encoded_frames_ - 1,
                             metadata.payload_size_bytes);
 
@@ -2209,19 +2090,8 @@
 
 void VEAClient::VerifyMinFPS() {
   DCHECK(thread_checker_.CalledOnValidThread());
-  if (test_perf_) {
-    if (input_coded_size_.GetArea() >= k2160PSizeInPixels) {
-      // When |input_coded_size_| is 2160p or more, it is expected that the
-      // calculated FPS might be lower than kMinPerfFPS. Log as warning instead
-      // of failing the test in this case.
-      if (frames_per_second() < kMinPerfFPS) {
-        LOG(WARNING) << "Measured FPS: " << frames_per_second()
-                     << " is below min required: " << kMinPerfFPS << " FPS.";
-      }
-    } else {
+  if (test_perf_)
       EXPECT_GE(frames_per_second(), kMinPerfFPS);
-    }
-  }
 }
 
 void VEAClient::VerifyStreamProperties() {
@@ -2245,13 +2115,13 @@
   }
 }
 
-void VEAClient::WriteIvfFileHeader(uint32_t fourcc) {
+void VEAClient::WriteIvfFileHeader() {
   DCHECK(thread_checker_.CalledOnValidThread());
   IvfFileHeader header = {};
   memcpy(header.signature, kIvfHeaderSignature, sizeof(header.signature));
   header.version = 0;
   header.header_size = sizeof(header);
-  header.fourcc = fourcc;  // VP80 or VP90
+  header.fourcc = 0x30385056;  // VP80
   header.width =
       base::checked_cast<uint16_t>(test_stream_->visible_size.width());
   header.height =
@@ -2524,13 +2394,9 @@
 // - If true, switch framerate mid-stream.
 // - If true, verify the output frames of encoder.
 // - If true, verify the timestamps of output frames.
-// - If true, verify the output level is as provided in input stream. Only
-//   available for H264 encoder for now.
 class VideoEncodeAcceleratorTest
     : public ::testing::TestWithParam<
-          std::
-              tuple<int, bool, int, bool, bool, bool, bool, bool, bool, bool>> {
-};
+          std::tuple<int, bool, int, bool, bool, bool, bool, bool, bool>> {};
 
 TEST_P(VideoEncodeAcceleratorTest, TestSimpleEncode) {
   size_t num_concurrent_encoders = std::get<0>(GetParam());
@@ -2543,32 +2409,6 @@
   const bool verify_output =
       std::get<7>(GetParam()) || g_env->verify_all_output();
   const bool verify_output_timestamp = std::get<8>(GetParam());
-  const bool force_level = std::get<9>(GetParam());
-
-  if (force_level) {
-    // Skip ForceLevel test if "--force_level=false".
-    if (!g_force_level) {
-      LOG(WARNING) << "ForceLevel test is disabled.";
-      return;
-    }
-
-    // Skip ForceLevel test for non-H264 test stream.
-    for (auto it = g_env->test_streams_.begin();
-         it != g_env->test_streams_.end();) {
-      if (!IsH264((*it)->requested_profile) || !(*it)->requested_level) {
-        LOG(WARNING) << "Skip ForceLevel for stream: " << (*it)->in_filename
-                     << " (Non-H264 codec or level is not assigned).";
-        it = g_env->test_streams_.erase(it);
-      } else {
-        ASSERT_TRUE(CheckH264InitConfigValidity(it->get()));
-        ++it;
-      }
-    }
-    if (g_env->test_streams_.empty()) {
-      LOG(WARNING) << "ForceLevel test is totally skipped.";
-      return;
-    }
-  }
 
   std::vector<
       std::unique_ptr<media::test::ClientStateNotification<ClientState>>>
@@ -2592,7 +2432,7 @@
         g_env->test_streams_[test_stream_index].get(), notes.back().get(),
         encoder_save_to_file, keyframe_period, force_bitrate, test_perf,
         mid_stream_bitrate_switch, mid_stream_framerate_switch, verify_output,
-        verify_output_timestamp, force_level));
+        verify_output_timestamp));
 
     g_env->GetRenderingTaskRunner()->PostTask(
         FROM_HERE, base::BindOnce(&VEAClient::CreateEncoder,
@@ -2680,33 +2520,19 @@
 #if defined(OS_CHROMEOS) || defined(OS_LINUX)
 // TODO(kcwu): add back test of verify_output=true after
 // https://crbug.com/694131 fixed.
-INSTANTIATE_TEST_SUITE_P(SimpleEncode,
+INSTANTIATE_TEST_CASE_P(
+    SimpleEncode,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           true,
-                                                           0,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(1, true, 0, false, false, false, false, false, false)));
 
-INSTANTIATE_TEST_SUITE_P(EncoderPerf,
+INSTANTIATE_TEST_CASE_P(
+    EncoderPerf,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           false,
-                                                           0,
-                                                           false,
-                                                           true,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(1, false, 0, false, true, false, false, false, false)));
 
-INSTANTIATE_TEST_SUITE_P(ForceKeyframes,
+INSTANTIATE_TEST_CASE_P(ForceKeyframes,
                          VideoEncodeAcceleratorTest,
                          ::testing::Values(std::make_tuple(1,
                                                            false,
@@ -2716,144 +2542,63 @@
                                                            false,
                                                            false,
                                                            false,
-                                                           false,
                                                            false)));
 
-INSTANTIATE_TEST_SUITE_P(ForceBitrate,
+INSTANTIATE_TEST_CASE_P(
+    ForceBitrate,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           false,
-                                                           0,
-                                                           true,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(1, false, 0, true, false, false, false, false, false)));
 
-INSTANTIATE_TEST_SUITE_P(MidStreamParamSwitchBitrate,
+INSTANTIATE_TEST_CASE_P(
+    MidStreamParamSwitchBitrate,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           false,
-                                                           0,
-                                                           true,
-                                                           false,
-                                                           true,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(1, false, 0, true, false, true, false, false, false)));
 
 // TODO(kcwu): add back bitrate test after https://crbug.com/693336 fixed.
-INSTANTIATE_TEST_SUITE_P(DISABLED_MidStreamParamSwitchFPS,
-                         VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           false,
-                                                           0,
-                                                           true,
-                                                           false,
-                                                           false,
-                                                           true,
-                                                           false,
-                                                           false,
-                                                           false)));
-
-INSTANTIATE_TEST_SUITE_P(MultipleEncoders,
+INSTANTIATE_TEST_CASE_P(
+    DISABLED_MidStreamParamSwitchFPS,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(3,
-                                                           false,
-                                                           0,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false),
-                                           std::make_tuple(3,
-                                                           false,
-                                                           0,
-                                                           true,
-                                                           false,
-                                                           true,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(1, false, 0, true, false, false, true, false, false)));
 
-INSTANTIATE_TEST_SUITE_P(VerifyTimestamp,
+INSTANTIATE_TEST_CASE_P(
+    MultipleEncoders,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           false,
-                                                           0,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           true,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(3, false, 0, false, false, false, false, false, false),
+        std::make_tuple(3, false, 0, true, false, true, false, false, false)));
 
-INSTANTIATE_TEST_SUITE_P(ForceLevel,
+INSTANTIATE_TEST_CASE_P(
+    VerifyTimestamp,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           false,
-                                                           0,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           true)));
+    ::testing::Values(
+        std::make_tuple(1, false, 0, false, false, false, false, false, true)));
 
-INSTANTIATE_TEST_SUITE_P(NoInputTest,
+INSTANTIATE_TEST_CASE_P(NoInputTest,
                          VideoEncodeAcceleratorSimpleTest,
                          ::testing::Values(0));
 
-INSTANTIATE_TEST_SUITE_P(CacheLineUnalignedInputTest,
+INSTANTIATE_TEST_CASE_P(CacheLineUnalignedInputTest,
                          VideoEncodeAcceleratorSimpleTest,
                          ::testing::Values(1));
 
 #elif defined(OS_MACOSX) || defined(OS_WIN)
-INSTANTIATE_TEST_SUITE_P(SimpleEncode,
+INSTANTIATE_TEST_CASE_P(
+    SimpleEncode,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           true,
-                                                           0,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false),
-                                           std::make_tuple(1,
-                                                           true,
-                                                           0,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           true,
-                                                           false,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(1, true, 0, false, false, false, false, false, false),
+        std::make_tuple(1, true, 0, false, false, false, false, true, false)));
 
-INSTANTIATE_TEST_SUITE_P(EncoderPerf,
+INSTANTIATE_TEST_CASE_P(
+    EncoderPerf,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           false,
-                                                           0,
-                                                           false,
-                                                           true,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(1, false, 0, false, true, false, false, false, false)));
 
-INSTANTIATE_TEST_SUITE_P(MultipleEncoders,
+INSTANTIATE_TEST_CASE_P(MultipleEncoders,
                          VideoEncodeAcceleratorTest,
                          ::testing::Values(std::make_tuple(3,
                                                            false,
@@ -2863,35 +2608,20 @@
                                                            false,
                                                            false,
                                                            false,
-                                                           false,
                                                            false)));
 
-INSTANTIATE_TEST_SUITE_P(VerifyTimestamp,
+INSTANTIATE_TEST_CASE_P(
+    VerifyTimestamp,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           false,
-                                                           0,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           true,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(1, false, 0, false, false, false, false, false, true)));
 
 #if defined(OS_WIN)
-INSTANTIATE_TEST_SUITE_P(ForceBitrate,
+INSTANTIATE_TEST_CASE_P(
+    ForceBitrate,
                          VideoEncodeAcceleratorTest,
-                         ::testing::Values(std::make_tuple(1,
-                                                           false,
-                                                           0,
-                                                           true,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false,
-                                                           false)));
+    ::testing::Values(
+        std::make_tuple(1, false, 0, true, false, false, false, false, false)));
 #endif  // defined(OS_WIN)
 
 #endif  // defined(OS_CHROMEOS) || defined(OS_LINUX)
@@ -2928,9 +2658,6 @@
                     media::g_verify_all_output)));
 
 #if BUILDFLAG(USE_VAAPI)
-    base::test::ScopedFeatureList scoped_feature_list;
-    // TODO(crbug.com/811912): remove once enabled by default.
-    scoped_feature_list.InitAndEnableFeature(media::kVaapiVP9Encoder);
     media::VaapiWrapper::PreSandboxInitialization();
 #elif defined(OS_WIN)
     media::MediaFoundationVideoEncodeAccelerator::PreSandboxInitialization();
@@ -2997,12 +2724,6 @@
       media::g_verify_all_output = true;
       continue;
     }
-    if (it->first == "force_level") {
-      std::string input(it->second.begin(), it->second.end());
-      // Only set |g_force_level| to true if input is "true"; false otherwise.
-      media::g_force_level = input == "true";
-      continue;
-    }
 
     if (it->first == "native_input") {
 #if defined(OS_CHROMEOS)
--- a/media/gpu/vp8_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vp8_decoder.cc	2019-05-17 18:53:34.280000000 +0300
@@ -7,10 +7,6 @@
 
 namespace media {
 
-namespace {
-constexpr size_t kVP8NumFramesActive = 4;
-}
-
 VP8Decoder::VP8Accelerator::VP8Accelerator() {}
 
 VP8Decoder::VP8Accelerator::~VP8Accelerator() {}
@@ -169,13 +165,9 @@
 }
 
 size_t VP8Decoder::GetRequiredNumOfPictures() const {
-  constexpr size_t kPicsInPipeline = limits::kMaxVideoFrames + 1;
+  const size_t kVP8NumFramesActive = 4;
+  const size_t kPicsInPipeline = limits::kMaxVideoFrames + 2;
   return kVP8NumFramesActive + kPicsInPipeline;
 }
 
-size_t VP8Decoder::GetNumReferenceFrames() const {
-  // Maximum number of reference frames.
-  return kVP8NumFramesActive;
-}
-
 }  // namespace media
--- a/media/gpu/vp8_decoder.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vp8_decoder.h	2019-05-17 18:53:34.280000000 +0300
@@ -72,7 +72,6 @@
   DecodeResult Decode() override WARN_UNUSED_RESULT;
   gfx::Size GetPicSize() const override;
   size_t GetRequiredNumOfPictures() const override;
-  size_t GetNumReferenceFrames() const override;
 
  private:
   bool DecodeAndOutputCurrentFrame(scoped_refptr<VP8Picture> pic);
--- a/media/gpu/vp8_reference_frame_vector.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vp8_reference_frame_vector.h	2019-05-17 18:53:34.280000000 +0300
@@ -7,7 +7,7 @@
 
 #include <array>
 
-#include "base/memory/scoped_refptr.h"
+#include "base/memory/ref_counted.h"
 #include "base/sequence_checker.h"
 #include "media/filters/vp8_parser.h"
 
--- a/media/gpu/vp9_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vp9_decoder.cc	2019-05-17 18:53:34.280000000 +0300
@@ -261,13 +261,9 @@
 }
 
 size_t VP9Decoder::GetRequiredNumOfPictures() const {
-  constexpr size_t kPicsInPipeline = limits::kMaxVideoFrames + 1;
-  return kPicsInPipeline + GetNumReferenceFrames();
-}
-
-size_t VP9Decoder::GetNumReferenceFrames() const {
-  // Maximum number of reference frames
-  return kVp9NumRefFrames;
+  // kMaxVideoFrames to keep higher level media pipeline populated, +2 for the
+  // pictures being parsed and decoded currently.
+  return limits::kMaxVideoFrames + kVp9NumRefFrames + 2;
 }
 
 }  // namespace media
--- a/media/gpu/vp9_decoder.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vp9_decoder.h	2019-05-17 18:53:34.280000000 +0300
@@ -106,7 +106,6 @@
   DecodeResult Decode() override WARN_UNUSED_RESULT;
   gfx::Size GetPicSize() const override;
   size_t GetRequiredNumOfPictures() const override;
-  size_t GetNumReferenceFrames() const override;
 
  private:
   // Update ref_frames_ based on the information in current frame header.
--- a/media/gpu/vp9_picture.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vp9_picture.cc	2019-05-17 18:53:34.280000000 +0300
@@ -6,7 +6,7 @@
 
 namespace media {
 
-VP9Picture::VP9Picture() : frame_hdr(new Vp9FrameHeader()) {}
+VP9Picture::VP9Picture() = default;
 
 VP9Picture::~VP9Picture() = default;
 
--- a/media/gpu/vt_video_decode_accelerator_mac.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vt_video_decode_accelerator_mac.cc	2019-05-17 18:53:34.280000000 +0300
@@ -319,7 +319,7 @@
 
   // The named transfer function.
   gfx::ColorSpace::TransferID transfer_id = gfx::ColorSpace::TransferID::BT709;
-  skcms_TransferFunction custom_tr_fn = {2.2f, 1, 0, 1, 0, 0, 0};
+  SkColorSpaceTransferFn custom_tr_fn = {2.2f, 1, 0, 1, 0, 0, 0};
   struct {
     const CFStringRef cfstr;
     gfx::ColorSpace::TransferID id;
@@ -354,7 +354,7 @@
       CGFloat gamma_float = 0;
       if (CFNumberGetValue(gamma_number, kCFNumberCGFloatType, &gamma_float)) {
         transfer_id = gfx::ColorSpace::TransferID::CUSTOM;
-        custom_tr_fn.g = gamma_float;
+        custom_tr_fn.fG = gamma_float;
       } else {
         DLOG(ERROR) << "Filed to get CVImageBufferRef gamma level as float.";
       }
@@ -854,8 +854,8 @@
   // no image.
   if (!frame->has_slice) {
     gpu_task_runner_->PostTask(
-        FROM_HERE, base::BindOnce(&VTVideoDecodeAccelerator::DecodeDone,
-                                  weak_this_, frame));
+        FROM_HERE,
+        base::Bind(&VTVideoDecodeAccelerator::DecodeDone, weak_this_, frame));
     return;
   }
 
@@ -1010,7 +1010,7 @@
   frame->image.reset(image_buffer, base::scoped_policy::RETAIN);
   gpu_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&VTVideoDecodeAccelerator::DecodeDone, weak_this_, frame));
+      base::Bind(&VTVideoDecodeAccelerator::DecodeDone, weak_this_, frame));
 }
 
 void VTVideoDecodeAccelerator::DecodeDone(Frame* frame) {
@@ -1057,7 +1057,7 @@
   // Queue a task even if flushing fails, so that destruction always completes.
   gpu_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&VTVideoDecodeAccelerator::FlushDone, weak_this_, type));
+      base::Bind(&VTVideoDecodeAccelerator::FlushDone, weak_this_, type));
 }
 
 void VTVideoDecodeAccelerator::FlushDone(TaskType type) {
@@ -1121,7 +1121,7 @@
   // future work after that happens.
   gpu_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&VTVideoDecodeAccelerator::ProcessWorkQueues, weak_this_));
+      base::Bind(&VTVideoDecodeAccelerator::ProcessWorkQueues, weak_this_));
 }
 
 void VTVideoDecodeAccelerator::ReusePictureBuffer(int32_t picture_id) {
@@ -1369,7 +1369,7 @@
   if (!gpu_task_runner_->BelongsToCurrentThread()) {
     gpu_task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&VTVideoDecodeAccelerator::NotifyError, weak_this_,
+        base::Bind(&VTVideoDecodeAccelerator::NotifyError, weak_this_,
                        vda_error_type, session_failure_type));
   } else if (state_ == STATE_DECODING) {
     state_ = STATE_ERROR;
--- a/media/gpu/vt_video_encode_accelerator_mac.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/vt_video_encode_accelerator_mac.cc	2019-05-17 18:53:34.280000000 +0300
@@ -171,9 +171,9 @@
   }
 
   client_task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&Client::RequireBitstreamBuffers, client_,
-                                kNumInputBuffers, input_visible_size_,
-                                bitstream_buffer_size_));
+      FROM_HERE,
+      base::Bind(&Client::RequireBitstreamBuffers, client_, kNumInputBuffers,
+                 input_visible_size_, bitstream_buffer_size_));
   return true;
 }
 
@@ -212,8 +212,8 @@
 
   encoder_thread_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&VTVideoEncodeAccelerator::UseOutputBitstreamBufferTask,
-                     base::Unretained(this), std::move(buffer_ref)));
+      base::Bind(&VTVideoEncodeAccelerator::UseOutputBitstreamBufferTask,
+                 base::Unretained(this), base::Passed(&buffer_ref)));
 }
 
 void VTVideoEncodeAccelerator::RequestEncodingParametersChange(
@@ -225,8 +225,7 @@
 
   encoder_thread_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(
-          &VTVideoEncodeAccelerator::RequestEncodingParametersChangeTask,
+      base::Bind(&VTVideoEncodeAccelerator::RequestEncodingParametersChangeTask,
           base::Unretained(this), bitrate, framerate));
 }
 
@@ -399,7 +398,7 @@
       FROM_HERE,
       base::BindOnce(&VTVideoEncodeAccelerator::CompressionCallbackTask,
                      encoder->encoder_weak_ptr_, status,
-                     std::move(encode_output)));
+                     base::Passed(&encode_output)));
 }
 
 void VTVideoEncodeAccelerator::CompressionCallbackTask(
@@ -436,9 +435,9 @@
     DVLOG(2) << " frame dropped";
     client_task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&Client::BitstreamBufferReady, client_, buffer_ref->id,
-                       BitstreamBufferMetadata(
-                           0, false, encode_output->capture_timestamp)));
+        base::Bind(&Client::BitstreamBufferReady, client_, buffer_ref->id,
+                   BitstreamBufferMetadata(0, false,
+                                           encode_output->capture_timestamp)));
     return;
   }
 
@@ -461,8 +460,7 @@
 
   client_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(
-          &Client::BitstreamBufferReady, client_, buffer_ref->id,
+      base::Bind(&Client::BitstreamBufferReady, client_, buffer_ref->id,
           BitstreamBufferMetadata(used_buffer_size, keyframe,
                                   encode_output->capture_timestamp)));
 }
--- a/media/gpu/windows/d3d11_cdm_proxy.cc	2019-05-17 17:45:41.284000000 +0300
+++ b/media/gpu/windows/d3d11_cdm_proxy.cc	2019-05-17 18:53:34.280000000 +0300
@@ -9,7 +9,6 @@
 
 #include "base/bind.h"
 #include "base/logging.h"
-#include "base/memory/ptr_util.h"
 #include "base/power_monitor/power_monitor.h"
 #include "base/power_monitor/power_observer.h"
 #include "base/stl_util.h"
@@ -25,10 +24,10 @@
 namespace {
 
 // Checks whether there is a hardware protected key exhange method.
-// https://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/dn894125(v=vs.85).aspx
+// https://msdn.microsoft.com/en-us/library/windows/desktop/dn894125(v=vs.85).aspx
 // The key exhange capabilities are checked using these.
-// https://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/hh447640%28v=vs.85%29.aspx?f=255&MSPPError=-2147217396
-// https://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/hh447782(v=vs.85).aspx
+// https://msdn.microsoft.com/en-us/library/windows/desktop/hh447640%28v=vs.85%29.aspx?f=255&MSPPError=-2147217396
+// https://msdn.microsoft.com/en-us/library/windows/desktop/hh447782(v=vs.85).aspx
 bool CanDoHardwareProtectedKeyExchange(
     Microsoft::WRL::ComPtr<ID3D11VideoDevice> video_device,
     const GUID& crypto_type) {
@@ -164,7 +163,7 @@
   // Return true on success.
   bool RegisterHardwareContentProtectionTeardown(ComPtr<ID3D11Device> device);
 
-  // Regiesters for power events, specifically power resume event.
+  // Regiesters for power events, specifically power suspend event.
   // Returns true on success.
   bool RegisterPowerEvents();
 
@@ -173,7 +172,7 @@
 
   // base::PowerObserver implementation. Other power events are not relevant to
   // this class.
-  void OnResume() override;
+  void OnSuspend() override;
 
   // Stops watching for events. Good for clean up.
   void StopWatching();
@@ -544,26 +543,6 @@
 void D3D11CdmProxy::NotifyHardwareContentProtectionTeardown() {
   cdm_context_->OnHardwareReset();
   client_->NotifyHardwareReset();
-  Reset();
-}
-
-void D3D11CdmProxy::Reset() {
-  client_ = nullptr;
-  initialized_ = false;
-  crypto_session_map_.clear();
-  device_.Reset();
-  device_context_.Reset();
-  video_device_.Reset();
-  video_device1_.Reset();
-  video_context_.Reset();
-  video_context1_.Reset();
-  // Note that this deregisters hardware reset event watcher. It shouldn't
-  // notify the clients until this is reinitialized. Also the client is set to
-  // null in this method.
-  hardware_event_watcher_ = nullptr;
-  crypto_session_map_.clear();
-  private_input_size_ = 0;
-  private_output_size_ = 0;
 }
 
 D3D11CdmProxy::HardwareEventWatcher::~HardwareEventWatcher() {
@@ -648,7 +627,7 @@
   teardown_callback_.Run();
 }
 
-void D3D11CdmProxy::HardwareEventWatcher::OnResume() {
+void D3D11CdmProxy::HardwareEventWatcher::OnSuspend() {
   teardown_callback_.Run();
 }
 
--- a/media/gpu/windows/d3d11_cdm_proxy.h	2019-05-17 17:45:41.284000000 +0300
+++ b/media/gpu/windows/d3d11_cdm_proxy.h	2019-05-17 18:53:34.280000000 +0300
@@ -32,11 +32,11 @@
   // includes creating a crypto session with
   // ID3D11VideoDevice::CreateCryptoSession(). This is "a GUID that specifies
   // the type of encryption to use".
-  // https://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/hh447785(v=vs.85).aspx
+  // https://msdn.microsoft.com/en-us/library/windows/desktop/hh447785(v=vs.85).aspx
   // This is also used ot call
   // ID3D11VideoDevice1::GetCryptoSessionPrivateDataSize(). It "Indicates the
   // crypto type for which the private input and output size is queried."
-  // https://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/dn894143(v=vs.85).aspx
+  // https://msdn.microsoft.com/en-us/library/windows/desktop/dn894143(v=vs.85).aspx
   // |protocol| determines what protocol this is operating in. This
   // value is passed to callbacks that require a protocol enum value.
   // |function_id_map| maps Function enum to an integer.
@@ -75,9 +75,6 @@
 
   void NotifyHardwareContentProtectionTeardown();
 
-  // Reset the state of this instance to be reinitializable.
-  void Reset();
-
   const GUID crypto_type_;
   const CdmProxy::Protocol protocol_;
   const FunctionIdMap function_id_map_;
@@ -93,13 +90,11 @@
   // Counter for assigning IDs to crypto sessions.
   uint32_t next_crypto_session_id_ = 1;
 
-  // Everything from here until weak ptr factory (which must be at the end)
-  // should be reset in Reset().
   Client* client_ = nullptr;
   bool initialized_ = false;
 
   // These ComPtrs are refcounted pointers.
-  // https://msdn.m1cr050ft.qjz9zk/en-us/library/br244983.aspx
+  // https://msdn.microsoft.com/en-us/library/br244983.aspx
   ComPtr<ID3D11Device> device_;
   ComPtr<ID3D11DeviceContext> device_context_;
   // TODO(crbug.com/788880): Remove ID3D11VideoDevice and ID3D11VideoContext if
--- a/media/gpu/windows/d3d11_cdm_proxy_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/windows/d3d11_cdm_proxy_unittest.cc	2019-05-17 18:53:34.280000000 +0300
@@ -24,7 +24,6 @@
 using ::testing::_;
 using ::testing::AllOf;
 using ::testing::AtLeast;
-using ::testing::AtMost;
 using ::testing::DoAll;
 using ::testing::Invoke;
 using ::testing::InvokeWithoutArgs;
@@ -52,13 +51,8 @@
 
 class MockPowerMonitorSource : public base::PowerMonitorSource {
  public:
-  // Use this method to send a power resume event.
-  void Resume() {
-    // Due to how ProcessPowerEvent() works, it has to be suspended first to
-    // resume.
-    ProcessPowerEvent(SUSPEND_EVENT);
-    ProcessPowerEvent(RESUME_EVENT);
-  }
+  // Use this method to send a power suspend event.
+  void Suspend() { ProcessPowerEvent(SUSPEND_EVENT); }
 
   MOCK_METHOD0(Shutdown, void());
   MOCK_METHOD0(IsOnBatteryPowerImpl, bool());
@@ -98,7 +92,7 @@
     function_id_map[kTestFunction] = kTestFunctionId;
 
     // Use NiceMock because we don't care about base::PowerMonitorSource events
-    // other than calling Resume() directly.
+    // other than calling Suspend() directly.
     auto mock_power_monitor_source =
         std::make_unique<NiceMock<MockPowerMonitorSource>>();
     mock_power_monitor_source_ = mock_power_monitor_source.get();
@@ -195,8 +189,8 @@
                              Return(S_OK)));
   }
 
-  // Helper method to do Initialize(). The returned mock objects are accessible
-  // thru member variables.
+  // Helper method to do Initialize(). Only useful if the test doesn't require
+  // access to the mocks later.
   void Initialize(CdmProxy::Client* client, CdmProxy::InitializeCB callback) {
     EXPECT_CALL(create_device_mock_,
                 Create(_, D3D_DRIVER_TYPE_HARDWARE, _, _, _, _, _, _, _, _));
@@ -241,33 +235,10 @@
     Mock::VerifyAndClearExpectations(video_context1_mock_.Get());
   }
 
-  // Test case where the proxy is initialized and then hardware content
-  // protection teardown is notified.
-  void HardwareContentProtectionTeardown() {
-    base::RunLoop run_loop;
-
-    EXPECT_CALL(callback_mock_,
-                InitializeCallback(CdmProxy::Status::kOk, _, _));
-    ASSERT_NO_FATAL_FAILURE(Initialize(
-        &client_, base::BindOnce(&CallbackMock::InitializeCallback,
-                                 base::Unretained(&callback_mock_))));
-
-    EXPECT_CALL(client_, NotifyHardwareReset());
-
-    base::MockCallback<CdmContext::EventCB> event_cb;
-    auto callback_registration =
-        proxy_->GetCdmContext()->RegisterEventCB(event_cb.Get());
-    EXPECT_CALL(event_cb, Run(CdmContext::Event::kHardwareContextLost))
-        .WillOnce(InvokeWithoutArgs([&run_loop]() { run_loop.Quit(); }));
-
-    SetEvent(teardown_event_);
-    run_loop.Run();
-  }
-
   MockProxyClient client_;
   std::unique_ptr<D3D11CdmProxy> proxy_;
   std::unique_ptr<base::PowerMonitor> power_monitor_;
-  // Owned by power_monitor_. Use this to simulate a power-resume.
+  // Owned by power_monitor_. Use this to simulate a power-suspend.
   MockPowerMonitorSource* mock_power_monitor_source_;
 
   D3D11CreateDeviceMock create_device_mock_;
@@ -320,16 +291,22 @@
 // Hardware content protection teardown is notified to the proxy.
 // Verify that the client is notified.
 TEST_F(D3D11CdmProxyTest, HardwareContentProtectionTeardown) {
-  EXPECT_NO_FATAL_FAILURE(HardwareContentProtectionTeardown());
-}
+  base::RunLoop run_loop;
+
+  EXPECT_CALL(client_, NotifyHardwareReset());
+
+  base::MockCallback<CdmContext::EventCB> event_cb;
+  auto callback_registration =
+      proxy_->GetCdmContext()->RegisterEventCB(event_cb.Get());
+  EXPECT_CALL(event_cb, Run(CdmContext::Event::kHardwareContextLost))
+      .WillOnce(InvokeWithoutArgs([&run_loop]() { run_loop.Quit(); }));
 
-// Verify that initialization after hardware content protection teardown works..
-TEST_F(D3D11CdmProxyTest, HardwareContentProtectionTeardownThenInitialize) {
-  ASSERT_NO_FATAL_FAILURE(HardwareContentProtectionTeardown());
   EXPECT_CALL(callback_mock_, InitializeCallback(CdmProxy::Status::kOk, _, _));
   ASSERT_NO_FATAL_FAILURE(
       Initialize(&client_, base::BindOnce(&CallbackMock::InitializeCallback,
                                           base::Unretained(&callback_mock_))));
+  SetEvent(teardown_event_);
+  run_loop.Run();
 }
 
 // Verify that failing to register to hardware content protection teardown
@@ -349,41 +326,18 @@
 }
 
 // Verify that the client is notified on power suspend.
-TEST_F(D3D11CdmProxyTest, PowerResume) {
+TEST_F(D3D11CdmProxyTest, PowerSuspend) {
   base::RunLoop run_loop;
 
-  EXPECT_CALL(callback_mock_, InitializeCallback(CdmProxy::Status::kOk, _, _));
-  ASSERT_NO_FATAL_FAILURE(
-      Initialize(&client_, base::BindOnce(&CallbackMock::InitializeCallback,
-                                          base::Unretained(&callback_mock_))));
-
   EXPECT_CALL(client_, NotifyHardwareReset()).WillOnce(Invoke([&run_loop]() {
     run_loop.Quit();
   }));
 
-  mock_power_monitor_source_->Resume();
-  run_loop.Run();
-}
-
-// IRL power resume is notified and then hardware content protection teardown
-// is notified. Make sure that the two notifications don't signal the clients
-// more than once (without being reinitialized in between the notifications).
-// Note that this test uses QuitWhenIdle(). If both notifications are processed
-// this test will run forever.
-TEST_F(D3D11CdmProxyTest, PowerResumeAndHardwareContentProtectionTeardown) {
-  base::RunLoop run_loop;
-
   EXPECT_CALL(callback_mock_, InitializeCallback(CdmProxy::Status::kOk, _, _));
   ASSERT_NO_FATAL_FAILURE(
       Initialize(&client_, base::BindOnce(&CallbackMock::InitializeCallback,
                                           base::Unretained(&callback_mock_))));
-
-  EXPECT_CALL(client_, NotifyHardwareReset())
-      .Times(1)
-      .WillOnce(Invoke([&run_loop]() { run_loop.QuitWhenIdle(); }));
-
-  mock_power_monitor_source_->Resume();
-  SetEvent(teardown_event_);
+  mock_power_monitor_source_->Suspend();
   run_loop.Run();
 }
 
@@ -790,6 +744,14 @@
 TEST_F(D3D11CdmProxyTest, ClearKeysAfterHardwareContentProtectionTeardown) {
   base::RunLoop run_loop;
 
+  EXPECT_CALL(client_, NotifyHardwareReset()).WillOnce(Invoke([&run_loop]() {
+    run_loop.Quit();
+  }));
+
+  base::WeakPtr<CdmContext> context = proxy_->GetCdmContext();
+  ASSERT_TRUE(context);
+  CdmProxyContext* proxy_context = context->GetCdmProxyContext();
+
   uint32_t crypto_session_id_from_initialize = 0;
   EXPECT_CALL(callback_mock_,
               InitializeCallback(CdmProxy::Status::kOk, kTestProtocol, _))
@@ -813,17 +775,9 @@
                  base::BindOnce(&CallbackMock::SetKeyCallback,
                                 base::Unretained(&callback_mock_)));
 
-  EXPECT_CALL(client_, NotifyHardwareReset()).WillOnce(Invoke([&run_loop]() {
-    run_loop.Quit();
-  }));
-
   SetEvent(teardown_event_);
   run_loop.Run();
 
-  base::WeakPtr<CdmContext> context = proxy_->GetCdmContext();
-  ASSERT_TRUE(context);
-  CdmProxyContext* proxy_context = context->GetCdmProxyContext();
-
   std::string key_id_str(kKeyId.begin(), kKeyId.end());
   auto decrypt_context =
       proxy_context->GetD3D11DecryptContext(kTestKeyType, key_id_str);
--- a/media/gpu/windows/d3d11_decryptor.cc	2019-05-17 17:45:41.288000000 +0300
+++ b/media/gpu/windows/d3d11_decryptor.cc	2019-05-17 18:53:34.284000000 +0300
@@ -14,7 +14,7 @@
 namespace {
 
 // "A buffer is defined as a single subresource."
-// https://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/ff476901(v=vs.85).aspx
+// https://msdn.microsoft.com/en-us/library/windows/desktop/ff476901(v=vs.85).aspx
 const UINT kSubresourceIndex = 0;
 const UINT kWaitIfGPUBusy = 0;
 
@@ -106,7 +106,7 @@
 
 // Checks whether |device1| is the same component as |device2|.
 // Note that comparing COM pointers require using their IUnknowns.
-// https://docs.m1cr050ft.qjz9zk/en-us/windows/desktop/api/unknwn/nf-unknwn-iunknown-queryinterface(q_)
+// https://docs.microsoft.com/en-us/windows/desktop/api/unknwn/nf-unknwn-iunknown-queryinterface(q_)
 bool SameDevices(Microsoft::WRL::ComPtr<ID3D11Device> device1,
                  Microsoft::WRL::ComPtr<ID3D11Device> device2) {
   // For the case where both are nullptrs, they aren't devices, so returning
--- a/media/gpu/windows/d3d11_h264_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/windows/d3d11_h264_accelerator.cc	2019-05-17 18:53:34.284000000 +0300
@@ -197,134 +197,6 @@
   return true;
 }
 
-void D3D11H264Accelerator::FillPicParamsWithConstants(
-    DXVA_PicParams_H264* pic) {
-  // From "DirectX Video Acceleration Specification for H.264/AVC Decoding":
-  // "The value shall be 1 unless the restricted-mode profile in use
-  // explicitly supports the value 0."
-  pic->MbsConsecutiveFlag = 1;
-
-  // The latest DXVA decoding guide says to set this to 3 if the software
-  // decoder (this class) is following the guide.
-  pic->Reserved16Bits = 3;
-
-  // |ContinuationFlag| indicates that we've filled in the remaining fields.
-  pic->ContinuationFlag = 1;
-
-  // Must be zero unless bit 13 of ConfigDecoderSpecific is set.
-  pic->Reserved8BitsA = 0;
-
-  // Unused, should always be zero.
-  pic->Reserved8BitsB = 0;
-
-  // Should always be 1.
-  pic->StatusReportFeedbackNumber = 1;
-
-  // UNUSED: slice_group_map_type (undocumented)
-  // UNUSED: slice_group_change_rate (undocumented)
-}
-
-#define ARG_SEL(_1, _2, NAME, ...) NAME
-
-#define SPS_TO_PP1(a) pic_param->a = sps->a;
-#define SPS_TO_PP2(a, b) pic_param->a = sps->b;
-#define SPS_TO_PP(...) ARG_SEL(__VA_ARGS__, SPS_TO_PP2, SPS_TO_PP1)(__VA_ARGS__)
-void D3D11H264Accelerator::PicParamsFromSPS(DXVA_PicParams_H264* pic_param,
-                                            const H264SPS* sps,
-                                            bool field_pic) {
-  // The H.264 specification now calls this |max_num_ref_frames|, while
-  // DXVA_PicParams_H264 continues to use the old name, |num_ref_frames|.
-  // See DirectX Video Acceleration for H.264/MPEG-4 AVC Decoding (4.2).
-  SPS_TO_PP(num_ref_frames, max_num_ref_frames);
-  SPS_TO_PP(wFrameWidthInMbsMinus1, pic_width_in_mbs_minus1);
-  SPS_TO_PP(wFrameHeightInMbsMinus1, pic_height_in_map_units_minus1);
-  SPS_TO_PP(residual_colour_transform_flag, separate_colour_plane_flag);
-  SPS_TO_PP(chroma_format_idc);
-  SPS_TO_PP(frame_mbs_only_flag);
-  SPS_TO_PP(bit_depth_luma_minus8);
-  SPS_TO_PP(bit_depth_chroma_minus8);
-  SPS_TO_PP(log2_max_frame_num_minus4);
-  SPS_TO_PP(pic_order_cnt_type);
-  SPS_TO_PP(log2_max_pic_order_cnt_lsb_minus4);
-  SPS_TO_PP(delta_pic_order_always_zero_flag);
-  SPS_TO_PP(direct_8x8_inference_flag);
-
-  pic_param->MbaffFrameFlag = sps->mb_adaptive_frame_field_flag && field_pic;
-  pic_param->field_pic_flag = field_pic;
-
-  pic_param->MinLumaBipredSize8x8Flag = sps->level_idc >= 31;
-}
-#undef SPS_TO_PP
-#undef SPS_TO_PP2
-#undef SPS_TO_PP1
-
-#define PPS_TO_PP1(a) pic_param->a = pps->a;
-#define PPS_TO_PP2(a, b) pic_param->a = pps->b;
-#define PPS_TO_PP(...) ARG_SEL(__VA_ARGS__, PPS_TO_PP2, PPS_TO_PP1)(__VA_ARGS__)
-bool D3D11H264Accelerator::PicParamsFromPPS(DXVA_PicParams_H264* pic_param,
-                                            const H264PPS* pps) {
-  PPS_TO_PP(constrained_intra_pred_flag);
-  PPS_TO_PP(weighted_pred_flag);
-  PPS_TO_PP(weighted_bipred_idc);
-
-  PPS_TO_PP(transform_8x8_mode_flag);
-  PPS_TO_PP(pic_init_qs_minus26);
-  PPS_TO_PP(chroma_qp_index_offset);
-  PPS_TO_PP(second_chroma_qp_index_offset);
-  PPS_TO_PP(pic_init_qp_minus26);
-  PPS_TO_PP(num_ref_idx_l0_active_minus1, num_ref_idx_l0_default_active_minus1);
-  PPS_TO_PP(num_ref_idx_l1_active_minus1, num_ref_idx_l1_default_active_minus1);
-  PPS_TO_PP(entropy_coding_mode_flag);
-  PPS_TO_PP(pic_order_present_flag,
-            bottom_field_pic_order_in_frame_present_flag);
-  PPS_TO_PP(deblocking_filter_control_present_flag);
-  PPS_TO_PP(redundant_pic_cnt_present_flag);
-
-  PPS_TO_PP(num_slice_groups_minus1);
-  if (pic_param->num_slice_groups_minus1) {
-    // TODO(liberato): UMA?
-    // TODO(liberato): media log?
-    LOG(ERROR) << "num_slice_groups_minus1 == "
-               << pic_param->num_slice_groups_minus1;
-    return false;
-  }
-  return true;
-}
-#undef PPS_TO_PP
-#undef PPS_TO_PP2
-#undef PPS_TO_PP1
-
-#undef ARG_SEL
-
-void D3D11H264Accelerator::PicParamsFromSliceHeader(
-    DXVA_PicParams_H264* pic_param,
-    const H264SliceHeader* slice_hdr) {
-  pic_param->sp_for_switch_flag = slice_hdr->sp_for_switch_flag;
-  pic_param->field_pic_flag = slice_hdr->field_pic_flag;
-  pic_param->CurrPic.AssociatedFlag = slice_hdr->bottom_field_flag;
-  pic_param->IntraPicFlag = slice_hdr->IsISlice();
-}
-
-void D3D11H264Accelerator::PicParamsFromPic(
-    DXVA_PicParams_H264* pic_param,
-    const scoped_refptr<H264Picture>& pic) {
-  pic_param->CurrPic.Index7Bits =
-      static_cast<D3D11H264Picture*>(pic.get())->level_;
-  pic_param->RefPicFlag = pic->ref;
-  pic_param->frame_num = pic->frame_num;
-
-  if (pic_param->field_pic_flag && pic_param->CurrPic.AssociatedFlag) {
-    pic_param->CurrFieldOrderCnt[1] = pic->bottom_field_order_cnt;
-    pic_param->CurrFieldOrderCnt[0] = 0;
-  } else if (pic_param->field_pic_flag && !pic_param->CurrPic.AssociatedFlag) {
-    pic_param->CurrFieldOrderCnt[0] = pic->top_field_order_cnt;
-    pic_param->CurrFieldOrderCnt[1] = 0;
-  } else {
-    pic_param->CurrFieldOrderCnt[0] = pic->top_field_order_cnt;
-    pic_param->CurrFieldOrderCnt[1] = pic->bottom_field_order_cnt;
-  }
-}
-
 Status D3D11H264Accelerator::SubmitSlice(
     const H264PPS* pps,
     const H264SliceHeader* slice_hdr,
@@ -337,24 +209,98 @@
   scoped_refptr<D3D11H264Picture> our_pic(
       static_cast<D3D11H264Picture*>(pic.get()));
   DXVA_PicParams_H264 pic_param = {};
-  FillPicParamsWithConstants(&pic_param);
-
-  PicParamsFromSPS(&pic_param, &sps_, slice_hdr->field_pic_flag);
-  if (!PicParamsFromPPS(&pic_param, pps))
-    return Status::kFail;
-  PicParamsFromSliceHeader(&pic_param, slice_hdr);
-  PicParamsFromPic(&pic_param, pic);
+#define FROM_SPS_TO_PP(a) pic_param.a = sps_.a
+#define FROM_SPS_TO_PP2(a, b) pic_param.a = sps_.b
+#define FROM_PPS_TO_PP(a) pic_param.a = pps->a
+#define FROM_PPS_TO_PP2(a, b) pic_param.a = pps->b
+#define FROM_SLICE_TO_PP(a) pic_param.a = slice_hdr->a
+#define FROM_SLICE_TO_PP2(a, b) pic_param.a = slice_hdr->b
+  FROM_SPS_TO_PP2(wFrameWidthInMbsMinus1, pic_width_in_mbs_minus1);
+  FROM_SPS_TO_PP2(wFrameHeightInMbsMinus1, pic_height_in_map_units_minus1);
+  pic_param.CurrPic.Index7Bits = our_pic->level_;
+  pic_param.CurrPic.AssociatedFlag = slice_hdr->bottom_field_flag;
+  // The H.264 specification now calls this |max_num_ref_frames|, while
+  // DXVA_PicParams_H264 continues to use the old name, |num_ref_frames|.
+  // See DirectX Video Acceleration for H.264/MPEG-4 AVC Decoding (4.2).
+  FROM_SPS_TO_PP2(num_ref_frames, max_num_ref_frames);
 
+  FROM_SLICE_TO_PP(field_pic_flag);
+  pic_param.MbaffFrameFlag =
+      sps_.mb_adaptive_frame_field_flag && pic_param.field_pic_flag;
+  FROM_SPS_TO_PP2(residual_colour_transform_flag, separate_colour_plane_flag);
+  FROM_SLICE_TO_PP(sp_for_switch_flag);
+  FROM_SPS_TO_PP(chroma_format_idc);
+  pic_param.RefPicFlag = pic->ref;
+  FROM_PPS_TO_PP(constrained_intra_pred_flag);
+  FROM_PPS_TO_PP(weighted_pred_flag);
+  FROM_PPS_TO_PP(weighted_bipred_idc);
+  // From "DirectX Video Acceleration Specification for H.264/AVC Decoding":
+  // "The value shall be 1 unless the restricted-mode profile in use explicitly
+  // supports the value 0."
+  pic_param.MbsConsecutiveFlag = 1;
+  FROM_SPS_TO_PP(frame_mbs_only_flag);
+  FROM_PPS_TO_PP(transform_8x8_mode_flag);
+  pic_param.MinLumaBipredSize8x8Flag = sps_.level_idc >= 31;
+  pic_param.IntraPicFlag = slice_hdr->IsISlice();
+  FROM_SPS_TO_PP(bit_depth_luma_minus8);
+  FROM_SPS_TO_PP(bit_depth_chroma_minus8);
+  // The latest DXVA decoding guide says to set this to 3 if the software
+  // decoder (this class) is following the guide.
+  pic_param.Reserved16Bits = 3;
   memcpy(pic_param.RefFrameList, ref_frame_list_,
          sizeof pic_param.RefFrameList);
-
+  if (pic_param.field_pic_flag && pic_param.CurrPic.AssociatedFlag) {
+    pic_param.CurrFieldOrderCnt[1] = pic->bottom_field_order_cnt;
+    pic_param.CurrFieldOrderCnt[0] = 0;
+  } else if (pic_param.field_pic_flag && !pic_param.CurrPic.AssociatedFlag) {
+    pic_param.CurrFieldOrderCnt[0] = pic->top_field_order_cnt;
+    pic_param.CurrFieldOrderCnt[1] = 0;
+  } else {
+    pic_param.CurrFieldOrderCnt[0] = pic->top_field_order_cnt;
+    pic_param.CurrFieldOrderCnt[1] = pic->bottom_field_order_cnt;
+  }
   memcpy(pic_param.FieldOrderCntList, field_order_cnt_list_,
          sizeof pic_param.FieldOrderCntList);
-
+  FROM_PPS_TO_PP(pic_init_qs_minus26);
+  FROM_PPS_TO_PP(chroma_qp_index_offset);
+  FROM_PPS_TO_PP(second_chroma_qp_index_offset);
+  // |ContinuationFlag| indicates that we've filled in the remaining fields.
+  pic_param.ContinuationFlag = 1;
+  FROM_PPS_TO_PP(pic_init_qp_minus26);
+  FROM_PPS_TO_PP2(num_ref_idx_l0_active_minus1,
+                  num_ref_idx_l0_default_active_minus1);
+  FROM_PPS_TO_PP2(num_ref_idx_l1_active_minus1,
+                  num_ref_idx_l1_default_active_minus1);
+  // UNUSED: Reserved8BitsA.  Must be zero unless bit 13 of
+  // ConfigDecoderSpecific is set.
   memcpy(pic_param.FrameNumList, frame_num_list_,
          sizeof pic_param.FrameNumList);
   pic_param.UsedForReferenceFlags = used_for_reference_flags_;
   pic_param.NonExistingFrameFlags = non_existing_frame_flags_;
+  pic_param.frame_num = pic->frame_num;
+  FROM_SPS_TO_PP(log2_max_frame_num_minus4);
+  FROM_SPS_TO_PP(pic_order_cnt_type);
+  FROM_SPS_TO_PP(log2_max_pic_order_cnt_lsb_minus4);
+  FROM_SPS_TO_PP(delta_pic_order_always_zero_flag);
+  FROM_SPS_TO_PP(direct_8x8_inference_flag);
+  FROM_PPS_TO_PP(entropy_coding_mode_flag);
+  FROM_PPS_TO_PP2(pic_order_present_flag,
+                  bottom_field_pic_order_in_frame_present_flag);
+  FROM_PPS_TO_PP(num_slice_groups_minus1);
+  if (pic_param.num_slice_groups_minus1) {
+    // TODO(liberato): UMA?
+    // TODO(liberato): media log?
+    LOG(ERROR) << "num_slice_groups_minus1 == "
+               << pic_param.num_slice_groups_minus1;
+    return Status::kFail;
+  }
+  // UNUSED: slice_group_map_type (undocumented)
+  FROM_PPS_TO_PP(deblocking_filter_control_present_flag);
+  FROM_PPS_TO_PP(redundant_pic_cnt_present_flag);
+  // UNUSED: Reserved8BitsB (unused, should always be zero).
+  // UNUSED: slice_group_change_rate (undocumented)
+
+  pic_param.StatusReportFeedbackNumber = 1;
 
   UINT buffer_size;
   void* buffer;
--- a/media/gpu/windows/d3d11_h264_accelerator.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/windows/d3d11_h264_accelerator.h	2019-05-17 18:53:34.284000000 +0300
@@ -62,24 +62,6 @@
   void Reset() override;
   bool OutputPicture(const scoped_refptr<H264Picture>& pic) override;
 
-  // Gets a pic params struct with the constant fields set.
-  void FillPicParamsWithConstants(DXVA_PicParams_H264* pic_param);
-
-  // Populate the pic params with fields from the SPS structure.
-  void PicParamsFromSPS(DXVA_PicParams_H264* pic_param,
-                        const H264SPS* sps,
-                        bool field_pic);
-
-  // Populate the pic params with fields from the PPS structure.
-  bool PicParamsFromPPS(DXVA_PicParams_H264* pic_param, const H264PPS* pps);
-
-  // Populate the pic params with fields from the slice header structure.
-  void PicParamsFromSliceHeader(DXVA_PicParams_H264* pic_param,
-                                const H264SliceHeader* pps);
-
-  void PicParamsFromPic(DXVA_PicParams_H264* pic_param,
-                        const scoped_refptr<H264Picture>& pic);
-
  private:
   bool SubmitSliceData();
   bool RetrieveBitstreamBuffer();
--- a/media/gpu/windows/d3d11_video_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/windows/d3d11_video_decoder.cc	2019-05-17 18:53:34.284000000 +0300
@@ -613,14 +613,13 @@
   texture_desc.SampleDesc.Count = 1;
   texture_desc.Usage = D3D11_USAGE_DEFAULT;
   texture_desc.BindFlags = D3D11_BIND_DECODER | D3D11_BIND_SHADER_RESOURCE;
+  texture_desc.MiscFlags = D3D11_RESOURCE_MISC_SHARED;
   if (base::FeatureList::IsEnabled(
           features::kDirectCompositionUseNV12DecodeSwapChain)) {
     // Decode swap chains do not support shared resources.
     // TODO(sunnyps): Find a workaround for when the decoder moves to its own
     // thread and D3D device.  See https://crbug.com/911847
     texture_desc.MiscFlags = 0;
-  } else {
-    texture_desc.MiscFlags = D3D11_RESOURCE_MISC_SHARED;
   }
   if (config_.is_encrypted())
     texture_desc.MiscFlags |= D3D11_RESOURCE_MISC_HW_PROTECTED;
@@ -767,15 +766,11 @@
     return {};
   }
 
-  // This workaround accounts for almost half of all startup results, and it's
-  // unclear that it's relevant here.
-  if (!base::FeatureList::IsEnabled(kD3D11VideoDecoderIgnoreWorkarounds)) {
     if (gpu_workarounds.disable_dxgi_zero_copy_video) {
       UMA_HISTOGRAM_ENUMERATION(uma_name,
                                 NotSupportedReason::kZeroCopyVideoRequired);
       return {};
     }
-  }
 
   // Remember that this might query the angle device, so this won't work if
   // we're not on the GPU main thread.  Also remember that devices are thread
--- a/media/gpu/windows/d3d11_video_decoder_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/windows/d3d11_video_decoder_impl.cc	2019-05-17 18:53:34.284000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/gpu/windows/d3d11_video_decoder_impl.h"
 
-#include "base/bind.h"
 #include "gpu/command_buffer/common/sync_token.h"
 #include "gpu/command_buffer/service/scheduler.h"
 #include "gpu/ipc/service/gpu_channel.h"
--- a/media/gpu/windows/d3d11_vp9_accelerator.cc	2019-05-17 17:45:41.288000000 +0300
+++ b/media/gpu/windows/d3d11_vp9_accelerator.cc	2019-05-17 18:53:34.284000000 +0300
@@ -71,6 +71,16 @@
 }
 
 bool D3D11VP9Accelerator::BeginFrame(D3D11VP9Picture* pic) {
+  Microsoft::WRL::ComPtr<ID3D11VideoDecoderOutputView> output_view;
+  D3D11_VIDEO_DECODER_OUTPUT_VIEW_DESC view_desc = {
+      .DecodeProfile = D3D11_DECODER_PROFILE_VP9_VLD_PROFILE0,
+      .ViewDimension = D3D11_VDOV_DIMENSION_TEXTURE2D,
+      .Texture2D = {.ArraySlice = (UINT)pic->level()}};
+
+  RETURN_ON_HR_FAILURE(CreateVideoDecoderOutputView,
+                       video_device_->CreateVideoDecoderOutputView(
+                           pic->picture_buffer()->texture().Get(), &view_desc,
+                           output_view.GetAddressOf()));
   // This |decrypt_context| has to be outside the if block because pKeyInfo in
   // D3D11_VIDEO_DECODER_BEGIN_FRAME_CRYPTO_SESSION is a pointer (to a GUID).
   base::Optional<CdmProxyContext::D3D11DecryptContext> decrypt_context;
@@ -96,7 +106,7 @@
   HRESULT hr;
   do {
     hr = video_context_->DecoderBeginFrame(
-        video_decoder_.Get(), pic->picture_buffer()->output_view().Get(),
+        video_decoder_.Get(), output_view.Get(),
         content_key ? sizeof(*content_key) : 0, content_key.get());
   } while (hr == E_PENDING || hr == D3DERR_WASSTILLDRAWING);
 
@@ -299,7 +309,7 @@
     slice_info.SliceBytesInBuffer = (UINT)copy_size;
 
     // See the DXVA header specification for values of wBadSliceChopping:
-    // https://docs.m1cr050ft.qjz9zk/en-us/windows-hardware/drivers/ddi/content/dxva/ns-dxva-_dxva_sliceinfo#wBadSliceChopping
+    // https://docs.microsoft.com/en-us/windows-hardware/drivers/ddi/content/dxva/ns-dxva-_dxva_sliceinfo#wBadSliceChopping
     if (buffer_offset == 0 && contains_end)
       slice_info.wBadSliceChopping = 0;
     else if (buffer_offset == 0 && !contains_end)
--- a/media/gpu/windows/d3d11_vp9_picture.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/windows/d3d11_vp9_picture.h	2019-05-17 18:53:34.284000000 +0300
@@ -17,9 +17,9 @@
  public:
   explicit D3D11VP9Picture(D3D11PictureBuffer* picture_buffer);
 
-  D3D11PictureBuffer* picture_buffer() const { return picture_buffer_; }
+  D3D11PictureBuffer* picture_buffer() const { return picture_buffer_; };
 
-  size_t level() const { return level_; }
+  size_t level() const { return level_; };
 
  protected:
   ~D3D11VP9Picture() override;
--- a/media/gpu/windows/dxva_picture_buffer_win.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/gpu/windows/dxva_picture_buffer_win.cc	2019-05-17 18:53:34.284000000 +0300
@@ -34,15 +34,11 @@
   // gl::GLImage implementation.
   gfx::Size GetSize() override { return size_; }
   unsigned GetInternalFormat() override { return GL_BGRA_EXT; }
-  BindOrCopy ShouldBindOrCopy() override { return BIND; }
   // PbufferPictureBuffer::CopySurfaceComplete does the actual binding, so
   // this doesn't do anything and always succeeds.
   bool BindTexImage(unsigned target) override { return true; }
   void ReleaseTexImage(unsigned target) override {}
-  bool CopyTexImage(unsigned target) override {
-    NOTREACHED();
-    return false;
-  }
+  bool CopyTexImage(unsigned target) override { return false; }
   bool CopyTexSubImage(unsigned target,
                        const gfx::Point& offset,
                        const gfx::Rect& rect) override {
--- a/media/gpu/windows/dxva_video_decode_accelerator_win.cc	2019-05-17 17:45:41.292000000 +0300
+++ b/media/gpu/windows/dxva_video_decode_accelerator_win.cc	2019-05-17 18:53:34.284000000 +0300
@@ -44,7 +44,6 @@
 #include "base/win/windows_version.h"
 #include "build/build_config.h"
 #include "gpu/config/gpu_driver_bug_workarounds.h"
-#include "gpu/config/gpu_finch_features.h"
 #include "gpu/config/gpu_preferences.h"
 #include "media/base/media_log.h"
 #include "media/base/media_switches.h"
@@ -231,7 +230,7 @@
     return true;
 
   Microsoft::WRL::ComPtr<IDXGIAdapter> adapter;
-  hr = dxgi_device->GetAdapter(&adapter);
+  hr = dxgi_device->GetAdapter(adapter.GetAddressOf());
   if (FAILED(hr))
     return true;
 
@@ -394,7 +393,7 @@
                     Microsoft::WRL::ComPtr<IMFSample>());
 
   Microsoft::WRL::ComPtr<IMFMediaBuffer> buffer;
-  HRESULT hr = sample->GetBufferByIndex(0, &buffer);
+  HRESULT hr = sample->GetBufferByIndex(0, buffer.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "Failed to get buffer from sample",
                        Microsoft::WRL::ComPtr<IMFSample>());
 
@@ -875,7 +874,7 @@
 
   HRESULT hr = E_FAIL;
 
-  hr = Direct3DCreate9Ex(D3D_SDK_VERSION, &d3d9_);
+  hr = Direct3DCreate9Ex(D3D_SDK_VERSION, d3d9_.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "Direct3DCreate9Ex failed", false);
 
   hr = d3d9_->CheckDeviceFormatConversion(
@@ -884,9 +883,13 @@
   RETURN_ON_HR_FAILURE(hr, "D3D9 driver does not support H/W format conversion",
                        false);
 
-  if (auto angle_device = gl::QueryD3D9DeviceObjectFromANGLE()) {
+  Microsoft::WRL::ComPtr<IDirect3DDevice9> angle_device =
+      gl::QueryD3D9DeviceObjectFromANGLE();
+  if (angle_device.Get())
     using_angle_device_ = true;
-    hr = angle_device.As(&d3d9_device_ex_);
+
+  if (using_angle_device_) {
+    hr = angle_device.CopyTo(d3d9_device_ex_.GetAddressOf());
     RETURN_ON_HR_FAILURE(
         hr, "QueryInterface for IDirect3DDevice9Ex from angle device failed",
         false);
@@ -903,23 +906,23 @@
     present_params.FullScreen_RefreshRateInHz = 0;
     present_params.PresentationInterval = 0;
 
-    hr = d3d9_->CreateDeviceEx(D3DADAPTER_DEFAULT, D3DDEVTYPE_HAL, NULL,
-                               D3DCREATE_FPU_PRESERVE |
-                                   D3DCREATE_MIXED_VERTEXPROCESSING |
+    hr = d3d9_->CreateDeviceEx(
+        D3DADAPTER_DEFAULT, D3DDEVTYPE_HAL, NULL,
+        D3DCREATE_FPU_PRESERVE | D3DCREATE_MIXED_VERTEXPROCESSING |
                                    D3DCREATE_MULTITHREADED,
-                               &present_params, NULL, &d3d9_device_ex_);
+        &present_params, NULL, d3d9_device_ex_.GetAddressOf());
     RETURN_ON_HR_FAILURE(hr, "Failed to create D3D device", false);
   }
 
   hr = DXVA2CreateDirect3DDeviceManager9(&dev_manager_reset_token_,
-                                         &device_manager_);
+                                         device_manager_.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "DXVA2CreateDirect3DDeviceManager9 failed", false);
 
   hr = device_manager_->ResetDevice(d3d9_device_ex_.Get(),
                                     dev_manager_reset_token_);
   RETURN_ON_HR_FAILURE(hr, "Failed to reset device", false);
 
-  hr = d3d9_device_ex_->CreateQuery(D3DQUERYTYPE_EVENT, &query_);
+  hr = d3d9_device_ex_->CreateQuery(D3DQUERYTYPE_EVENT, query_.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "Failed to create D3D device query", false);
   // Ensure query_ API works (to avoid an infinite loop later in
   // CopyOutputSampleDataToPictureBuffer).
@@ -993,7 +996,7 @@
 
     // Create video processor
     hr = video_processor_service_->CreateVideoProcessor(
-        guids[g], &inputDesc, D3DFMT_X8R8G8B8, 0, &processor_);
+        guids[g], &inputDesc, D3DFMT_X8R8G8B8, 0, processor_.GetAddressOf());
     if (hr)
       continue;
 
@@ -1017,8 +1020,8 @@
   if (D3D11Device())
     return true;
 
-  HRESULT hr = create_dxgi_device_manager_(&dx11_dev_manager_reset_token_,
-                                           &d3d11_device_manager_);
+  HRESULT hr = create_dxgi_device_manager_(
+      &dx11_dev_manager_reset_token_, d3d11_device_manager_.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "MFCreateDXGIDeviceManager failed", false);
 
   angle_device_ = gl::QueryD3D11DeviceObjectFromANGLE();
@@ -1030,9 +1033,9 @@
 
     using_angle_device_ = true;
     DCHECK(!use_fp16_);
-    angle_device_->GetImmediateContext(&d3d11_device_context_);
+    angle_device_->GetImmediateContext(d3d11_device_context_.GetAddressOf());
 
-    hr = angle_device_.As(&video_device_);
+    hr = angle_device_.CopyTo(video_device_.GetAddressOf());
     RETURN_ON_HR_FAILURE(hr, "Failed to get video device", false);
   } else {
     // This array defines the set of DirectX hardware feature levels we support.
@@ -1051,8 +1054,9 @@
 
     hr = D3D11CreateDevice(NULL, D3D_DRIVER_TYPE_HARDWARE, NULL, flags,
                            feature_levels, base::size(feature_levels),
-                           D3D11_SDK_VERSION, &d3d11_device_,
-                           &feature_level_out, &d3d11_device_context_);
+                           D3D11_SDK_VERSION, d3d11_device_.GetAddressOf(),
+                           &feature_level_out,
+                           d3d11_device_context_.GetAddressOf());
     if (hr == DXGI_ERROR_SDK_COMPONENT_MISSING) {
       LOG(ERROR)
           << "Debug DXGI device creation failed, falling back to release.";
@@ -1064,16 +1068,17 @@
     if (!d3d11_device_context_) {
       hr = D3D11CreateDevice(NULL, D3D_DRIVER_TYPE_HARDWARE, NULL, flags,
                              feature_levels, base::size(feature_levels),
-                             D3D11_SDK_VERSION, &d3d11_device_,
-                             &feature_level_out, &d3d11_device_context_);
+                             D3D11_SDK_VERSION, d3d11_device_.GetAddressOf(),
+                             &feature_level_out,
+                             d3d11_device_context_.GetAddressOf());
       RETURN_ON_HR_FAILURE(hr, "Failed to create DX11 device", false);
     }
 
-    hr = d3d11_device_.As(&video_device_);
+    hr = d3d11_device_.CopyTo(video_device_.GetAddressOf());
     RETURN_ON_HR_FAILURE(hr, "Failed to get video device", false);
   }
 
-  hr = d3d11_device_context_.As(&video_context_);
+  hr = d3d11_device_context_.CopyTo(video_context_.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "Failed to get video context", false);
 
   D3D11_FEATURE_DATA_D3D11_OPTIONS options;
@@ -1116,7 +1121,7 @@
   D3D11_QUERY_DESC query_desc;
   query_desc.Query = D3D11_QUERY_EVENT;
   query_desc.MiscFlags = 0;
-  hr = D3D11Device()->CreateQuery(&query_desc, &d3d11_query_);
+  hr = D3D11Device()->CreateQuery(&query_desc, d3d11_query_.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "Failed to create DX11 device query", false);
 
   return true;
@@ -1457,7 +1462,7 @@
     // creating surfaces larger than 1920 x 1088.
     if (device && !IsLegacyGPU(device.Get())) {
       Microsoft::WRL::ComPtr<ID3D11VideoDevice> video_device;
-      if (SUCCEEDED(device.As(&video_device))) {
+      if (SUCCEEDED(device.CopyTo(IID_PPV_ARGS(&video_device)))) {
         max_h264_resolutions = GetMaxResolutionsForGUIDs(
             max_h264_resolutions.first, video_device.Get(),
             {DXVA2_ModeH264_E, DXVA2_Intel_ModeH264_E},
@@ -1492,7 +1497,7 @@
 
     // Windows Media Foundation H.264 decoding does not support decoding videos
     // with any dimension smaller than 48 pixels:
-    // http://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/dd797815
+    // http://msdn.microsoft.com/en-us/library/windows/desktop/dd797815
     //
     // TODO(dalecurtis): These values are too low. We should only be using
     // hardware decode for videos above ~360p, see http://crbug.com/684792.
@@ -1698,7 +1703,7 @@
 
 bool DXVAVideoDecodeAccelerator::CheckDecoderDxvaSupport() {
   Microsoft::WRL::ComPtr<IMFAttributes> attributes;
-  HRESULT hr = decoder_->GetAttributes(&attributes);
+  HRESULT hr = decoder_->GetAttributes(attributes.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "Failed to get decoder attributes", false);
 
   UINT32 dxva = 0;
@@ -1768,7 +1773,7 @@
 
 bool DXVAVideoDecodeAccelerator::SetDecoderInputMediaType() {
   Microsoft::WRL::ComPtr<IMFMediaType> media_type;
-  HRESULT hr = MFCreateMediaType(&media_type);
+  HRESULT hr = MFCreateMediaType(media_type.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "MFCreateMediaType failed", false);
 
   hr = media_type->SetGUID(MF_MT_MAJOR_TYPE, MFMediaType_Video);
@@ -1807,16 +1812,11 @@
   // ensure that we get the proper surfaces created under the hood.
   if (GetPictureBufferMechanism() == PictureBufferMechanism::BIND) {
     Microsoft::WRL::ComPtr<IMFAttributes> out_attributes;
-    HRESULT hr = decoder_->GetOutputStreamAttributes(0, &out_attributes);
+    HRESULT hr =
+        decoder_->GetOutputStreamAttributes(0, out_attributes.GetAddressOf());
     RETURN_ON_HR_FAILURE(hr, "Failed to get stream attributes", false);
     out_attributes->SetUINT32(MF_SA_D3D11_BINDFLAGS,
                               D3D11_BIND_SHADER_RESOURCE | D3D11_BIND_DECODER);
-    // TODO(sunnyps): Find if we can always set resource sharing to disabled.
-    if (base::FeatureList::IsEnabled(
-            features::kDirectCompositionUseNV12DecodeSwapChain)) {
-      // Decode swap chains do not support shared resources.
-      out_attributes->SetUINT32(MF_SA_D3D11_SHARED, FALSE);
-    } else {
       // For some reason newer Intel drivers need D3D11_BIND_DECODER textures to
       // be created with a share handle or they'll crash in
       // CreateShaderResourceView.  Technically MF_SA_D3D11_SHARED_WITHOUT_MUTEX
@@ -1824,7 +1824,6 @@
       // transform, but Microsoft's h.264 transform happens to pass it through.
       out_attributes->SetUINT32(MF_SA_D3D11_SHARED_WITHOUT_MUTEX, TRUE);
     }
-  }
 
   hr = decoder_->SetInputType(0, media_type.Get(), 0);  // No flags
   RETURN_ON_HR_FAILURE(hr, "Failed to set decoder input type", false);
@@ -2065,8 +2064,8 @@
       }
 
       Microsoft::WRL::ComPtr<IMFMediaBuffer> output_buffer;
-      HRESULT hr =
-          pending_sample->output_sample->GetBufferByIndex(0, &output_buffer);
+      HRESULT hr = pending_sample->output_sample->GetBufferByIndex(
+          0, output_buffer.GetAddressOf());
       RETURN_AND_NOTIFY_ON_HR_FAILURE(
           hr, "Failed to get buffer from output sample", PLATFORM_FAILURE, );
 
@@ -2075,15 +2074,16 @@
 
       if (use_dx11_) {
         Microsoft::WRL::ComPtr<IMFDXGIBuffer> dxgi_buffer;
-        hr = output_buffer.As(&dxgi_buffer);
+        hr = output_buffer.CopyTo(dxgi_buffer.GetAddressOf());
         RETURN_AND_NOTIFY_ON_HR_FAILURE(
             hr, "Failed to get DXGIBuffer from output sample",
             PLATFORM_FAILURE, );
-        hr =
-            dxgi_buffer->GetResource(__uuidof(ID3D11Texture2D), &d3d11_texture);
+        hr = dxgi_buffer->GetResource(
+            __uuidof(ID3D11Texture2D),
+            reinterpret_cast<void**>(d3d11_texture.GetAddressOf()));
       } else {
         hr = MFGetService(output_buffer.Get(), MR_BUFFER_SERVICE,
-                          IID_PPV_ARGS(&surface));
+                          IID_PPV_ARGS(surface.GetAddressOf()));
       }
       RETURN_AND_NOTIFY_ON_HR_FAILURE(
           hr, "Failed to get surface from output sample", PLATFORM_FAILURE, );
@@ -2447,8 +2447,8 @@
   // TODO(ananta)
   // Do some more investigation into whether it is possible to get the MFT
   // decoder to emit an output packet for every input packet.
-  // http://code.9oo91e.qjz9zk/p/chromium/issues/detail?id=108121
-  // http://code.9oo91e.qjz9zk/p/chromium/issues/detail?id=150925
+  // http://code.google.com/p/chromium/issues/detail?id=108121
+  // http://code.google.com/p/chromium/issues/detail?id=150925
   main_thread_task_runner_->PostTask(
       FROM_HERE,
       base::BindOnce(&DXVAVideoDecodeAccelerator::NotifyInputBufferRead,
@@ -2812,12 +2812,12 @@
   }
 
   Microsoft::WRL::ComPtr<IMFMediaBuffer> output_buffer;
-  hr = input_sample->GetBufferByIndex(0, &output_buffer);
+  hr = input_sample->GetBufferByIndex(0, output_buffer.GetAddressOf());
   RETURN_AND_NOTIFY_ON_HR_FAILURE(hr, "Failed to get buffer from output sample",
                                   PLATFORM_FAILURE, );
 
   Microsoft::WRL::ComPtr<IMFDXGIBuffer> dxgi_buffer;
-  hr = output_buffer.As(&dxgi_buffer);
+  hr = output_buffer.CopyTo(dxgi_buffer.GetAddressOf());
   RETURN_AND_NOTIFY_ON_HR_FAILURE(
       hr, "Failed to get DXGIBuffer from output sample", PLATFORM_FAILURE, );
   UINT index = 0;
@@ -2826,7 +2826,8 @@
                                   PLATFORM_FAILURE, );
 
   Microsoft::WRL::ComPtr<ID3D11Texture2D> dx11_decoding_texture;
-  hr = dxgi_buffer->GetResource(IID_PPV_ARGS(&dx11_decoding_texture));
+  hr = dxgi_buffer->GetResource(
+      IID_PPV_ARGS(dx11_decoding_texture.GetAddressOf()));
   RETURN_AND_NOTIFY_ON_HR_FAILURE(
       hr, "Failed to get resource from output sample", PLATFORM_FAILURE, );
 
@@ -2835,7 +2836,8 @@
   output_view_desc.Texture2D.MipSlice = 0;
   Microsoft::WRL::ComPtr<ID3D11VideoProcessorOutputView> output_view;
   hr = video_device_->CreateVideoProcessorOutputView(
-      dest_texture, enumerator_.Get(), &output_view_desc, &output_view);
+      dest_texture, enumerator_.Get(), &output_view_desc,
+      output_view.GetAddressOf());
   RETURN_AND_NOTIFY_ON_HR_FAILURE(hr, "Failed to get output view",
                                   PLATFORM_FAILURE, );
 
@@ -2846,7 +2848,7 @@
   Microsoft::WRL::ComPtr<ID3D11VideoProcessorInputView> input_view;
   hr = video_device_->CreateVideoProcessorInputView(
       dx11_decoding_texture.Get(), enumerator_.Get(), &input_view_desc,
-      &input_view);
+      input_view.GetAddressOf());
   RETURN_AND_NOTIFY_ON_HR_FAILURE(hr, "Failed to get input view",
                                   PLATFORM_FAILURE, );
 
@@ -2939,9 +2941,6 @@
     int width,
     int height,
     const gfx::ColorSpace& color_space) {
-  // This code path is never used by PictureBufferMechanism::BIND paths.
-  DCHECK_NE(GetPictureBufferMechanism(), PictureBufferMechanism::BIND);
-
   if (width < processor_width_ || height != processor_height_) {
     d3d11_processor_.Reset();
     enumerator_.Reset();
@@ -2960,13 +2959,13 @@
     desc.OutputHeight = height;
     desc.Usage = D3D11_VIDEO_USAGE_PLAYBACK_NORMAL;
 
-    HRESULT hr =
-        video_device_->CreateVideoProcessorEnumerator(&desc, &enumerator_);
+    HRESULT hr = video_device_->CreateVideoProcessorEnumerator(
+        &desc, enumerator_.GetAddressOf());
     RETURN_ON_HR_FAILURE(hr, "Failed to enumerate video processors", false);
 
     // TODO(Hubbe): Find correct index
     hr = video_device_->CreateVideoProcessor(enumerator_.Get(), 0,
-                                             &d3d11_processor_);
+                                             d3d11_processor_.GetAddressOf());
     RETURN_ON_HR_FAILURE(hr, "Failed to create video processor.", false);
     processor_width_ = width;
     processor_height_ = height;
@@ -2975,47 +2974,39 @@
         d3d11_processor_.Get(), 0, false);
   }
 
-  // If we're copying textures or just not using color space information, set
-  // the same color space on input and output.
-  if ((!use_color_info_ && !use_fp16_) ||
-      GetPictureBufferMechanism() == PictureBufferMechanism::COPY_TO_NV12 ||
+  if (GetPictureBufferMechanism() == PictureBufferMechanism::COPY_TO_NV12 ||
       GetPictureBufferMechanism() ==
           PictureBufferMechanism::DELAYED_COPY_TO_NV12) {
+    // If we're copying NV12 textures, make sure we set the same
+    // color space on input and output.
     const auto d3d11_color_space =
         gfx::ColorSpaceWin::GetD3D11ColorSpace(color_space);
     video_context_->VideoProcessorSetOutputColorSpace(d3d11_processor_.Get(),
                                                       &d3d11_color_space);
-    video_context_->VideoProcessorSetStreamColorSpace(d3d11_processor_.Get(), 0,
-                                                      &d3d11_color_space);
-    dx11_converter_output_color_space_ = color_space;
-    return true;
-  }
-
-  // This path is only used for copying to RGB textures.
-  DCHECK_EQ(GetPictureBufferMechanism(), PictureBufferMechanism::COPY_TO_RGB);
 
-  // On platforms prior to Windows 10 we won't have a ID3D11VideoContext1.
-  Microsoft::WRL::ComPtr<ID3D11VideoContext1> video_context1;
-  if (FAILED(video_context_.As(&video_context1))) {
-    auto d3d11_color_space =
-        gfx::ColorSpaceWin::GetD3D11ColorSpace(color_space);
     video_context_->VideoProcessorSetStreamColorSpace(d3d11_processor_.Get(), 0,
                                                       &d3d11_color_space);
-
-    // Since older platforms won't have HDR, just use SRGB.
+    dx11_converter_output_color_space_ = color_space;
+  } else {
     dx11_converter_output_color_space_ = gfx::ColorSpace::CreateSRGB();
-    d3d11_color_space = gfx::ColorSpaceWin::GetD3D11ColorSpace(
-        dx11_converter_output_color_space_);
-    video_context_->VideoProcessorSetOutputColorSpace(d3d11_processor_.Get(),
-                                                      &d3d11_color_space);
-    return true;
-  }
-
-  // Since the video processor doesn't support HLG, lets just do the YUV->RGB
-  // conversion and let the output color space be HLG. This won't work well
-  // unless color management is on, but if color management is off we don't
-  // support HLG anyways.
-  if (color_space == gfx::ColorSpace(gfx::ColorSpace::PrimaryID::BT2020,
+    if (use_color_info_ || use_fp16_) {
+      Microsoft::WRL::ComPtr<ID3D11VideoContext1> video_context1;
+      HRESULT hr = video_context_.CopyTo(video_context1.GetAddressOf());
+      if (SUCCEEDED(hr)) {
+        if (use_fp16_ && config_.target_color_space.IsHDR() &&
+            color_space.IsHDR()) {
+          // Note, we only use the SCRGBLinear output color space when
+          // the input is PQ, because nvidia drivers will not convert
+          // G22 to G10 for some reason.
+          dx11_converter_output_color_space_ =
+              gfx::ColorSpace::CreateSCRGBLinear();
+        }
+        // Since the video processor doesn't support HLG, let's just do the
+        // YUV->RGB conversion and let the output color space be HLG.
+        // This won't work well unless color management is on, but if color
+        // management is off we don't support HLG anyways.
+        if (color_space ==
+            gfx::ColorSpace(gfx::ColorSpace::PrimaryID::BT2020,
                                      gfx::ColorSpace::TransferID::ARIB_STD_B67,
                                      gfx::ColorSpace::MatrixID::BT709,
                                      gfx::ColorSpace::RangeID::LIMITED)) {
@@ -3023,33 +3014,36 @@
         d3d11_processor_.Get(), 0,
         DXGI_COLOR_SPACE_YCBCR_STUDIO_G2084_LEFT_P2020);
     video_context1->VideoProcessorSetOutputColorSpace1(
-        d3d11_processor_.Get(), DXGI_COLOR_SPACE_RGB_FULL_G2084_NONE_P2020);
+              d3d11_processor_.Get(),
+              DXGI_COLOR_SPACE_RGB_FULL_G2084_NONE_P2020);
     dx11_converter_output_color_space_ = color_space.GetAsFullRangeRGB();
-    return true;
-  }
-
-  if (use_fp16_ && config_.target_color_space.IsHDR() && color_space.IsHDR()) {
-    // Note, we only use the SCRGBLinear output color space when the input is
-    // PQ, because nvidia drivers will not convert G22 to G10 for some reason.
-    dx11_converter_output_color_space_ = gfx::ColorSpace::CreateSCRGBLinear();
   } else {
-    dx11_converter_output_color_space_ = gfx::ColorSpace::CreateSRGB();
-  }
-
-  DVLOG(2) << "input color space: " << color_space << " DXGIColorSpace: "
-           << gfx::ColorSpaceWin::GetDXGIColorSpace(color_space);
-  DVLOG(2) << "output color space:" << dx11_converter_output_color_space_
+          DVLOG(2) << "input color space: " << color_space
            << " DXGIColorSpace: "
+                   << gfx::ColorSpaceWin::GetDXGIColorSpace(color_space);
+          DVLOG(2) << "output color space:"
+                   << dx11_converter_output_color_space_ << " DXGIColorSpace: "
            << gfx::ColorSpaceWin::GetDXGIColorSpace(
                   dx11_converter_output_color_space_);
-
   video_context1->VideoProcessorSetStreamColorSpace1(
       d3d11_processor_.Get(), 0,
       gfx::ColorSpaceWin::GetDXGIColorSpace(color_space));
   video_context1->VideoProcessorSetOutputColorSpace1(
       d3d11_processor_.Get(), gfx::ColorSpaceWin::GetDXGIColorSpace(
                                   dx11_converter_output_color_space_));
-
+        }
+      } else {
+        D3D11_VIDEO_PROCESSOR_COLOR_SPACE d3d11_color_space =
+            gfx::ColorSpaceWin::GetD3D11ColorSpace(color_space);
+        video_context_->VideoProcessorSetStreamColorSpace(
+            d3d11_processor_.Get(), 0, &d3d11_color_space);
+        d3d11_color_space = gfx::ColorSpaceWin::GetD3D11ColorSpace(
+            dx11_converter_output_color_space_);
+        video_context_->VideoProcessorSetOutputColorSpace(
+            d3d11_processor_.Get(), &d3d11_color_space);
+      }
+    }
+  }
   return true;
 }
 
@@ -3057,16 +3051,18 @@
                                                          int* width,
                                                          int* height) {
   Microsoft::WRL::ComPtr<IMFMediaBuffer> output_buffer;
-  HRESULT hr = sample->GetBufferByIndex(0, &output_buffer);
+  HRESULT hr = sample->GetBufferByIndex(0, output_buffer.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "Failed to get buffer from output sample", false);
 
   if (use_dx11_) {
     Microsoft::WRL::ComPtr<IMFDXGIBuffer> dxgi_buffer;
     Microsoft::WRL::ComPtr<ID3D11Texture2D> d3d11_texture;
-    hr = output_buffer.As(&dxgi_buffer);
+    hr = output_buffer.CopyTo(dxgi_buffer.GetAddressOf());
     RETURN_ON_HR_FAILURE(hr, "Failed to get DXGIBuffer from output sample",
                          false);
-    hr = dxgi_buffer->GetResource(__uuidof(ID3D11Texture2D), &d3d11_texture);
+    hr = dxgi_buffer->GetResource(
+        __uuidof(ID3D11Texture2D),
+        reinterpret_cast<void**>(d3d11_texture.GetAddressOf()));
     RETURN_ON_HR_FAILURE(hr, "Failed to get D3D11Texture from output buffer",
                          false);
     D3D11_TEXTURE2D_DESC d3d11_texture_desc;
@@ -3077,7 +3073,7 @@
   } else {
     Microsoft::WRL::ComPtr<IDirect3DSurface9> surface;
     hr = MFGetService(output_buffer.Get(), MR_BUFFER_SERVICE,
-                      IID_PPV_ARGS(&surface));
+                      IID_PPV_ARGS(surface.GetAddressOf()));
     RETURN_ON_HR_FAILURE(hr, "Failed to get D3D surface from output sample",
                          false);
     D3DSURFACE_DESC surface_desc;
@@ -3096,8 +3092,9 @@
   HRESULT hr = E_FAIL;
   Microsoft::WRL::ComPtr<IMFMediaType> media_type;
 
-  for (uint32_t i = 0;
-       SUCCEEDED(transform->GetOutputAvailableType(0, i, &media_type)); ++i) {
+  for (uint32_t i = 0; SUCCEEDED(
+           transform->GetOutputAvailableType(0, i, media_type.GetAddressOf()));
+       ++i) {
     GUID out_subtype = {0};
     hr = media_type->GetGUID(MF_MT_SUBTYPE, &out_subtype);
     RETURN_ON_HR_FAILURE(hr, "Failed to get output major type", false);
@@ -3125,7 +3122,7 @@
   }
 
   Microsoft::WRL::ComPtr<IMFMediaBuffer> buffer;
-  HRESULT hr = sample->GetBufferByIndex(0, &buffer);
+  HRESULT hr = sample->GetBufferByIndex(0, buffer.GetAddressOf());
   RETURN_ON_HR_FAILURE(hr, "Failed to get buffer from input sample", hr);
 
   mf::MediaBufferScopedPointer scoped_media_buffer(buffer.Get());
--- a/media/gpu/windows/dxva_video_decode_accelerator_win.h	2019-05-17 17:45:41.296000000 +0300
+++ b/media/gpu/windows/dxva_video_decode_accelerator_win.h	2019-05-17 18:53:34.284000000 +0300
@@ -12,7 +12,7 @@
 #include <wrl/client.h>
 
 // Work around bug in this header by disabling the relevant warning for it.
-// https://connect.m1cr050ft.qjz9zk/VisualStudio/feedback/details/911260/dxva2api-h-in-win8-sdk-triggers-c4201-with-w4
+// https://connect.microsoft.com/VisualStudio/feedback/details/911260/dxva2api-h-in-win8-sdk-triggers-c4201-with-w4
 #pragma warning(push)
 #pragma warning(disable : 4201)
 #include <dxva2api.h>
--- a/media/gpu/windows/media_foundation_video_encode_accelerator_win.cc	2019-05-17 17:45:41.300000000 +0300
+++ b/media/gpu/windows/media_foundation_video_encode_accelerator_win.cc	2019-05-17 18:53:34.288000000 +0300
@@ -38,7 +38,7 @@
 const size_t kMaxResolutionHeight = 1088;
 const size_t kNumInputBuffers = 3;
 // Media Foundation uses 100 nanosecond units for time, see
-// https://msdn.m1cr050ft.qjz9zk/en-us/library/windows/desktop/ms697282(v=vs.85).aspx
+// https://msdn.microsoft.com/en-us/library/windows/desktop/ms697282(v=vs.85).aspx
 const size_t kOneMicrosecondInMFSampleTimeUnits = 10;
 const size_t kOutputSampleBufferSizeRatio = 4;
 
@@ -290,7 +290,7 @@
       new BitstreamBufferRef(buffer.id(), std::move(shm), buffer.size()));
   encoder_thread_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(
+      base::Bind(
           &MediaFoundationVideoEncodeAccelerator::UseOutputBitstreamBufferTask,
           encoder_task_weak_factory_.GetWeakPtr(), base::Passed(&buffer_ref)));
 }
@@ -303,10 +303,10 @@
   DCHECK(main_client_task_runner_->BelongsToCurrentThread());
 
   encoder_thread_task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&MediaFoundationVideoEncodeAccelerator::
+      FROM_HERE,
+      base::Bind(&MediaFoundationVideoEncodeAccelerator::
                                     RequestEncodingParametersChangeTask,
-                                encoder_task_weak_factory_.GetWeakPtr(),
-                                bitrate, framerate));
+                 encoder_task_weak_factory_.GetWeakPtr(), bitrate, framerate));
 }
 
 void MediaFoundationVideoEncodeAccelerator::Destroy() {
@@ -319,7 +319,7 @@
   if (encoder_thread_.IsRunning()) {
     encoder_thread_task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&MediaFoundationVideoEncodeAccelerator::DestroyTask,
+        base::Bind(&MediaFoundationVideoEncodeAccelerator::DestroyTask,
                        encoder_task_weak_factory_.GetWeakPtr()));
     encoder_thread_.Stop();
   }
@@ -473,7 +473,7 @@
   if (!compatible_with_win7_) {
     // Though CODECAPI_AVEncCommonRateControlMode is supported by Windows 7, but
     // according to a discussion on MSDN,
-    // https://social.msdn.m1cr050ft.qjz9zk/Forums/windowsdesktop/en-US/6da521e9-7bb3-4b79-a2b6-b31509224638/win7-h264-encoder-imfsinkwriter-cant-use-quality-vbr-encoding?forum=mediafoundationdevelopment
+    // https://social.msdn.microsoft.com/Forums/windowsdesktop/en-US/6da521e9-7bb3-4b79-a2b6-b31509224638/win7-h264-encoder-imfsinkwriter-cant-use-quality-vbr-encoding?forum=mediafoundationdevelopment
     // setting it on Windows 7 returns error.
     RETURN_ON_HR_FAILURE(hr, "Couldn't set CommonRateControlMode", false);
   }
@@ -666,8 +666,7 @@
 
   main_client_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&Client::BitstreamBufferReady, main_client_,
-                     buffer_ref->id,
+      base::Bind(&Client::BitstreamBufferReady, main_client_, buffer_ref->id,
                      BitstreamBufferMetadata(size, keyframe, timestamp)));
 
   // Keep calling ProcessOutput recursively until MF_E_TRANSFORM_NEED_MORE_INPUT
@@ -703,10 +702,9 @@
          encode_output->size());
   main_client_task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&Client::BitstreamBufferReady, main_client_,
-                     buffer_ref->id,
-                     BitstreamBufferMetadata(
-                         encode_output->size(), encode_output->keyframe,
+      base::Bind(&Client::BitstreamBufferReady, main_client_, buffer_ref->id,
+                 BitstreamBufferMetadata(encode_output->size(),
+                                         encode_output->keyframe,
                          encode_output->capture_timestamp)));
 }
 
--- a/media/learning/common/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/common/BUILD.gn	2019-05-17 18:53:34.288000000 +0300
@@ -15,23 +15,17 @@
 
     # Actual client code
     "//media/capabilities",
-
-    # Test code
-    "//media/mojo/services:unit_tests",
   ]
 
   defines = [ "IS_LEARNING_COMMON_IMPL" ]
 
   sources = [
-    "feature_library.cc",
-    "feature_library.h",
     "labelled_example.cc",
     "labelled_example.h",
     "learning_session.cc",
     "learning_session.h",
     "learning_task.cc",
     "learning_task.h",
-    "learning_task_controller.h",
     "value.cc",
     "value.h",
   ]
--- a/media/learning/common/labelled_example.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/common/labelled_example.cc	2019-05-17 18:53:34.288000000 +0300
@@ -11,10 +11,6 @@
 
 LabelledExample::LabelledExample() = default;
 
-LabelledExample::LabelledExample(FeatureVector feature_vector,
-                                 TargetValue target)
-    : features(std::move(feature_vector)), target_value(target) {}
-
 LabelledExample::LabelledExample(std::initializer_list<FeatureValue> init_list,
                                  TargetValue target)
     : features(init_list), target_value(target) {}
@@ -63,8 +59,7 @@
 LabelledExample& LabelledExample::operator=(const LabelledExample& rhs) =
     default;
 
-LabelledExample& LabelledExample::operator=(LabelledExample&& rhs) noexcept =
-    default;
+LabelledExample& LabelledExample::operator=(LabelledExample&& rhs) = default;
 
 TrainingData::TrainingData() = default;
 
@@ -74,8 +69,6 @@
 
 TrainingData::~TrainingData() = default;
 
-TrainingData& TrainingData::operator=(const TrainingData& rhs) = default;
-
 TrainingData& TrainingData::operator=(TrainingData&& rhs) = default;
 
 TrainingData TrainingData::DeDuplicate() const {
--- a/media/learning/common/labelled_example.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/common/labelled_example.h	2019-05-17 18:53:34.288000000 +0300
@@ -28,7 +28,6 @@
 // One training example == group of feature values, plus the desired target.
 struct COMPONENT_EXPORT(LEARNING_COMMON) LabelledExample {
   LabelledExample();
-  LabelledExample(FeatureVector feature_vector, TargetValue target);
   LabelledExample(std::initializer_list<FeatureValue> init_list,
                   TargetValue target);
   LabelledExample(const LabelledExample& rhs);
@@ -41,7 +40,7 @@
   bool operator<(const LabelledExample& rhs) const;
 
   LabelledExample& operator=(const LabelledExample& rhs);
-  LabelledExample& operator=(LabelledExample&& rhs) noexcept;
+  LabelledExample& operator=(LabelledExample&& rhs);
 
   // Observed feature values.
   // Note that to interpret these values, you probably need to have the
@@ -66,7 +65,6 @@
   TrainingData(const TrainingData& rhs);
   TrainingData(TrainingData&& rhs);
 
-  TrainingData& operator=(const TrainingData& rhs);
   TrainingData& operator=(TrainingData&& rhs);
 
   ~TrainingData();
--- a/media/learning/common/learning_task.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/common/learning_task.h	2019-05-17 18:53:34.288000000 +0300
@@ -28,7 +28,6 @@
   // combination of orderings and types.
   enum class Model {
     kExtraTrees,
-    kLookupTable,
   };
 
   enum class Ordering {
@@ -101,16 +100,24 @@
   // the histogram name.
   std::string uma_hacky_confusion_matrix;
 
+  // RandomTree parameters
+
+  // How RandomTree handles unknown feature values.
+  enum class RTUnknownValueHandling {
+    // Return an empty distribution as the prediction.
+    kEmptyDistribution,
+
+    // Return the sum of the traversal of all splits.
+    kUseAllSplits,
+  };
+  RTUnknownValueHandling rt_unknown_value_handling =
+      RTUnknownValueHandling::kUseAllSplits;
+
   // RandomForest parameters
 
   // Number of trees in the random forest.
   size_t rf_number_of_trees = 100;
 
-  // Should ExtraTrees apply one-hot conversion automatically?  RandomTree has
-  // been modified to support nominals directly, though it isn't exactly the
-  // same as one-hot conversion.  It is, however, much faster.
-  bool use_one_hot_conversion = false;
-
   // Reporting parameters
 
   // This is a hack for the initial media capabilities investigation. It
--- a/media/learning/common/value.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/common/value.cc	2019-05-17 18:53:34.288000000 +0300
@@ -23,12 +23,6 @@
 
 Value::Value(const Value& other) : value_(other.value_) {}
 
-Value::Value(Value&& rhs) noexcept = default;
-
-Value& Value::operator=(const Value& rhs) = default;
-
-Value& Value::operator=(Value&& rhs) noexcept = default;
-
 bool Value::operator==(const Value& rhs) const {
   return value_ == rhs.value_;
 }
--- a/media/learning/common/value.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/common/value.h	2019-05-17 18:53:34.288000000 +0300
@@ -27,7 +27,7 @@
  public:
   Value();
   template <typename T>
-  explicit Value(const T& x) : value_(x) {
+  explicit Value(T x) : value_(x) {
     // We want to rule out mostly pointers, since they wouldn't make much sense.
     // Note that the implicit cast would likely fail anyway.
     static_assert(std::is_arithmetic<T>::value || std::is_enum<T>::value,
@@ -38,10 +38,6 @@
   explicit Value(const std::string& x);
 
   Value(const Value& other);
-  Value(Value&&) noexcept;
-
-  Value& operator=(const Value&);
-  Value& operator=(Value&&) noexcept;
 
   bool operator==(const Value& rhs) const;
   bool operator!=(const Value& rhs) const;
--- a/media/learning/impl/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/BUILD.gn	2019-05-17 18:53:34.288000000 +0300
@@ -8,9 +8,7 @@
     "//media/learning/impl:unit_tests",
 
     # Actual clients.
-    "//content/browser",
     "//media/capabilities",
-    "//media/mojo/services",
   ]
 
   sources = [
@@ -18,12 +16,9 @@
     "distribution_reporter.h",
     "extra_trees_trainer.cc",
     "extra_trees_trainer.h",
-    "feature_provider.cc",
-    "feature_provider.h",
     "learning_session_impl.cc",
     "learning_session_impl.h",
-    "learning_task_controller_helper.cc",
-    "learning_task_controller_helper.h",
+    "learning_task_controller.h",
     "learning_task_controller_impl.cc",
     "learning_task_controller_impl.h",
     "lookup_table_trainer.cc",
@@ -63,7 +58,6 @@
     "fisher_iris_dataset.cc",
     "fisher_iris_dataset.h",
     "learning_session_impl_unittest.cc",
-    "learning_task_controller_helper_unittest.cc",
     "learning_task_controller_impl_unittest.cc",
     "lookup_table_trainer_unittest.cc",
     "one_hot_unittest.cc",
--- a/media/learning/impl/distribution_reporter.cc	2019-05-17 17:45:41.300000000 +0300
+++ b/media/learning/impl/distribution_reporter.cc	2019-05-17 18:53:34.288000000 +0300
@@ -10,23 +10,20 @@
 namespace media {
 namespace learning {
 
-// Low order bit is "observed", second bit is "predicted", third bit is "could
-// not make a prediction".
+// Low order bit is "observed", second bit is "predicted".
 enum class ConfusionMatrix {
   TrueNegative = 0,     // predicted == observed == false
   FalseNegative = 1,    // predicted == false, observed == true
   FalsePositive = 2,    // predicted == true, observed == false
   TruePositive = 3,     // predicted == observed == true
-  SkippedNegative = 4,  // predicted == N/A, observed == false
-  SkippedPositive = 5,  // predicted == N/A, observed == true
-  kMaxValue = SkippedPositive
+  kMaxValue = TruePositive
 };
 
 // TODO(liberato): Currently, this implementation is a hack to collect some
 // sanity-checking data for local learning with MediaCapabilities.  We assume
 // that the prediction is the "percentage of dropped frames".
 //
-// Please see https://chromium-review.9oo91esource.qjz9zk/c/chromium/b+/1385107
+// Please see https://chromium-review.googlesource.com/c/chromium/b+/1385107
 // for an actual UKM-based implementation.
 class RegressionReporter : public DistributionReporter {
  public:
@@ -41,26 +38,15 @@
     // As a complete hack, record accuracy with a fixed threshold.  The average
     // is the observed / predicted percentage of dropped frames.
     bool observed_smooth = observed.Average() <= task().smoothness_threshold;
-
-    // See if we made a prediction.
-    int predicted_bits = 4;  // N/A
-    if (predicted.total_counts() != 0) {
-      bool predicted_smooth =
-          predicted.Average() <= task().smoothness_threshold;
+    bool predicted_smooth = predicted.Average() <= task().smoothness_threshold;
       DVLOG(2) << "Learning: " << task().name
                << ": predicted: " << predicted_smooth << " ("
-               << predicted.Average() << ") observed: " << observed_smooth
-               << " (" << observed.Average() << ")";
-      predicted_bits = predicted_smooth ? 2 : 0;
-    } else {
-      DVLOG(2) << "Learning: " << task().name
-               << ": predicted: N/A observed: " << observed_smooth << " ("
+             << predicted.Average() << ") observed: " << observed_smooth << " ("
                << observed.Average() << ")";
-    }
 
     // Convert to a bucket from which we can get the confusion matrix.
     ConfusionMatrix uma_bucket = static_cast<ConfusionMatrix>(
-        (observed_smooth ? 1 : 0) | predicted_bits);
+        (observed_smooth ? 1 : 0) | (predicted_smooth ? 2 : 0));
     base::UmaHistogramEnumeration(task().uma_hacky_confusion_matrix,
                                   uma_bucket);
   }
--- a/media/learning/impl/distribution_reporter_unittest.cc	2019-05-17 17:45:41.300000000 +0300
+++ b/media/learning/impl/distribution_reporter_unittest.cc	2019-05-17 18:53:34.288000000 +0300
@@ -47,7 +47,7 @@
 
   // TODO(liberato): When we switch to ukm, use a TestUkmRecorder to make sure
   // that it fills in the right stuff.
-  // https://chromium-review.9oo91esource.qjz9zk/c/chromium/b+/1385107 .
+  // https://chromium-review.googlesource.com/c/chromium/b+/1385107 .
 }
 
 TEST_F(DistributionReporterTest, DistributionReporterNeedsUmaName) {
--- a/media/learning/impl/extra_trees_trainer.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/extra_trees_trainer.cc	2019-05-17 18:53:34.288000000 +0300
@@ -34,16 +34,11 @@
   if (!tree_trainer_)
     tree_trainer_ = std::make_unique<RandomTreeTrainer>(rng());
 
-  // We've modified RandomTree to handle nominals, so we don't need to do one-
-  // hot conversion normally.  It's slow.  However, the changes to RandomTree
-  // are only approximately the same thing.
-  if (task_.use_one_hot_conversion) {
+  // RandomTree requires one-hot vectors to properly choose split points the way
+  // that ExtraTrees require.
+  // TODO(liberato): Modify it not to need this.  It's slow.
     converter_ = std::make_unique<OneHotConverter>(task, training_data);
     converted_training_data_ = converter_->Convert(training_data);
-    task_ = converter_->converted_task();
-  } else {
-    converted_training_data_ = training_data;
-  }
 
   // Start training.  Send in nullptr to start the process.
   OnRandomTreeModel(std::move(model_cb), nullptr);
@@ -57,22 +52,17 @@
 
   // If this is the last tree, then return the finished model.
   if (trees_.size() == task_.rf_number_of_trees) {
-    std::unique_ptr<Model> model =
-        std::make_unique<VotingEnsemble>(std::move(trees_));
-    // If we have a converter, then wrap everything in a ConvertingModel.
-    if (converter_) {
-      model = std::make_unique<ConvertingModel>(std::move(converter_),
-                                                std::move(model));
-    }
-
-    std::move(model_cb).Run(std::move(model));
+    std::move(model_cb).Run(std::make_unique<ConvertingModel>(
+        std::move(converter_),
+        std::make_unique<VotingEnsemble>(std::move(trees_))));
     return;
   }
 
   // Train the next tree.
   auto cb = base::BindOnce(&ExtraTreesTrainer::OnRandomTreeModel, AsWeakPtr(),
                            std::move(model_cb));
-  tree_trainer_->Train(task_, converted_training_data_, std::move(cb));
+  tree_trainer_->Train(converter_->converted_task(), converted_training_data_,
+                       std::move(cb));
 }
 
 }  // namespace learning
--- a/media/learning/impl/extra_trees_trainer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/extra_trees_trainer_unittest.cc	2019-05-17 18:53:34.288000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/learning/impl/extra_trees_trainer.h"
 
-#include "base/bind.h"
 #include "base/memory/ref_counted.h"
 #include "base/test/scoped_task_environment.h"
 #include "media/learning/impl/fisher_iris_dataset.h"
@@ -203,7 +202,7 @@
   }
 }
 
-INSTANTIATE_TEST_SUITE_P(ExtraTreesTest,
+INSTANTIATE_TEST_CASE_P(ExtraTreesTest,
                          ExtraTreesTest,
                          testing::ValuesIn({LearningTask::Ordering::kUnordered,
                                             LearningTask::Ordering::kNumeric}));
--- a/media/learning/impl/learning_session_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/learning_session_impl.cc	2019-05-17 18:53:34.288000000 +0300
@@ -4,8 +4,6 @@
 
 #include "media/learning/impl/learning_session_impl.h"
 
-#include <utility>
-
 #include "base/bind.h"
 #include "base/logging.h"
 #include "media/learning/impl/distribution_reporter.h"
@@ -16,12 +14,10 @@
 
 LearningSessionImpl::LearningSessionImpl()
     : controller_factory_(
-          base::BindRepeating([](const LearningTask& task,
-                                 SequenceBoundFeatureProvider feature_provider)
+          base::BindRepeating([](const LearningTask& task)
                                   -> std::unique_ptr<LearningTaskController> {
             return std::make_unique<LearningTaskControllerImpl>(
-                task, DistributionReporter::Create(task),
-                std::move(feature_provider));
+                task, DistributionReporter::Create(task));
           })) {}
 
 LearningSessionImpl::~LearningSessionImpl() = default;
@@ -34,22 +30,13 @@
 void LearningSessionImpl::AddExample(const std::string& task_name,
                                      const LabelledExample& example) {
   auto iter = task_map_.find(task_name);
-  if (iter != task_map_.end()) {
-    // TODO(liberato): We shouldn't be adding examples.  We should provide the
-    // LearningTaskController instead, although ownership gets a bit weird.
-    LearningTaskController::ObservationId id = 1;
-    iter->second->BeginObservation(id, example.features);
-    iter->second->CompleteObservation(
-        id, ObservationCompletion(example.target_value, example.weight));
-  }
+  if (iter != task_map_.end())
+    iter->second->AddExample(example);
 }
 
-void LearningSessionImpl::RegisterTask(
-    const LearningTask& task,
-    SequenceBoundFeatureProvider feature_provider) {
+void LearningSessionImpl::RegisterTask(const LearningTask& task) {
   DCHECK(task_map_.count(task.name) == 0);
-  task_map_.emplace(task.name,
-                    controller_factory_.Run(task, std::move(feature_provider)));
+  task_map_.emplace(task.name, controller_factory_.Run(task));
 }
 
 }  // namespace learning
--- a/media/learning/impl/learning_session_impl.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/learning_session_impl.h	2019-05-17 18:53:34.288000000 +0300
@@ -8,10 +8,8 @@
 #include <map>
 
 #include "base/component_export.h"
-#include "base/threading/sequence_bound.h"
 #include "media/learning/common/learning_session.h"
-#include "media/learning/common/learning_task_controller.h"
-#include "media/learning/impl/feature_provider.h"
+#include "media/learning/impl/learning_task_controller.h"
 
 namespace media {
 namespace learning {
@@ -21,13 +19,12 @@
 class COMPONENT_EXPORT(LEARNING_IMPL) LearningSessionImpl
     : public LearningSession {
  public:
-  LearningSessionImpl();
+  explicit LearningSessionImpl();
   ~LearningSessionImpl() override;
 
   using CreateTaskControllerCB =
       base::RepeatingCallback<std::unique_ptr<LearningTaskController>(
-          const LearningTask&,
-          SequenceBoundFeatureProvider)>;
+          const LearningTask&)>;
 
   void SetTaskControllerFactoryCBForTesting(CreateTaskControllerCB cb);
 
@@ -37,9 +34,7 @@
 
   // Registers |task|, so that calls to AddExample with |task.name| will work.
   // This will create a new controller for the task.
-  void RegisterTask(const LearningTask& task,
-                    SequenceBoundFeatureProvider feature_provider =
-                        SequenceBoundFeatureProvider());
+  void RegisterTask(const LearningTask& task);
 
  private:
   // [task_name] = task controller.
--- a/media/learning/impl/learning_session_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/learning_session_impl_unittest.cc	2019-05-17 18:53:34.288000000 +0300
@@ -3,14 +3,11 @@
 // found in the LICENSE file.
 
 #include <memory>
-#include <utility>
 #include <vector>
 
 #include "base/bind.h"
-#include "base/test/scoped_task_environment.h"
-#include "base/threading/sequenced_task_runner_handle.h"
-#include "media/learning/common/learning_task_controller.h"
 #include "media/learning/impl/learning_session_impl.h"
+#include "media/learning/impl/learning_task_controller.h"
 #include "testing/gtest/include/gtest/gtest.h"
 
 namespace media {
@@ -20,63 +17,23 @@
  public:
   class FakeLearningTaskController : public LearningTaskController {
    public:
-    FakeLearningTaskController(const LearningTask& task,
-                               SequenceBoundFeatureProvider feature_provider)
-        : feature_provider_(std::move(feature_provider)) {
-      // As a complete hack, call the only public method on fp so that
-      // we can verify that it was given to us by the session.
-      if (!feature_provider_.is_null()) {
-        feature_provider_.Post(FROM_HERE, &FeatureProvider::AddFeatures,
-                               FeatureVector(),
-                               FeatureProvider::FeatureVectorCB());
-      }
-    }
-
-    void BeginObservation(ObservationId id,
-                          const FeatureVector& features) override {
-      id_ = id;
-      features_ = features;
-    }
+    FakeLearningTaskController(const LearningTask& task) {}
 
-    void CompleteObservation(ObservationId id,
-                             const ObservationCompletion& completion) override {
-      EXPECT_EQ(id_, id);
-      example_.features = std::move(features_);
-      example_.target_value = completion.target_value;
-      example_.weight = completion.weight;
+    void AddExample(const LabelledExample& example) override {
+      example_ = example;
     }
 
-    void CancelObservation(ObservationId id) override { ASSERT_TRUE(false); }
-
-    SequenceBoundFeatureProvider feature_provider_;
-    ObservationId id_ = 0;
-    FeatureVector features_;
     LabelledExample example_;
   };
 
-  class FakeFeatureProvider : public FeatureProvider {
-   public:
-    FakeFeatureProvider(bool* flag_ptr) : flag_ptr_(flag_ptr) {}
-
-    // Do nothing, except note that we were called.
-    void AddFeatures(FeatureVector features,
-                     FeatureProvider::FeatureVectorCB cb) override {
-      *flag_ptr_ = true;
-    }
-
-    bool* flag_ptr_ = nullptr;
-  };
-
   using ControllerVector = std::vector<FakeLearningTaskController*>;
 
   LearningSessionImplTest() {
     session_ = std::make_unique<LearningSessionImpl>();
     session_->SetTaskControllerFactoryCBForTesting(base::BindRepeating(
-        [](ControllerVector* controllers, const LearningTask& task,
-           SequenceBoundFeatureProvider feature_provider)
+        [](ControllerVector* controllers, const LearningTask& task)
             -> std::unique_ptr<LearningTaskController> {
-          auto controller = std::make_unique<FakeLearningTaskController>(
-              task, std::move(feature_provider));
+          auto controller = std::make_unique<FakeLearningTaskController>(task);
           controllers->push_back(controller.get());
           return controller;
         },
@@ -86,8 +43,6 @@
     task_1_.name = "task_1";
   }
 
-  base::test::ScopedTaskEnvironment scoped_task_environment_;
-
   std::unique_ptr<LearningSessionImpl> session_;
 
   LearningTask task_0_;
@@ -119,17 +74,5 @@
   EXPECT_EQ(task_controllers_[1]->example_, example_1);
 }
 
-TEST_F(LearningSessionImplTest, FeatureProviderIsForwarded) {
-  // Verify that a FeatureProvider actually gets forwarded to the LTC.
-  bool flag = false;
-  session_->RegisterTask(task_0_,
-                         base::SequenceBound<FakeFeatureProvider>(
-                             base::SequencedTaskRunnerHandle::Get(), &flag));
-  scoped_task_environment_.RunUntilIdle();
-  // Registering the task should create a FakeLearningTaskController, which will
-  // call AddFeatures on the fake FeatureProvider.
-  EXPECT_TRUE(flag);
-}
-
 }  // namespace learning
 }  // namespace media
--- a/media/learning/impl/learning_task_controller_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/learning_task_controller_impl.cc	2019-05-17 18:53:34.288000000 +0300
@@ -5,56 +5,30 @@
 #include "media/learning/impl/learning_task_controller_impl.h"
 
 #include <memory>
-#include <utility>
 
 #include "base/bind.h"
 #include "media/learning/impl/extra_trees_trainer.h"
-#include "media/learning/impl/lookup_table_trainer.h"
+#include "media/learning/impl/random_tree_trainer.h"
 
 namespace media {
 namespace learning {
 
 LearningTaskControllerImpl::LearningTaskControllerImpl(
     const LearningTask& task,
-    std::unique_ptr<DistributionReporter> reporter,
-    SequenceBoundFeatureProvider feature_provider)
+    std::unique_ptr<DistributionReporter> reporter)
     : task_(task),
       training_data_(std::make_unique<TrainingData>()),
-      reporter_(std::move(reporter)),
-      helper_(std::make_unique<LearningTaskControllerHelper>(
-          task,
-          base::BindRepeating(&LearningTaskControllerImpl::AddFinishedExample,
-                              AsWeakPtr()),
-          std::move(feature_provider))) {
+      reporter_(std::move(reporter)) {
   switch (task_.model) {
     case LearningTask::Model::kExtraTrees:
       trainer_ = std::make_unique<ExtraTreesTrainer>();
       break;
-    case LearningTask::Model::kLookupTable:
-      trainer_ = std::make_unique<LookupTableTrainer>();
-      break;
   }
 }
 
 LearningTaskControllerImpl::~LearningTaskControllerImpl() = default;
 
-void LearningTaskControllerImpl::BeginObservation(
-    ObservationId id,
-    const FeatureVector& features) {
-  helper_->BeginObservation(id, features);
-}
-
-void LearningTaskControllerImpl::CompleteObservation(
-    ObservationId id,
-    const ObservationCompletion& completion) {
-  helper_->CompleteObservation(id, completion);
-}
-
-void LearningTaskControllerImpl::CancelObservation(ObservationId id) {
-  helper_->CancelObservation(id);
-}
-
-void LearningTaskControllerImpl::AddFinishedExample(LabelledExample example) {
+void LearningTaskControllerImpl::AddExample(const LabelledExample& example) {
   if (training_data_->size() >= task_.max_data_set_size) {
     // Replace a random example.  We don't necessarily want to replace the
     // oldest, since we don't necessarily want to enforce an ad-hoc recency
@@ -88,6 +62,7 @@
 
   num_untrained_examples_ = 0;
 
+  // TODO(liberato): don't do this if one is in-flight.
   TrainedModelCB model_cb =
       base::BindOnce(&LearningTaskControllerImpl::OnModelTrained, AsWeakPtr());
   training_is_in_progress_ = true;
--- a/media/learning/impl/learning_task_controller_impl.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/learning_task_controller_impl.h	2019-05-17 18:53:34.288000000 +0300
@@ -10,10 +10,8 @@
 #include "base/callback.h"
 #include "base/component_export.h"
 #include "base/memory/weak_ptr.h"
-#include "media/learning/common/learning_task_controller.h"
 #include "media/learning/impl/distribution_reporter.h"
-#include "media/learning/impl/feature_provider.h"
-#include "media/learning/impl/learning_task_controller_helper.h"
+#include "media/learning/impl/learning_task_controller.h"
 #include "media/learning/impl/random_number_generator.h"
 #include "media/learning/impl/training_algorithm.h"
 
@@ -22,15 +20,6 @@
 
 class LearningTaskControllerImplTest;
 
-// Controller for a single learning task.  Takes training examples, and forwards
-// them to the learner(s).  Responsible for things like:
-//  - Managing underlying learner(s) based on the learning task
-//  - Feature subset selection
-//  - UMA reporting on accuracy / feature importance
-//
-// The idea is that one can create a LearningTask, give it to an LTCI, and the
-// LTCI will do the work of building / evaluating the model based on training
-// examples that are provided to it.
 class COMPONENT_EXPORT(LEARNING_IMPL) LearningTaskControllerImpl
     : public LearningTaskController,
       public HasRandomNumberGenerator,
@@ -38,22 +27,13 @@
  public:
   LearningTaskControllerImpl(
       const LearningTask& task,
-      std::unique_ptr<DistributionReporter> reporter = nullptr,
-      SequenceBoundFeatureProvider feature_provider =
-          SequenceBoundFeatureProvider());
+      std::unique_ptr<DistributionReporter> reporter = nullptr);
   ~LearningTaskControllerImpl() override;
 
   // LearningTaskController
-  void BeginObservation(ObservationId id,
-                        const FeatureVector& features) override;
-  void CompleteObservation(ObservationId id,
-                           const ObservationCompletion& completion) override;
-  void CancelObservation(ObservationId id) override;
+  void AddExample(const LabelledExample& example) override;
 
  private:
-  // Add |example| to the training data, and process it.
-  void AddFinishedExample(LabelledExample example);
-
   // Called by |training_cb_| when the model is trained.
   void OnModelTrained(std::unique_ptr<Model> model);
 
@@ -80,9 +60,6 @@
   // Optional reporter for training accuracy.
   std::unique_ptr<DistributionReporter> reporter_;
 
-  // Helper that we use to handle deferred examples.
-  std::unique_ptr<LearningTaskControllerHelper> helper_;
-
   friend class LearningTaskControllerImplTest;
 };
 
--- a/media/learning/impl/learning_task_controller_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/learning_task_controller_impl_unittest.cc	2019-05-17 18:53:34.288000000 +0300
@@ -4,11 +4,7 @@
 
 #include "media/learning/impl/learning_task_controller_impl.h"
 
-#include <utility>
-
 #include "base/bind.h"
-#include "base/test/scoped_task_environment.h"
-#include "base/threading/sequenced_task_runner_handle.h"
 #include "media/learning/impl/distribution_reporter.h"
 #include "testing/gtest/include/gtest/gtest.h"
 
@@ -65,80 +61,44 @@
                const TrainingData& training_data,
                TrainedModelCB model_cb) override {
       (*num_models_)++;
-      training_data_ = training_data;
       std::move(model_cb).Run(std::make_unique<FakeModel>(target_value_));
     }
 
-    const TrainingData& training_data() const { return training_data_; }
-
    private:
     int* num_models_ = nullptr;
     TargetValue target_value_;
-
-    // Most recently provided training data.
-    TrainingData training_data_;
-  };
-
-  // Increments feature 0.
-  class FakeFeatureProvider : public FeatureProvider {
-   public:
-    void AddFeatures(FeatureVector features, FeatureVectorCB cb) override {
-      features[0] = FeatureValue(features[0].value() + 1);
-      std::move(cb).Run(features);
-    }
   };
 
   LearningTaskControllerImplTest()
       : predicted_target_(123), not_predicted_target_(456) {
-    // Set the name so that we can check it later.
-    task_.name = "TestTask";
     // Don't require too many training examples per report.
     task_.max_data_set_size = 20;
     task_.min_new_data_fraction = 0.1;
-  }
 
-  void CreateController(SequenceBoundFeatureProvider feature_provider =
-                            SequenceBoundFeatureProvider()) {
     std::unique_ptr<FakeDistributionReporter> reporter =
         std::make_unique<FakeDistributionReporter>(task_);
     reporter_raw_ = reporter.get();
 
     controller_ = std::make_unique<LearningTaskControllerImpl>(
-        task_, std::move(reporter), std::move(feature_provider));
-
-    auto fake_trainer =
-        std::make_unique<FakeTrainer>(&num_models_, predicted_target_);
-    trainer_raw_ = fake_trainer.get();
-    controller_->SetTrainerForTesting(std::move(fake_trainer));
-  }
-
-  void AddExample(const LabelledExample& example) {
-    LearningTaskController::ObservationId id = 1;
-    controller_->BeginObservation(id, example.features);
-    controller_->CompleteObservation(
-        id, ObservationCompletion(example.target_value, example.weight));
+        task_, std::move(reporter));
+    controller_->SetTrainerForTesting(
+        std::make_unique<FakeTrainer>(&num_models_, predicted_target_));
   }
 
-  base::test::ScopedTaskEnvironment scoped_task_environment_;
-
   // Number of models that we trained.
   int num_models_ = 0;
-  FakeModel* last_model_ = nullptr;
 
   // Two distinct targets.
   const TargetValue predicted_target_;
   const TargetValue not_predicted_target_;
 
   FakeDistributionReporter* reporter_raw_ = nullptr;
-  FakeTrainer* trainer_raw_ = nullptr;
 
   LearningTask task_;
   std::unique_ptr<LearningTaskControllerImpl> controller_;
 };
 
 TEST_F(LearningTaskControllerImplTest, AddingExamplesTrainsModelAndReports) {
-  CreateController();
-
   LabelledExample example;
 
   // Up to the first 1/training_fraction examples should train on each example.
@@ -146,7 +106,7 @@
   example.target_value = predicted_target_;
   int count = static_cast<int>(1.0 / task_.min_new_data_fraction);
   for (int i = 0; i < count; i++) {
-    AddExample(example);
+    controller_->AddExample(example);
     EXPECT_EQ(num_models_, i + 1);
     // All examples except the first should be reported as correct.  For the
     // first, there's no model to test again.
@@ -155,14 +115,14 @@
   }
   // The next |count| should train every other one.
   for (int i = 0; i < count; i++) {
-    AddExample(example);
+    controller_->AddExample(example);
     EXPECT_EQ(num_models_, count + (i + 1) / 2);
   }
 
   // The next |count| should be the same, since we've reached the max training
   // set size.
   for (int i = 0; i < count; i++) {
-    AddExample(example);
+    controller_->AddExample(example);
     EXPECT_EQ(num_models_, count + count / 2 + (i + 1) / 2);
   }
 
@@ -174,26 +134,10 @@
   // Adding a value that doesn't match should report one more attempt, with an
   // incorrect prediction.
   example.target_value = not_predicted_target_;
-  AddExample(example);
+  controller_->AddExample(example);
   EXPECT_EQ(reporter_raw_->num_reported_, count * 3);
   EXPECT_EQ(reporter_raw_->num_correct_, count * 3 - 1);  // Unchanged.
 }
 
-TEST_F(LearningTaskControllerImplTest, FeatureProviderIsUsed) {
-  // If a FeatureProvider factory is provided, make sure that it's used to
-  // adjust new examples.
-  SequenceBoundFeatureProvider feature_provider =
-      base::SequenceBound<FakeFeatureProvider>(
-          base::SequencedTaskRunnerHandle::Get());
-  CreateController(std::move(feature_provider));
-  LabelledExample example;
-  example.features.push_back(FeatureValue(123));
-  example.weight = 321u;
-  AddExample(example);
-  scoped_task_environment_.RunUntilIdle();
-  EXPECT_EQ(trainer_raw_->training_data()[0].features[0], FeatureValue(124));
-  EXPECT_EQ(trainer_raw_->training_data()[0].weight, example.weight);
-}
-
 }  // namespace learning
 }  // namespace media
--- a/media/learning/impl/random_tree_trainer.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/random_tree_trainer.cc	2019-05-17 18:53:34.292000000 +0300
@@ -36,6 +36,7 @@
                int split_index,
                FeatureValue split_point)
       : split_index_(split_index),
+        rt_unknown_value_handling_(task.rt_unknown_value_handling),
         ordering_(task.feature_descriptions[split_index].ordering),
         split_point_(split_point) {}
 
@@ -46,8 +47,8 @@
     FeatureValue f;
     switch (ordering_) {
       case LearningTask::Ordering::kUnordered:
-        // Use 0 for "!=" and 1 for "==".
-        f = FeatureValue(features[split_index_] == split_point_);
+        // Use the nominal value directly.
+        f = features[split_index_];
         break;
       case LearningTask::Ordering::kNumeric:
         // Use 0 for "<=" and 1 for ">".
@@ -57,9 +58,18 @@
 
     auto iter = children_.find(f);
 
-    // If we've never seen this feature value, then return nothing.
-    if (iter == children_.end())
+    // If we've never seen this feature value, then average all our branches.
+    // This is an attempt to mimic one-hot encoding, where we'll take the zero
+    // branch but it depends on the tree structure which of the one-hot values
+    // we're choosing.
+    if (iter == children_.end()) {
+      switch (rt_unknown_value_handling_) {
+        case LearningTask::RTUnknownValueHandling::kEmptyDistribution:
       return TargetDistribution();
+        case LearningTask::RTUnknownValueHandling::kUseAllSplits:
+          return PredictDistributionWithMissingValues(features);
+      }
+    }
 
     return iter->second->PredictDistribution(features);
   }
@@ -88,6 +98,9 @@
   int split_index_ = -1;
   base::flat_map<FeatureValue, std::unique_ptr<Model>> children_;
 
+  // How we handle unknown values.
+  LearningTask::RTUnknownValueHandling rt_unknown_value_handling_;
+
   // How is our feature value ordered?
   LearningTask::Ordering ordering_;
 
@@ -214,11 +227,6 @@
   }
 
   // Select the feature subset to consider at this leaf.
-  // TODO(liberato): For nominals, with one-hot encoding, we'd give an equal
-  // chance to each feature's value.  For example, if F1 has {A, B} and F2 has
-  // {C,D,E,F}, then we would pick uniformly over {A,B,C,D,E,F}.  However, now
-  // we pick between {F1, F2} then pick between either {A,B} or {C,D,E,F}.  We
-  // do this because it's simpler and doesn't seem to hurt anything.
   FeatureSet feature_candidates = new_unused_set;
   // TODO(liberato): Let our caller override this.
   const size_t features_per_split =
@@ -259,6 +267,17 @@
   std::unique_ptr<InteriorNode> node = std::make_unique<InteriorNode>(
       task, best_potential_split.split_index, best_potential_split.split_point);
 
+  // Don't let the subtree use this feature if this is nominal split, since
+  // there's nothing left to split.  For numeric splits, we might want to split
+  // it further.  Note that if there is only one branch for this split, then
+  // we returned a leaf anyway.
+  if (task.feature_descriptions[best_potential_split.split_index].ordering ==
+      LearningTask::Ordering::kUnordered) {
+    DCHECK(new_unused_set.find(best_potential_split.split_index) !=
+           new_unused_set.end());
+    new_unused_set.erase(best_potential_split.split_index);
+  }
+
   for (auto& branch_iter : best_potential_split.branch_infos) {
     node->AddChild(branch_iter.first,
                    Build(task, training_data, branch_iter.second.training_idx,
@@ -278,21 +297,18 @@
   DCHECK_GT(training_idx.size(), 0u);
 
   Split split(split_index);
-
-  bool is_numeric = task.feature_descriptions[split_index].ordering ==
-                    LearningTask::Ordering::kNumeric;
+  base::Optional<FeatureValue> split_point;
 
   // TODO(liberato): Consider removing nominal feature support and RF.  That
   // would make this code somewhat simpler.
 
   // For a numeric split, find the split point.  Otherwise, we'll split on every
   // nominal value that this feature has in |training_data|.
-  if (is_numeric) {
-    split.split_point =
-        FindSplitPoint_Numeric(split.split_index, training_data, training_idx);
-  } else {
-    split.split_point =
-        FindSplitPoint_Nominal(split.split_index, training_data, training_idx);
+  if (task.feature_descriptions[split_index].ordering ==
+      LearningTask::Ordering::kNumeric) {
+    split_point =
+        FindNumericSplitPoint(split.split_index, training_data, training_idx);
+    split.split_point = *split_point;
   }
 
   // Find the split's feature values and construct the training set for each.
@@ -307,14 +323,13 @@
     FeatureValue v_i = example.features[split.split_index];
 
     // Figure out what value this example would use for splitting.  For nominal,
-    // it's 1 or 0, based on whether |v_i| is equal to the split or not.  For
-    // numeric, it's whether |v_i| is <= the split point or not (0 for <=, and 1
-    // for >).
+    // it's just |v_i|.  For numeric, it's whether |v_i| is <= the split point
+    // or not (0 for <=, 1 for >).
     FeatureValue split_feature;
-    if (is_numeric)
-      split_feature = FeatureValue(v_i > split.split_point);
+    if (split_point)
+      split_feature = FeatureValue(v_i > *split_point);
     else
-      split_feature = FeatureValue(v_i == split.split_point);
+      split_feature = v_i;
 
     // Add |v_i| to the right training set.  Remember that emplace will do
     // nothing if the key already exists.
@@ -330,17 +345,17 @@
   // Figure out how good / bad this split is.
   switch (task.target_description.ordering) {
     case LearningTask::Ordering::kUnordered:
-      ComputeSplitScore_Nominal(&split, total_weight);
+      ComputeNominalSplitScore(&split, total_weight);
       break;
     case LearningTask::Ordering::kNumeric:
-      ComputeSplitScore_Numeric(&split, total_weight);
+      ComputeNumericSplitScore(&split, total_weight);
       break;
   }
 
   return split;
 }
 
-void RandomTreeTrainer::ComputeSplitScore_Nominal(Split* split,
+void RandomTreeTrainer::ComputeNominalSplitScore(Split* split,
                                                   double total_weight) {
   // Compute the nats given that we're at this node.
   split->nats_remaining = 0;
@@ -359,7 +374,7 @@
   }
 }
 
-void RandomTreeTrainer::ComputeSplitScore_Numeric(Split* split,
+void RandomTreeTrainer::ComputeNumericSplitScore(Split* split,
                                                   double total_weight) {
   // Compute the nats given that we're at this node.
   split->nats_remaining = 0;
@@ -387,7 +402,7 @@
   }
 }
 
-FeatureValue RandomTreeTrainer::FindSplitPoint_Numeric(
+FeatureValue RandomTreeTrainer::FindNumericSplitPoint(
     size_t split_index,
     const TrainingData& training_data,
     const std::vector<size_t>& training_idx) {
@@ -429,42 +444,5 @@
   return v_split;
 }
 
-FeatureValue RandomTreeTrainer::FindSplitPoint_Nominal(
-    size_t split_index,
-    const TrainingData& training_data,
-    const std::vector<size_t>& training_idx) {
-  // We should not be given a training set of size 0, since there's no need to
-  // check an empty split.
-  DCHECK_GT(training_idx.size(), 0u);
-
-  // Construct a set of all values for |training_idx|.  We don't care about
-  // their relative frequency, since one-hot encoding doesn't.
-  // For example, if a feature has 10 "yes" instances and 1 "no" instance, then
-  // there's a 50% chance for each to be chosen here.  This is because one-hot
-  // encoding would do roughly the same thing: when choosing features, the
-  // "is_yes" and "is_no" features that come out of one-hot encoding would be
-  // equally likely to be chosen.
-  //
-  // Important but subtle note: we can't choose a value that's been chosen
-  // before for this feature, since that would be like splitting on the same
-  // one-hot feature more than once.  Luckily, we won't be asked to do that.  If
-  // we choose "Yes" at some level in the tree, then the "==" branch will have
-  // trivial features which will be removed from consideration early (we never
-  // consider features with only one value), and the != branch won't have any
-  // "Yes" values for us to pick at a lower level.
-  std::set<FeatureValue> values;
-  for (size_t idx : training_idx) {
-    const LabelledExample& example = training_data[idx];
-    values.insert(example.features[split_index]);
-  }
-
-  // Select one uniformly at random.
-  size_t which = rng()->Generate(values.size());
-  auto it = values.begin();
-  for (; which > 0; it++, which--)
-    ;
-  return *it;
-}
-
 }  // namespace learning
 }  // namespace media
--- a/media/learning/impl/random_tree_trainer.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/random_tree_trainer.h	2019-05-17 18:53:34.292000000 +0300
@@ -160,18 +160,13 @@
 
   // Fill in |nats_remaining| for |split| for a nominal target.  |total_weight|
   // is the total weight of all instances coming into this split.
-  void ComputeSplitScore_Nominal(Split* split, double total_weight);
+  void ComputeNominalSplitScore(Split* split, double total_weight);
 
   // Fill in |nats_remaining| for |split| for a numeric target.
-  void ComputeSplitScore_Numeric(Split* split, double total_weight);
-
-  // Compute the split point for |training_data| for a nominal feature.
-  FeatureValue FindSplitPoint_Nominal(size_t index,
-                                      const TrainingData& training_data,
-                                      const std::vector<size_t>& training_idx);
+  void ComputeNumericSplitScore(Split* split, double total_weight);
 
   // Compute the split point for |training_data| for a numeric feature.
-  FeatureValue FindSplitPoint_Numeric(size_t index,
+  FeatureValue FindNumericSplitPoint(size_t index,
                                       const TrainingData& training_data,
                                       const std::vector<size_t>& training_idx);
 
--- a/media/learning/impl/random_tree_trainer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/impl/random_tree_trainer_unittest.cc	2019-05-17 18:53:34.292000000 +0300
@@ -169,17 +169,30 @@
   training_data.push_back(example_1);
   training_data.push_back(example_2);
 
-  auto model = Train(task_, training_data);
-  auto distribution =
+  task_.rt_unknown_value_handling =
+      LearningTask::RTUnknownValueHandling::kEmptyDistribution;
+  std::unique_ptr<Model> model = Train(task_, training_data);
+  TargetDistribution distribution =
       model->PredictDistribution(FeatureVector({FeatureValue(789)}));
   if (ordering_ == LearningTask::Ordering::kUnordered) {
-    // OOV data could be split on either feature first, so we don't really know
-    // which to expect.  We assert that there should be exactly one example, but
-    // whether it's |example_1| or |example_2| isn't clear.
+    // OOV data should return an empty distribution (nominal).
+    EXPECT_EQ(distribution.size(), 0u);
+  } else {
+    // OOV data should end up in the |example_2| bucket, since the feature is
+    // numerically higher.
     EXPECT_EQ(distribution.size(), 1u);
-    EXPECT_EQ(distribution[example_1.target_value] +
-                  distribution[example_2.target_value],
-              1u);
+    EXPECT_EQ(distribution[example_2.target_value], 1u);
+  }
+
+  task_.rt_unknown_value_handling =
+      LearningTask::RTUnknownValueHandling::kUseAllSplits;
+  model = Train(task_, training_data);
+  distribution = model->PredictDistribution(FeatureVector({FeatureValue(789)}));
+  if (ordering_ == LearningTask::Ordering::kUnordered) {
+    // OOV data should return with the sum of all splits.
+    EXPECT_EQ(distribution.size(), 2u);
+    EXPECT_EQ(distribution[example_1.target_value], 1u);
+    EXPECT_EQ(distribution[example_2.target_value], 1u);
   } else {
     // The unknown feature is numerically higher than |example_2|, so we
     // expect it to fall into that bucket.
@@ -199,6 +212,8 @@
     training_data.push_back(example);
   }
 
+  task_.rt_unknown_value_handling =
+      LearningTask::RTUnknownValueHandling::kEmptyDistribution;
   std::unique_ptr<Model> model = Train(task_, training_data);
   for (size_t i = 0; i < 4; i++) {
     // Get a prediction for the |i|-th feature value.
@@ -212,7 +227,7 @@
   }
 }
 
-INSTANTIATE_TEST_SUITE_P(RandomTreeTest,
+INSTANTIATE_TEST_CASE_P(RandomTreeTest,
                          RandomTreeTest,
                          testing::ValuesIn({LearningTask::Ordering::kUnordered,
                                             LearningTask::Ordering::kNumeric}));
--- a/media/learning/mojo/mojo_learning_session_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/learning/mojo/mojo_learning_session_impl_unittest.cc	2019-05-17 18:53:34.292000000 +0300
@@ -8,7 +8,6 @@
 #include "base/memory/ptr_util.h"
 #include "media/learning/common/learning_session.h"
 #include "media/learning/mojo/mojo_learning_session_impl.h"
-#include "media/learning/mojo/public/mojom/learning_types.mojom.h"
 #include "testing/gtest/include/gtest/gtest.h"
 
 namespace media {
--- a/media/media_options.gni	2019-05-01 01:22:52.000000000 +0300
+++ b/media/media_options.gni	2019-05-17 18:53:34.292000000 +0300
@@ -5,10 +5,8 @@
 import("//build/config/chrome_build.gni")
 import("//build/config/chromecast_build.gni")
 import("//build/config/features.gni")
-import("//build/config/jumbo.gni")
 import("//media/gpu/args.gni")
 import("//testing/libfuzzer/fuzzer_test.gni")
-import("//third_party/libaom/options.gni")
 
 # Do not expand this list without double-checking with OWNERS, this is a list of
 # all targets which roll up into the //media component. It controls visibility
@@ -79,36 +77,17 @@
   # Enable HLS with SAMPLE-AES decryption.
   enable_hls_sample_aes = proprietary_codecs && is_chromecast
 
-  # Enable logging override, e.g. enable DVLOGs at build time. Must not be
-  # enabled when |use_jumbo_build| is true, in which case multiple .cc files
-  # are combined and we could override more logging than expected.
-  enable_logging_override = !use_jumbo_build && is_chromecast
+  # Enable logging override, e.g. enable DVLOGs at build time.
+  enable_logging_override = is_chromecast
 
   # If true, use cast CMA backend instead of default chromium media pipeline.
   # TODO(sanfin): Remove this flag when all builds enable CMA.
   is_cast_using_cma_backend = true
-
-  if (is_win && target_cpu == "arm64") {
-    # TODO: Enable dav1d for Windows ARM64. https://crbug.com/941022
-    enable_dav1d_decoder = false
-  } else if (is_fuchsia && target_cpu == "x64") {
-    # TODO: Fix media_unittests for fuchsia X64. https://crbug.com/930300
-    enable_dav1d_decoder = false
-  } else {
-    enable_dav1d_decoder = !is_android && !is_ios
-  }
-}
-
-declare_args() {
-  enable_av1_decoder = enable_dav1d_decoder || enable_libaom_decoder
 }
 
 # enable_hls_sample_aes can only be true if enable_mse_mpeg2ts_stream_parser is.
 assert(enable_mse_mpeg2ts_stream_parser || !enable_hls_sample_aes)
 
-# Logging override must not be enabled in jumbo builds.
-assert(!use_jumbo_build || !enable_logging_override)
-
 if (media_use_ffmpeg) {
   media_subcomponent_deps += [ "//media/ffmpeg" ]
 }
--- a/media/midi/midi_manager_alsa.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/midi/midi_manager_alsa.h	2019-05-17 18:53:34.296000000 +0300
@@ -212,7 +212,7 @@
    public:
     iterator erase(iterator position) {
       return MidiPortStateBase::erase(position);
-    }
+    };
     void push_back(std::unique_ptr<MidiPort> port) {
       MidiPortStateBase::push_back(std::move(port));
     }
@@ -359,7 +359,7 @@
   struct SndMidiEventDeleter {
     void operator()(snd_midi_event_t* coder) const {
       snd_midi_event_free(coder);
-    }
+    };
   };
 
   using SourceMap = std::unordered_map<int, uint32_t>;
--- a/media/midi/midi_manager_android.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/midi/midi_manager_android.cc	2019-05-17 18:53:34.296000000 +0300
@@ -5,7 +5,6 @@
 #include "media/midi/midi_manager_android.h"
 
 #include "base/android/build_info.h"
-#include "base/bind.h"
 #include "base/feature_list.h"
 #include "base/metrics/field_trial_params.h"
 #include "base/strings/stringprintf.h"
--- a/media/midi/midi_manager_mac.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/midi/midi_manager_mac.cc	2019-05-17 18:53:34.300000000 +0300
@@ -19,7 +19,7 @@
 #include "media/midi/midi_service.h"
 #include "media/midi/task_service.h"
 
-using base::NumberToString;
+using base::IntToString;
 using base::SysCFStringRefToUTF8;
 using midi::mojom::PortState;
 using midi::mojom::Result;
@@ -72,7 +72,7 @@
   result = MIDIObjectGetIntegerProperty(
       endpoint, kMIDIPropertyDriverVersion, &version_number);
   if (result == noErr) {
-    version = NumberToString(version_number);
+    version = IntToString(version_number);
   } else {
     // kMIDIPropertyDriverVersion is not supported in IAC driver providing
     // endpoints, and the result will be kMIDIUnknownProperty (-10835).
@@ -85,7 +85,7 @@
   result = MIDIObjectGetIntegerProperty(
       endpoint, kMIDIPropertyUniqueID, &id_number);
   if (result == noErr) {
-    id = NumberToString(id_number);
+    id = IntToString(id_number);
   } else {
     // On connecting some devices, e.g., nano KONTROL2, unknown endpoints
     // appear and disappear quickly and they fail on queries.
--- a/media/midi/midi_manager_win.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/midi/midi_manager_win.cc	2019-05-17 18:53:34.300000000 +0300
@@ -15,7 +15,6 @@
 #include <limits>
 #include <string>
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/callback.h"
 #include "base/logging.h"
--- a/media/midi/task_service.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/midi/task_service.h	2019-05-17 18:53:34.300000000 +0300
@@ -115,6 +115,6 @@
   DISALLOW_COPY_AND_ASSIGN(TaskService);
 };
 
-}  // namespace midi
+};  // namespace midi
 
 #endif  // MEDIA_MIDI_TASK_SERVICE_H_
--- a/media/mojo/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/BUILD.gn	2019-05-17 18:53:34.304000000 +0300
@@ -36,6 +36,9 @@
         assert(false, "Invalid mojo media service: $service")
       }
     }
+    assert(
+        enable_mojo_renderer || !enable_runtime_media_renderer_selection,
+        "The mojo renderer must be enabled to use runtime media renderer selection.")
 
     if (mojo_media_host == "browser") {
       enable_mojo_media_in_browser_process = true
--- a/media/mojo/clients/mojo_audio_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/clients/mojo_audio_decoder.cc	2019-05-17 18:53:34.304000000 +0300
@@ -88,16 +88,16 @@
   DCHECK(task_runner_->BelongsToCurrentThread());
 
   if (remote_decoder_.encountered_error()) {
-    task_runner_->PostTask(
-        FROM_HERE, base::BindOnce(decode_cb, DecodeStatus::DECODE_ERROR));
+    task_runner_->PostTask(FROM_HERE,
+                           base::Bind(decode_cb, DecodeStatus::DECODE_ERROR));
     return;
   }
 
   mojom::DecoderBufferPtr buffer =
       mojo_decoder_buffer_writer_->WriteDecoderBuffer(std::move(media_buffer));
   if (!buffer) {
-    task_runner_->PostTask(
-        FROM_HERE, base::BindOnce(decode_cb, DecodeStatus::DECODE_ERROR));
+    task_runner_->PostTask(FROM_HERE,
+                           base::Bind(decode_cb, DecodeStatus::DECODE_ERROR));
     return;
   }
 
--- a/media/mojo/clients/mojo_audio_decoder_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/clients/mojo_audio_decoder_unittest.cc	2019-05-17 18:53:34.304000000 +0300
@@ -60,7 +60,7 @@
     mojom::AudioDecoderPtr remote_audio_decoder;
     service_task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&MojoAudioDecoderTest::ConnectToService,
+        base::Bind(&MojoAudioDecoderTest::ConnectToService,
                        base::Unretained(this),
                        base::Passed(mojo::MakeRequest(&remote_audio_decoder))));
     mojo_audio_decoder_.reset(new MojoAudioDecoder(
--- a/media/mojo/clients/mojo_cdm.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/clients/mojo_cdm.cc	2019-05-17 18:53:34.304000000 +0300
@@ -364,7 +364,7 @@
       DCHECK(decryptor_task_runner_);
       decryptor_task_runner_->PostTask(
           FROM_HERE,
-          base::BindOnce(&MojoCdm::OnKeyAdded, weak_factory_.GetWeakPtr()));
+          base::Bind(&MojoCdm::OnKeyAdded, weak_factory_.GetWeakPtr()));
     }
   }
 
--- a/media/mojo/clients/mojo_renderer_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/clients/mojo_renderer_factory.cc	2019-05-17 18:53:34.308000000 +0300
@@ -4,10 +4,9 @@
 
 #include "media/mojo/clients/mojo_renderer_factory.h"
 
-#include <utility>
+#include <memory>
 
 #include "base/single_thread_task_runner.h"
-#include "build/build_config.h"
 #include "media/mojo/clients/mojo_renderer.h"
 #include "media/renderers/decrypting_renderer.h"
 #include "media/renderers/video_overlay_factory.h"
@@ -18,15 +17,22 @@
 namespace media {
 
 MojoRendererFactory::MojoRendererFactory(
+    mojom::HostedRendererType type,
     const GetGpuFactoriesCB& get_gpu_factories_cb,
     media::mojom::InterfaceFactory* interface_factory)
     : get_gpu_factories_cb_(get_gpu_factories_cb),
-      interface_factory_(interface_factory) {
+      interface_factory_(interface_factory),
+      hosted_renderer_type_(type) {
   DCHECK(interface_factory_);
 }
 
 MojoRendererFactory::~MojoRendererFactory() = default;
 
+void MojoRendererFactory::SetGetTypeSpecificIdCB(
+    const GetTypeSpecificIdCB& get_type_specific_id) {
+  get_type_specific_id_ = get_type_specific_id;
+}
+
 std::unique_ptr<Renderer> MojoRendererFactory::CreateRenderer(
     const scoped_refptr<base::SingleThreadTaskRunner>& media_task_runner,
     const scoped_refptr<base::TaskRunner>& /* worker_task_runner */,
@@ -34,46 +40,37 @@
     VideoRendererSink* video_renderer_sink,
     const RequestOverlayInfoCB& /* request_overlay_info_cb */,
     const gfx::ColorSpace& /* target_color_space */) {
-  DCHECK(interface_factory_);
+  std::unique_ptr<VideoOverlayFactory> overlay_factory;
 
-  DCHECK(get_gpu_factories_cb_);
-  auto overlay_factory =
+  // |get_gpu_factories_cb_| can be null in the HLS/MediaPlayerRenderer case,
+  // when we do not need to create video overlays.
+  if (get_gpu_factories_cb_) {
+    overlay_factory =
       std::make_unique<VideoOverlayFactory>(get_gpu_factories_cb_.Run());
+  }
 
-  mojom::RendererPtr renderer_ptr;
-  interface_factory_->CreateDefaultRenderer(std::string(),
-                                            mojo::MakeRequest(&renderer_ptr));
-
-  return std::make_unique<MojoRenderer>(
-      media_task_runner, std::move(overlay_factory), video_renderer_sink,
-      std::move(renderer_ptr));
+  // MediaPlayerRendererClientFactory depends on |this| always returning a MR,
+  // since it uses a static_cast to use some MojoRenderer specific interfaces.
+  // Therefore, |this| should never return anything else than a MojoRenderer.
+  return std::make_unique<MojoRenderer>(media_task_runner,
+                                        std::move(overlay_factory),
+                                        video_renderer_sink, GetRendererPtr());
 }
 
-#if defined(OS_ANDROID)
-std::unique_ptr<MojoRenderer> MojoRendererFactory::CreateFlingingRenderer(
-    const std::string& presentation_id,
-    const scoped_refptr<base::SingleThreadTaskRunner>& media_task_runner,
-    VideoRendererSink* video_renderer_sink) {
-  DCHECK(interface_factory_);
+mojom::RendererPtr MojoRendererFactory::GetRendererPtr() {
   mojom::RendererPtr renderer_ptr;
-  interface_factory_->CreateFlingingRenderer(presentation_id,
-                                             mojo::MakeRequest(&renderer_ptr));
-
-  return std::make_unique<MojoRenderer>(
-      media_task_runner, nullptr, video_renderer_sink, std::move(renderer_ptr));
-}
 
-std::unique_ptr<MojoRenderer> MojoRendererFactory::CreateMediaPlayerRenderer(
-    const scoped_refptr<base::SingleThreadTaskRunner>& media_task_runner,
-    VideoRendererSink* video_renderer_sink) {
-  DCHECK(interface_factory_);
-  mojom::RendererPtr renderer_ptr;
-  interface_factory_->CreateMediaPlayerRenderer(
+  if (interface_factory_) {
+    interface_factory_->CreateRenderer(hosted_renderer_type_,
+                                       get_type_specific_id_.is_null()
+                                           ? std::string()
+                                           : get_type_specific_id_.Run(),
       mojo::MakeRequest(&renderer_ptr));
+  } else {
+    NOTREACHED();
+  }
 
-  return std::make_unique<MojoRenderer>(
-      media_task_runner, nullptr, video_renderer_sink, std::move(renderer_ptr));
+  return renderer_ptr;
 }
-#endif  // defined(OS_ANDROID)
 
 }  // namespace media
--- a/media/mojo/clients/mojo_renderer_factory.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/clients/mojo_renderer_factory.h	2019-05-17 18:53:34.308000000 +0300
@@ -6,10 +6,8 @@
 #define MEDIA_MOJO_CLIENTS_MOJO_RENDERER_FACTORY_H_
 
 #include <memory>
-#include <string>
 
 #include "base/macros.h"
-#include "build/build_config.h"
 #include "media/base/renderer_factory.h"
 #include "media/mojo/interfaces/interface_factory.mojom.h"
 #include "media/mojo/interfaces/renderer.mojom.h"
@@ -21,7 +19,6 @@
 namespace media {
 
 class GpuVideoAcceleratorFactories;
-class MojoRenderer;
 
 // The default factory class for creating MojoRenderer.
 //
@@ -32,12 +29,17 @@
 // wrapper factories that use MRF, rather than creating derived MojoRenderer
 // types, or extending MRF. See DecryptingRendererFactory and
 // MediaPlayerRendererClientFactory for examples of small wrappers around MRF.
+//
+// NOTE: MediaPlayerRendererClientFactory uses MojoRenderer specific methods,
+//       and uses a static_cast<MojoRenderer*> internally. |this| should
+//       never return anything but a MojoRenderer. See crbug.com/919494.
 class MojoRendererFactory : public RendererFactory {
  public:
   using GetGpuFactoriesCB = base::Callback<GpuVideoAcceleratorFactories*()>;
   using GetTypeSpecificIdCB = base::Callback<std::string()>;
 
-  MojoRendererFactory(const GetGpuFactoriesCB& get_gpu_factories_cb,
+  MojoRendererFactory(mojom::HostedRendererType type,
+                      const GetGpuFactoriesCB& get_gpu_factories_cb,
                       media::mojom::InterfaceFactory* interface_factory);
 
   ~MojoRendererFactory() final;
@@ -50,24 +52,27 @@
       const RequestOverlayInfoCB& request_overlay_info_cb,
       const gfx::ColorSpace& target_color_space) final;
 
-#if defined(OS_ANDROID)
-  std::unique_ptr<MojoRenderer> CreateFlingingRenderer(
-      const std::string& presentation_id,
-      const scoped_refptr<base::SingleThreadTaskRunner>& media_task_runner,
-      VideoRendererSink* video_renderer_sink);
-
-  std::unique_ptr<MojoRenderer> CreateMediaPlayerRenderer(
-      const scoped_refptr<base::SingleThreadTaskRunner>& media_task_runner,
-      VideoRendererSink* video_renderer_sink);
-#endif  // defined (OS_ANDROID)
+  // Sets the callback that will fetch the TypeSpecificId when
+  // InterfaceFactory::CreateRenderer() is called. What the string represents
+  // depends on the value of |hosted_renderer_type_|. Currently, we only use it
+  // with mojom::HostedRendererType::kFlinging, in which case
+  // |get_type_specific_id| should return the presentation ID to be given to the
+  // FlingingRenderer in the browser process.
+  void SetGetTypeSpecificIdCB(const GetTypeSpecificIdCB& get_type_specific_id);
 
  private:
+  mojom::RendererPtr GetRendererPtr();
+
   GetGpuFactoriesCB get_gpu_factories_cb_;
+  GetTypeSpecificIdCB get_type_specific_id_;
 
   // InterfaceFactory or InterfaceProvider used to create or connect to remote
   // renderer.
   media::mojom::InterfaceFactory* interface_factory_ = nullptr;
 
+  // Underlying renderer type that will be hosted by the MojoRenderer.
+  mojom::HostedRendererType hosted_renderer_type_;
+
   DISALLOW_COPY_AND_ASSIGN(MojoRendererFactory);
 };
 
--- a/media/mojo/clients/mojo_video_decoder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/clients/mojo_video_decoder.cc	2019-05-17 18:53:34.308000000 +0300
@@ -216,7 +216,7 @@
 
   if (has_connection_error_) {
     task_runner_->PostTask(
-        FROM_HERE, base::BindOnce(bound_decode_cb, DecodeStatus::DECODE_ERROR));
+        FROM_HERE, base::Bind(bound_decode_cb, DecodeStatus::DECODE_ERROR));
     return;
   }
 
@@ -224,7 +224,7 @@
       mojo_decoder_buffer_writer_->WriteDecoderBuffer(std::move(buffer));
   if (!mojo_buffer) {
     task_runner_->PostTask(
-        FROM_HERE, base::BindOnce(bound_decode_cb, DecodeStatus::DECODE_ERROR));
+        FROM_HERE, base::Bind(bound_decode_cb, DecodeStatus::DECODE_ERROR));
     return;
   }
 
--- a/media/mojo/clients/mojo_video_encode_accelerator.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/clients/mojo_video_encode_accelerator.cc	2019-05-17 18:53:34.308000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/mojo/clients/mojo_video_encode_accelerator.h"
 
-#include "base/bind.h"
 #include "base/logging.h"
 #include "gpu/ipc/client/gpu_channel_host.h"
 #include "media/base/video_frame.h"
--- a/media/mojo/common/mojo_data_pipe_read_write.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/common/mojo_data_pipe_read_write.cc	2019-05-17 18:53:34.308000000 +0300
@@ -6,7 +6,6 @@
 
 #include <memory>
 
-#include "base/bind.h"
 #include "base/logging.h"
 #include "base/memory/ptr_util.h"
 #include "media/mojo/common/mojo_pipe_read_write_util.h"
--- a/media/mojo/common/mojo_decoder_buffer_converter.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/common/mojo_decoder_buffer_converter.cc	2019-05-17 18:53:34.308000000 +0300
@@ -6,7 +6,6 @@
 
 #include <memory>
 
-#include "base/bind.h"
 #include "base/logging.h"
 #include "base/memory/ptr_util.h"
 #include "base/single_thread_task_runner.h"
--- a/media/mojo/common/mojo_shared_buffer_video_frame.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/common/mojo_shared_buffer_video_frame.h	2019-05-17 18:53:34.308000000 +0300
@@ -97,7 +97,7 @@
 
   uint8_t* shared_buffer_data() {
     return reinterpret_cast<uint8_t*>(shared_buffer_mapping_.get());
-  }
+  };
 
   mojo::ScopedSharedBufferHandle shared_buffer_handle_;
   mojo::ScopedSharedBufferMapping shared_buffer_mapping_;
--- a/media/mojo/interfaces/interface_factory.mojom	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/interfaces/interface_factory.mojom	2019-05-17 18:53:34.312000000 +0300
@@ -12,24 +12,48 @@
 import "media/mojo/interfaces/video_decoder.mojom";
 import "mojo/public/mojom/base/token.mojom";
 
+// Defines the types of renderers that can be hosted by a mojo Renderer.
+enum HostedRendererType {
+  // media::DefaultRenderer: Used to offload normal rendering scenarios to a
+  // different process, for stability or performance reasons.
+  kDefault,
+
+  // content::MediaPlayerRenderer: Used to handle HLS videos on Android. Also
+  // used on older Android devices, that don't have platform decode support, and
+  // are better off using the native Android MediaPlayer.
+  [EnableIf=is_android]
+  kMediaPlayer,
+
+  // content::FlingingRenderer: Used to control a CastSession, in the context
+  // of RemotePlayback. The CastSession must have already been started via the
+  // PresentationService. This renderer does not actually render anything on the
+  // local device, but instead serves as a link to/from media content playing on
+  // a cast device.
+  [EnableIf=is_android]
+  kFlinging,
+};
+
 // A factory for creating media mojo interfaces. Renderers can only access
 // ContentDecryptionModules created with the same factory.
 interface InterfaceFactory {
   CreateAudioDecoder(AudioDecoder& audio_decoder);
   CreateVideoDecoder(VideoDecoder& video_decoder);
 
-  // Creates a regular media::Renderer (DefaultRendererFactory).
-  CreateDefaultRenderer(string audio_device_id, Renderer& renderer);
-
-  [EnableIf=is_android]
-  // Creates a MediaPlayerRenderer (MediaPlayerRendererFactory).
-  CreateMediaPlayerRenderer(Renderer& renderer);
-
-  [EnableIf=is_android]
-  // Creates a FlingingRenderer (FlingingRendererFactory).
-  // The |presentation_id| is used to find an already set-up RemotePlayback
-  // session (see blink::RemotePlayback).
-  CreateFlingingRenderer(string presentation_id, Renderer& renderer);
+  // Creates a Renderer, using |type| to choose which concrete media::Renderer
+  // implementation to host. Different values of |type| might affect how the
+  // request is ultimately routed (i.e. |type| will determine in which process
+  // the mojom::Renderer is created).
+  // |type_specific_id| represents a different kind of ID, based off of |type|.
+  // The usage of |type_specific_id| per |type| is defined as follows:
+  // - kDefault: represents an audio device ID, which is defined in
+  //   media/audio/audio_device_description.h.
+  //   If |type_specific_id| is empty, kDefaultDeviceId will be used.
+  // - kMediaPlayer: unused.
+  // - kFlinging: represents a PresentationID for a session that has already
+  //   been started. If the ID cannot be found (e.g. the session has already
+  //   ended), CreateRenderer will be a no-op.
+  CreateRenderer(HostedRendererType type, string type_specific_id,
+                 Renderer& renderer);
 
   // Creates a CDM based on the |key_system| provided. A |key_system| is a
   // generic term for a decryption mechanism and/or content protection provider.
--- a/media/mojo/interfaces/media_drm_storage.mojom	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/interfaces/media_drm_storage.mojom	2019-05-17 18:53:34.312000000 +0300
@@ -23,15 +23,12 @@
 // service is the true source for the persistent license and origin
 // provisioning.
 interface MediaDrmStorage {
-  // Initializes |this| and if successful (|success| = true) returns an origin
-  // ID that can be used to identify the current origin. If |origin_id| is
-  // not empty, then it must be valid and unique among all origins. The
-  // implementation will persist the information (e.g. origin ID, provision
-  // time) in the storage. If |origin_id| is empty, then device-wide
-  // provisioning is to be used. If Initialize() fails or returns an empty
-  // origin ID then the other methods below should not be called, and will
-  // fail if they are called.
-  Initialize() => (bool success, mojo_base.mojom.UnguessableToken? origin_id);
+  // Initializes |this| and returns a random identifier than can be used to
+  // identify the current origin. The origin ID should be randomly generated if
+  // it doesn't exist. |origin_id| must be valid and unique among all origins.
+  // If origin information doesn't exist, the implementation will persist the
+  // the information (e.g. origin ID, provision time) in the storage.
+  Initialize() => (mojo_base.mojom.UnguessableToken origin_id);
 
   // Saves origin information (e.g. origin ID, provision time) in the storage
   // after MediaDrm is provisioned for current origin. It will clear all
--- a/media/mojo/services/android_mojo_media_client.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/android_mojo_media_client.cc	2019-05-17 18:53:34.316000000 +0300
@@ -8,7 +8,6 @@
 
 #include <memory>
 
-#include "base/bind.h"
 #include "media/base/android/android_cdm_factory.h"
 #include "media/base/audio_decoder.h"
 #include "media/base/cdm_factory.h"
--- a/media/mojo/services/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/BUILD.gn	2019-05-17 18:53:34.316000000 +0300
@@ -2,10 +2,11 @@
 # Use of this source code is governed by a BSD-style license that can be
 # found in the LICENSE file.
 
-import("//build/config/chromecast_build.gni")
 import("//build/config/jumbo.gni")
 import("//media/media_options.gni")
+import("//services/catalog/public/tools/catalog.gni")
 import("//services/service_manager/public/cpp/service_executable.gni")
+import("//services/service_manager/public/service_manifest.gni")
 import("//testing/test.gni")
 
 jumbo_component("services") {
@@ -99,7 +100,6 @@
     "//media/gpu",
     "//media/gpu:buildflags",
     "//media/gpu/ipc/service",
-    "//media/learning/impl",
     "//media/mojo/common",
     "//media/mojo/common:mojo_shared_buffer_video_frame",
     "//services/metrics/public/cpp:metrics_cpp",
@@ -147,34 +147,14 @@
   }
 }
 
-source_set("cdm_manifest") {
-  sources = [
-    "cdm_manifest.cc",
-    "cdm_manifest.h",
-  ]
-  deps = [
-    "//base",
-    "//media/mojo/interfaces",
-    "//media/mojo/interfaces:constants",
-    "//services/service_manager/public/cpp",
-  ]
+service_manifest("cdm_manifest") {
+  name = "cdm"
+  source = "cdm_manifest.json"
 }
 
-source_set("media_manifest") {
-  sources = [
-    "media_manifest.cc",
-    "media_manifest.h",
-  ]
-  deps = [
-    "//base",
-    "//media/mojo/interfaces",
-    "//media/mojo/interfaces:constants",
-    "//services/service_manager/public/cpp",
-  ]
-  if (is_chromecast) {
-    defines = [ "IS_CHROMECAST" ]
-    deps += [ "//chromecast/common/mojom" ]
-  }
+service_manifest("media_manifest") {
+  name = "media"
+  source = "media_manifest.json"
 }
 
 # Unit Tests
@@ -190,10 +170,7 @@
     "mojo_audio_output_stream_unittest.cc",
     "mojo_jpeg_decode_accelerator_service_unittest.cc",
     "mojo_video_encode_accelerator_service_unittest.cc",
-    "test_helpers.cc",
-    "test_helpers.h",
     "video_decode_perf_history_unittest.cc",
-    "video_decode_stats_recorder_unittest.cc",
     "watch_time_recorder_unittest.cc",
   ]
 
@@ -202,7 +179,6 @@
     "//base/test:test_support",
     "//components/ukm:test_support",
     "//media:test_support",
-    "//media/learning/common",
     "//media/mojo:test_support",
     "//mojo/core/embedder",
     "//mojo/public/interfaces/bindings/tests:test_interfaces",
@@ -260,7 +236,7 @@
   }
 
   deps = [
-    ":media_manifest",
+    ":service_tests_catalog_source",
     ":services",
     "//base",
     "//base/test:test_support",
@@ -271,7 +247,6 @@
     "//media/mojo/interfaces",
     "//media/mojo/interfaces:constants",
     "//mojo/core/test:run_all_unittests",
-    "//services/service_manager/public/cpp",
     "//services/service_manager/public/cpp/test:test_support",
     "//testing/gmock",
     "//testing/gtest",
@@ -282,6 +257,23 @@
   ]
 }
 
+service_manifest("media_service_unittest_manifest") {
+  name = "media_service_unittests"
+  source = "test_manifest.json"
+}
+
+catalog("service_tests_catalog") {
+  testonly = true
+  embedded_services = [ ":media_service_unittest_manifest" ]
+  standalone_services = [ ":media_manifest" ]
+}
+
+catalog_cpp_source("service_tests_catalog_source") {
+  testonly = true
+  catalog = ":service_tests_catalog"
+  generated_function_name = "media::test::CreateServiceTestCatalog"
+}
+
 # media_pipeline_integration_unittests is out of date and disabled by default.
 test("media_pipeline_integration_unittests") {
   testonly = true
@@ -295,3 +287,19 @@
     ":media",
   ]
 }
+
+service_manifest("pipeline_test_manifest") {
+  name = "media_pipeline_integration_unittests"
+  source = "pipeline_test_manifest.json"
+}
+
+catalog("media_pipeline_integration_unittests_catalog") {
+  embedded_services = [ ":pipeline_test_manifest" ]
+  standalone_services = [ ":media_manifest" ]
+}
+
+catalog_cpp_source("media_pipeline_integration_unittests_catalog_source") {
+  testonly = true
+  catalog = ":media_pipeline_integration_unittests_catalog"
+  generated_function_name = "media::test::CreatePipelineIntegrationTestCatalog"
+}
--- a/media/mojo/services/cdm_service.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/cdm_service.cc	2019-05-17 18:53:34.320000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/mojo/services/cdm_service.h"
 
-#include "base/bind.h"
 #include "base/logging.h"
 #include "media/base/cdm_factory.h"
 #include "media/cdm/cdm_module.h"
--- a/media/mojo/services/deferred_destroy_strong_binding_set.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/deferred_destroy_strong_binding_set.h	2019-05-17 18:53:34.320000000 +0300
@@ -10,7 +10,6 @@
 #include <map>
 #include <memory>
 
-#include "base/bind.h"
 #include "base/macros.h"
 #include "base/memory/ptr_util.h"
 #include "base/memory/weak_ptr.h"
--- a/media/mojo/services/gpu_mojo_media_client.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/gpu_mojo_media_client.cc	2019-05-17 18:53:34.320000000 +0300
@@ -68,16 +68,7 @@
   if (!channel)
     return nullptr;
 
-  gpu::CommandBufferStub* stub = channel->LookupCommandBuffer(route_id);
-  if (!stub)
-    return nullptr;
-
-  // Only allow stubs that have a ContextGroup, that is, the GLES2 ones. Later
-  // code assumes the ContextGroup is valid.
-  if (!stub->decoder_context()->GetContextGroup())
-    return nullptr;
-
-  return stub;
+  return channel->LookupCommandBuffer(route_id);
 }
 #endif
 
@@ -123,16 +114,20 @@
 
 std::vector<SupportedVideoDecoderConfig>
 GpuMojoMediaClient::GetSupportedVideoDecoderConfigs() {
-#if defined(OS_ANDROID)
-  static std::vector<SupportedVideoDecoderConfig> supported_configs =
-      MediaCodecVideoDecoder::GetSupportedConfigs();
-  return supported_configs;
-#else
+  // TODO(liberato): Implement for D3D11VideoDecoder and MediaCodecVideoDecoder.
+  VideoDecodeAccelerator::Capabilities capabilities =
+      GpuVideoAcceleratorUtil::ConvertGpuToMediaDecodeCapabilities(
+          GpuVideoDecodeAcceleratorFactory::GetDecoderCapabilities(
+              gpu_preferences_, gpu_workarounds_));
+  bool allow_encrypted =
+      capabilities.flags &
+      VideoDecodeAccelerator::Capabilities::SUPPORTS_ENCRYPTED_STREAMS;
+
   std::vector<SupportedVideoDecoderConfig> supported_configs;
 
-#if defined(OS_WIN)
-  // Start with the configurations supported by D3D11VideoDecoder.
-  // VdaVideoDecoder is still used as a fallback.
+#if defined(OS_ANDROID)
+  // TODO(liberato): Add MCVD.
+#elif defined(OS_WIN)
   if (!d3d11_supported_configs_) {
     d3d11_supported_configs_ =
         D3D11VideoDecoder::GetSupportedVideoDecoderConfigs(
@@ -141,16 +136,7 @@
   supported_configs = *d3d11_supported_configs_;
 #endif
 
-  // VdaVideoDecoder will be used to wrap a VDA. Add the configs supported
-  // by the VDA implementation.
-  // TODO(sandersd): Move conversion code into VdaVideoDecoder.
-  VideoDecodeAccelerator::Capabilities capabilities =
-      GpuVideoAcceleratorUtil::ConvertGpuToMediaDecodeCapabilities(
-          GpuVideoDecodeAcceleratorFactory::GetDecoderCapabilities(
-              gpu_preferences_, gpu_workarounds_));
-  bool allow_encrypted =
-      capabilities.flags &
-      VideoDecodeAccelerator::Capabilities::SUPPORTS_ENCRYPTED_STREAMS;
+  // Merge the VDA supported profiles.
   for (const auto& supported_profile : capabilities.supported_profiles) {
     supported_configs.push_back(SupportedVideoDecoderConfig(
         supported_profile.profile,           // profile_min
@@ -160,9 +146,7 @@
         allow_encrypted,                     // allow_encrypted
         supported_profile.encrypted_only));  // require_encrypted
   }
-
   return supported_configs;
-#endif  // defined(OS_ANDROID)
 }
 
 std::unique_ptr<VideoDecoder> GpuMojoMediaClient::CreateVideoDecoder(
--- a/media/mojo/services/interface_factory_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/interface_factory_impl.cc	2019-05-17 18:53:34.320000000 +0300
@@ -5,7 +5,6 @@
 #include "media/mojo/services/interface_factory_impl.h"
 
 #include <memory>
-#include "base/bind.h"
 #include "base/guid.h"
 
 #include "base/logging.h"
@@ -95,11 +94,22 @@
 #endif  // BUILDFLAG(ENABLE_MOJO_VIDEO_DECODER)
 }
 
-void InterfaceFactoryImpl::CreateDefaultRenderer(
-    const std::string& audio_device_id,
+void InterfaceFactoryImpl::CreateRenderer(
+    media::mojom::HostedRendererType type,
+    const std::string& type_specific_id,
     mojo::InterfaceRequest<mojom::Renderer> request) {
   DVLOG(2) << __func__;
 #if BUILDFLAG(ENABLE_MOJO_RENDERER)
+  // Creation requests for non default renderers should have already been
+  // handled by now, in a different layer.
+  if (type != media::mojom::HostedRendererType::kDefault) {
+    DLOG(ERROR) << "Creation of specialized renderers is not supported.";
+    return;
+  }
+
+  // For HostedRendererType::kDefault type, |type_specific_id| represents an
+  // audio device ID. See interface_factory.mojom.
+  const std::string& audio_device_id = type_specific_id;
   auto renderer = mojo_media_client_->CreateRenderer(
       interfaces_.get(), base::ThreadTaskRunnerHandle::Get(), &media_log_,
       audio_device_id);
@@ -127,19 +137,6 @@
 #endif  // BUILDFLAG(ENABLE_MOJO_RENDERER)
 }
 
-#if defined(OS_ANDROID)
-void InterfaceFactoryImpl::CreateMediaPlayerRenderer(
-    mojo::InterfaceRequest<mojom::Renderer> request) {
-  NOTREACHED();
-}
-
-void InterfaceFactoryImpl::CreateFlingingRenderer(
-    const std::string& audio_device_id,
-    mojo::InterfaceRequest<mojom::Renderer> request) {
-  NOTREACHED();
-}
-#endif  // defined(OS_ANDROID)
-
 void InterfaceFactoryImpl::CreateCdm(
     const std::string& /* key_system */,
     mojo::InterfaceRequest<mojom::ContentDecryptionModule> request) {
--- a/media/mojo/services/interface_factory_impl.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/interface_factory_impl.h	2019-05-17 18:53:34.320000000 +0300
@@ -8,16 +8,9 @@
 #include <memory>
 
 #include "base/macros.h"
-#include "build/build_config.h"
 #include "media/base/media_util.h"
 #include "media/mojo/buildflags.h"
-#include "media/mojo/interfaces/audio_decoder.mojom.h"
-#include "media/mojo/interfaces/cdm_proxy.mojom.h"
-#include "media/mojo/interfaces/content_decryption_module.mojom.h"
-#include "media/mojo/interfaces/decryptor.mojom.h"
 #include "media/mojo/interfaces/interface_factory.mojom.h"
-#include "media/mojo/interfaces/renderer.mojom.h"
-#include "media/mojo/interfaces/video_decoder.mojom.h"
 #include "media/mojo/services/deferred_destroy_strong_binding_set.h"
 #include "media/mojo/services/mojo_cdm_service_context.h"
 #include "mojo/public/cpp/bindings/strong_binding_set.h"
@@ -40,13 +33,9 @@
   // mojom::InterfaceFactory implementation.
   void CreateAudioDecoder(mojom::AudioDecoderRequest request) final;
   void CreateVideoDecoder(mojom::VideoDecoderRequest request) final;
-  void CreateDefaultRenderer(const std::string& audio_device_id,
+  void CreateRenderer(media::mojom::HostedRendererType type,
+                      const std::string& type_specific_id,
                              mojom::RendererRequest request) final;
-#if defined(OS_ANDROID)
-  void CreateMediaPlayerRenderer(mojom::RendererRequest request) final;
-  void CreateFlingingRenderer(const std::string& presentation_id,
-                              mojom::RendererRequest request) final;
-#endif  // defined(OS_ANDROID)
   void CreateCdm(const std::string& key_system,
                  mojom::ContentDecryptionModuleRequest request) final;
   void CreateDecryptor(int cdm_id, mojom::DecryptorRequest request) final;
--- a/media/mojo/services/media_metrics_provider_unittest.cc	2019-05-17 17:45:41.300000000 +0300
+++ b/media/mojo/services/media_metrics_provider_unittest.cc	2019-05-17 18:53:34.320000000 +0300
@@ -22,7 +22,7 @@
 
 namespace media {
 
-constexpr char kTestOrigin[] = "https://test.9oo91e.qjz9zk/";
+constexpr char kTestOrigin[] = "https://test.google.com/";
 
 class MediaMetricsProviderTest : public testing::Test {
  public:
@@ -99,7 +99,7 @@
   }
 
   // Now try one with different values and optional parameters set.
-  const std::string kTestOrigin2 = "https://test2.9oo91e.qjz9zk/";
+  const std::string kTestOrigin2 = "https://test2.google.com/";
   const base::TimeDelta kMetadataTime = base::TimeDelta::FromSeconds(1);
   const base::TimeDelta kFirstFrameTime = base::TimeDelta::FromSeconds(2);
   const base::TimeDelta kPlayReadyTime = base::TimeDelta::FromSeconds(3);
--- a/media/mojo/services/media_service.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/media_service.cc	2019-05-17 18:53:34.320000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/mojo/services/media_service.h"
 
-#include "base/bind.h"
 #include "base/logging.h"
 #include "media/media_buildflags.h"
 #include "media/mojo/services/interface_factory_impl.h"
--- a/media/mojo/services/media_service_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/media_service_unittest.cc	2019-05-17 18:53:34.320000000 +0300
@@ -21,7 +21,6 @@
 #include "media/mojo/clients/mojo_decryptor.h"
 #include "media/mojo/clients/mojo_demuxer_stream_impl.h"
 #include "media/mojo/common/media_type_converters.h"
-#include "media/mojo/interfaces/cdm_proxy.mojom.h"
 #include "media/mojo/interfaces/constants.mojom.h"
 #include "media/mojo/interfaces/content_decryption_module.mojom.h"
 #include "media/mojo/interfaces/decryptor.mojom.h"
@@ -29,10 +28,9 @@
 #include "media/mojo/interfaces/media_service.mojom.h"
 #include "media/mojo/interfaces/renderer.mojom.h"
 #include "media/mojo/services/media_interface_provider.h"
-#include "media/mojo/services/media_manifest.h"
+#include "media/mojo/services/service_tests_catalog_source.h"
 #include "mojo/public/cpp/bindings/associated_binding.h"
 #include "mojo/public/cpp/bindings/interface_request.h"
-#include "services/service_manager/public/cpp/manifest_builder.h"
 #include "services/service_manager/public/cpp/test/test_service.h"
 #include "services/service_manager/public/cpp/test/test_service_manager.h"
 #include "testing/gmock/include/gmock/gmock.h"
@@ -120,8 +118,6 @@
   base::PostTask(FROM_HERE, run_loop->QuitClosure());
 }
 
-const char kTestServiceName[] = "media_service_unittests";
-
 // Tests MediaService built into a standalone mojo service binary (see
 // ServiceMain() in main.cc) where MediaService uses TestMojoMediaClient.
 // TestMojoMediaClient supports CDM creation using DefaultCdmFactory (only
@@ -130,14 +126,9 @@
 class MediaServiceTest : public testing::Test {
  public:
   MediaServiceTest()
-      : test_service_manager_(
-            {GetMediaManifest(),
-             service_manager::ManifestBuilder()
-                 .WithServiceName(kTestServiceName)
-                 .RequireCapability(mojom::kMediaServiceName, "media:media")
-                 .Build()}),
-        test_service_(
-            test_service_manager_.RegisterTestInstance(kTestServiceName)),
+      : test_service_manager_(test::CreateServiceTestCatalog()),
+        test_service_(test_service_manager_.RegisterTestInstance(
+            "media_service_unittests")),
         cdm_proxy_client_binding_(&cdm_proxy_client_),
         renderer_client_binding_(&renderer_client_),
         video_stream_(DemuxerStream::VIDEO) {}
@@ -240,7 +231,8 @@
   void InitializeRenderer(const VideoDecoderConfig& video_config,
                           bool expected_result) {
     base::RunLoop run_loop;
-    interface_factory_->CreateDefaultRenderer(std::string(),
+    interface_factory_->CreateRenderer(
+        media::mojom::HostedRendererType::kDefault, std::string(),
                                               mojo::MakeRequest(&renderer_));
 
     video_stream_.set_video_decoder_config(video_config);
--- a/media/mojo/services/mojo_audio_input_stream.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_audio_input_stream.cc	2019-05-17 18:53:34.320000000 +0300
@@ -7,7 +7,6 @@
 #include <memory>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/callback_helpers.h"
 #include "base/memory/read_only_shared_memory_region.h"
 #include "base/sync_socket.h"
--- a/media/mojo/services/mojo_audio_input_stream_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_audio_input_stream_unittest.cc	2019-05-17 18:53:34.324000000 +0300
@@ -6,7 +6,6 @@
 
 #include <utility>
 
-#include "base/bind.h"
 #include "base/memory/read_only_shared_memory_region.h"
 #include "base/message_loop/message_loop.h"
 #include "base/run_loop.h"
--- a/media/mojo/services/mojo_audio_output_stream.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_audio_output_stream.cc	2019-05-17 18:53:34.324000000 +0300
@@ -7,11 +7,9 @@
 #include <memory>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/callback_helpers.h"
 #include "base/memory/unsafe_shared_memory_region.h"
 #include "base/sync_socket.h"
-#include "media/mojo/interfaces/audio_data_pipe.mojom.h"
 #include "mojo/public/cpp/system/platform_handle.h"
 
 namespace media {
--- a/media/mojo/services/mojo_audio_output_stream_provider.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_audio_output_stream_provider.cc	2019-05-17 18:53:34.324000000 +0300
@@ -6,9 +6,7 @@
 
 #include <utility>
 
-#include "base/bind.h"
 #include "build/build_config.h"
-#include "media/mojo/interfaces/audio_data_pipe.mojom.h"
 #include "mojo/public/cpp/bindings/message.h"
 
 namespace media {
--- a/media/mojo/services/mojo_audio_output_stream_provider_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_audio_output_stream_provider_unittest.cc	2019-05-17 18:53:34.324000000 +0300
@@ -6,7 +6,6 @@
 
 #include <utility>
 
-#include "base/bind.h"
 #include "base/message_loop/message_loop.h"
 #include "base/run_loop.h"
 #include "base/test/mock_callback.h"
--- a/media/mojo/services/mojo_audio_output_stream_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_audio_output_stream_unittest.cc	2019-05-17 18:53:34.324000000 +0300
@@ -6,13 +6,11 @@
 
 #include <utility>
 
-#include "base/bind.h"
 #include "base/memory/unsafe_shared_memory_region.h"
 #include "base/message_loop/message_loop.h"
 #include "base/run_loop.h"
 #include "base/sync_socket.h"
 #include "media/audio/audio_output_controller.h"
-#include "media/mojo/interfaces/audio_data_pipe.mojom.h"
 #include "mojo/public/cpp/system/message_pipe.h"
 #include "mojo/public/cpp/system/platform_handle.h"
 #include "testing/gmock/include/gmock/gmock.h"
--- a/media/mojo/services/mojo_cdm_helper_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_cdm_helper_unittest.cc	2019-05-17 18:53:34.324000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/mojo/services/mojo_cdm_helper.h"
 
-#include "base/bind.h"
 #include "base/files/file.h"
 #include "base/files/file_util.h"
 #include "base/files/scoped_temp_dir.h"
--- a/media/mojo/services/mojo_jpeg_decode_accelerator_service.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_jpeg_decode_accelerator_service.cc	2019-05-17 18:53:34.328000000 +0300
@@ -9,7 +9,6 @@
 #include <memory>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/logging.h"
 #include "base/memory/ptr_util.h"
 #include "base/memory/shared_memory.h"
--- a/media/mojo/services/mojo_jpeg_encode_accelerator_service.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_jpeg_encode_accelerator_service.cc	2019-05-17 18:53:34.328000000 +0300
@@ -9,7 +9,6 @@
 #include <memory>
 #include <utility>
 
-#include "base/bind.h"
 #include "base/bind_helpers.h"
 #include "base/logging.h"
 #include "base/memory/ptr_util.h"
--- a/media/mojo/services/mojo_media_drm_storage.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_media_drm_storage.cc	2019-05-17 18:53:34.328000000 +0300
@@ -9,8 +9,6 @@
 
 #include "base/bind.h"
 #include "base/bind_helpers.h"
-#include "base/optional.h"
-#include "base/unguessable_token.h"
 #include "mojo/public/cpp/bindings/callback_helpers.h"
 
 namespace media {
@@ -30,8 +28,8 @@
 void MojoMediaDrmStorage::Initialize(InitCB init_cb) {
   DVLOG(1) << __func__;
   media_drm_storage_ptr_->Initialize(
-      mojo::WrapCallbackWithDefaultInvokeIfNotRun(std::move(init_cb), false,
-                                                  base::nullopt));
+      mojo::WrapCallbackWithDefaultInvokeIfNotRun(std::move(init_cb),
+                                                  base::UnguessableToken()));
 }
 
 void MojoMediaDrmStorage::OnProvisioned(ResultCB result_cb) {
--- a/media/mojo/services/mojo_media_log.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_media_log.cc	2019-05-17 18:53:34.328000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/mojo/services/mojo_media_log.h"
 
-#include "base/bind.h"
 #include "base/logging.h"
 #include "base/threading/sequenced_task_runner_handle.h"
 
--- a/media/mojo/services/mojo_video_decoder_service.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_video_decoder_service.cc	2019-05-17 18:53:34.328000000 +0300
@@ -13,7 +13,6 @@
 #include "base/threading/thread_task_runner_handle.h"
 #include "media/base/cdm_context.h"
 #include "media/base/decoder_buffer.h"
-#include "media/base/simple_sync_token_client.h"
 #include "media/base/video_decoder.h"
 #include "media/base/video_decoder_config.h"
 #include "media/base/video_frame.h"
@@ -42,6 +41,26 @@
 const char kDecodeTraceName[] = "MojoVideoDecoderService::Decode";
 const char kResetTraceName[] = "MojoVideoDecoderService::Reset";
 
+class StaticSyncTokenClient : public VideoFrame::SyncTokenClient {
+ public:
+  explicit StaticSyncTokenClient(const gpu::SyncToken& sync_token)
+      : sync_token_(sync_token) {}
+
+  // VideoFrame::SyncTokenClient implementation
+  void GenerateSyncToken(gpu::SyncToken* sync_token) final {
+    *sync_token = sync_token_;
+  }
+
+  void WaitSyncToken(const gpu::SyncToken& sync_token) final {
+    // NOP; we don't care what the old sync token was.
+  }
+
+ private:
+  gpu::SyncToken sync_token_;
+
+  DISALLOW_COPY_AND_ASSIGN(StaticSyncTokenClient);
+};
+
 }  // namespace
 
 class VideoFrameHandleReleaserImpl final
@@ -73,7 +92,7 @@
       mojo::ReportBadMessage("Unknown |release_token|.");
       return;
     }
-    SimpleSyncTokenClient client(release_sync_token);
+    StaticSyncTokenClient client(release_sync_token);
     it->second->UpdateReleaseSyncToken(&client);
     video_frames_.erase(it);
   }
--- a/media/mojo/services/mojo_video_encode_accelerator_service_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/mojo_video_encode_accelerator_service_unittest.cc	2019-05-17 18:53:34.328000000 +0300
@@ -4,8 +4,6 @@
 
 #include <stddef.h>
 
-#include "base/bind.h"
-#include "base/bind_helpers.h"
 #include "base/message_loop/message_loop.h"
 #include "base/run_loop.h"
 #include "gpu/config/gpu_preferences.h"
--- a/media/mojo/services/OWNERS	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/OWNERS	2019-05-17 18:53:34.316000000 +0300
@@ -1,12 +1,17 @@
-per-file cdm_manifest.cc=set noparent
-per-file cdm_manifest.cc=file://ipc/SECURITY_OWNERS
-per-file cdm_manifest.h=set noparent
-per-file cdm_manifest.h=file://ipc/SECURITY_OWNERS
+per-file cdm_manifest.json=set noparent
+per-file cdm_manifest.json=file://ipc/SECURITY_OWNERS
 
-per-file media_manifest.cc=set noparent
-per-file media_manifest.cc=file://ipc/SECURITY_OWNERS
-per-file media_manifest.h=set noparent
-per-file media_manifest.h=file://ipc/SECURITY_OWNERS
+per-file media_manifest.json=set noparent
+per-file media_manifest.json=file://ipc/SECURITY_OWNERS
+
+per-file pipeline_test_manifest.json=set noparent
+per-file pipeline_test_manifest.json=file://ipc/SECURITY_OWNERS
+
+per-file test_manifest.json=set noparent
+per-file test_manifest.json=file://ipc/SECURITY_OWNERS
+
+per-file pipeline_apptest_manifest.json=set noparent
+per-file pipeline_apptest_manifest.json=file://ipc/SECURITY_OWNERS
 
 per-file mojo_audio_output*=file://media/audio/OWNERS
 per-file mojo_audio_input*=file://media/audio/OWNERS
--- a/media/mojo/services/video_decode_perf_history.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/video_decode_perf_history.cc	2019-05-17 18:53:34.328000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/mojo/services/video_decode_perf_history.h"
 
-#include "base/bind.h"
 #include "base/callback.h"
 #include "base/format_macros.h"
 #include "base/logging.h"
@@ -15,7 +14,6 @@
 #include "media/base/media_switches.h"
 #include "media/base/video_codecs.h"
 #include "media/capabilities/learning_helper.h"
-#include "media/mojo/interfaces/media_types.mojom.h"
 #include "mojo/public/cpp/bindings/strong_binding.h"
 #include "services/metrics/public/cpp/ukm_builders.h"
 #include "services/metrics/public/cpp/ukm_recorder.h"
@@ -39,11 +37,9 @@
 }
 
 VideoDecodePerfHistory::VideoDecodePerfHistory(
-    std::unique_ptr<VideoDecodeStatsDB> db,
-    learning::FeatureProviderFactoryCB feature_factory_cb)
+    std::unique_ptr<VideoDecodeStatsDB> db)
     : db_(std::move(db)),
       db_init_status_(UNINITIALIZED),
-      feature_factory_cb_(std::move(feature_factory_cb)),
       weak_ptr_factory_(this) {
   DVLOG(2) << __func__;
   DCHECK(db_);
@@ -51,7 +47,7 @@
   // If the local learning experiment is enabled, then also create
   // |learning_helper_| to send data to it.
   if (base::FeatureList::IsEnabled(kMediaLearningExperiment))
-    learning_helper_ = std::make_unique<LearningHelper>(feature_factory_cb_);
+    learning_helper_ = std::make_unique<LearningHelper>();
 }
 
 VideoDecodePerfHistory::~VideoDecodePerfHistory() {
@@ -357,7 +353,7 @@
   // If we have a learning helper, then replace it.  This will erase any data
   // that it currently has.
   if (learning_helper_)
-    learning_helper_ = std::make_unique<LearningHelper>(feature_factory_cb_);
+    learning_helper_ = std::make_unique<LearningHelper>();
 
   if (db_init_status_ == FAILED) {
     DVLOG(3) << __func__ << " Can't clear history - No DB!";
--- a/media/mojo/services/video_decode_perf_history.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/video_decode_perf_history.h	2019-05-17 18:53:34.332000000 +0300
@@ -16,7 +16,6 @@
 #include "media/base/video_codecs.h"
 #include "media/capabilities/video_decode_stats_db.h"
 #include "media/capabilities/video_decode_stats_db_provider.h"
-#include "media/learning/impl/feature_provider.h"
 #include "media/mojo/interfaces/video_decode_perf_history.mojom.h"
 #include "media/mojo/services/media_mojo_export.h"
 #include "mojo/public/cpp/bindings/binding_set.h"
@@ -54,10 +53,7 @@
  public:
   static const char kMaxSmoothDroppedFramesPercentParamName[];
 
-  explicit VideoDecodePerfHistory(
-      std::unique_ptr<VideoDecodeStatsDB> db,
-      learning::FeatureProviderFactoryCB feature_factory_cb =
-          learning::FeatureProviderFactoryCB());
+  explicit VideoDecodePerfHistory(std::unique_ptr<VideoDecodeStatsDB> db);
   ~VideoDecodePerfHistory() override;
 
   // Bind the mojo request to this instance. Single instance will be used to
@@ -189,9 +185,6 @@
   // Optional helper for local learning.
   std::unique_ptr<LearningHelper> learning_helper_;
 
-  // Optional callback to create a FeatureProvider for |learning_helper_|.
-  learning::FeatureProviderFactoryCB feature_factory_cb_;
-
   // Ensures all access to class members come on the same sequence.
   SEQUENCE_CHECKER(sequence_checker_);
 
--- a/media/mojo/services/video_decode_perf_history_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/video_decode_perf_history_unittest.cc	2019-05-17 18:53:34.332000000 +0300
@@ -5,7 +5,6 @@
 #include <map>
 #include <string>
 
-#include "base/bind.h"
 #include "base/memory/ptr_util.h"
 #include "base/metrics/field_trial_param_associator.h"
 #include "base/metrics/field_trial_params.h"
@@ -18,8 +17,6 @@
 #include "components/ukm/test_ukm_recorder.h"
 #include "media/base/media_switches.h"
 #include "media/capabilities/video_decode_stats_db.h"
-#include "media/mojo/interfaces/media_types.mojom.h"
-#include "media/mojo/services/test_helpers.h"
 #include "media/mojo/services/video_decode_perf_history.h"
 #include "services/metrics/public/cpp/ukm_builders.h"
 #include "testing/gmock/include/gmock/gmock.h"
@@ -28,6 +25,7 @@
 #include "url/origin.h"
 
 using UkmEntry = ukm::builders::Media_VideoDecodePerfRecord;
+using testing::Eq;
 using testing::IsNull;
 using testing::_;
 
@@ -163,6 +161,34 @@
 
   MOCK_METHOD1(MockGetVideoDecodeStatsDBCB, void(VideoDecodeStatsDB* db));
 
+  mojom::PredictionFeatures MakeFeatures(VideoCodecProfile profile,
+                                         gfx::Size video_size,
+                                         int frames_per_sec) {
+    mojom::PredictionFeatures features;
+    features.profile = profile;
+    features.video_size = video_size;
+    features.frames_per_sec = frames_per_sec;
+    return features;
+  }
+
+  mojom::PredictionFeaturesPtr MakeFeaturesPtr(VideoCodecProfile profile,
+                                               gfx::Size video_size,
+                                               int frames_per_sec) {
+    mojom::PredictionFeaturesPtr features = mojom::PredictionFeatures::New();
+    *features = MakeFeatures(profile, video_size, frames_per_sec);
+    return features;
+  }
+
+  mojom::PredictionTargets MakeTargets(uint32_t frames_decoded,
+                                       uint32_t frames_dropped,
+                                       uint32_t frames_power_efficient) {
+    mojom::PredictionTargets targets;
+    targets.frames_decoded = frames_decoded;
+    targets.frames_dropped = frames_dropped;
+    targets.frames_power_efficient = frames_power_efficient;
+    return targets;
+  }
+
   void SavePerfRecord(const url::Origin& origin,
                       bool is_top_frame,
                       mojom::PredictionFeatures features,
@@ -708,7 +734,7 @@
   }
 }
 
-INSTANTIATE_TEST_SUITE_P(VaryDBInitTiming,
+INSTANTIATE_TEST_CASE_P(VaryDBInitTiming,
                          VideoDecodePerfHistoryParamTest,
                          ::testing::Values(true, false));
 
--- a/media/mojo/services/video_decode_stats_recorder.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/video_decode_stats_recorder.cc	2019-05-17 18:53:34.332000000 +0300
@@ -34,10 +34,8 @@
   DCHECK_GT(features->frames_per_sec, 0);
   DCHECK(features->video_size.width() > 0 && features->video_size.height() > 0);
 
-  // DO THIS FIRST! Finalize existing stats with the current state.
-  FinalizeRecord();
-
   features_ = *features;
+  FinalizeRecord();
 
   DVLOG(2) << __func__ << "profile: " << features_.profile
            << " sz:" << features_.video_size.ToString()
--- a/media/mojo/services/video_decode_stats_recorder.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/services/video_decode_stats_recorder.h	2019-05-17 18:53:34.332000000 +0300
@@ -10,7 +10,6 @@
 
 #include "base/time/time.h"
 #include "media/base/video_codecs.h"
-#include "media/mojo/interfaces/media_types.mojom.h"
 #include "media/mojo/interfaces/video_decode_stats_recorder.mojom.h"
 #include "media/mojo/services/media_mojo_export.h"
 #include "media/mojo/services/video_decode_perf_history.h"
--- a/media/mojo/services/watch_time_recorder_unittest.cc	2019-05-17 17:45:41.304000000 +0300
+++ b/media/mojo/services/watch_time_recorder_unittest.cc	2019-05-17 18:53:34.332000000 +0300
@@ -27,7 +27,7 @@
 
 namespace media {
 
-constexpr char kTestOrigin[] = "https://test.9oo91e.qjz9zk/";
+constexpr char kTestOrigin[] = "https://test.google.com/";
 
 class WatchTimeRecorderTest : public testing::Test {
  public:
--- a/media/mojo/test/mojo_video_encode_accelerator_integration_test.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/mojo/test/mojo_video_encode_accelerator_integration_test.cc	2019-05-17 18:53:34.332000000 +0300
@@ -5,7 +5,6 @@
 #include <stddef.h>
 #include <memory>
 
-#include "base/bind.h"
 #include "base/message_loop/message_loop.h"
 #include "base/run_loop.h"
 #include "base/test/gtest_util.h"
--- a/media/muxers/webm_muxer.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/muxers/webm_muxer.cc	2019-05-17 18:53:34.332000000 +0300
@@ -25,7 +25,6 @@
   // Set Opus version.
   header[OPUS_EXTRADATA_VERSION_OFFSET] = 1;
   // Set channel count.
-  DCHECK_LE(params.channels(), 2);
   header[OPUS_EXTRADATA_CHANNELS_OFFSET] = params.channels();
   // Set pre-skip
   uint16_t skip = 0;
@@ -38,7 +37,23 @@
   uint16_t gain = 0;
   memcpy(header + OPUS_EXTRADATA_GAIN_OFFSET, &gain, 2);
 
+  // Set channel mapping.
+  if (params.channels() > 2) {
+    // Also possible to have a multistream, not supported for now.
+    DCHECK_LE(params.channels(), OPUS_MAX_VORBIS_CHANNELS);
+    header[OPUS_EXTRADATA_CHANNEL_MAPPING_OFFSET] = 1;
+    // Assuming no coupled streams. This should actually be
+    // channels() - |coupled_streams|.
+    header[OPUS_EXTRADATA_NUM_STREAMS_OFFSET] = params.channels();
+    header[OPUS_EXTRADATA_NUM_COUPLED_OFFSET] = 0;
+    // Set the actual stream map.
+    for (int i = 0; i < params.channels(); ++i) {
+      header[OPUS_EXTRADATA_STREAM_MAP_OFFSET + i] =
+          kOpusVorbisChannelMap[params.channels() - 1][i];
+    }
+  } else {
   header[OPUS_EXTRADATA_CHANNEL_MAPPING_OFFSET] = 0;
+  }
 }
 
 static double GetFrameRate(const WebmMuxer::VideoParameters& params) {
@@ -278,8 +293,6 @@
   DCHECK(audio_track);
   DCHECK_EQ(params.sample_rate(), audio_track->sample_rate());
   DCHECK_EQ(params.channels(), static_cast<int>(audio_track->channels()));
-  DCHECK_LE(params.channels(), 2)
-      << "Only 1 or 2 channels supported, requested " << params.channels();
 
   // Audio data is always pcm_f32le.
   audio_track->set_bit_depth(32u);
--- a/media/muxers/webm_muxer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/muxers/webm_muxer_unittest.cc	2019-05-17 18:53:34.332000000 +0300
@@ -327,6 +327,6 @@
     {kCodecVP8, kCodecPCM, 1, 1},
 };
 
-INSTANTIATE_TEST_SUITE_P(, WebmMuxerTest, ValuesIn(kTestCases));
+INSTANTIATE_TEST_CASE_P(, WebmMuxerTest, ValuesIn(kTestCases));
 
 }  // namespace media
--- a/media/PRESUBMIT.py	2019-05-17 17:45:41.224000000 +0300
+++ b/media/PRESUBMIT.py	2019-05-17 18:53:34.016000000 +0300
@@ -4,7 +4,7 @@
 
 """Top-level presubmit script for Chromium media component.
 
-See http://dev.ch40m1um.qjz9zk/developers/how-tos/depottools/presubmit-scripts
+See http://dev.chromium.org/developers/how-tos/depottools/presubmit-scripts
 for more details about the presubmit API built into depot_tools.
 """
 
@@ -144,7 +144,7 @@
       ' - The max value (3rd argument) should be an enum value equal to the\n'
       '   last valid value, e.g. FOO_MAX = LAST_VALID_FOO.\n'
       ' - 1 must be added to that max value.\n'
-      'Contact dalecurtis@ch40m1um.qjz9zk if you have questions.' , problems)]
+      'Contact dalecurtis@chromium.org if you have questions.' , problems)]
 
   return []
 
--- a/media/remoting/courier_renderer.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/remoting/courier_renderer.cc	2019-05-17 18:53:34.332000000 +0300
@@ -144,7 +144,7 @@
       FROM_HERE,
       base::BindOnce(
           &RendererController::StartDataPipe, controller_,
-          std::move(audio_data_pipe), std::move(video_data_pipe),
+          base::Passed(&audio_data_pipe), base::Passed(&video_data_pipe),
           base::BindOnce(&CourierRenderer::OnDataPipeCreatedOnMainThread,
                          media_task_runner_, weak_factory_.GetWeakPtr(),
                          rpc_broker_)));
@@ -295,9 +295,9 @@
     mojo::ScopedDataPipeProducerHandle video_handle) {
   media_task_runner->PostTask(
       FROM_HERE,
-      base::BindOnce(&CourierRenderer::OnDataPipeCreated, self,
-                     std::move(audio), std::move(video),
-                     std::move(audio_handle), std::move(video_handle),
+      base::Bind(&CourierRenderer::OnDataPipeCreated, self,
+                 base::Passed(&audio), base::Passed(&video),
+                 base::Passed(&audio_handle), base::Passed(&video_handle),
                      rpc_broker ? rpc_broker->GetUniqueHandle()
                                 : RpcBroker::kInvalidHandle,
                      rpc_broker ? rpc_broker->GetUniqueHandle()
@@ -370,9 +370,9 @@
     scoped_refptr<base::SingleThreadTaskRunner> media_task_runner,
     base::WeakPtr<CourierRenderer> self,
     std::unique_ptr<pb::RpcMessage> message) {
-  media_task_runner->PostTask(
-      FROM_HERE, base::BindOnce(&CourierRenderer::OnReceivedRpc, self,
-                                std::move(message)));
+  media_task_runner->PostTask(FROM_HERE,
+                              base::Bind(&CourierRenderer::OnReceivedRpc, self,
+                                         base::Passed(&message)));
 }
 
 void CourierRenderer::OnReceivedRpc(std::unique_ptr<pb::RpcMessage> message) {
@@ -438,7 +438,7 @@
   DCHECK(main_task_runner_);
   main_task_runner_->PostTask(
       FROM_HERE, base::BindOnce(&RpcBroker::SendMessageToRemote, rpc_broker_,
-                                std::move(message)));
+                                base::Passed(&message)));
 }
 
 void CourierRenderer::AcquireRendererDone(
--- a/media/remoting/courier_renderer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/remoting/courier_renderer_unittest.cc	2019-05-17 18:53:34.336000000 +0300
@@ -6,7 +6,6 @@
 
 #include <memory>
 
-#include "base/bind.h"
 #include "base/run_loop.h"
 #include "base/test/scoped_task_environment.h"
 #include "base/test/simple_test_tick_clock.h"
--- a/media/remoting/demuxer_stream_adapter.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/remoting/demuxer_stream_adapter.cc	2019-05-17 18:53:34.336000000 +0300
@@ -329,8 +329,8 @@
 
   // Contiune to read decoder buffer until reaching |read_until_count_| or
   // end of stream.
-  media_task_runner_->PostTask(
-      FROM_HERE, base::BindOnce(&DemuxerStreamAdapter::RequestBuffer,
+  media_task_runner_->PostTask(FROM_HERE,
+                               base::Bind(&DemuxerStreamAdapter::RequestBuffer,
                                 weak_factory_.GetWeakPtr()));
 }
 
--- a/media/remoting/demuxer_stream_adapter_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/remoting/demuxer_stream_adapter_unittest.cc	2019-05-17 18:53:34.336000000 +0300
@@ -7,7 +7,6 @@
 #include <memory>
 #include <vector>
 
-#include "base/bind.h"
 #include "base/callback_helpers.h"
 #include "base/message_loop/message_loop.h"
 #include "base/run_loop.h"
--- a/media/remoting/fake_remoter.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/remoting/fake_remoter.cc	2019-05-17 18:53:34.336000000 +0300
@@ -124,11 +124,11 @@
   if (start_will_fail_) {
     base::ThreadTaskRunnerHandle::Get()->PostTask(
         FROM_HERE,
-        base::BindOnce(&FakeRemoter::StartFailed, weak_factory_.GetWeakPtr()));
+        base::Bind(&FakeRemoter::StartFailed, weak_factory_.GetWeakPtr()));
   } else {
     base::ThreadTaskRunnerHandle::Get()->PostTask(
         FROM_HERE,
-        base::BindOnce(&FakeRemoter::Started, weak_factory_.GetWeakPtr()));
+        base::Bind(&FakeRemoter::Started, weak_factory_.GetWeakPtr()));
   }
 }
 
@@ -152,8 +152,8 @@
 
 void FakeRemoter::Stop(mojom::RemotingStopReason reason) {
   base::ThreadTaskRunnerHandle::Get()->PostTask(
-      FROM_HERE, base::BindOnce(&FakeRemoter::Stopped,
-                                weak_factory_.GetWeakPtr(), reason));
+      FROM_HERE,
+      base::Bind(&FakeRemoter::Stopped, weak_factory_.GetWeakPtr(), reason));
 }
 
 void FakeRemoter::SendMessageToSink(const std::vector<uint8_t>& message) {}
--- a/media/remoting/integration_test.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/remoting/integration_test.cc	2019-05-17 18:53:34.336000000 +0300
@@ -6,8 +6,8 @@
 
 #include "media/base/test_data_util.h"
 #include "media/remoting/end2end_test_renderer.h"
+#include "media/test/mock_media_source.h"
 #include "media/test/pipeline_integration_test_base.h"
-#include "media/test/test_media_source.h"
 
 namespace media {
 namespace remoting {
@@ -64,7 +64,7 @@
 }
 
 TEST_F(MediaRemotingIntegrationTest, BasicPlayback_MediaSource) {
-  TestMediaSource source("bear-320x240.webm", 219229);
+  MockMediaSource source("bear-320x240.webm", 219229);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -75,7 +75,7 @@
 }
 
 TEST_F(MediaRemotingIntegrationTest, MediaSource_ConfigChange_WebM) {
-  TestMediaSource source("bear-320x240-16x9-aspect.webm", kAppendWholeFile);
+  MockMediaSource source("bear-320x240-16x9-aspect.webm", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
 
   EXPECT_CALL(*this, OnVideoNaturalSizeChange(gfx::Size(640, 360))).Times(1);
--- a/media/remoting/OWNERS	2019-05-01 01:22:52.000000000 +0300
+++ b/media/remoting/OWNERS	2019-05-17 18:53:34.332000000 +0300
@@ -1,4 +1,5 @@
 erickung@chromium.org
 miu@chromium.org
+xjz@chromium.org
 
 # COMPONENT: Internals>Cast>Streaming
--- a/media/remoting/renderer_controller.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/remoting/renderer_controller.h	2019-05-17 18:53:34.336000000 +0300
@@ -17,7 +17,6 @@
 #include "media/base/media_observer.h"
 #include "media/media_buildflags.h"
 #include "media/mojo/interfaces/remoting.mojom.h"
-#include "media/mojo/interfaces/remoting_common.mojom.h"
 #include "media/remoting/metrics.h"
 #include "mojo/public/cpp/bindings/binding.h"
 
--- a/media/renderers/audio_renderer_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/audio_renderer_impl_unittest.cc	2019-05-17 18:53:34.340000000 +0300
@@ -48,6 +48,22 @@
   int value;
 };
 
+class MockMediaClient : public MediaClient {
+ public:
+  MockMediaClient() = default;
+  ~MockMediaClient() override = default;
+
+  void AddSupportedKeySystems(
+      std::vector<std::unique_ptr<KeySystemProperties>>* key_systems) override {
+  }
+  bool IsKeySystemsUpdateNeeded() override { return false; }
+  bool IsSupportedAudioType(const AudioType& type) override { return true; }
+  bool IsSupportedVideoType(const VideoType& type) override { return true; }
+  bool IsSupportedBitstreamAudioCodec(AudioCodec codec) override {
+    return true;
+  }
+};
+
 }  // namespace
 
 // Constants to specify the type of audio data used.
@@ -196,8 +212,6 @@
   }
 
   void InitializeBitstreamFormat() {
-    EXPECT_CALL(media_client_, IsSupportedBitstreamAudioCodec(_))
-        .WillRepeatedly(Return(true));
     SetMediaClient(&media_client_);
 
     hardware_params_.Reset(AudioParameters::AUDIO_BITSTREAM_EAC3,
--- a/media/renderers/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/BUILD.gn	2019-05-17 18:53:34.340000000 +0300
@@ -21,6 +21,8 @@
     "default_decoder_factory.h",
     "default_renderer_factory.cc",
     "default_renderer_factory.h",
+    "flinging_renderer_client_factory.cc",
+    "flinging_renderer_client_factory.h",
     "paint_canvas_video_renderer.cc",
     "paint_canvas_video_renderer.h",
     "remote_playback_client_wrapper.h",
@@ -41,13 +43,12 @@
     "//components/viz/client",
     "//gpu/command_buffer/client:gles2_interface",
     "//gpu/command_buffer/common",
-    "//media:media_buildflags",
     "//media/audio",
     "//media/base",
     "//media/filters",
     "//media/video",
     "//skia",
-    "//third_party/libaom:libaom_buildflags",
+    "//third_party/libaom:av1_buildflags",
     "//third_party/libyuv",
     "//ui/gfx:geometry_skia",
     "//ui/gfx:memory_buffer",
--- a/media/renderers/default_decoder_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/default_decoder_factory.cc	2019-05-17 18:53:34.340000000 +0300
@@ -16,7 +16,7 @@
 #include "media/filters/gpu_video_decoder.h"
 #include "media/media_buildflags.h"
 #include "media/video/gpu_video_accelerator_factories.h"
-#include "third_party/libaom/libaom_buildflags.h"
+#include "third_party/libaom/av1_buildflags.h"
 
 #if !defined(OS_ANDROID)
 #include "media/filters/decrypting_audio_decoder.h"
@@ -27,14 +27,10 @@
 #include "media/filters/fuchsia/fuchsia_video_decoder.h"
 #endif
 
-#if BUILDFLAG(ENABLE_LIBAOM_DECODER)
+#if BUILDFLAG(ENABLE_AV1_DECODER)
 #include "media/filters/aom_video_decoder.h"
 #endif
 
-#if BUILDFLAG(ENABLE_DAV1D_DECODER)
-#include "media/filters/dav1d_video_decoder.h"
-#endif
-
 #if BUILDFLAG(ENABLE_FFMPEG)
 #include "media/filters/ffmpeg_audio_decoder.h"
 #endif
@@ -62,7 +58,7 @@
 #if !defined(OS_ANDROID)
   // DecryptingAudioDecoder is only needed in External Clear Key testing to
   // cover the audio decrypt-and-decode path.
-  if (base::FeatureList::IsEnabled(kExternalClearKeyForTesting)) {
+  if (base::FeatureList::IsEnabled(media::kExternalClearKeyForTesting)) {
     audio_decoders->push_back(
         std::make_unique<DecryptingAudioDecoder>(task_runner, media_log));
   }
@@ -93,7 +89,9 @@
 
   // Perfer an external decoder since one will only exist if it is hardware
   // accelerated.
-  if (gpu_factories && gpu_factories->IsGpuVideoAcceleratorEnabled()) {
+  // Remember that |gpu_factories| will be null if HW video decode is turned
+  // off in chrome://flags.
+  if (gpu_factories) {
     // |gpu_factories_| requires that its entry points be called on its
     // |GetTaskRunner()|. Since |pipeline_| will own decoders created from the
     // factories, require that their message loops are identical.
@@ -101,7 +99,7 @@
 
     // MojoVideoDecoder replaces any VDA for this platform when it's enabled.
     if (external_decoder_factory_ &&
-        base::FeatureList::IsEnabled(kMojoVideoDecoder)) {
+        base::FeatureList::IsEnabled(media::kMojoVideoDecoder)) {
       external_decoder_factory_->CreateVideoDecoders(
           task_runner, gpu_factories, media_log, request_overlay_info_cb,
           target_color_space, video_decoders);
@@ -120,9 +118,7 @@
   video_decoders->push_back(std::make_unique<OffloadingVpxVideoDecoder>());
 #endif
 
-#if BUILDFLAG(ENABLE_DAV1D_DECODER)
-  video_decoders->push_back(std::make_unique<Dav1dVideoDecoder>(media_log));
-#elif BUILDFLAG(ENABLE_LIBAOM_DECODER)
+#if BUILDFLAG(ENABLE_AV1_DECODER)
   video_decoders->push_back(std::make_unique<AomVideoDecoder>(media_log));
 #endif
 
--- a/media/renderers/paint_canvas_video_renderer.cc	2019-05-17 17:45:41.308000000 +0300
+++ b/media/renderers/paint_canvas_video_renderer.cc	2019-05-17 18:53:34.344000000 +0300
@@ -7,7 +7,6 @@
 #include <GLES3/gl3.h>
 #include <limits>
 
-#include "base/bind.h"
 #include "base/macros.h"
 #include "base/memory/ptr_util.h"
 #include "cc/paint/paint_canvas.h"
@@ -212,13 +211,13 @@
   if (mailbox_holder.texture_target != GL_TEXTURE_2D) {
     // TODO(dcastagna): At the moment Skia doesn't support targets different
     // than GL_TEXTURE_2D.  Avoid this copy once
-    // https://code.9oo91e.qjz9zk/p/skia/issues/detail?id=3868 is addressed.
+    // https://code.google.com/p/skia/issues/detail?id=3868 is addressed.
     gl->GenTextures(1, &source_texture);
     DCHECK(source_texture);
     gl->BindTexture(GL_TEXTURE_2D, source_texture);
     PaintCanvasVideoRenderer::CopyVideoFrameSingleTextureToGLTexture(
         gl, video_frame, GL_TEXTURE_2D, source_texture, GL_RGBA, GL_RGBA,
-        GL_UNSIGNED_BYTE, 0, false, false);
+        GL_UNSIGNED_BYTE, 0, true, false);
   } else {
     gl->WaitSyncTokenCHROMIUM(mailbox_holder.sync_token.GetConstData());
     source_texture =
--- a/media/renderers/paint_canvas_video_renderer_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/paint_canvas_video_renderer_unittest.cc	2019-05-17 18:53:34.344000000 +0300
@@ -5,7 +5,6 @@
 #include <GLES3/gl3.h>
 #include <stdint.h>
 
-#include "base/bind.h"
 #include "base/macros.h"
 #include "base/memory/aligned_memory.h"
 #include "base/message_loop/message_loop.h"
--- a/media/renderers/video_overlay_factory.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/video_overlay_factory.cc	2019-05-17 18:53:34.344000000 +0300
@@ -71,8 +71,7 @@
 
 VideoOverlayFactory::VideoOverlayFactory(
     GpuVideoAcceleratorFactories* gpu_factories)
-    : overlay_plane_id_(base::UnguessableToken::Create()),
-      gpu_factories_(gpu_factories) {}
+    : gpu_factories_(gpu_factories) {}
 
 VideoOverlayFactory::~VideoOverlayFactory() = default;
 
@@ -109,8 +108,6 @@
   // video frame in its place.
   frame->metadata()->SetBoolean(VideoFrameMetadata::PROTECTED_VIDEO, true);
   frame->metadata()->SetBoolean(VideoFrameMetadata::HW_PROTECTED, true);
-  frame->metadata()->SetUnguessableToken(VideoFrameMetadata::OVERLAY_PLANE_ID,
-                                         overlay_plane_id_);
   return frame;
 }
 
--- a/media/renderers/video_overlay_factory.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/video_overlay_factory.h	2019-05-17 18:53:34.344000000 +0300
@@ -7,7 +7,6 @@
 
 #include "base/macros.h"
 #include "base/memory/ref_counted.h"
-#include "base/unguessable_token.h"
 #include "media/base/media_export.h"
 
 namespace gfx {
@@ -29,16 +28,11 @@
   ~VideoOverlayFactory();
 
   scoped_refptr<::media::VideoFrame> CreateFrame(const gfx::Size& size);
-  const base::UnguessableToken& overlay_plane_id() const {
-    return overlay_plane_id_;
-  }
 
  private:
   class Texture;
   Texture* GetTexture();
 
-  // |overlay_plane_id_| identifies the instances of VideoOverlayFactory.
-  const base::UnguessableToken overlay_plane_id_;
   ::media::GpuVideoAcceleratorFactories* gpu_factories_;
   std::unique_ptr<Texture> texture_;
 
--- a/media/renderers/video_renderer_impl.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/video_renderer_impl.cc	2019-05-17 18:53:34.344000000 +0300
@@ -4,7 +4,6 @@
 
 #include "media/renderers/video_renderer_impl.h"
 
-#include <algorithm>
 #include <utility>
 
 #include "base/bind.h"
@@ -132,7 +131,7 @@
       painted_first_frame_(false),
       min_buffered_frames_(limits::kMaxVideoFrames),
       weak_factory_(this),
-      cancel_on_flush_weak_factory_(this) {
+      frame_callback_weak_factory_(this) {
   DCHECK(create_video_decoders_cb_);
 }
 
@@ -176,7 +175,7 @@
   pending_read_ = false;
   if (gpu_memory_buffer_pool_)
     gpu_memory_buffer_pool_->Abort();
-  cancel_on_flush_weak_factory_.InvalidateWeakPtrs();
+  frame_callback_weak_factory_.InvalidateWeakPtrs();
   video_decoder_stream_->Reset(
       base::BindOnce(&VideoRendererImpl::OnVideoDecoderStreamResetDone,
                      weak_factory_.GetWeakPtr()));
@@ -270,8 +269,7 @@
     base::TimeTicks deadline_min,
     base::TimeTicks deadline_max,
     bool background_rendering) {
-  TRACE_EVENT_BEGIN1("media", "VideoRendererImpl::Render", "id",
-                     media_log_->id());
+  TRACE_EVENT1("media", "VideoRendererImpl::Render", "id", media_log_->id());
   base::AutoLock auto_lock(lock_);
   DCHECK_EQ(state_, kPlaying);
   last_render_time_ = tick_clock_->NowTicks();
@@ -318,8 +316,6 @@
                      weak_factory_.GetWeakPtr(), result->format(),
                      result->natural_size()));
 
-  TRACE_EVENT_END1("media", "VideoRendererImpl::Render", "frame",
-                   result->AsHumanReadableString());
   return result;
 }
 
@@ -501,7 +497,8 @@
                             static_cast<int>(VideoFrameColorSpaceUMA::MAX) + 1);
   const bool is_eos =
       frame->metadata()->IsTrue(VideoFrameMetadata::END_OF_STREAM);
-  const bool is_before_start_time = !is_eos && IsBeforeStartTime(*frame);
+  const bool is_before_start_time =
+      !is_eos && IsBeforeStartTime(frame->timestamp());
   const bool cant_read = !video_decoder_stream_->CanReadWithoutStalling();
 
   if (is_eos) {
@@ -660,7 +657,7 @@
       pending_read_ = true;
       video_decoder_stream_->Read(
           base::BindOnce(&VideoRendererImpl::FrameReady,
-                         cancel_on_flush_weak_factory_.GetWeakPtr()));
+                         frame_callback_weak_factory_.GetWeakPtr()));
       return;
     case kUninitialized:
     case kInitializing:
@@ -754,36 +751,15 @@
   if (!time_progressing && algorithm_->frames_queued())
     return;
 
-  // Fire ended if we have no more effective frames, only ever had one frame, or
-  // we only have 1 effective frame and there's less than one render interval
-  // left before the ended event should execute.
-  base::TimeDelta ended_event_delay;
-  bool should_render_end_of_stream = false;
-  if (!algorithm_->effective_frames_queued()) {
-    should_render_end_of_stream = true;
-  } else if (algorithm_->frames_queued() == 1u &&
-             algorithm_->average_frame_duration().is_zero()) {
-    should_render_end_of_stream = true;
-  } else if (algorithm_->frames_queued() == 1u &&
-             algorithm_->effective_frames_queued() == 1) {
-    const auto end_delay =
-        std::max(base::TimeDelta(),
-                 algorithm_->last_frame_end_time() - tick_clock_->NowTicks());
-    if (end_delay < algorithm_->render_interval()) {
-      should_render_end_of_stream = true;
-      ended_event_delay = end_delay;
-    }
-  }
-
-  if (!should_render_end_of_stream)
-    return;
-
+  // Fire ended if we have no more effective frames or only ever had one frame.
+  if (!algorithm_->effective_frames_queued() ||
+      (algorithm_->frames_queued() == 1u &&
+       algorithm_->average_frame_duration().is_zero())) {
   rendered_end_of_stream_ = true;
-  task_runner_->PostDelayedTask(
-      FROM_HERE,
+    task_runner_->PostTask(FROM_HERE,
       base::BindOnce(&VideoRendererImpl::OnPlaybackEnded,
-                     cancel_on_flush_weak_factory_.GetWeakPtr()),
-      ended_event_delay);
+                                          weak_factory_.GetWeakPtr()));
+  }
 }
 
 base::TimeTicks VideoRendererImpl::ConvertMediaTimestamp(
@@ -801,15 +777,8 @@
   return current_time[0];
 }
 
-bool VideoRendererImpl::IsBeforeStartTime(const VideoFrame& frame) {
-  // Prefer the actual frame duration over the average if available.
-  base::TimeDelta metadata_frame_duration;
-  if (frame.metadata()->GetTimeDelta(VideoFrameMetadata::FRAME_DURATION,
-                                     &metadata_frame_duration)) {
-    return frame.timestamp() + metadata_frame_duration < start_timestamp_;
-  }
-
-  return frame.timestamp() + video_decoder_stream_->AverageDuration() <
+bool VideoRendererImpl::IsBeforeStartTime(base::TimeDelta timestamp) {
+  return timestamp + video_decoder_stream_->AverageDuration() <
          start_timestamp_;
 }
 
--- a/media/renderers/video_renderer_impl.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/video_renderer_impl.h	2019-05-17 18:53:34.344000000 +0300
@@ -155,7 +155,7 @@
 
   // Helper method for checking if a frame timestamp plus the frame's expected
   // duration is before |start_timestamp_|.
-  bool IsBeforeStartTime(const VideoFrame& frame);
+  bool IsBeforeStartTime(base::TimeDelta timestamp);
 
   // Attempts to remove frames which are no longer effective for rendering when
   // |buffering_state_| == BUFFERING_HAVE_NOTHING or |was_background_rendering_|
@@ -311,7 +311,7 @@
   // This is useful when doing video frame copies asynchronously since we
   // want to discard video frames that might be received after the stream has
   // been reset.
-  base::WeakPtrFactory<VideoRendererImpl> cancel_on_flush_weak_factory_;
+  base::WeakPtrFactory<VideoRendererImpl> frame_callback_weak_factory_;
 
   DISALLOW_COPY_AND_ASSIGN(VideoRendererImpl);
 };
--- a/media/renderers/video_renderer_impl_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/video_renderer_impl_unittest.cc	2019-05-17 18:53:34.344000000 +0300
@@ -11,6 +11,7 @@
 #include "base/callback.h"
 #include "base/callback_helpers.h"
 #include "base/containers/circular_deque.h"
+#include "base/debug/stack_trace.h"
 #include "base/macros.h"
 #include "base/memory/ptr_util.h"
 #include "base/run_loop.h"
--- a/media/renderers/video_resource_updater.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/video_resource_updater.cc	2019-05-17 18:53:34.344000000 +0300
@@ -19,18 +19,15 @@
 #include "base/trace_event/memory_dump_manager.h"
 #include "base/trace_event/process_memory_dump.h"
 #include "base/trace_event/trace_event.h"
-#include "base/unguessable_token.h"
 #include "build/build_config.h"
 #include "cc/base/math_util.h"
 #include "cc/paint/skia_paint_canvas.h"
 #include "components/viz/client/client_resource_provider.h"
 #include "components/viz/client/shared_bitmap_reporter.h"
 #include "components/viz/common/gpu/context_provider.h"
-#include "components/viz/common/gpu/raster_context_provider.h"
 #include "components/viz/common/quads/render_pass.h"
 #include "components/viz/common/quads/stream_video_draw_quad.h"
 #include "components/viz/common/quads/texture_draw_quad.h"
-#include "components/viz/common/quads/video_hole_draw_quad.h"
 #include "components/viz/common/quads/yuv_video_draw_quad.h"
 #include "components/viz/common/resources/bitmap_allocation.h"
 #include "components/viz/common/resources/resource_sizes.h"
@@ -76,10 +73,11 @@
             return VideoFrameResourceType::STREAM_TEXTURE;
           FALLTHROUGH;
         case GL_TEXTURE_2D:
-        case GL_TEXTURE_RECTANGLE_ARB:
           return (format == PIXEL_FORMAT_XRGB)
                      ? VideoFrameResourceType::RGB
                      : VideoFrameResourceType::RGBA_PREMULTIPLIED;
+        case GL_TEXTURE_RECTANGLE_ARB:
+          return VideoFrameResourceType::RGB;
         default:
           NOTREACHED();
           break;
@@ -335,16 +333,11 @@
                         viz::ResourceFormat format,
                         const gfx::ColorSpace& color_space,
                         bool use_gpu_memory_buffer_resources,
-                        viz::ContextProvider* context_provider,
-                        viz::RasterContextProvider* raster_context_provider)
+                        viz::ContextProvider* context_provider)
       : PlaneResource(plane_resource_id, size, format, /*is_software=*/false),
-        context_provider_(context_provider),
-        raster_context_provider_(raster_context_provider) {
-    DCHECK(context_provider_ || raster_context_provider_);
-    const gpu::Capabilities& caps =
-        raster_context_provider_
-            ? raster_context_provider_->ContextCapabilities()
-            : context_provider_->ContextCapabilities();
+        context_provider_(context_provider) {
+    DCHECK(context_provider_);
+    const gpu::Capabilities& caps = context_provider_->ContextCapabilities();
     overlay_candidate_ = use_gpu_memory_buffer_resources &&
                          caps.texture_storage_image &&
                          IsGpuMemoryBufferFormatSupported(format);
@@ -355,17 +348,24 @@
       texture_target_ = gpu::GetBufferTextureTarget(gfx::BufferUsage::SCANOUT,
                                                     BufferFormat(format), caps);
     }
-    auto* sii = SharedImageInterface();
+    auto* sii = context_provider_->SharedImageInterface();
+    DCHECK(sii);
+    auto* gl = context_provider_->ContextGL();
+    DCHECK(gl);
+
     mailbox_ =
         sii->CreateSharedImage(format, size, color_space, shared_image_usage);
-    ContextGL()->WaitSyncTokenCHROMIUM(
-        sii->GenUnverifiedSyncToken().GetConstData());
+    gl->WaitSyncTokenCHROMIUM(sii->GenUnverifiedSyncToken().GetConstData());
   }
 
   ~HardwarePlaneResource() override {
+    auto* sii = context_provider_->SharedImageInterface();
+    DCHECK(sii);
+    auto* gl = context_provider_->ContextGL();
+    DCHECK(gl);
     gpu::SyncToken sync_token;
-    ContextGL()->GenUnverifiedSyncTokenCHROMIUM(sync_token.GetData());
-    SharedImageInterface()->DestroySharedImage(sync_token, mailbox_);
+    gl->GenUnverifiedSyncTokenCHROMIUM(sync_token.GetData());
+    sii->DestroySharedImage(sync_token, mailbox_);
   }
 
   const gpu::Mailbox& mailbox() const { return mailbox_; }
@@ -374,23 +374,7 @@
   bool overlay_candidate() const { return overlay_candidate_; }
 
  private:
-  gpu::SharedImageInterface* SharedImageInterface() {
-    auto* sii = raster_context_provider_
-                    ? raster_context_provider_->SharedImageInterface()
-                    : context_provider_->SharedImageInterface();
-    DCHECK(sii);
-    return sii;
-  }
-
-  gpu::gles2::GLES2Interface* ContextGL() {
-    auto* gl = raster_context_provider_ ? raster_context_provider_->ContextGL()
-                                        : context_provider_->ContextGL();
-    DCHECK(gl);
-    return gl;
-  }
-
   viz::ContextProvider* const context_provider_;
-  viz::RasterContextProvider* const raster_context_provider_;
   gpu::Mailbox mailbox_;
   GLenum texture_target_ = GL_TEXTURE_2D;
   bool overlay_candidate_ = false;
@@ -412,7 +396,6 @@
 
 VideoResourceUpdater::VideoResourceUpdater(
     viz::ContextProvider* context_provider,
-    viz::RasterContextProvider* raster_context_provider,
     viz::SharedBitmapReporter* shared_bitmap_reporter,
     viz::ClientResourceProvider* resource_provider,
     bool use_stream_video_draw_quad,
@@ -420,7 +403,6 @@
     bool use_r16_texture,
     int max_resource_size)
     : context_provider_(context_provider),
-      raster_context_provider_(raster_context_provider),
       shared_bitmap_reporter_(shared_bitmap_reporter),
       resource_provider_(resource_provider),
       use_stream_video_draw_quad_(use_stream_video_draw_quad),
@@ -429,8 +411,7 @@
       max_resource_size_(max_resource_size),
       tracing_id_(g_next_video_resource_updater_id.GetNext()),
       weak_ptr_factory_(this) {
-  DCHECK(context_provider_ || raster_context_provider_ ||
-         shared_bitmap_reporter_);
+  DCHECK(context_provider_ || shared_bitmap_reporter_);
 
   base::trace_event::MemoryDumpManager::GetInstance()->RegisterDumpProvider(
       this, "media::VideoResourceUpdater", base::ThreadTaskRunnerHandle::Get());
@@ -502,9 +483,6 @@
   const float tex_height_scale =
       static_cast<float>(visible_rect.height()) / coded_size.height();
 
-  const gfx::PointF uv_top_left(0.f, 0.f);
-  const gfx::PointF uv_bottom_right(tex_width_scale, tex_height_scale);
-
   switch (frame_resource_type_) {
     case VideoFrameResourceType::YUV: {
       const gfx::Size ya_tex_size = coded_size;
@@ -578,7 +556,8 @@
         break;
       bool premultiplied_alpha =
           frame_resource_type_ == VideoFrameResourceType::RGBA_PREMULTIPLIED;
-
+      gfx::PointF uv_top_left(0.f, 0.f);
+      gfx::PointF uv_bottom_right(tex_width_scale, tex_height_scale);
       float opacity[] = {1.0f, 1.0f, 1.0f, 1.0f};
       bool flipped = false;
       bool nearest_neighbor = false;
@@ -591,19 +570,6 @@
           protected_video_type = ui::ProtectedVideoType::kSoftwareProtected;
       }
 
-      base::UnguessableToken overlay_plane_id;
-      if (frame->metadata()->GetUnguessableToken(
-              VideoFrameMetadata::OVERLAY_PLANE_ID, &overlay_plane_id)) {
-        // Valid |overlay_plane_id| is present, this frame is generated by cast
-        // and we should punch the video hole accordingly.
-        auto* video_hole_quad =
-            render_pass->CreateAndAppendDrawQuad<viz::VideoHoleDrawQuad>();
-        video_hole_quad->SetNew(shared_quad_state, quad_rect, visible_quad_rect,
-                                overlay_plane_id);
-      } else {
-        // TODO(guohuideng): Consider replacing TextureDrawQuad with
-        // VideoHoleDrawQuad here if the quad is for video hole punching
-        // purpose.
         auto* texture_quad =
             render_pass->CreateAndAppendDrawQuad<viz::TextureDrawQuad>();
         texture_quad->SetNew(shared_quad_state, quad_rect, visible_quad_rect,
@@ -615,20 +581,19 @@
         for (viz::ResourceId resource_id : texture_quad->resources) {
           resource_provider_->ValidateResource(resource_id);
         }
-      }
-
       break;
     }
     case VideoFrameResourceType::STREAM_TEXTURE: {
       DCHECK_EQ(frame_resources_.size(), 1u);
       if (frame_resources_.size() < 1u)
         break;
+      gfx::Transform scale;
+      scale.Scale(tex_width_scale, tex_height_scale);
       auto* stream_video_quad =
           render_pass->CreateAndAppendDrawQuad<viz::StreamVideoDrawQuad>();
       stream_video_quad->SetNew(shared_quad_state, quad_rect, visible_quad_rect,
                                 needs_blending, frame_resources_[0].id,
-                                frame_resources_[0].size_in_pixels, uv_top_left,
-                                uv_bottom_right);
+                                frame_resources_[0].size_in_pixels, scale);
       for (viz::ResourceId resource_id : stream_video_quad->resources) {
         resource_provider_->ValidateResource(resource_id);
       }
@@ -654,10 +619,8 @@
 
 viz::ResourceFormat VideoResourceUpdater::YuvResourceFormat(
     int bits_per_channel) {
-  DCHECK(raster_context_provider_ || context_provider_);
-  const auto& caps = raster_context_provider_
-                         ? raster_context_provider_->ContextCapabilities()
-                         : context_provider_->ContextCapabilities();
+  DCHECK(context_provider_);
+  const auto& caps = context_provider_->ContextCapabilities();
   if (caps.disable_one_component_textures)
     return viz::RGBA_8888;
   if (bits_per_channel <= 8)
@@ -721,8 +684,7 @@
   } else {
     all_resources_.push_back(std::make_unique<HardwarePlaneResource>(
         plane_resource_id, plane_size, format, color_space,
-        use_gpu_memory_buffer_resources_, context_provider_,
-        raster_context_provider_));
+        use_gpu_memory_buffer_resources_, context_provider_));
   }
   return all_resources_.back().get();
 }
@@ -749,8 +711,7 @@
   DCHECK_EQ(hardware_resource->texture_target(),
             static_cast<GLenum>(GL_TEXTURE_2D));
 
-  auto* gl = raster_context_provider_ ? raster_context_provider_->ContextGL()
-                                      : context_provider_->ContextGL();
+  gpu::gles2::GLES2Interface* gl = context_provider_->ContextGL();
 
   gl->WaitSyncTokenCHROMIUM(mailbox_holder.sync_token.GetConstData());
   // TODO(piman): convert to CreateAndTexStorage2DSharedImageCHROMIUM once
@@ -785,7 +746,7 @@
     scoped_refptr<VideoFrame> video_frame) {
   TRACE_EVENT0("cc", "VideoResourceUpdater::CreateForHardwarePlanes");
   DCHECK(video_frame->HasTextures());
-  if (!context_provider_ && !raster_context_provider_)
+  if (!context_provider_)
     return VideoFrameExternalResources();
 
   VideoFrameExternalResources external_resources;
@@ -983,9 +944,7 @@
             video_frame.get(), upload_pixels_.get(), bytes_per_row);
 
         // Copy pixels into texture.
-        auto* gl = raster_context_provider_
-                       ? raster_context_provider_->ContextGL()
-                       : context_provider_->ContextGL();
+        auto* gl = context_provider_->ContextGL();
 
         const gfx::Size& plane_size = hardware_resource->resource_size();
         {
@@ -1013,10 +972,7 @@
       HardwarePlaneResource* hardware_resource = plane_resource->AsHardware();
       external_resources.type = VideoFrameResourceType::RGBA;
       gpu::SyncToken sync_token;
-      auto* gl = raster_context_provider_
-                     ? raster_context_provider_->ContextGL()
-                     : context_provider_->ContextGL();
-      GenerateCompositorSyncToken(gl, &sync_token);
+      GenerateCompositorSyncToken(context_provider_->ContextGL(), &sync_token);
       transferable_resource = viz::TransferableResource::MakeGLOverlay(
           hardware_resource->mailbox(), GL_LINEAR,
           hardware_resource->texture_target(), sync_token,
@@ -1147,8 +1103,7 @@
 
     // Copy pixels into texture. TexSubImage2D() is applicable because
     // |yuv_resource_format| is LUMINANCE_F16, R16_EXT, LUMINANCE_8 or RED_8.
-    auto* gl = raster_context_provider_ ? raster_context_provider_->ContextGL()
-                                        : context_provider_->ContextGL();
+    auto* gl = context_provider_->ContextGL();
     DCHECK(GLSupportsFormat(plane_resource_format));
     {
       HardwarePlaneResource::ScopedTexture scope(gl, plane_resource);
@@ -1165,9 +1120,7 @@
 
   // Set the sync token otherwise resource is assumed to be synchronized.
   gpu::SyncToken sync_token;
-  auto* gl = raster_context_provider_ ? raster_context_provider_->ContextGL()
-                                      : context_provider_->ContextGL();
-  GenerateCompositorSyncToken(gl, &sync_token);
+  GenerateCompositorSyncToken(context_provider_->ContextGL(), &sync_token);
 
   for (size_t i = 0; i < plane_resources.size(); ++i) {
     HardwarePlaneResource* plane_resource = plane_resources[i]->AsHardware();
@@ -1196,9 +1149,7 @@
     return;
 
   // The video frame will insert a wait on the previous release sync token.
-  auto* gl = raster_context_provider_ ? raster_context_provider_->ContextGL()
-                                      : context_provider_->ContextGL();
-  SyncTokenClientImpl client(gl, sync_token);
+  SyncTokenClientImpl client(context_provider_->ContextGL(), sync_token);
   video_frame->UpdateReleaseSyncToken(&client);
 }
 
@@ -1215,9 +1166,8 @@
     return;
 
   if (context_provider_ && sync_token.HasData()) {
-    auto* gl = raster_context_provider_ ? raster_context_provider_->ContextGL()
-                                        : context_provider_->ContextGL();
-    gl->WaitSyncTokenCHROMIUM(sync_token.GetConstData());
+    context_provider_->ContextGL()->WaitSyncTokenCHROMIUM(
+        sync_token.GetConstData());
   }
 
   if (lost_resource) {
--- a/media/renderers/video_resource_updater.h	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/video_resource_updater.h	2019-05-17 18:53:34.344000000 +0300
@@ -33,7 +33,6 @@
 namespace viz {
 class ClientResourceProvider;
 class ContextProvider;
-class RasterContextProvider;
 class RenderPass;
 class SharedBitmapReporter;
 }  // namespace viz
@@ -80,7 +79,6 @@
   // compositing |shared_bitmap_reporter| should be provided. If there is a
   // non-null |context_provider| we assume GPU compositing.
   VideoResourceUpdater(viz::ContextProvider* context_provider,
-                       viz::RasterContextProvider* raster_context_provider,
                        viz::SharedBitmapReporter* shared_bitmap_reporter,
                        viz::ClientResourceProvider* resource_provider,
                        bool use_stream_video_draw_quad,
@@ -133,9 +131,7 @@
     gfx::Size size_in_pixels;
   };
 
-  bool software_compositor() const {
-    return context_provider_ == nullptr && raster_context_provider_ == nullptr;
-  }
+  bool software_compositor() const { return context_provider_ == nullptr; }
 
   // Obtain a resource of the right format by either recycling an
   // unreferenced but appropriately formatted resource, or by
@@ -187,7 +183,6 @@
                     base::trace_event::ProcessMemoryDump* pmd) override;
 
   viz::ContextProvider* const context_provider_;
-  viz::RasterContextProvider* const raster_context_provider_;
   viz::SharedBitmapReporter* const shared_bitmap_reporter_;
   viz::ClientResourceProvider* const resource_provider_;
   const bool use_stream_video_draw_quad_;
--- a/media/renderers/video_resource_updater_unittest.cc	2019-05-01 01:22:52.000000000 +0300
+++ b/media/renderers/video_resource_updater_unittest.cc	2019-05-17 18:53:34.344000000 +0300
@@ -86,16 +86,14 @@
   std::unique_ptr<VideoResourceUpdater> CreateUpdaterForHardware(
       bool use_stream_video_draw_quad = false) {
     return std::make_unique<VideoResourceUpdater>(
-        context_provider_.get(), /*raster_context_provider=*/nullptr, nullptr,
-        resource_provider_.get(), use_stream_video_draw_quad,
-        /*use_gpu_memory_buffer_resources=*/false,
+        context_provider_.get(), nullptr, resource_provider_.get(),
+        use_stream_video_draw_quad, /*use_gpu_memory_buffer_resources=*/false,
         /*use_r16_texture=*/use_r16_texture_, /*max_resource_size=*/10000);
   }
 
   std::unique_ptr<VideoResourceUpdater> CreateUpdaterForSoftware() {
     return std::make_unique<VideoResourceUpdater>(
-        /*context_provider=*/nullptr, /*raster_context_provider=*/nullptr,
-        &shared_bitmap_reporter_, resource_provider_.get(),
+        nullptr, &shared_bitmap_reporter_, resource_provider_.get(),
         /*use_stream_video_draw_quad=*/false,
         /*use_gpu_memory_buffer_resources=*/false,
         /*use_r16_texture=*/false,
--- a/media/test/BUILD.gn	2019-05-01 01:22:52.000000000 +0300
+++ b/media/test/BUILD.gn	2019-05-17 18:53:34.344000000 +0300
@@ -31,10 +31,10 @@
     sources = [
       "fake_encrypted_media.cc",
       "fake_encrypted_media.h",
+      "mock_media_source.cc",
+      "mock_media_source.h",
       "pipeline_integration_test_base.cc",
       "pipeline_integration_test_base.h",
-      "test_media_source.cc",
-      "test_media_source.h",
     ]
 
     configs += [ "//media:media_config" ]
@@ -42,11 +42,10 @@
     deps = [
       "//base",
       "//base/test:test_support",
-      "//media:media_buildflags",
       "//media:test_support",
       "//testing/gmock",
       "//testing/gtest",
-      "//third_party/libaom:libaom_buildflags",
+      "//third_party/libaom:av1_buildflags",
       "//url",
     ]
   }
@@ -68,9 +67,9 @@
     deps = [
       ":pipeline_integration_test_base",
       "//base",
-      "//media:media_buildflags",
       "//media:test_support",
       "//media/mojo/clients",
+      "//third_party/libaom:av1_buildflags",
 
       # Needed for the opus_config
       "//third_party/opus",
@@ -122,16 +121,14 @@
       ":pipeline_integration_test_base",
       "//base",
       "//base/test:test_support",
-      "//media:media_buildflags",
       "//media:test_support",
       "//media/mojo/clients",
       "//media/mojo/interfaces",
-      "//media/mojo/interfaces:constants",
       "//media/mojo/services",
-      "//media/mojo/services:media_manifest",
-      "//services/service_manager/public/cpp",
+      "//media/mojo/services:media_pipeline_integration_unittests_catalog_source",
       "//services/service_manager/public/cpp/test:test_support",
       "//testing/gtest",
+      "//third_party/libaom:av1_buildflags",
       "//ui/gfx:test_support",
       "//ui/gfx/geometry",
 
@@ -220,6 +217,7 @@
       # header for pipeline_integration_test_base.h.  This should be
       # moved into the .cc file to avoid the extra dependency here.
       "//testing/gmock",
+      "//third_party/libaom:av1_buildflags",
       "//ui/gfx:test_support",
     ]
 
--- a/media/test/pipeline_integration_fuzzertest.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/test/pipeline_integration_fuzzertest.cc	2019-05-17 18:53:35.000000000 +0300
@@ -19,9 +19,9 @@
 #include "media/base/media.h"
 #include "media/base/media_switches.h"
 #include "media/base/pipeline_status.h"
-#include "media/media_buildflags.h"
+#include "media/test/mock_media_source.h"
 #include "media/test/pipeline_integration_test_base.h"
-#include "media/test/test_media_source.h"
+#include "third_party/libaom/av1_buildflags.h"
 
 namespace {
 
@@ -191,7 +191,7 @@
     scoped_refptr<media::DecoderBuffer> buffer(
         DecoderBuffer::CopyFrom(data, size));
 
-    TestMediaSource source(buffer, mimetype, kAppendWholeFile);
+    MockMediaSource source(buffer, mimetype, kAppendWholeFile);
 
     // Prevent timeout in the case of not enough media appended to complete
     // demuxer initialization, yet no error in the media appended.  The
@@ -203,9 +203,9 @@
         base::Bind(&OnEncryptedMediaInitData, this));
 
     // Allow parsing to either pass or fail without emitting a gtest failure
-    // from TestMediaSource.
+    // from MockMediaSource.
     source.set_expected_append_result(
-        TestMediaSource::ExpectedAppendResult::kSuccessOrFailure);
+        MockMediaSource::ExpectedAppendResult::kSuccessOrFailure);
 
     // TODO(wolenetz): Vary the behavior (abort/remove/seek/endOfStream/Append
     // in pieces/append near play-head/vary append mode/etc), perhaps using
@@ -248,10 +248,8 @@
   FuzzerVariant variant = PIPELINE_FUZZER_VARIANT;
 
   if (variant == SRC) {
-    {
       media::ProgressivePipelineIntegrationFuzzerTest test;
       test.RunTest(data, size);
-    }
   } else {
     // Sequentially fuzz with new and old MSE buffering APIs.  See
     // https://crbug.com/718641.
--- a/media/test/pipeline_integration_perftest.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/test/pipeline_integration_perftest.cc	2019-05-17 18:53:35.000000000 +0300
@@ -68,7 +68,7 @@
 
 // For simplicity we only test codecs with above 2% daily usage as measured by
 // the Media.AudioCodec histogram.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     /* no prefix */,
     ClocklessAudioPipelineIntegrationPerfTest,
     testing::ValuesIn(kAudioTestFiles));
--- a/media/test/pipeline_integration_test_base.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/test/pipeline_integration_test_base.cc	2019-05-17 18:53:35.000000000 +0300
@@ -12,7 +12,6 @@
 #include "base/memory/ref_counted.h"
 #include "base/run_loop.h"
 #include "base/single_thread_task_runner.h"
-#include "base/time/time.h"
 #include "media/base/media_log.h"
 #include "media/base/media_switches.h"
 #include "media/base/media_tracks.h"
@@ -23,17 +22,13 @@
 #include "media/renderers/audio_renderer_impl.h"
 #include "media/renderers/renderer_impl.h"
 #include "media/test/fake_encrypted_media.h"
-#include "media/test/test_media_source.h"
-#include "third_party/libaom/libaom_buildflags.h"
+#include "media/test/mock_media_source.h"
+#include "third_party/libaom/av1_buildflags.h"
 
-#if BUILDFLAG(ENABLE_LIBAOM_DECODER)
+#if BUILDFLAG(ENABLE_AV1_DECODER)
 #include "media/filters/aom_video_decoder.h"
 #endif
 
-#if BUILDFLAG(ENABLE_DAV1D_DECODER)
-#include "media/filters/dav1d_video_decoder.h"
-#endif
-
 #if BUILDFLAG(ENABLE_FFMPEG)
 #include "media/filters/ffmpeg_audio_decoder.h"
 #include "media/filters/ffmpeg_demuxer.h"
@@ -73,9 +68,7 @@
   video_decoders.push_back(std::make_unique<OffloadingVpxVideoDecoder>());
 #endif
 
-#if BUILDFLAG(ENABLE_DAV1D_DECODER)
-  video_decoders.push_back(std::make_unique<Dav1dVideoDecoder>(media_log));
-#elif BUILDFLAG(ENABLE_LIBAOM_DECODER)
+#if BUILDFLAG(ENABLE_AV1_DECODER)
   video_decoders.push_back(std::make_unique<AomVideoDecoder>(media_log));
 #endif
 
@@ -543,7 +536,7 @@
     return;
   last_frame_ = frame;
   DVLOG(3) << __func__ << " pts=" << frame->timestamp().InSecondsF();
-  VideoFrame::HashFrameForTesting(&md5_context_, *frame.get());
+  VideoFrame::HashFrameForTesting(&md5_context_, frame);
 }
 
 void PipelineIntegrationTestBase::CheckDuration() {
@@ -584,28 +577,26 @@
 }
 
 PipelineStatus PipelineIntegrationTestBase::StartPipelineWithMediaSource(
-    TestMediaSource* source) {
+    MockMediaSource* source) {
   return StartPipelineWithMediaSource(source, kNormal, nullptr);
 }
 
 PipelineStatus PipelineIntegrationTestBase::StartPipelineWithEncryptedMedia(
-    TestMediaSource* source,
+    MockMediaSource* source,
     FakeEncryptedMedia* encrypted_media) {
   return StartPipelineWithMediaSource(source, kNormal, encrypted_media);
 }
 
 PipelineStatus PipelineIntegrationTestBase::StartPipelineWithMediaSource(
-    TestMediaSource* source,
+    MockMediaSource* source,
     uint8_t test_type,
     FakeEncryptedMedia* encrypted_media) {
   ParseTestTypeFlags(test_type);
 
-  if (fuzzing_) {
+  if (fuzzing_)
     EXPECT_CALL(*source, InitSegmentReceivedMock(_)).Times(AnyNumber());
-    EXPECT_CALL(*source, OnParseWarningMock(_)).Times(AnyNumber());
-  } else if (!(test_type & kExpectDemuxerFailure)) {
+  else if (!(test_type & kExpectDemuxerFailure))
     EXPECT_CALL(*source, InitSegmentReceivedMock(_)).Times(AtLeast(1));
-  }
 
   EXPECT_CALL(*this, OnMetadata(_))
       .Times(AtMost(1))
--- a/media/test/pipeline_integration_test_base.h	2019-05-01 01:22:53.000000000 +0300
+++ b/media/test/pipeline_integration_test_base.h	2019-05-17 18:53:35.000000000 +0300
@@ -10,7 +10,6 @@
 
 #include "base/callback_forward.h"
 #include "base/md5.h"
-#include "base/run_loop.h"
 #include "base/test/scoped_feature_list.h"
 #include "base/test/scoped_task_environment.h"
 #include "media/audio/clockless_audio_sink.h"
@@ -36,7 +35,7 @@
 namespace media {
 
 class FakeEncryptedMedia;
-class TestMediaSource;
+class MockMediaSource;
 
 // Empty MD5 hash string.  Used to verify empty video tracks.
 extern const char kNullVideoHash[];
@@ -159,10 +158,6 @@
   bool webaudio_attached_;
   bool mono_output_;
   bool fuzzing_;
-#if defined(ADDRESS_SANITIZER) || defined(UNDEFINED_SANITIZER)
-  // TODO(https://crbug.com/924030): ASAN causes Run() timeouts to be reached.
-  const base::RunLoop::ScopedDisableRunTimeoutForTest disable_run_timeout_;
-#endif
   std::unique_ptr<Demuxer> demuxer_;
   std::unique_ptr<DataSource> data_source_;
   std::unique_ptr<PipelineImpl> pipeline_;
@@ -196,12 +191,12 @@
       CreateAudioDecodersCB prepend_audio_decoders_cb =
           CreateAudioDecodersCB());
 
-  PipelineStatus StartPipelineWithMediaSource(TestMediaSource* source);
+  PipelineStatus StartPipelineWithMediaSource(MockMediaSource* source);
   PipelineStatus StartPipelineWithEncryptedMedia(
-      TestMediaSource* source,
+      MockMediaSource* source,
       FakeEncryptedMedia* encrypted_media);
   PipelineStatus StartPipelineWithMediaSource(
-      TestMediaSource* source,
+      MockMediaSource* source,
       uint8_t test_type,
       FakeEncryptedMedia* encrypted_media);
 
--- a/media/test/pipeline_integration_test.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/test/pipeline_integration_test.cc	2019-05-17 18:53:35.000000000 +0300
@@ -32,18 +32,17 @@
 #include "media/media_buildflags.h"
 #include "media/renderers/renderer_impl.h"
 #include "media/test/fake_encrypted_media.h"
+#include "media/test/mock_media_source.h"
 #include "media/test/pipeline_integration_test_base.h"
-#include "media/test/test_media_source.h"
 #include "testing/gmock/include/gmock/gmock.h"
+#include "third_party/libaom/av1_buildflags.h"
 #include "url/gurl.h"
 
 #if defined(MOJO_RENDERER)
 #include "media/mojo/clients/mojo_renderer.h"
-#include "media/mojo/interfaces/constants.mojom.h"  // nogncheck
 #include "media/mojo/interfaces/interface_factory.mojom.h"
 #include "media/mojo/interfaces/renderer.mojom.h"
-#include "media/mojo/services/media_manifest.h"                    // nogncheck
-#include "services/service_manager/public/cpp/manifest_builder.h"  // nogncheck
+#include "media/mojo/services/media_pipeline_integration_unittests_catalog_source.h"  // nogncheck
 #include "services/service_manager/public/cpp/test/test_service.h"  // nogncheck
 #include "services/service_manager/public/cpp/test/test_service_manager.h"  // nogncheck
 
@@ -77,7 +76,7 @@
 // To use MAYBE_EME in a parameterized test fixture, don't directly use TEST_P
 // (because "MAYBE_EME" will literally be used as part of the test name).
 // Instead, use this wrapper macro to ensure that this first level of  parameter
-// expansion is done before the INSTANTIATE_TEST_SUITE_P macro is processed.
+// expansion is done before the INSTANTIATE_TEST_CASE_P macro is processed.
 // For precedent, see similar IN_PROC_BROWSER_TEST_P definition.
 #define MAYBE_EME_TEST_P(fixture, test) TEST_P(fixture, test)
 
@@ -412,20 +411,13 @@
 //               preferably by eliminating multiple inheritance here which is
 //               banned by Google C++ style.
 #if defined(MOJO_RENDERER) && defined(ENABLE_MOJO_PIPELINE_INTEGRATION_TEST)
-const char kTestServiceName[] = "media_pipeline_integration_unittests";
-
 class PipelineIntegrationTest : public testing::Testing,
                                 public PipelineIntegrationTestBase {
  public:
   PipelineIntegrationTest()
-      : test_service_manager_(
-            {GetMediaManifest(),
-             service_manager::ManifestBuilder()
-                 .WithServiceName(kTestServiceName)
-                 .RequireCapability(mojom::kMediaServiceName, "media:media")
-                 .Build()}),
-        test_service_(
-            test_service_manager_.RegisterTestInstance(kTestServiceName)) {}
+      : test_service_manager_(test::CreatePipelineIntegrationTestCatalog()),
+        test_service_(test_service_manager_.RegisterTestInstance(
+            "media_pipeline_integration_shelltests")) {}
 
   void SetUp() override {
     InitializeMediaLibrary();
@@ -435,7 +427,7 @@
   std::unique_ptr<Renderer> CreateRenderer(
       CreateVideoDecodersCB prepend_video_decoders_cb,
       CreateAudioDecodersCB prepend_audio_decoders_cb) override {
-    test_service_.connector()->BindInterface(mojom::kMediaServiceName,
+    test_service_.connector()->BindInterface("media",
                                              &media_interface_factory_);
 
     mojom::RendererPtr mojo_renderer;
@@ -464,7 +456,7 @@
                           base::TimeDelta seek_time,
                           int seek_file_position,
                           int seek_append_size) {
-    TestMediaSource source(filename, initial_append_size);
+    MockMediaSource source(filename, initial_append_size);
 
     if (StartPipelineWithMediaSource(&source, kNoClockless, nullptr) !=
         PIPELINE_OK) {
@@ -536,7 +528,7 @@
   void PlayToEnd() {
     MSEPlaybackTestData data = GetParam();
 
-    TestMediaSource source(data.filename, data.append_bytes);
+    MockMediaSource source(data.filename, data.append_bytes);
     ASSERT_EQ(PIPELINE_OK,
               StartPipelineWithMediaSource(&source, kNormal, nullptr));
     source.EndOfStream();
@@ -581,7 +573,7 @@
 
 const PlaybackTestData kOpenCodecsTests[] = {{"bear-vp9-i422.webm", 0, 2736}};
 
-INSTANTIATE_TEST_SUITE_P(OpenCodecs,
+INSTANTIATE_TEST_CASE_P(OpenCodecs,
                          BasicPlaybackTest,
                          testing::ValuesIn(kOpenCodecsTests));
 
@@ -595,7 +587,7 @@
 };
 
 // TODO(chcunningham): Migrate other basic playback tests to TEST_P.
-INSTANTIATE_TEST_SUITE_P(ProprietaryCodecs,
+INSTANTIATE_TEST_CASE_P(ProprietaryCodecs,
                          BasicPlaybackTest,
                          testing::ValuesIn(kADTSTests));
 
@@ -607,7 +599,7 @@
 };
 
 // TODO(chcunningham): Migrate other basic MSE playback tests to TEST_P.
-INSTANTIATE_TEST_SUITE_P(ProprietaryCodecs,
+INSTANTIATE_TEST_CASE_P(ProprietaryCodecs,
                          BasicMSEPlaybackTest,
                          testing::ValuesIn(kMediaSourceADTSTests));
 
@@ -656,7 +648,7 @@
     // TODO(wolenetz): Switch back to 'segments' mode once we have some
     // incubation of a way to flexibly allow playback through unbuffered
     // regions. Known test media requiring sequence mode: MP3-in-MP2T
-    TestMediaSource source(file_one.filename, file_one.append_bytes, true);
+    MockMediaSource source(file_one.filename, file_one.append_bytes, true);
     ASSERT_EQ(PIPELINE_OK,
               StartPipelineWithMediaSource(&source, kNormal, nullptr));
     source.EndOfStream();
@@ -765,14 +757,14 @@
 #endif  // BUILDFLAG(USE_PROPRIETARY_CODECS)
 };
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     AudioOnly,
     MSEChangeTypeTest,
     testing::Combine(testing::ValuesIn(kMediaSourceAudioFiles),
                      testing::ValuesIn(kMediaSourceAudioFiles)),
     MSEChangeTypeTest::PrintToStringParamName());
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     VideoOnly,
     MSEChangeTypeTest,
     testing::Combine(testing::ValuesIn(kMediaSourceVideoFiles),
@@ -1166,7 +1158,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlaybackOpusWebmTrimmingHashed) {
-  TestMediaSource source("opus-trimming-test.webm", kAppendWholeFile);
+  MockMediaSource source("opus-trimming-test.webm", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithMediaSource(&source, kHashed, nullptr));
   source.EndOfStream();
@@ -1195,7 +1187,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlaybackOpusMp4TrimmingHashed) {
-  TestMediaSource source("opus-trimming-test.mp4", kAppendWholeFile);
+  MockMediaSource source("opus-trimming-test.mp4", kAppendWholeFile);
 
   // TODO(dalecurtis): The test clip currently does not have the edit list
   // entries required to achieve correctness here, so we're manually specifying
@@ -1305,7 +1297,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlaybackOpusPrerollExceedsCodecDelay) {
-  TestMediaSource source("bear-opus.webm", kAppendWholeFile);
+  MockMediaSource source("bear-opus.webm", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithMediaSource(&source, kHashed, nullptr));
   source.EndOfStream();
@@ -1334,7 +1326,7 @@
 
 TEST_P(MSEPipelineIntegrationTest,
        BasicPlaybackOpusMp4PrerollExceedsCodecDelay) {
-  TestMediaSource source("bear-opus.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-opus.mp4", kAppendWholeFile);
 
   // TODO(dalecurtis): The test clip currently does not have the edit list
   // entries required to achieve correctness here, so we're manually specifying
@@ -1428,7 +1420,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback) {
-  TestMediaSource source("bear-320x240.webm", 219229);
+  MockMediaSource source("bear-320x240.webm", 219229);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -1451,7 +1443,7 @@
   // stream should let the test complete with error indicating failure to open
   // demuxer. Here we append only the first 10 bytes of a test WebM, definitely
   // less than the ~4400 bytes needed to parse its full initialization segment.
-  TestMediaSource source("bear-320x240.webm", 10);
+  MockMediaSource source("bear-320x240.webm", 10);
   source.set_do_eos_after_next_append(true);
   EXPECT_EQ(
       DEMUXER_ERROR_COULD_NOT_OPEN,
@@ -1462,16 +1454,16 @@
   // After successful initialization segment append completing demuxer opening,
   // immediately append a corrupted media segment to trigger parse error while
   // pipeline is still completing renderer setup.
-  TestMediaSource source("bear-320x240_corrupted_after_init_segment.webm",
+  MockMediaSource source("bear-320x240_corrupted_after_init_segment.webm",
                          4380);
   source.set_expected_append_result(
-      TestMediaSource::ExpectedAppendResult::kFailure);
+      MockMediaSource::ExpectedAppendResult::kFailure);
   EXPECT_EQ(CHUNK_DEMUXER_ERROR_APPEND_FAILED,
             StartPipelineWithMediaSource(&source));
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_Live) {
-  TestMediaSource source("bear-320x240-live.webm", 219221);
+  MockMediaSource source("bear-320x240-live.webm", 219221);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -1491,7 +1483,7 @@
 
 #if BUILDFLAG(ENABLE_AV1_DECODER)
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_AV1_WebM) {
-  TestMediaSource source("bear-av1.webm", 18898);
+  MockMediaSource source("bear-av1.webm", 18898);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -1508,7 +1500,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_AV1_10bit_WebM) {
-  TestMediaSource source("bear-av1-320x180-10bit.webm", 19076);
+  MockMediaSource source("bear-av1-320x180-10bit.webm", 19076);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -1528,7 +1520,7 @@
 #endif
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_VP9_WebM) {
-  TestMediaSource source("bear-vp9.webm", 67504);
+  MockMediaSource source("bear-vp9.webm", 67504);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -1545,7 +1537,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_VP9_BlockGroup_WebM) {
-  TestMediaSource source("bear-vp9-blockgroup.webm", 67871);
+  MockMediaSource source("bear-vp9-blockgroup.webm", 67871);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -1562,7 +1554,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_VP8A_WebM) {
-  TestMediaSource source("bear-vp8a.webm", kAppendWholeFile);
+  MockMediaSource source("bear-vp8a.webm", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -1580,7 +1572,7 @@
 
 #if BUILDFLAG(ENABLE_AV1_DECODER)
 TEST_P(MSEPipelineIntegrationTest, ConfigChange_AV1_WebM) {
-  TestMediaSource source("bear-av1-480x360.webm", kAppendWholeFile);
+  MockMediaSource source("bear-av1-480x360.webm", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
 
   const gfx::Size kNewSize(640, 480);
@@ -1608,7 +1600,7 @@
 #endif  // BUILDFLAG(ENABLE_AV1_DECODER)
 
 TEST_P(MSEPipelineIntegrationTest, ConfigChange_WebM) {
-  TestMediaSource source("bear-320x240-16x9-aspect.webm", kAppendWholeFile);
+  MockMediaSource source("bear-320x240-16x9-aspect.webm", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
 
   const gfx::Size kNewSize(640, 360);
@@ -1635,7 +1627,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, AudioConfigChange_WebM) {
-  TestMediaSource source("bear-320x240-audio-only.webm", kAppendWholeFile);
+  MockMediaSource source("bear-320x240-audio-only.webm", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
 
   const int kNewSampleRate = 48000;
@@ -1669,7 +1661,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, Remove_Updates_BufferedRanges) {
-  TestMediaSource source("bear-320x240.webm", kAppendWholeFile);
+  MockMediaSource source("bear-320x240.webm", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
 
   auto buffered_ranges = pipeline_->GetBufferedTimeRanges();
@@ -1698,7 +1690,7 @@
 // will no longer start at 0.
 TEST_P(MSEPipelineIntegrationTest, FillUp_Buffer) {
   const char* input_filename = "bear-320x240.webm";
-  TestMediaSource source(input_filename, kAppendWholeFile);
+  MockMediaSource source(input_filename, kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.SetMemoryLimits(1048576);
 
@@ -1727,7 +1719,7 @@
 
 TEST_P(MSEPipelineIntegrationTest, GCWithDisabledVideoStream) {
   const char* input_filename = "bear-320x240.webm";
-  TestMediaSource source(input_filename, kAppendWholeFile);
+  MockMediaSource source(input_filename, kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   scoped_refptr<DecoderBuffer> file = ReadTestDataFile(input_filename);
   // The input file contains audio + video data. Assuming video data size is
@@ -1757,7 +1749,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(ConfigChange_Encrypted_WebM)) {
-  TestMediaSource source("bear-320x240-16x9-aspect-av_enc-av.webm",
+  MockMediaSource source("bear-320x240-16x9-aspect-av_enc-av.webm",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -1789,7 +1781,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(ConfigChange_ClearThenEncrypted_WebM)) {
-  TestMediaSource source("bear-320x240-16x9-aspect.webm", kAppendWholeFile);
+  MockMediaSource source("bear-320x240-16x9-aspect.webm", kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -1822,7 +1814,7 @@
 // supported by the Renderer.
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(ConfigChange_EncryptedThenClear_WebM)) {
-  TestMediaSource source("bear-320x240-16x9-aspect-av_enc-av.webm",
+  MockMediaSource source("bear-320x240-16x9-aspect-av_enc-av.webm",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -1872,7 +1864,7 @@
 
 #if BUILDFLAG(ENABLE_AV1_DECODER)
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_AV1_MP4) {
-  TestMediaSource source("bear-av1.mp4", 24355);
+  MockMediaSource source("bear-av1.mp4", 24355);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -1889,7 +1881,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_AV1_10bit_MP4) {
-  TestMediaSource source("bear-av1-320x180-10bit.mp4", 19658);
+  MockMediaSource source("bear-av1-320x180-10bit.mp4", 19658);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -1908,7 +1900,7 @@
 #endif
 
 TEST_P(MSEPipelineIntegrationTest, FlacInMp4_Hashed) {
-  TestMediaSource source("sfx-flac_frag.mp4", kAppendWholeFile);
+  MockMediaSource source("sfx-flac_frag.mp4", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithMediaSource(&source, kHashed, nullptr));
   source.EndOfStream();
@@ -1992,33 +1984,33 @@
 }
 
 // CBR seeks should always be fast and accurate.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     CBRSeek_HasTOC,
     Mp3FastSeekIntegrationTest,
     ::testing::Values(Mp3FastSeekParams("bear-audio-10s-CBR-has-TOC.mp3",
                                         "-0.58,0.61,3.08,2.55,0.90,-1.20,")));
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     CBRSeeks_NoTOC,
     Mp3FastSeekIntegrationTest,
     ::testing::Values(Mp3FastSeekParams("bear-audio-10s-CBR-no-TOC.mp3",
                                         "1.16,0.68,1.25,0.60,1.66,0.93,")));
 
 // VBR seeks can be fast *OR* accurate, but not both. We chose fast.
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     VBRSeeks_HasTOC,
     Mp3FastSeekIntegrationTest,
     ::testing::Values(Mp3FastSeekParams("bear-audio-10s-VBR-has-TOC.mp3",
                                         "-0.08,-0.53,0.75,0.89,2.44,0.73,")));
 
-INSTANTIATE_TEST_SUITE_P(
+INSTANTIATE_TEST_CASE_P(
     VBRSeeks_NoTOC,
     Mp3FastSeekIntegrationTest,
     ::testing::Values(Mp3FastSeekParams("bear-audio-10s-VBR-no-TOC.mp3",
                                         "-0.22,0.80,1.19,0.73,-0.31,-1.12,")));
 
 TEST_P(MSEPipelineIntegrationTest, MP3) {
-  TestMediaSource source("sfx.mp3", kAppendWholeFile);
+  MockMediaSource source("sfx.mp3", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithMediaSource(&source, kHashed, nullptr));
   source.EndOfStream();
@@ -2036,7 +2028,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, MP3_TimestampOffset) {
-  TestMediaSource source("sfx.mp3", kAppendWholeFile);
+  MockMediaSource source("sfx.mp3", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   EXPECT_EQ(313, source.last_timestamp_offset().InMilliseconds());
 
@@ -2063,7 +2055,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, MP3_Icecast) {
-  TestMediaSource source("icy_sfx.mp3", kAppendWholeFile);
+  MockMediaSource source("icy_sfx.mp3", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -2075,7 +2067,7 @@
 #if BUILDFLAG(USE_PROPRIETARY_CODECS)
 
 TEST_P(MSEPipelineIntegrationTest, ADTS) {
-  TestMediaSource source("sfx.adts", kAppendWholeFile);
+  MockMediaSource source("sfx.adts", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithMediaSource(&source, kHashed, nullptr));
   source.EndOfStream();
@@ -2093,7 +2085,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, ADTS_TimestampOffset) {
-  TestMediaSource source("sfx.adts", kAppendWholeFile);
+  MockMediaSource source("sfx.adts", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithMediaSource(&source, kHashed, nullptr));
   EXPECT_EQ(325, source.last_timestamp_offset().InMilliseconds());
@@ -2177,10 +2169,10 @@
   Play();
 
   ASSERT_TRUE(WaitUntilOnEnded());
-}
+};
 
 TEST_P(MSEPipelineIntegrationTest, ConfigChange_MP4) {
-  TestMediaSource source("bear-640x360-av_frag.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-640x360-av_frag.mp4", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
 
   const gfx::Size kNewSize(1280, 720);
@@ -2213,7 +2205,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(ConfigChange_Encrypted_MP4_CENC_VideoOnly)) {
-  TestMediaSource source("bear-640x360-v_frag-cenc-mdat.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-640x360-v_frag-cenc-mdat.mp4", kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -2247,7 +2239,7 @@
 MAYBE_EME_TEST_P(
     MSEPipelineIntegrationTest,
     MAYBE_EME(ConfigChange_Encrypted_MP4_CENC_KeyRotation_VideoOnly)) {
-  TestMediaSource source("bear-640x360-v_frag-cenc-key_rotation.mp4",
+  MockMediaSource source("bear-640x360-v_frag-cenc-key_rotation.mp4",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new RotatingKeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -2277,7 +2269,7 @@
 
 TEST_P(MSEPipelineIntegrationTest,
        MAYBE_EME(ConfigChange_ClearThenEncrypted_MP4_CENC)) {
-  TestMediaSource source("bear-640x360-v_frag.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-640x360-v_frag.mp4", kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -2286,7 +2278,7 @@
   scoped_refptr<DecoderBuffer> second_file =
       ReadTestDataFile("bear-1280x720-v_frag-cenc.mp4");
   source.set_expected_append_result(
-      TestMediaSource::ExpectedAppendResult::kFailure);
+      MockMediaSource::ExpectedAppendResult::kFailure);
   source.AppendAtTime(base::TimeDelta::FromSeconds(kAppendTimeSec),
                       second_file->data(), second_file->data_size());
 
@@ -2310,7 +2302,7 @@
 // Config changes from encrypted to clear are not currently supported.
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(ConfigChange_EncryptedThenClear_MP4_CENC)) {
-  TestMediaSource source("bear-640x360-v_frag-cenc-mdat.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-640x360-v_frag-cenc-mdat.mp4", kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -2319,7 +2311,7 @@
       ReadTestDataFile("bear-1280x720-av_frag.mp4");
 
   source.set_expected_append_result(
-      TestMediaSource::ExpectedAppendResult::kFailure);
+      MockMediaSource::ExpectedAppendResult::kFailure);
   source.AppendAtTime(base::TimeDelta::FromSeconds(kAppendTimeSec),
                       second_file->data(), second_file->data_size());
 
@@ -2362,7 +2354,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_WebM)) {
-  TestMediaSource source("bear-320x240-av_enc-av.webm", 219816);
+  MockMediaSource source("bear-320x240-av_enc-av.webm", 219816);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -2379,7 +2371,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_ClearStart_WebM)) {
-  TestMediaSource source("bear-320x240-av_enc-av_clear-1s.webm",
+  MockMediaSource source("bear-320x240-av_enc-av_clear-1s.webm",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -2397,7 +2389,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_NoEncryptedFrames_WebM)) {
-  TestMediaSource source("bear-320x240-av_enc-av_clear-all.webm",
+  MockMediaSource source("bear-320x240-av_enc-av_clear-all.webm",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new NoResponseApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -2415,7 +2407,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_MP4_VP9_CENC_VideoOnly)) {
-  TestMediaSource source("bear-320x240-v_frag-vp9-cenc.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-320x240-v_frag-vp9-cenc.mp4", kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -2430,7 +2422,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_VideoOnly_MP4_VP9) {
-  TestMediaSource source("bear-320x240-v_frag-vp9.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-320x240-v_frag-vp9.mp4", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
   ASSERT_EQ(PIPELINE_OK, pipeline_status_);
@@ -2445,7 +2437,7 @@
 #if BUILDFLAG(USE_PROPRIETARY_CODECS)
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_MP4_CENC_VideoOnly)) {
-  TestMediaSource source("bear-1280x720-v_frag-cenc.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-1280x720-v_frag-cenc.mp4", kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -2462,7 +2454,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_MP4_CENC_AudioOnly)) {
-  TestMediaSource source("bear-1280x720-a_frag-cenc.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-1280x720-a_frag-cenc.mp4", kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -2480,7 +2472,7 @@
 MAYBE_EME_TEST_P(
     MSEPipelineIntegrationTest,
     MAYBE_EME(EncryptedPlayback_NoEncryptedFrames_MP4_CENC_VideoOnly)) {
-  TestMediaSource source("bear-1280x720-v_frag-cenc_clear-all.mp4",
+  MockMediaSource source("bear-1280x720-v_frag-cenc_clear-all.mp4",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new NoResponseApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -2496,7 +2488,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, Mp2ts_AAC_HE_SBR_Audio) {
-  TestMediaSource source("bear-1280x720-aac_he.ts", kAppendWholeFile);
+  MockMediaSource source("bear-1280x720-aac_he.ts", kAppendWholeFile);
 #if BUILDFLAG(ENABLE_MSE_MPEG2TS_STREAM_PARSER)
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
@@ -2515,7 +2507,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, Mpeg2ts_MP3Audio_Mp4a_6B) {
-  TestMediaSource source("bear-audio-mp4a.6B.ts",
+  MockMediaSource source("bear-audio-mp4a.6B.ts",
                          "video/mp2t; codecs=\"mp4a.6B\"", kAppendWholeFile);
 #if BUILDFLAG(ENABLE_MSE_MPEG2TS_STREAM_PARSER)
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
@@ -2529,7 +2521,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, Mpeg2ts_MP3Audio_Mp4a_69) {
-  TestMediaSource source("bear-audio-mp4a.69.ts",
+  MockMediaSource source("bear-audio-mp4a.69.ts",
                          "video/mp2t; codecs=\"mp4a.69\"", kAppendWholeFile);
 #if BUILDFLAG(ENABLE_MSE_MPEG2TS_STREAM_PARSER)
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
@@ -2545,7 +2537,7 @@
 MAYBE_EME_TEST_P(
     MSEPipelineIntegrationTest,
     MAYBE_EME(EncryptedPlayback_NoEncryptedFrames_MP4_CENC_AudioOnly)) {
-  TestMediaSource source("bear-1280x720-a_frag-cenc_clear-all.mp4",
+  MockMediaSource source("bear-1280x720-a_frag-cenc_clear-all.mp4",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new NoResponseApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -2564,7 +2556,7 @@
 // beginning of mdat box.
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_MP4_CENC_MDAT_Video)) {
-  TestMediaSource source("bear-640x360-v_frag-cenc-mdat.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-640x360-v_frag-cenc-mdat.mp4", kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -2580,7 +2572,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_MP4_CENC_SENC_Video)) {
-  TestMediaSource source("bear-640x360-v_frag-cenc-senc.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-640x360-v_frag-cenc-senc.mp4", kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
             StartPipelineWithEncryptedMedia(&source, &encrypted_media));
@@ -2602,7 +2594,7 @@
 MAYBE_EME_TEST_P(
     MSEPipelineIntegrationTest,
     MAYBE_EME(EncryptedPlayback_MP4_CENC_SENC_NO_SAIZ_SAIO_Video)) {
-  TestMediaSource source("bear-640x360-v_frag-cenc-senc-no-saiz-saio.mp4",
+  MockMediaSource source("bear-640x360-v_frag-cenc-senc-no-saiz-saio.mp4",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new KeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -2619,7 +2611,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_MP4_CENC_KeyRotation_Video)) {
-  TestMediaSource source("bear-1280x720-v_frag-cenc-key_rotation.mp4",
+  MockMediaSource source("bear-1280x720-v_frag-cenc-key_rotation.mp4",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new RotatingKeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -2636,7 +2628,7 @@
 
 MAYBE_EME_TEST_P(MSEPipelineIntegrationTest,
                  MAYBE_EME(EncryptedPlayback_MP4_CENC_KeyRotation_Audio)) {
-  TestMediaSource source("bear-1280x720-a_frag-cenc-key_rotation.mp4",
+  MockMediaSource source("bear-1280x720-a_frag-cenc-key_rotation.mp4",
                          kAppendWholeFile);
   FakeEncryptedMedia encrypted_media(new RotatingKeyProvidingApp());
   EXPECT_EQ(PIPELINE_OK,
@@ -2652,7 +2644,7 @@
 }
 
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_VideoOnly_MP4_AVC3) {
-  TestMediaSource source("bear-1280x720-v_frag-avc3.mp4", kAppendWholeFile);
+  MockMediaSource source("bear-1280x720-v_frag-avc3.mp4", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
 
@@ -2675,7 +2667,7 @@
   // to actually demux and decode the stream. On platforms that support both
   // demuxing and decoding we'll get PIPELINE_OK.
   const char kMp4HevcVideoOnly[] = "video/mp4; codecs=\"hvc1.1.6.L93.B0\"";
-  TestMediaSource source("bear-320x240-v_frag-hevc.mp4", kMp4HevcVideoOnly,
+  MockMediaSource source("bear-320x240-v_frag-hevc.mp4", kMp4HevcVideoOnly,
                          kAppendWholeFile);
 #if BUILDFLAG(ENABLE_HEVC_DEMUXING)
   PipelineStatus status = StartPipelineWithMediaSource(&source);
@@ -2690,7 +2682,7 @@
 // Same test as above but using a different mime type.
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_VideoOnly_MP4_HEV1) {
   const char kMp4Hev1VideoOnly[] = "video/mp4; codecs=\"hev1.1.6.L93.B0\"";
-  TestMediaSource source("bear-320x240-v_frag-hevc.mp4", kMp4Hev1VideoOnly,
+  MockMediaSource source("bear-320x240-v_frag-hevc.mp4", kMp4Hev1VideoOnly,
                          kAppendWholeFile);
 #if BUILDFLAG(ENABLE_HEVC_DEMUXING)
   PipelineStatus status = StartPipelineWithMediaSource(&source);
@@ -2978,7 +2970,7 @@
 
 // Same as above but using MediaSource.
 TEST_P(MSEPipelineIntegrationTest, BasicPlayback_Opus441kHz) {
-  TestMediaSource source("sfx-opus-441.webm", kAppendWholeFile);
+  MockMediaSource source("sfx-opus-441.webm", kAppendWholeFile);
   EXPECT_EQ(PIPELINE_OK, StartPipelineWithMediaSource(&source));
   source.EndOfStream();
   Play();
@@ -3040,10 +3032,10 @@
             demuxer_->GetStartTime());
 }
 
-INSTANTIATE_TEST_SUITE_P(LegacyByDts,
+INSTANTIATE_TEST_CASE_P(LegacyByDts,
                          MSEPipelineIntegrationTest,
                          ::testing::Values(BufferingApi::kLegacyByDts));
-INSTANTIATE_TEST_SUITE_P(NewByPts,
+INSTANTIATE_TEST_CASE_P(NewByPts,
                          MSEPipelineIntegrationTest,
                          ::testing::Values(BufferingApi::kNewByPts));
 
--- a/media/test/PRESUBMIT.py	2019-05-17 17:45:41.308000000 +0300
+++ b/media/test/PRESUBMIT.py	2019-05-17 18:53:34.344000000 +0300
@@ -4,7 +4,7 @@
 
 """Top-level presubmit script for media/test/.
 
-See http://dev.ch40m1um.qjz9zk/developers/how-tos/depottools/presubmit-scripts
+See http://dev.chromium.org/developers/how-tos/depottools/presubmit-scripts
 for more details about the presubmit API built into depot_tools.
 """
 
--- a/media/video/BUILD.gn	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/BUILD.gn	2019-05-17 18:53:35.004000000 +0300
@@ -57,7 +57,6 @@
   ]
 
   deps = [
-    "//gpu/command_buffer/client",
     "//gpu/command_buffer/common",
     "//media/base",
     "//third_party/libyuv",
@@ -119,7 +118,6 @@
   deps = [
     "//base",
     "//base/test:test_support",
-    "//components/viz/test:test_support",
     "//gpu:test_support",
     "//gpu/command_buffer/client:gles2_interface",
     "//gpu/command_buffer/common",
--- a/media/video/fake_video_encode_accelerator.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/fake_video_encode_accelerator.cc	2019-05-17 18:53:35.004000000 +0300
@@ -52,7 +52,7 @@
   client_ = client;
   task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&FakeVideoEncodeAccelerator::DoRequireBitstreamBuffers,
+      base::Bind(&FakeVideoEncodeAccelerator::DoRequireBitstreamBuffers,
                      weak_this_factory_.GetWeakPtr(), kMinimumInputCount,
                      config.input_visible_size, kMinimumOutputBufferSize));
   return true;
@@ -89,8 +89,11 @@
 void FakeVideoEncodeAccelerator::SendDummyFrameForTesting(bool key_frame) {
   task_runner_->PostTask(
       FROM_HERE,
-      base::BindOnce(&FakeVideoEncodeAccelerator::DoBitstreamBufferReady,
-                     weak_this_factory_.GetWeakPtr(), 0, 23, key_frame));
+        base::Bind(&FakeVideoEncodeAccelerator::DoBitstreamBufferReady,
+                   weak_this_factory_.GetWeakPtr(),
+                   0,
+                   23,
+                   key_frame));
 }
 
 void FakeVideoEncodeAccelerator::SetWillInitializationSucceed(
@@ -116,9 +119,11 @@
     next_frame_is_first_frame_ = false;
     task_runner_->PostTask(
         FROM_HERE,
-        base::BindOnce(&FakeVideoEncodeAccelerator::DoBitstreamBufferReady,
-                       weak_this_factory_.GetWeakPtr(), bitstream_buffer_id,
-                       kMinimumOutputBufferSize, key_frame));
+        base::Bind(&FakeVideoEncodeAccelerator::DoBitstreamBufferReady,
+                   weak_this_factory_.GetWeakPtr(),
+                   bitstream_buffer_id,
+                   kMinimumOutputBufferSize,
+                   key_frame));
   }
 }
 
--- a/media/video/gpu_memory_buffer_video_frame_pool.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/gpu_memory_buffer_video_frame_pool.cc	2019-05-17 18:53:35.004000000 +0300
@@ -29,8 +29,7 @@
 #include "base/trace_event/trace_event.h"
 #include "build/build_config.h"
 #include "gpu/GLES2/gl2extchromium.h"
-#include "gpu/command_buffer/client/shared_image_interface.h"
-#include "gpu/command_buffer/common/shared_image_usage.h"
+#include "gpu/command_buffer/client/gles2_interface.h"
 #include "media/base/bind_to_current_loop.h"
 #include "media/video/gpu_video_accelerator_factories.h"
 #include "third_party/libyuv/include/libyuv.h"
@@ -96,6 +95,8 @@
   struct PlaneResource {
     gfx::Size size;
     std::unique_ptr<gfx::GpuMemoryBuffer> gpu_memory_buffer;
+    unsigned texture_id = 0u;
+    unsigned image_id = 0u;
     gpu::Mailbox mailbox;
   };
 
@@ -263,6 +264,40 @@
   return gfx::BufferFormat::BGRA_8888;
 }
 
+unsigned ImageInternalFormat(GpuVideoAcceleratorFactories::OutputFormat format,
+                             size_t plane) {
+  switch (format) {
+    case GpuVideoAcceleratorFactories::OutputFormat::I420:
+      DCHECK_LE(plane, 2u);
+      return GL_RED_EXT;
+    case GpuVideoAcceleratorFactories::OutputFormat::NV12_DUAL_GMB:
+      DCHECK_LE(plane, 1u);
+      return plane == 0 ? GL_RED_EXT : GL_RG_EXT;
+    case GpuVideoAcceleratorFactories::OutputFormat::NV12_SINGLE_GMB:
+      DCHECK_LE(plane, 1u);
+      return GL_RGB_YCBCR_420V_CHROMIUM;
+    case GpuVideoAcceleratorFactories::OutputFormat::UYVY:
+      DCHECK_EQ(0u, plane);
+      return GL_RGB_YCBCR_422_CHROMIUM;
+    case GpuVideoAcceleratorFactories::OutputFormat::XR30:
+    case GpuVideoAcceleratorFactories::OutputFormat::XB30:
+      DCHECK_EQ(0u, plane);
+      // Technically speaking we should say GL_RGB10_EXT, but that format is not
+      // supported in OpenGLES.
+      return GL_RGB10_A2_EXT;
+    case GpuVideoAcceleratorFactories::OutputFormat::RGBA:
+      DCHECK_EQ(0u, plane);
+      return GL_RGBA;
+    case GpuVideoAcceleratorFactories::OutputFormat::BGRA:
+      DCHECK_EQ(0u, plane);
+      return GL_BGRA_EXT;
+    case GpuVideoAcceleratorFactories::OutputFormat::UNDEFINED:
+      NOTREACHED();
+      break;
+  }
+  return 0;
+}
+
 // The number of output planes to be copied in each iteration.
 size_t PlanesPerCopy(GpuVideoAcceleratorFactories::OutputFormat format) {
   switch (format) {
@@ -894,8 +929,8 @@
     BindAndCreateMailboxesHardwareFrameResources(
         const scoped_refptr<VideoFrame>& video_frame,
         FrameResources* frame_resources) {
-  gpu::SharedImageInterface* sii = gpu_factories_->SharedImageInterface();
-  if (!sii) {
+  gpu::gles2::GLES2Interface* gles2 = gpu_factories_->ContextGL();
+  if (!gles2) {
     frame_resources->MarkUnused(tick_clock_->NowTicks());
     CompleteCopyRequestAndMaybeStartNextCopy(video_frame);
     return;
@@ -910,26 +945,29 @@
         GpuMemoryBufferFormat(output_format_, i);
     unsigned texture_target = gpu_factories_->ImageTextureTarget(buffer_format);
     // Bind the texture and create or rebind the image.
-    if (plane_resource.gpu_memory_buffer && plane_resource.mailbox.IsZero()) {
-      uint32_t usage =
-          gpu::SHARED_IMAGE_USAGE_GLES2 | gpu::SHARED_IMAGE_USAGE_RASTER |
-          gpu::SHARED_IMAGE_USAGE_DISPLAY | gpu::SHARED_IMAGE_USAGE_SCANOUT;
-      plane_resource.mailbox =
-          sii->CreateSharedImage(plane_resource.gpu_memory_buffer.get(),
-                                 gpu_factories_->GpuMemoryBufferManager(),
-                                 video_frame->ColorSpace(), usage);
-    } else if (!plane_resource.mailbox.IsZero()) {
-      // The sync token was waited on the client side before reuse.
-      sii->UpdateSharedImage(gpu::SyncToken(), plane_resource.mailbox);
+    gles2->BindTexture(texture_target, plane_resource.texture_id);
+    if (plane_resource.gpu_memory_buffer && !plane_resource.image_id) {
+      const size_t width = VideoFrame::Columns(i, VideoFormat(output_format_),
+                                               coded_size.width());
+      const size_t height =
+          VideoFrame::Rows(i, VideoFormat(output_format_), coded_size.height());
+      plane_resource.image_id = gles2->CreateImageCHROMIUM(
+          plane_resource.gpu_memory_buffer->AsClientBuffer(), width, height,
+          ImageInternalFormat(output_format_, i));
+    } else if (plane_resource.image_id) {
+      gles2->ReleaseTexImage2DCHROMIUM(texture_target, plane_resource.image_id);
     }
+    if (plane_resource.image_id)
+      gles2->BindTexImage2DCHROMIUM(texture_target, plane_resource.image_id);
     mailbox_holders[i] = gpu::MailboxHolder(plane_resource.mailbox,
                                             gpu::SyncToken(), texture_target);
   }
 
   // Insert a sync_token, this is needed to make sure that the textures the
   // mailboxes refer to will be used only after all the previous commands posted
-  // in the SharedImageInterface have been processed.
-  gpu::SyncToken sync_token = sii->GenUnverifiedSyncToken();
+  // in the command buffer have been processed.
+  gpu::SyncToken sync_token;
+  gles2->GenUnverifiedSyncTokenCHROMIUM(sync_token.GetData());
   for (size_t i = 0; i < NumGpuMemoryBuffers(output_format_); i++)
     mailbox_holders[i].sync_token = sync_token;
 
@@ -1052,6 +1090,11 @@
   }
 
   // Create the resources.
+  gpu::gles2::GLES2Interface* gles2 = gpu_factories_->ContextGL();
+  if (!gles2)
+    return nullptr;
+
+  gles2->ActiveTexture(GL_TEXTURE0);
   FrameResources* frame_resources = new FrameResources(size);
   resources_pool_.push_back(frame_resources);
   for (size_t i = 0; i < NumGpuMemoryBuffers(output_format_); i++) {
@@ -1066,6 +1109,16 @@
     plane_resource.gpu_memory_buffer = gpu_factories_->CreateGpuMemoryBuffer(
         plane_resource.size, buffer_format,
         gfx::BufferUsage::SCANOUT_CPU_READ_WRITE);
+
+    unsigned texture_target = gpu_factories_->ImageTextureTarget(buffer_format);
+    gles2->GenTextures(1, &plane_resource.texture_id);
+    gles2->BindTexture(texture_target, plane_resource.texture_id);
+    gles2->TexParameteri(texture_target, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
+    gles2->TexParameteri(texture_target, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
+    gles2->TexParameteri(texture_target, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
+    gles2->TexParameteri(texture_target, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);
+    gles2->ProduceTextureDirectCHROMIUM(plane_resource.texture_id,
+                                        plane_resource.mailbox.name);
   }
   return frame_resources;
 }
@@ -1088,16 +1141,15 @@
   // TODO(dcastagna): As soon as the context lost is dealt with in media,
   // make sure that we won't execute this callback (use a weak pointer to
   // the old context).
-  gpu::SharedImageInterface* sii = gpu_factories->SharedImageInterface();
-  if (!sii)
+  gpu::gles2::GLES2Interface* gles2 = gpu_factories->ContextGL();
+  if (!gles2)
     return;
 
   for (PlaneResource& plane_resource : frame_resources->plane_resources) {
-    if (!plane_resource.mailbox.IsZero()) {
-      // The sync token was already waited on the client side in
-      // MailboxHoldersReleased.
-      sii->DestroySharedImage(gpu::SyncToken(), plane_resource.mailbox);
-    }
+    if (plane_resource.image_id)
+      gles2->DestroyImageCHROMIUM(plane_resource.image_id);
+    if (plane_resource.texture_id)
+      gles2->DeleteTextures(1, &plane_resource.texture_id);
   }
 }
 
--- a/media/video/gpu_memory_buffer_video_frame_pool_unittest.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/gpu_memory_buffer_video_frame_pool_unittest.cc	2019-05-17 18:53:35.004000000 +0300
@@ -10,7 +10,7 @@
 #include "base/test/test_simple_task_runner.h"
 #include "base/threading/thread_task_runner_handle.h"
 #include "base/time/time.h"
-#include "components/viz/test/test_context_provider.h"
+#include "gpu/command_buffer/client/gles2_interface_stub.h"
 #include "media/base/video_frame.h"
 #include "media/video/gpu_memory_buffer_video_frame_pool.h"
 #include "media/video/mock_gpu_video_accelerator_factories.h"
@@ -21,6 +21,50 @@
 
 namespace media {
 
+namespace {
+class TestGLES2Interface : public gpu::gles2::GLES2InterfaceStub {
+ public:
+  void GenTextures(GLsizei n, GLuint* textures) override {
+    DCHECK_EQ(1, n);
+    *textures = ++gen_textures_count_;
+  }
+
+  void GenSyncTokenCHROMIUM(GLbyte* sync_token) override {
+    gpu::SyncToken sync_token_data;
+    sync_token_data.Set(gpu::CommandBufferNamespace::GPU_IO,
+                        gpu::CommandBufferId(), next_fence_sync_++);
+    sync_token_data.SetVerifyFlush();
+    memcpy(sync_token, &sync_token_data, sizeof(sync_token_data));
+  }
+
+  void GenUnverifiedSyncTokenCHROMIUM(GLbyte* sync_token) override {
+    gpu::SyncToken sync_token_data;
+    sync_token_data.Set(gpu::CommandBufferNamespace::GPU_IO,
+                        gpu::CommandBufferId(), next_fence_sync_++);
+    memcpy(sync_token, &sync_token_data, sizeof(sync_token_data));
+  }
+
+  void ProduceTextureDirectCHROMIUM(GLuint texture, GLbyte* mailbox) override {
+    *reinterpret_cast<unsigned*>(mailbox) = ++mailbox_;
+  }
+
+  void DeleteTextures(GLsizei n, const GLuint* textures) override {
+    ++deleted_textures_;
+    DCHECK_LE(deleted_textures_, gen_textures_count_);
+  }
+
+  unsigned gen_textures_count() const { return gen_textures_count_; }
+  unsigned deleted_textures_count() const { return deleted_textures_; }
+
+ private:
+  uint64_t next_fence_sync_ = 1u;
+  unsigned mailbox_ = 0u;
+  unsigned gen_textures_count_ = 0u;
+  unsigned deleted_textures_ = 0u;
+};
+
+}  // unnamed namespace
+
 class GpuMemoryBufferVideoFramePoolTest : public ::testing::Test {
  public:
   GpuMemoryBufferVideoFramePoolTest() = default;
@@ -29,12 +73,13 @@
     // empty base::TimeTicks values.
     test_clock_.Advance(base::TimeDelta::FromSeconds(1234));
 
-    sii_.reset(new viz::TestSharedImageInterface);
+    gles2_.reset(new TestGLES2Interface);
     media_task_runner_ = base::MakeRefCounted<base::TestSimpleTaskRunner>();
     copy_task_runner_ = base::MakeRefCounted<base::TestSimpleTaskRunner>();
     media_task_runner_handle_.reset(
         new base::ThreadTaskRunnerHandle(media_task_runner_));
-    mock_gpu_factories_.reset(new MockGpuVideoAcceleratorFactories(sii_.get()));
+    mock_gpu_factories_.reset(
+        new MockGpuVideoAcceleratorFactories(gles2_.get()));
     EXPECT_CALL(*mock_gpu_factories_.get(), SignalSyncToken(_, _))
         .Times(AtLeast(0));
     gpu_memory_buffer_pool_.reset(new GpuMemoryBufferVideoFramePool(
@@ -135,7 +180,7 @@
   // GpuMemoryBufferVideoFramePool uses BindToCurrentLoop(), which requires
   // ThreadTaskRunnerHandle initialization.
   std::unique_ptr<base::ThreadTaskRunnerHandle> media_task_runner_handle_;
-  std::unique_ptr<viz::TestSharedImageInterface> sii_;
+  std::unique_ptr<TestGLES2Interface> gles2_;
 };
 
 void MaybeCreateHardwareFrameCallback(
@@ -175,7 +220,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_I420, frame->format());
   EXPECT_EQ(3u, frame->NumTextures());
-  EXPECT_EQ(3u, sii_->shared_image_count());
+  EXPECT_EQ(3u, gles2_->gen_textures_count());
 }
 
 // Tests the current workaround for odd positioned video frame input. Once
@@ -202,7 +247,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_I420, frame->format());
   EXPECT_EQ(3u, frame->NumTextures());
-  EXPECT_EQ(3u, sii_->shared_image_count());
+  EXPECT_EQ(3u, gles2_->gen_textures_count());
 }
 
 TEST_F(GpuMemoryBufferVideoFramePoolTest, ReuseFirstResource) {
@@ -215,7 +260,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   gpu::Mailbox mailbox = frame->mailbox_holder(0).mailbox;
   const gpu::SyncToken sync_token = frame->mailbox_holder(0).sync_token;
-  EXPECT_EQ(3u, sii_->shared_image_count());
+  EXPECT_EQ(3u, gles2_->gen_textures_count());
 
   scoped_refptr<VideoFrame> frame2;
   gpu_memory_buffer_pool_->MaybeCreateHardwareFrame(
@@ -225,7 +270,7 @@
 
   EXPECT_NE(software_frame.get(), frame2.get());
   EXPECT_NE(mailbox, frame2->mailbox_holder(0).mailbox);
-  EXPECT_EQ(6u, sii_->shared_image_count());
+  EXPECT_EQ(6u, gles2_->gen_textures_count());
 
   frame = nullptr;
   frame2 = nullptr;
@@ -236,7 +281,7 @@
   RunUntilIdle();
 
   EXPECT_NE(software_frame.get(), frame.get());
-  EXPECT_EQ(6u, sii_->shared_image_count());
+  EXPECT_EQ(6u, gles2_->gen_textures_count());
   EXPECT_EQ(frame->mailbox_holder(0).mailbox, mailbox);
   EXPECT_NE(frame->mailbox_holder(0).sync_token, sync_token);
 }
@@ -248,13 +293,7 @@
       base::BindOnce(MaybeCreateHardwareFrameCallback, &frame));
   RunUntilIdle();
 
-  EXPECT_EQ(3u, sii_->shared_image_count());
-  // Check that the mailboxes in the VideoFrame were properly created.
-  gpu::Mailbox old_mailboxes[3];
-  for (size_t i = 0; i < 3; ++i) {
-    old_mailboxes[i] = frame->mailbox_holder(i).mailbox;
-    EXPECT_TRUE(sii_->CheckSharedImageExists(old_mailboxes[i]));
-  }
+  EXPECT_EQ(3u, gles2_->gen_textures_count());
 
   frame = nullptr;
   RunUntilIdle();
@@ -262,13 +301,7 @@
       CreateTestYUVVideoFrame(4),
       base::BindOnce(MaybeCreateHardwareFrameCallback, &frame));
   RunUntilIdle();
-  // Check that the mailboxes in the old VideoFrame were properly destroyed.
-  for (const auto& mailbox : old_mailboxes)
-    EXPECT_FALSE(sii_->CheckSharedImageExists(mailbox));
-  EXPECT_EQ(3u, sii_->shared_image_count());
-  // Check that the mailboxes in the new VideoFrame were properly created.
-  for (size_t i = 0; i < 3; ++i)
-    EXPECT_TRUE(sii_->CheckSharedImageExists(frame->mailbox_holder(i).mailbox));
+  EXPECT_EQ(6u, gles2_->gen_textures_count());
 }
 
 TEST_F(GpuMemoryBufferVideoFramePoolTest, CreateOneHardwareUYUVFrame) {
@@ -284,7 +317,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_UYVY, frame->format());
   EXPECT_EQ(1u, frame->NumTextures());
-  EXPECT_EQ(1u, sii_->shared_image_count());
+  EXPECT_EQ(1u, gles2_->gen_textures_count());
   EXPECT_TRUE(frame->metadata()->IsTrue(
       media::VideoFrameMetadata::READ_LOCK_FENCES_ENABLED));
 }
@@ -302,7 +335,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_NV12, frame->format());
   EXPECT_EQ(1u, frame->NumTextures());
-  EXPECT_EQ(1u, sii_->shared_image_count());
+  EXPECT_EQ(1u, gles2_->gen_textures_count());
   EXPECT_TRUE(frame->metadata()->IsTrue(
       media::VideoFrameMetadata::READ_LOCK_FENCES_ENABLED));
 }
@@ -320,7 +353,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_NV12, frame->format());
   EXPECT_EQ(2u, frame->NumTextures());
-  EXPECT_EQ(2u, sii_->shared_image_count());
+  EXPECT_EQ(2u, gles2_->gen_textures_count());
   EXPECT_TRUE(frame->metadata()->IsTrue(
       media::VideoFrameMetadata::READ_LOCK_FENCES_ENABLED));
 }
@@ -338,7 +371,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_ARGB, frame->format());
   EXPECT_EQ(1u, frame->NumTextures());
-  EXPECT_EQ(1u, sii_->shared_image_count());
+  EXPECT_EQ(1u, gles2_->gen_textures_count());
   EXPECT_TRUE(frame->metadata()->IsTrue(
       media::VideoFrameMetadata::READ_LOCK_FENCES_ENABLED));
 
@@ -363,7 +396,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_ARGB, frame->format());
   EXPECT_EQ(1u, frame->NumTextures());
-  EXPECT_EQ(1u, sii_->shared_image_count());
+  EXPECT_EQ(1u, gles2_->gen_textures_count());
   EXPECT_TRUE(frame->metadata()->IsTrue(
       media::VideoFrameMetadata::READ_LOCK_FENCES_ENABLED));
 
@@ -388,7 +421,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_ARGB, frame->format());
   EXPECT_EQ(1u, frame->NumTextures());
-  EXPECT_EQ(1u, sii_->shared_image_count());
+  EXPECT_EQ(1u, gles2_->gen_textures_count());
   EXPECT_TRUE(frame->metadata()->IsTrue(
       media::VideoFrameMetadata::READ_LOCK_FENCES_ENABLED));
 
@@ -412,7 +445,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_RGB32, frame->format());
   EXPECT_EQ(1u, frame->NumTextures());
-  EXPECT_EQ(1u, sii_->shared_image_count());
+  EXPECT_EQ(1u, gles2_->gen_textures_count());
   EXPECT_TRUE(frame->metadata()->IsTrue(
       media::VideoFrameMetadata::READ_LOCK_FENCES_ENABLED));
 }
@@ -430,7 +463,7 @@
   EXPECT_NE(software_frame.get(), frame.get());
   EXPECT_EQ(PIXEL_FORMAT_RGB32, frame->format());
   EXPECT_EQ(1u, frame->NumTextures());
-  EXPECT_EQ(1u, sii_->shared_image_count());
+  EXPECT_EQ(1u, gles2_->gen_textures_count());
   EXPECT_TRUE(frame->metadata()->IsTrue(
       media::VideoFrameMetadata::READ_LOCK_FENCES_ENABLED));
 }
@@ -461,7 +494,7 @@
 }
 
 // CreateGpuMemoryBuffer can return null (e.g: when the GPU process is down).
-// This test checks that in that case we don't crash and don't create the
+// This test checks that in that case we don't crash and still create the
 // textures.
 TEST_F(GpuMemoryBufferVideoFramePoolTest, CreateGpuMemoryBufferFail) {
   scoped_refptr<VideoFrame> software_frame = CreateTestYUVVideoFrame(10);
@@ -473,7 +506,7 @@
   RunUntilIdle();
 
   EXPECT_NE(software_frame.get(), frame.get());
-  EXPECT_EQ(0u, sii_->shared_image_count());
+  EXPECT_EQ(3u, gles2_->gen_textures_count());
 }
 
 TEST_F(GpuMemoryBufferVideoFramePoolTest, ShutdownReleasesUnusedResources) {
@@ -494,19 +527,22 @@
   EXPECT_NE(software_frame.get(), frame_2.get());
   EXPECT_NE(frame_1.get(), frame_2.get());
 
-  EXPECT_EQ(6u, sii_->shared_image_count());
+  EXPECT_EQ(6u, gles2_->gen_textures_count());
+  EXPECT_EQ(0u, gles2_->deleted_textures_count());
 
   // Drop frame and verify that resources are still available for reuse.
   frame_1 = nullptr;
   RunUntilIdle();
-  EXPECT_EQ(6u, sii_->shared_image_count());
+  EXPECT_EQ(6u, gles2_->gen_textures_count());
+  EXPECT_EQ(0u, gles2_->deleted_textures_count());
 
   // While still holding onto the second frame, destruct the frame pool and
   // verify that the inner pool releases the resources for the first frame.
   gpu_memory_buffer_pool_.reset();
   RunUntilIdle();
 
-  EXPECT_EQ(3u, sii_->shared_image_count());
+  EXPECT_EQ(6u, gles2_->gen_textures_count());
+  EXPECT_EQ(3u, gles2_->deleted_textures_count());
 }
 
 TEST_F(GpuMemoryBufferVideoFramePoolTest, StaleFramesAreExpired) {
@@ -527,19 +563,22 @@
   EXPECT_NE(software_frame.get(), frame_2.get());
   EXPECT_NE(frame_1.get(), frame_2.get());
 
-  EXPECT_EQ(6u, sii_->shared_image_count());
+  EXPECT_EQ(6u, gles2_->gen_textures_count());
+  EXPECT_EQ(0u, gles2_->deleted_textures_count());
 
   // Drop frame and verify that resources are still available for reuse.
   frame_1 = nullptr;
   RunUntilIdle();
-  EXPECT_EQ(6u, sii_->shared_image_count());
+  EXPECT_EQ(6u, gles2_->gen_textures_count());
+  EXPECT_EQ(0u, gles2_->deleted_textures_count());
 
   // Advance clock far enough to hit stale timer; ensure only frame_1 has its
   // resources released.
   test_clock_.Advance(base::TimeDelta::FromMinutes(1));
   frame_2 = nullptr;
   RunUntilIdle();
-  EXPECT_EQ(3u, sii_->shared_image_count());
+  EXPECT_EQ(6u, gles2_->gen_textures_count());
+  EXPECT_EQ(3u, gles2_->deleted_textures_count());
 }
 
 // Test when we request two copies in a row, there should be at most one frame
@@ -657,7 +696,7 @@
   EXPECT_NE(software_frame_1.get(), frame_1.get());
   EXPECT_EQ(PIXEL_FORMAT_RGB32, frame_1->format());
   EXPECT_EQ(1u, frame_1->NumTextures());
-  EXPECT_EQ(1u, sii_->shared_image_count());
+  EXPECT_EQ(1u, gles2_->gen_textures_count());
   EXPECT_TRUE(frame_1->metadata()->IsTrue(
       media::VideoFrameMetadata::READ_LOCK_FENCES_ENABLED));
 
--- a/media/video/gpu_video_accelerator_factories.h	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/gpu_video_accelerator_factories.h	2019-05-17 18:53:35.004000000 +0300
@@ -36,8 +36,6 @@
 
 namespace gpu {
 class ContextSupport;
-class GpuMemoryBufferManager;
-class SharedImageInterface;
 struct SyncToken;
 }
 
@@ -89,7 +87,8 @@
 
   virtual std::unique_ptr<media::VideoDecoder> CreateVideoDecoder(
       MediaLog* media_log,
-      const RequestOverlayInfoCB& request_overlay_info_cb) = 0;
+      const RequestOverlayInfoCB& request_overlay_info_cb,
+      const gfx::ColorSpace& target_color_space) = 0;
 
   // Caller owns returned pointer, but should call Destroy() on it (instead of
   // directly deleting) for proper destruction, as per the
@@ -140,17 +139,6 @@
   // the context was lost.
   virtual gpu::gles2::GLES2Interface* ContextGL() = 0;
 
-  // Returns a SharedImageInterface that can be used (on any thread) to allocate
-  // and update shared images.
-  // nullptr will be returned in cases where a context couldn't be created or
-  // the context was lost.
-  virtual gpu::SharedImageInterface* SharedImageInterface() = 0;
-
-  // Returns the GpuMemoryBufferManager that is used to allocate
-  // GpuMemoryBuffers. May return null if
-  // ShouldUseGpuMemoryBuffersForVideoFrames return false.
-  virtual gpu::GpuMemoryBufferManager* GpuMemoryBufferManager() = 0;
-
   // Allocate & return a shared memory segment.
   virtual std::unique_ptr<base::SharedMemory> CreateSharedMemory(
       size_t size) = 0;
--- a/media/video/mock_gpu_video_accelerator_factories.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/mock_gpu_video_accelerator_factories.cc	2019-05-17 18:53:35.008000000 +0300
@@ -93,8 +93,8 @@
 }  // unnamed namespace
 
 MockGpuVideoAcceleratorFactories::MockGpuVideoAcceleratorFactories(
-    gpu::SharedImageInterface* sii)
-    : sii_(sii) {}
+    gpu::gles2::GLES2Interface* gles2)
+    : gles2_(gles2) {}
 
 MockGpuVideoAcceleratorFactories::~MockGpuVideoAcceleratorFactories() = default;
 
--- a/media/video/mock_gpu_video_accelerator_factories.h	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/mock_gpu_video_accelerator_factories.h	2019-05-17 18:53:35.008000000 +0300
@@ -28,7 +28,7 @@
 
 class MockGpuVideoAcceleratorFactories : public GpuVideoAcceleratorFactories {
  public:
-  explicit MockGpuVideoAcceleratorFactories(gpu::SharedImageInterface* sii);
+  explicit MockGpuVideoAcceleratorFactories(gpu::gles2::GLES2Interface* gles2);
   ~MockGpuVideoAcceleratorFactories() override;
 
   bool IsGpuVideoAcceleratorEnabled() override;
@@ -37,10 +37,10 @@
   MOCK_METHOD0(GetCommandBufferRouteId, int32_t());
 
   MOCK_METHOD1(IsDecoderConfigSupported, bool(const VideoDecoderConfig&));
-  MOCK_METHOD2(
-      CreateVideoDecoder,
+  MOCK_METHOD3(CreateVideoDecoder,
       std::unique_ptr<media::VideoDecoder>(MediaLog*,
-                                           const RequestOverlayInfoCB&));
+                                                    const RequestOverlayInfoCB&,
+                                                    const gfx::ColorSpace&));
 
   // CreateVideo{Decode,Encode}Accelerator returns scoped_ptr, which the mocking
   // framework does not want.  Trampoline them.
@@ -80,17 +80,13 @@
   unsigned ImageTextureTarget(gfx::BufferFormat format) override;
   OutputFormat VideoFrameOutputFormat(VideoPixelFormat pixel_format) override {
     return video_frame_output_format_;
-  }
+  };
 
-  gpu::gles2::GLES2Interface* ContextGL() override { return nullptr; }
-  gpu::SharedImageInterface* SharedImageInterface() override { return sii_; }
-  gpu::GpuMemoryBufferManager* GpuMemoryBufferManager() override {
-    return nullptr;
-  }
+  gpu::gles2::GLES2Interface* ContextGL() override { return gles2_; }
 
   void SetVideoFrameOutputFormat(const OutputFormat video_frame_output_format) {
     video_frame_output_format_ = video_frame_output_format;
-  }
+  };
 
   void SetFailToAllocateGpuMemoryBufferForTesting(bool fail) {
     fail_to_allocate_gpu_memory_buffer_ = fail;
@@ -106,6 +102,8 @@
   std::unique_ptr<VideoEncodeAccelerator> CreateVideoEncodeAccelerator()
       override;
 
+  gpu::gles2::GLES2Interface* GetGLES2Interface() { return gles2_; }
+
   const std::vector<gfx::GpuMemoryBuffer*>& created_memory_buffers() {
     return created_memory_buffers_;
   }
@@ -118,7 +116,7 @@
 
   bool fail_to_allocate_gpu_memory_buffer_ = false;
 
-  gpu::SharedImageInterface* sii_;
+  gpu::gles2::GLES2Interface* gles2_;
 
   std::vector<gfx::GpuMemoryBuffer*> created_memory_buffers_;
 };
--- a/media/video/picture.h	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/picture.h	2019-05-17 18:53:35.008000000 +0300
@@ -114,7 +114,7 @@
   // without requesting new PictureBuffers. GpuVideoDecoder should read this
   // as a signal to update the size of the corresponding PicutreBuffer using
   // visible_rect() upon receiving this Picture from a VDA.
-  bool size_changed() const { return size_changed_; }
+  bool size_changed() const { return size_changed_; };
 
   void set_size_changed(bool size_changed) { size_changed_ = size_changed; }
 
--- a/media/video/video_encode_accelerator.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/video_encode_accelerator.cc	2019-05-17 18:53:35.008000000 +0300
@@ -84,16 +84,6 @@
       max_framerate_denominator(0) {
 }
 
-VideoEncodeAccelerator::SupportedProfile::SupportedProfile(
-    VideoCodecProfile profile,
-    const gfx::Size& max_resolution,
-    uint32_t max_framerate_numerator,
-    uint32_t max_framerate_denominator)
-    : profile(profile),
-      max_resolution(max_resolution),
-      max_framerate_numerator(max_framerate_numerator),
-      max_framerate_denominator(max_framerate_denominator) {}
-
 VideoEncodeAccelerator::SupportedProfile::~SupportedProfile() = default;
 
 void VideoEncodeAccelerator::Flush(FlushCallback flush_callback) {
--- a/media/video/video_encode_accelerator.h	2019-05-01 01:22:53.000000000 +0300
+++ b/media/video/video_encode_accelerator.h	2019-05-17 18:53:35.008000000 +0300
@@ -71,10 +71,6 @@
   // Specification of an encoding profile supported by an encoder.
   struct MEDIA_EXPORT SupportedProfile {
     SupportedProfile();
-    SupportedProfile(VideoCodecProfile profile,
-                     const gfx::Size& max_resolution,
-                     uint32_t max_framerate_numerator = 0u,
-                     uint32_t max_framerate_denominator = 1u);
     ~SupportedProfile();
     VideoCodecProfile profile;
     gfx::Size max_resolution;
--- a/media/webrtc/audio_processor.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/webrtc/audio_processor.cc	2019-05-17 18:53:35.012000000 +0300
@@ -7,7 +7,6 @@
 #include <utility>
 #include <vector>
 
-#include "base/bind.h"
 #include "base/command_line.h"
 #include "base/feature_list.h"
 #include "base/memory/ptr_util.h"
@@ -76,6 +75,9 @@
   StopEchoCancellationDump();
   if (audio_processing_)
     audio_processing_->UpdateHistogramsOnCallEnd();
+  // EchoInformation does this by itself on destruction, but since the stats are
+  // reset, they won't get doubly reported.
+  echo_information_.ReportAndResetAecDivergentFilterStats();
 }
 
 // Process the audio from source and return a pointer to the processed data.
@@ -135,6 +137,12 @@
   DCHECK_EQ(apm_error, webrtc::AudioProcessing::kNoError);
 }
 
+void AudioProcessor::UpdateInternalStats() {
+  if (audio_processing_)
+    echo_information_.UpdateAecStats(
+        audio_processing_->GetStatistics(has_reverse_stream_));
+}
+
 void AudioProcessor::GetStats(GetStatsCB callback) {
   webrtc::AudioProcessorInterface::AudioProcessorStatistics out = {};
   if (audio_processing_) {
@@ -190,7 +198,8 @@
   // 2" in those cases.
 
   // If we use nothing but, possibly, audio mirroring, don't initialize the APM.
-  if (settings_.echo_cancellation != EchoCancellationType::kAec3 &&
+  if (settings_.echo_cancellation != EchoCancellationType::kAec2 &&
+      settings_.echo_cancellation != EchoCancellationType::kAec3 &&
       settings_.noise_suppression == NoiseSuppressionType::kDisabled &&
       settings_.automatic_gain_control == AutomaticGainControlType::kDisabled &&
       !settings_.high_pass_filter && !settings_.typing_detection) {
@@ -202,6 +211,13 @@
 
   // AEC setup part 1.
 
+  // AEC2 options. Doesn't do anything if AEC2 isn't used.
+  ap_config.Set<webrtc::RefinedAdaptiveFilter>(
+      new webrtc::RefinedAdaptiveFilter(
+          base::CommandLine::ForCurrentProcess()->HasSwitch(
+              switches::kAecRefinedAdaptiveFilter)));
+  ap_config.Set<webrtc::ExtendedFilter>(new webrtc::ExtendedFilter(true));
+  ap_config.Set<webrtc::DelayAgnostic>(new webrtc::DelayAgnostic(true));
 
   // Echo cancellation is configured both before and after AudioProcessing
   // construction, but before Initialize.
@@ -268,6 +284,7 @@
 
   // AEC setup part 2.
   apm_config.echo_canceller.enabled =
+      settings_.echo_cancellation == EchoCancellationType::kAec2 ||
       settings_.echo_cancellation == EchoCancellationType::kAec3;
   apm_config.echo_canceller.mobile_mode = false;
 
--- a/media/webrtc/audio_processor.h	2019-05-01 01:22:53.000000000 +0300
+++ b/media/webrtc/audio_processor.h	2019-05-17 18:53:35.012000000 +0300
@@ -15,10 +15,11 @@
 #include "base/optional.h"
 #include "base/time/time.h"
 #include "media/audio/audio_io.h"
+#include "media/audio/audio_processing.h"
 #include "media/base/audio_parameters.h"
-#include "media/base/audio_processing.h"
 #include "media/webrtc/audio_delay_stats_reporter.h"
 #include "media/webrtc/audio_processor_controls.h"
+#include "media/webrtc/echo_information.h"
 #include "third_party/webrtc/modules/audio_processing/include/audio_processing.h"
 #include "third_party/webrtc/modules/audio_processing/typing_detection.h"
 #include "third_party/webrtc/rtc_base/task_queue.h"
@@ -62,6 +63,8 @@
                       const AudioParameters& parameters,
                       base::TimeTicks playout_time);
 
+  void UpdateInternalStats();
+
   void set_has_reverse_stream(bool has_reverse_stream) {
     has_reverse_stream_ = has_reverse_stream;
   }
@@ -106,6 +109,8 @@
   // thread.
   std::unique_ptr<rtc::TaskQueue> worker_queue_;
 
+  EchoInformation echo_information_;
+
   DISALLOW_COPY_AND_ASSIGN(AudioProcessor);
 };
 
--- a/media/webrtc/audio_processor_unittest.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/webrtc/audio_processor_unittest.cc	2019-05-17 18:53:35.012000000 +0300
@@ -144,7 +144,7 @@
 
   AudioProcessingSettings GetEnabledAudioProcessingSettings() const {
     AudioProcessingSettings settings;
-    settings.echo_cancellation = EchoCancellationType::kAec3;
+    settings.echo_cancellation = EchoCancellationType::kAec2;
     settings.noise_suppression = NoiseSuppressionType::kExperimental;
     settings.automatic_gain_control = AutomaticGainControlType::kExperimental;
     settings.high_pass_filter = true;
--- a/media/webrtc/BUILD.gn	2019-05-01 01:22:53.000000000 +0300
+++ b/media/webrtc/BUILD.gn	2019-05-17 18:53:35.008000000 +0300
@@ -18,6 +18,8 @@
   sources = [
     "audio_delay_stats_reporter.cc",
     "audio_delay_stats_reporter.h",
+    "echo_information.cc",
+    "echo_information.h",
     "webrtc_switches.cc",
     "webrtc_switches.h",
   ]
--- a/media/webrtc/webrtc_switches.cc	2019-05-01 01:22:53.000000000 +0300
+++ b/media/webrtc/webrtc_switches.cc	2019-05-17 18:53:35.012000000 +0300
@@ -6,6 +6,14 @@
 
 namespace switches {
 
+// Enables a new tuning of the WebRTC Acoustic Echo Canceler (AEC). The new
+// tuning aims at resolving two issues with the AEC:
+// https://bugs.chromium.org/p/webrtc/issues/detail?id=5777
+// https://bugs.chromium.org/p/webrtc/issues/detail?id=5778
+// TODO(hlundin): Remove this switch when experimentation is over;
+// crbug.com/603821.
+const char kAecRefinedAdaptiveFilter[] = "aec-refined-adaptive-filter";
+
 // Override the default minimum starting volume of the Automatic Gain Control
 // algorithm in WebRTC used with audio tracks from getUserMedia.
 // The valid range is 12-255. Values outside that range will be clamped
--- a/media/webrtc/webrtc_switches.h	2019-05-01 01:22:53.000000000 +0300
+++ b/media/webrtc/webrtc_switches.h	2019-05-17 18:53:35.012000000 +0300
@@ -12,6 +12,7 @@
 
 namespace switches {
 
+COMPONENT_EXPORT(MEDIA_WEBRTC) extern const char kAecRefinedAdaptiveFilter[];
 COMPONENT_EXPORT(MEDIA_WEBRTC) extern const char kAgcStartupMinVolume[];
 
 }  // namespace switches
--- a/media/audio/audio_processing.cc	1970-01-01 03:00:00.000000000 +0300
+++ b/media/audio/audio_processing.cc	2019-05-17 18:53:34.028000000 +0300
@@ -0,0 +1,60 @@
+// Copyright 2018 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "media/audio/audio_processing.h"
+
+#include "base/strings/strcat.h"
+
+namespace media {
+
+std::string AudioProcessingSettings::ToString() const {
+  auto agc_to_string = [](AutomaticGainControlType type) -> const char* {
+    switch (type) {
+      case AutomaticGainControlType::kDisabled:
+        return "disabled";
+      case AutomaticGainControlType::kDefault:
+        return "default";
+      case AutomaticGainControlType::kExperimental:
+        return "experimental";
+      case AutomaticGainControlType::kHybridExperimental:
+        return "hybrid experimental";
+    }
+  };
+
+  auto aec_to_string = [](EchoCancellationType type) -> const char* {
+    switch (type) {
+      case EchoCancellationType::kDisabled:
+        return "disabled";
+      case EchoCancellationType::kAec2:
+        return "aec2";
+      case EchoCancellationType::kAec3:
+        return "aec3";
+      case EchoCancellationType::kSystemAec:
+        return "system aec";
+    }
+  };
+
+  auto ns_to_string = [](NoiseSuppressionType type) -> const char* {
+    switch (type) {
+      case NoiseSuppressionType::kDisabled:
+        return "disabled";
+      case NoiseSuppressionType::kDefault:
+        return "default";
+      case NoiseSuppressionType::kExperimental:
+        return "experimental";
+    }
+  };
+
+  auto bool_to_yes_no = [](bool b) -> const char* { return b ? "yes" : "no"; };
+
+  return base::StrCat(
+      {"agc: ", agc_to_string(automatic_gain_control),
+       ", aec: ", aec_to_string(echo_cancellation),
+       ", ns: ", ns_to_string(noise_suppression),
+       ", high pass filter: ", bool_to_yes_no(high_pass_filter),
+       ", typing detection: ", bool_to_yes_no(typing_detection),
+       ", stereo mirroring: ", bool_to_yes_no(stereo_mirroring)});
+}
+
+}  // namespace media
--- a/media/audio/audio_processing.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/audio/audio_processing.h	2019-05-17 18:53:34.028000000 +0300
@@ -0,0 +1,59 @@
+// Copyright 2018 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef MEDIA_AUDIO_AUDIO_PROCESSING_H_
+#define MEDIA_AUDIO_AUDIO_PROCESSING_H_
+
+#include <string>
+
+#include "base/files/file.h"
+#include "base/time/time.h"
+#include "base/unguessable_token.h"
+#include "media/base/media_export.h"
+
+namespace media {
+
+enum class AutomaticGainControlType {
+  kDisabled,
+  kDefault,
+  kExperimental,
+  kHybridExperimental
+};
+enum class EchoCancellationType { kDisabled, kAec2, kAec3, kSystemAec };
+enum class NoiseSuppressionType { kDisabled, kDefault, kExperimental };
+
+struct MEDIA_EXPORT AudioProcessingSettings {
+  EchoCancellationType echo_cancellation = EchoCancellationType::kDisabled;
+  NoiseSuppressionType noise_suppression = NoiseSuppressionType::kDisabled;
+  AutomaticGainControlType automatic_gain_control =
+      AutomaticGainControlType::kDisabled;
+  bool high_pass_filter = false;
+  bool typing_detection = false;
+  bool stereo_mirroring = false;
+
+  bool operator==(const AudioProcessingSettings& b) const {
+    return echo_cancellation == b.echo_cancellation &&
+           noise_suppression == b.noise_suppression &&
+           automatic_gain_control == b.automatic_gain_control &&
+           high_pass_filter == b.high_pass_filter &&
+           typing_detection == b.typing_detection &&
+           stereo_mirroring == b.stereo_mirroring;
+  }
+
+  // Indicates whether WebRTC will be required to perform the audio processing.
+  bool requires_apm() const {
+    return echo_cancellation == EchoCancellationType::kAec2 ||
+           echo_cancellation == EchoCancellationType::kAec3 ||
+           noise_suppression != NoiseSuppressionType::kDisabled ||
+           automatic_gain_control != AutomaticGainControlType::kDisabled ||
+           high_pass_filter || typing_detection || stereo_mirroring;
+  }
+
+  // Stringifies the settings for human-readable logging.
+  std::string ToString() const;
+};
+
+}  // namespace media
+
+#endif  // MEDIA_AUDIO_AUDIO_PROCESSING_H_
--- a/media/base/android/java/borg/chromium/media/MediaSwitches.java	1970-01-01 03:00:00.000000000 +0300
+++ b/media/base/android/java/borg/chromium/media/MediaSwitches.java	2019-05-17 18:53:34.052000000 +0300
@@ -0,0 +1,20 @@
+// Copyright 2017 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+package org.chromium.media;
+
+/**
+ * Contains command line switches that are specific to the media layer.
+ */
+public abstract class MediaSwitches {
+    // Set the autoplay policy to ignore user gesture requirements
+    public static final String AUTOPLAY_NO_GESTURE_REQUIRED_POLICY =
+            "autoplay-policy=no-user-gesture-required";
+
+    // TODO(819383): Remove this and its usage.
+    public static final String USE_MODERN_MEDIA_CONTROLS = "UseModernMediaControls";
+
+    // Prevents instantiation.
+    private MediaSwitches() {}
+}
--- a/media/base/android/media_player_android.cc	1970-01-01 03:00:00.000000000 +0300
+++ b/media/base/android/media_player_android.cc	2019-05-17 18:53:34.056000000 +0300
@@ -0,0 +1,128 @@
+// Copyright (c) 2013 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "media/base/android/media_player_android.h"
+
+#include <algorithm>
+
+#include "base/android/scoped_java_ref.h"
+#include "base/logging.h"
+#include "base/single_thread_task_runner.h"
+#include "base/threading/thread_task_runner_handle.h"
+#include "media/base/android/media_drm_bridge.h"
+#include "media/base/android/media_player_manager.h"
+
+using base::android::JavaRef;
+
+namespace {
+
+const double kDefaultVolume = 1.0;
+
+}  // namespace
+
+namespace media {
+
+const double MediaPlayerAndroid::kDefaultVolumeMultiplier = 1.0;
+
+MediaPlayerAndroid::MediaPlayerAndroid(
+    int player_id,
+    MediaPlayerManager* manager,
+    const OnDecoderResourcesReleasedCB& on_decoder_resources_released_cb,
+    const GURL& frame_url)
+    : on_decoder_resources_released_cb_(on_decoder_resources_released_cb),
+      player_id_(player_id),
+      volume_(kDefaultVolume),
+      volume_multiplier_(kDefaultVolumeMultiplier),
+      manager_(manager),
+      frame_url_(frame_url),
+      weak_factory_(this) {
+  listener_.reset(new MediaPlayerListener(base::ThreadTaskRunnerHandle::Get(),
+                                          weak_factory_.GetWeakPtr()));
+}
+
+MediaPlayerAndroid::~MediaPlayerAndroid() {}
+
+void MediaPlayerAndroid::SetVolume(double volume) {
+  volume_ = std::max(0.0, std::min(volume, 1.0));
+  UpdateEffectiveVolume();
+}
+
+void MediaPlayerAndroid::SetVolumeMultiplier(double volume_multiplier) {
+  volume_multiplier_ = std::max(0.0, std::min(volume_multiplier, 1.0));
+  UpdateEffectiveVolume();
+}
+
+double MediaPlayerAndroid::GetEffectiveVolume() const {
+  return volume_ * volume_multiplier_;
+}
+
+void MediaPlayerAndroid::UpdateEffectiveVolume() {
+  UpdateEffectiveVolumeInternal(GetEffectiveVolume());
+}
+
+// For most subclasses we can delete on the caller thread.
+void MediaPlayerAndroid::DeleteOnCorrectThread() {
+  delete this;
+}
+
+GURL MediaPlayerAndroid::GetUrl() {
+  return GURL();
+}
+
+GURL MediaPlayerAndroid::GetSiteForCookies() {
+  return GURL();
+}
+
+void MediaPlayerAndroid::SetCdm(
+    const scoped_refptr<ContentDecryptionModule>& /* cdm */) {
+  // Players that support EME should override this.
+  LOG(ERROR) << "EME not supported on base MediaPlayerAndroid class.";
+  return;
+}
+
+void MediaPlayerAndroid::OnVideoSizeChanged(int width, int height) {
+  manager_->OnVideoSizeChanged(player_id(), width, height);
+}
+
+void MediaPlayerAndroid::OnMediaError(int error_type) {
+  manager_->OnError(player_id(), error_type);
+}
+
+void MediaPlayerAndroid::OnBufferingUpdate(int percent) {
+  manager_->OnBufferingUpdate(player_id(), percent);
+}
+
+void MediaPlayerAndroid::OnPlaybackComplete() {
+  manager_->OnPlaybackComplete(player_id());
+}
+
+void MediaPlayerAndroid::OnMediaInterrupted() {
+  manager_->OnMediaInterrupted(player_id());
+}
+
+void MediaPlayerAndroid::OnSeekComplete() {
+  manager_->OnSeekComplete(player_id(), GetCurrentTime());
+}
+
+void MediaPlayerAndroid::OnMediaPrepared() {}
+
+void MediaPlayerAndroid::AttachListener(
+    const JavaRef<jobject>& j_media_player) {
+  listener_->CreateMediaPlayerListener(j_media_player);
+}
+
+void MediaPlayerAndroid::DetachListener() {
+  listener_->ReleaseMediaPlayerListenerResources();
+}
+
+void MediaPlayerAndroid::DestroyListenerOnUIThread() {
+  weak_factory_.InvalidateWeakPtrs();
+  listener_.reset();
+}
+
+base::WeakPtr<MediaPlayerAndroid> MediaPlayerAndroid::WeakPtrForUIThread() {
+  return weak_factory_.GetWeakPtr();
+}
+
+}  // namespace media
--- a/media/base/android/media_player_android.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/base/android/media_player_android.h	2019-05-17 18:53:34.056000000 +0300
@@ -0,0 +1,188 @@
+// Copyright (c) 2013 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef MEDIA_BASE_ANDROID_MEDIA_PLAYER_ANDROID_H_
+#define MEDIA_BASE_ANDROID_MEDIA_PLAYER_ANDROID_H_
+
+#include <jni.h>
+
+#include <memory>
+#include <string>
+
+#include "base/android/scoped_java_ref.h"
+#include "base/callback.h"
+#include "base/macros.h"
+#include "base/memory/weak_ptr.h"
+#include "base/time/time.h"
+#include "media/base/android/media_player_listener.h"
+#include "media/base/media_export.h"
+#include "ui/gfx/geometry/size.h"
+#include "ui/gl/android/scoped_java_surface.h"
+#include "url/gurl.h"
+
+namespace media {
+
+class ContentDecryptionModule;
+class MediaPlayerManager;
+
+// This class serves as the base class for different media player
+// implementations on Android. Subclasses need to provide their own
+// MediaPlayerAndroid::Create() implementation.
+class MEDIA_EXPORT MediaPlayerAndroid {
+ public:
+  virtual ~MediaPlayerAndroid();
+
+  // Error types for MediaErrorCB.
+  enum MediaErrorType {
+    MEDIA_ERROR_FORMAT,
+    MEDIA_ERROR_DECODE,
+    MEDIA_ERROR_NOT_VALID_FOR_PROGRESSIVE_PLAYBACK,
+    MEDIA_ERROR_INVALID_CODE,
+    MEDIA_ERROR_SERVER_DIED,
+  };
+
+  static const double kDefaultVolumeMultiplier;
+
+  // Callback when the player releases decoding resources.
+  typedef base::Callback<void(int player_id)> OnDecoderResourcesReleasedCB;
+
+  // Virtual destructor.
+  // For most subclasses we can delete on the caller thread.
+  virtual void DeleteOnCorrectThread();
+
+  // Passing an external java surface object to the player.
+  virtual void SetVideoSurface(gl::ScopedJavaSurface surface) = 0;
+
+  // Start playing the media.
+  virtual void Start() = 0;
+
+  // Pause the media.
+  virtual void Pause(bool is_media_related_action) = 0;
+
+  // Seek to a particular position, based on renderer signaling actual seek
+  // with MediaPlayerHostMsg_Seek. If eventual success, OnSeekComplete() will be
+  // called.
+  virtual void SeekTo(base::TimeDelta timestamp) = 0;
+
+  // Release the player resources.
+  virtual void Release() = 0;
+
+  // Set the player volume, and take effect immediately.
+  // The volume should be between 0.0 and 1.0.
+  void SetVolume(double volume);
+
+  // Set the player volume multiplier, and take effect immediately.
+  // The volume should be between 0.0 and 1.0.
+  void SetVolumeMultiplier(double volume_multiplier);
+
+  // Get the media information from the player.
+  virtual bool HasVideo() const = 0;
+  virtual bool HasAudio() const = 0;
+  virtual int GetVideoWidth() = 0;
+  virtual int GetVideoHeight() = 0;
+  virtual base::TimeDelta GetDuration() = 0;
+  virtual base::TimeDelta GetCurrentTime() = 0;
+  virtual bool IsPlaying() = 0;
+  virtual bool CanPause() = 0;
+  virtual bool CanSeekForward() = 0;
+  virtual bool CanSeekBackward() = 0;
+  virtual bool IsPlayerReady() = 0;
+  virtual GURL GetUrl();
+  virtual GURL GetSiteForCookies();
+
+  // Associates the |cdm| with this player.
+  virtual void SetCdm(const scoped_refptr<ContentDecryptionModule>& cdm);
+
+  // Requests playback permission from MediaPlayerManager.
+  // Overridden in MediaCodecPlayer to pass data between threads.
+  virtual void RequestPermissionAndPostResult(base::TimeDelta duration,
+                                              bool has_audio) {}
+
+  // Overridden in MediaCodecPlayer to pass data between threads.
+  virtual void OnMediaMetadataChanged(base::TimeDelta duration,
+                                      const gfx::Size& video_size) {}
+
+  // Overridden in MediaCodecPlayer to pass data between threads.
+  virtual void OnTimeUpdate(base::TimeDelta current_timestamp,
+                            base::TimeTicks current_time_ticks) {}
+
+  int player_id() { return player_id_; }
+
+  GURL frame_url() { return frame_url_; }
+
+  // Attach/Detaches |listener_| for listening to all the media events. If
+  // |j_media_player| is NULL, |listener_| only listens to the system media
+  // events. Otherwise, it also listens to the events from |j_media_player|.
+  void AttachListener(const base::android::JavaRef<jobject>& j_media_player);
+  void DetachListener();
+
+ protected:
+  MediaPlayerAndroid(
+      int player_id,
+      MediaPlayerManager* manager,
+      const OnDecoderResourcesReleasedCB& on_decoder_resources_released_cb,
+      const GURL& frame_url);
+
+  // TODO(qinmin): Simplify the MediaPlayerListener class to only listen to
+  // media interrupt events. And have a separate child class to listen to all
+  // the events needed by MediaPlayerBridge. http://crbug.com/422597.
+  // MediaPlayerListener callbacks.
+  virtual void OnVideoSizeChanged(int width, int height);
+  virtual void OnMediaError(int error_type);
+  virtual void OnBufferingUpdate(int percent);
+  virtual void OnPlaybackComplete();
+  virtual void OnMediaInterrupted();
+  virtual void OnSeekComplete();
+  virtual void OnMediaPrepared();
+
+  double GetEffectiveVolume() const;
+  void UpdateEffectiveVolume();
+
+  // When destroying a subclassed object on a non-UI thread
+  // it is still required to destroy the |listener_| related stuff
+  // on the UI thread.
+  void DestroyListenerOnUIThread();
+
+  MediaPlayerManager* manager() { return manager_; }
+
+  base::WeakPtr<MediaPlayerAndroid> WeakPtrForUIThread();
+
+  OnDecoderResourcesReleasedCB on_decoder_resources_released_cb_;
+
+ private:
+  // Set the effective player volume, implemented by children classes.
+  virtual void UpdateEffectiveVolumeInternal(double effective_volume) = 0;
+
+  friend class MediaPlayerListener;
+
+  // Player ID assigned to this player.
+  int player_id_;
+
+  // The player volume. Should be between 0.0 and 1.0.
+  double volume_;
+
+  // The player volume multiplier. Should be between 0.0 and 1.0.  This
+  // should be a cached version of the MediaSession volume multiplier,
+  // and should keep updated.
+  double volume_multiplier_;
+
+  // Resource manager for all the media players.
+  MediaPlayerManager* manager_;
+
+  // Url for the frame that contains this player.
+  GURL frame_url_;
+
+  // Listener object that listens to all the media player events.
+  std::unique_ptr<MediaPlayerListener> listener_;
+
+  // Weak pointer passed to |listener_| for callbacks.
+  // NOTE: Weak pointers must be invalidated before all other member variables.
+  base::WeakPtrFactory<MediaPlayerAndroid> weak_factory_;
+
+  DISALLOW_COPY_AND_ASSIGN(MediaPlayerAndroid);
+};
+
+}  // namespace media
+
+#endif  // MEDIA_BASE_ANDROID_MEDIA_PLAYER_ANDROID_H_
--- a/media/base/android/media_player_manager.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/base/android/media_player_manager.h	2019-05-17 18:53:34.060000000 +0300
@@ -0,0 +1,78 @@
+// Copyright (c) 2012 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef MEDIA_BASE_ANDROID_MEDIA_PLAYER_MANAGER_H_
+#define MEDIA_BASE_ANDROID_MEDIA_PLAYER_MANAGER_H_
+
+#include "base/time/time.h"
+#include "media/base/media_export.h"
+
+namespace media {
+
+class MediaPlayerAndroid;
+class MediaResourceGetter;
+class MediaUrlInterceptor;
+
+// This class is responsible for managing active MediaPlayerAndroid objects.
+class MEDIA_EXPORT MediaPlayerManager {
+ public:
+  virtual ~MediaPlayerManager() {}
+
+  // Returns a pointer to the MediaResourceGetter object.
+  virtual MediaResourceGetter* GetMediaResourceGetter() = 0;
+
+  // Returns a pointer to the MediaUrlInterceptor object or null.
+  virtual MediaUrlInterceptor* GetMediaUrlInterceptor() = 0;
+
+  // Called when time update messages need to be sent. Args: player ID,
+  // current timestamp, current time ticks.
+  virtual void OnTimeUpdate(int player_id,
+                            base::TimeDelta current_timestamp,
+                            base::TimeTicks current_time_ticks) = 0;
+
+  // Called when media metadata changed. Args: player ID, duration of the
+  // media, width, height, whether the metadata is successfully extracted.
+  virtual void OnMediaMetadataChanged(
+      int player_id,
+      base::TimeDelta duration,
+      int width,
+      int height,
+      bool success) = 0;
+
+  // Called when playback completed. Args: player ID.
+  virtual void OnPlaybackComplete(int player_id) = 0;
+
+  // Called when media download was interrupted. Args: player ID.
+  virtual void OnMediaInterrupted(int player_id) = 0;
+
+  // Called when buffering has changed. Args: player ID, percentage
+  // of the media.
+  virtual void OnBufferingUpdate(int player_id, int percentage) = 0;
+
+  // Called when seek completed. Args: player ID, current time.
+  virtual void OnSeekComplete(
+      int player_id,
+      const base::TimeDelta& current_time) = 0;
+
+  // Called when error happens. Args: player ID, error type.
+  virtual void OnError(int player_id, int error) = 0;
+
+  // Called when video size has changed. Args: player ID, width, height.
+  virtual void OnVideoSizeChanged(int player_id, int width, int height) = 0;
+
+  // Returns the player with the specified id.
+  virtual MediaPlayerAndroid* GetPlayer(int player_id) = 0;
+
+  // Called by the player to request the playback for given duration. The
+  // manager should use this opportunity to check if the current context is
+  // appropriate for a media to play.
+  // Returns whether the request was granted.
+  virtual bool RequestPlay(int player_id,
+                           base::TimeDelta duration,
+                           bool has_audio) = 0;
+};
+
+}  // namespace media
+
+#endif  // MEDIA_BASE_ANDROID_MEDIA_PLAYER_MANAGER_H_
--- a/media/blink/renderer_media_player_interface.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/blink/renderer_media_player_interface.h	2019-05-17 18:53:34.116000000 +0300
@@ -0,0 +1,122 @@
+// Copyright 2016 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef MEDIA_BLINK_RENDERER_MEDIA_PLAYER_INTERFACE_H_
+#define MEDIA_BLINK_RENDERER_MEDIA_PLAYER_INTERFACE_H_
+
+// This file contains interfaces modeled after classes in
+// content/renderer/media/android for the purposes of letting clases in
+// this directory implement and/or interact with those classes.
+// It's a stop-gap used to support cast on android until a better solution
+// is implemented: crbug/575276
+
+#include <string>
+#include "base/time/time.h"
+#include "media/blink/webmediaplayer_delegate.h"
+#include "ui/gfx/geometry/rect_f.h"
+#include "url/gurl.h"
+
+namespace blink {
+enum class WebRemotePlaybackAvailability;
+}
+
+// Dictates which type of media playback is being initialized.
+enum MediaPlayerHostMsg_Initialize_Type {
+  MEDIA_PLAYER_TYPE_URL,
+  MEDIA_PLAYER_TYPE_REMOTE_ONLY,
+  MEDIA_PLAYER_TYPE_LAST = MEDIA_PLAYER_TYPE_REMOTE_ONLY
+};
+
+namespace media {
+
+class RendererMediaPlayerInterface {
+ public:
+  virtual void OnMediaMetadataChanged(base::TimeDelta duration,
+                                      int width,
+                                      int height,
+                                      bool success) = 0;
+  virtual void OnPlaybackComplete() = 0;
+  virtual void OnBufferingUpdate(int percentage) = 0;
+  virtual void OnSeekRequest(base::TimeDelta time_to_seek) = 0;
+  virtual void OnSeekComplete(base::TimeDelta current_time) = 0;
+  virtual void OnMediaError(int error_type) = 0;
+  virtual void OnVideoSizeChanged(int width, int height) = 0;
+
+  // Called to update the current time.
+  virtual void OnTimeUpdate(base::TimeDelta current_timestamp,
+                            base::TimeTicks current_time_ticks) = 0;
+
+  virtual void OnPlayerReleased() = 0;
+
+  // Functions called when media player status changes.
+  virtual void OnConnectedToRemoteDevice(
+      const std::string& remote_playback_message) = 0;
+  virtual void OnDisconnectedFromRemoteDevice() = 0;
+  virtual void OnRemotePlaybackStarted() = 0;
+  virtual void OnCancelledRemotePlaybackRequest() = 0;
+  virtual void OnDidExitFullscreen() = 0;
+  virtual void OnMediaPlayerPlay() = 0;
+  virtual void OnMediaPlayerPause() = 0;
+  virtual void OnRemoteRouteAvailabilityChanged(
+      blink::WebRemotePlaybackAvailability availability) = 0;
+
+  // This function is called by the RendererMediaPlayerManager to pause the
+  // video and release the media player and surface texture when we switch tabs.
+  // However, the actual GlTexture is not released to keep the video screenshot.
+  virtual void SuspendAndReleaseResources() = 0;
+};
+
+class RendererMediaPlayerManagerInterface {
+ public:
+  // Initializes a MediaPlayerAndroid object in browser process.
+  virtual void Initialize(MediaPlayerHostMsg_Initialize_Type type,
+                          int player_id,
+                          const GURL& url,
+                          const GURL& site_for_cookies,
+                          const GURL& frame_url,
+                          bool allow_credentials,
+                          int delegate_id) = 0;
+
+  // Starts the player.
+  virtual void Start(int player_id) = 0;
+
+  // Pauses the player.
+  // is_media_related_action should be true if this pause is coming from an
+  // an action that explicitly pauses the video (user pressing pause, JS, etc.)
+  // Otherwise it should be false if Pause is being called due to other reasons
+  // (cleanup, freeing resources, etc.)
+  virtual void Pause(int player_id, bool is_media_related_action) = 0;
+
+  // Performs seek on the player.
+  virtual void Seek(int player_id, base::TimeDelta time) = 0;
+
+  // Sets the player volume.
+  virtual void SetVolume(int player_id, double volume) = 0;
+
+  // Sets the poster image.
+  virtual void SetPoster(int player_id, const GURL& poster) = 0;
+
+  // Releases resources for the player after being suspended.
+  virtual void SuspendAndReleaseResources(int player_id) = 0;
+
+  // Destroys the player in the browser process
+  virtual void DestroyPlayer(int player_id) = 0;
+
+  // Requests remote playback if possible
+  virtual void RequestRemotePlayback(int player_id) = 0;
+
+  // Requests control of remote playback
+  virtual void RequestRemotePlaybackControl(int player_id) = 0;
+
+  // Requests stopping remote playback
+  virtual void RequestRemotePlaybackStop(int player_id) = 0;
+
+  // Registers and unregisters a RendererMediaPlayerInterface object.
+  virtual int RegisterMediaPlayer(RendererMediaPlayerInterface* player) = 0;
+  virtual void UnregisterMediaPlayer(int player_id) = 0;
+};
+
+}  // namespace media
+
+#endif  // MEDIA_BLINK_RENDERER_MEDIA_PLAYER_INTERFACE_H_
--- a/media/blink/webmediaplayer_cast_android.cc	1970-01-01 03:00:00.000000000 +0300
+++ b/media/blink/webmediaplayer_cast_android.cc	2019-05-17 18:53:34.120000000 +0300
@@ -0,0 +1,373 @@
+// Copyright 2016 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "media/blink/webmediaplayer_cast_android.h"
+
+#include "gpu/GLES2/gl2extchromium.h"
+#include "gpu/command_buffer/client/gles2_interface.h"
+#include "gpu/command_buffer/common/sync_token.h"
+#include "media/base/android/media_common_android.h"
+#include "media/base/bind_to_current_loop.h"
+#include "media/blink/webmediaplayer_impl.h"
+#include "media/blink/webmediaplayer_params.h"
+#include "third_party/blink/public/platform/web_media_player_client.h"
+#include "third_party/blink/public/web/web_document.h"
+#include "third_party/blink/public/web/web_local_frame.h"
+#include "third_party/skia/include/core/SkCanvas.h"
+#include "third_party/skia/include/core/SkFont.h"
+#include "third_party/skia/include/core/SkFontStyle.h"
+#include "third_party/skia/include/core/SkPaint.h"
+#include "third_party/skia/include/core/SkTypeface.h"
+
+using gpu::gles2::GLES2Interface;
+
+namespace media {
+
+namespace {
+// File-static function is to allow it to run even after WMPI is deleted.
+void OnReleaseTexture(
+    const base::Callback<gpu::gles2::GLES2Interface*()>& context_3d_cb,
+    GLuint texture_id,
+    const gpu::SyncToken& sync_token) {
+  GLES2Interface* gl = context_3d_cb.Run();
+  if (!gl)
+    return;
+
+  gl->WaitSyncTokenCHROMIUM(sync_token.GetConstData());
+  gl->DeleteTextures(1, &texture_id);
+  // Flush to ensure that the texture gets deleted in a timely fashion.
+  gl->ShallowFlushCHROMIUM();
+}
+
+GLES2Interface* GLCBShim(scoped_refptr<viz::ContextProvider> context_provider) {
+  return context_provider->ContextGL();
+}
+
+}  // namespace
+
+scoped_refptr<VideoFrame> MakeTextFrameForCast(
+    const std::string& remote_playback_message,
+    gfx::Size canvas_size,
+    gfx::Size natural_size,
+    const base::Callback<gpu::gles2::GLES2Interface*()>& context_3d_cb) {
+  SkBitmap bitmap;
+  bitmap.allocN32Pixels(canvas_size.width(), canvas_size.height());
+
+  // Create the canvas and draw the "Casting to <Chromecast>" text on it.
+  SkCanvas canvas(bitmap);
+  canvas.drawColor(SK_ColorBLACK);
+
+  const SkScalar kTextSize(40);
+  const SkScalar kMinPadding(40);
+
+  SkPaint paint;
+  paint.setColor(SK_ColorWHITE);
+
+  SkFont font;
+  font.setTypeface(SkTypeface::MakeFromName("sans", SkFontStyle::Bold()));
+  font.setSize(kTextSize);
+
+  // Calculate the vertical margin from the top
+  SkFontMetrics font_metrics;
+  font.getMetrics(&font_metrics);
+  SkScalar sk_vertical_margin = kMinPadding - font_metrics.fAscent;
+
+  // Measure the width of the entire text to display
+  size_t display_text_width =
+      font.measureText(remote_playback_message.c_str(),
+                       remote_playback_message.size(), kUTF8_SkTextEncoding);
+  std::string display_text(remote_playback_message);
+
+  if (display_text_width + (kMinPadding * 2) > canvas_size.width()) {
+    // The text is too long to fit in one line, truncate it and append ellipsis
+    // to the end.
+
+    // First, figure out how much of the canvas the '...' will take up.
+    const std::string kTruncationEllipsis("\xE2\x80\xA6");
+    SkScalar sk_ellipse_width =
+        font.measureText(kTruncationEllipsis.c_str(),
+                         kTruncationEllipsis.size(), kUTF8_SkTextEncoding);
+
+    // Then calculate how much of the text can be drawn with the '...' appended
+    // to the end of the string.
+    SkScalar sk_max_original_text_width(canvas_size.width() -
+                                        (kMinPadding * 2) - sk_ellipse_width);
+    size_t sk_max_original_text_length = font.breakText(
+        remote_playback_message.c_str(), remote_playback_message.size(),
+        kUTF8_SkTextEncoding, sk_max_original_text_width);
+
+    // Remove the part of the string that doesn't fit and append '...'.
+    display_text.erase(
+        sk_max_original_text_length,
+        remote_playback_message.size() - sk_max_original_text_length);
+    display_text.append(kTruncationEllipsis);
+    display_text_width = font.measureText(
+        display_text.c_str(), display_text.size(), kUTF8_SkTextEncoding);
+  }
+
+  // Center the text horizontally.
+  SkScalar sk_horizontal_margin =
+      (canvas_size.width() - display_text_width) / 2.0;
+  canvas.drawSimpleText(display_text.c_str(), display_text.size(),
+                        kUTF8_SkTextEncoding, sk_horizontal_margin,
+                        sk_vertical_margin, font, paint);
+
+  GLES2Interface* gl = context_3d_cb.Run();
+
+  // GPU Process crashed.
+  if (!gl)
+    return nullptr;
+  GLuint remote_playback_texture_id = 0;
+  gl->GenTextures(1, &remote_playback_texture_id);
+  GLuint texture_target = GL_TEXTURE_2D;
+  gl->BindTexture(texture_target, remote_playback_texture_id);
+  gl->TexParameteri(texture_target, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
+  gl->TexParameteri(texture_target, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
+  gl->TexParameteri(texture_target, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
+  gl->TexParameteri(texture_target, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);
+
+  gl->TexImage2D(texture_target, 0 /* level */, GL_RGBA /* internalformat */,
+                 bitmap.width(), bitmap.height(), 0 /* border */,
+                 GL_RGBA /* format */, GL_UNSIGNED_BYTE /* type */,
+                 bitmap.getPixels());
+
+  gpu::Mailbox texture_mailbox;
+  gl->ProduceTextureDirectCHROMIUM(remote_playback_texture_id,
+                                   texture_mailbox.name);
+
+  gpu::SyncToken texture_mailbox_sync_token;
+  gl->GenUnverifiedSyncTokenCHROMIUM(texture_mailbox_sync_token.GetData());
+
+  gpu::MailboxHolder holders[media::VideoFrame::kMaxPlanes] = {
+      gpu::MailboxHolder(texture_mailbox, texture_mailbox_sync_token,
+                         texture_target)};
+  return VideoFrame::WrapNativeTextures(
+      media::PIXEL_FORMAT_ARGB, holders,
+      media::BindToCurrentLoop(base::Bind(&OnReleaseTexture, context_3d_cb,
+                                          remote_playback_texture_id)),
+      canvas_size /* coded_size */, gfx::Rect(canvas_size) /* visible_rect */,
+      natural_size /* natural_size */, base::TimeDelta() /* timestamp */);
+}
+
+WebMediaPlayerCast::WebMediaPlayerCast(
+    WebMediaPlayerImpl* impl,
+    blink::WebMediaPlayerClient* client,
+    scoped_refptr<viz::ContextProvider> context_provider)
+    : webmediaplayer_(impl),
+      client_(client),
+      context_provider_(context_provider) {}
+
+WebMediaPlayerCast::~WebMediaPlayerCast() {
+  if (player_manager_) {
+    if (is_player_initialized_)
+      player_manager_->DestroyPlayer(player_id_);
+
+    player_manager_->UnregisterMediaPlayer(player_id_);
+  }
+}
+
+void WebMediaPlayerCast::Initialize(const GURL& url,
+                                    blink::WebLocalFrame* frame,
+                                    int delegate_id) {
+  player_manager_->Initialize(MEDIA_PLAYER_TYPE_REMOTE_ONLY, player_id_, url,
+                              frame->GetDocument().SiteForCookies(),
+                              frame->GetDocument().Url(), true, delegate_id);
+  is_player_initialized_ = true;
+}
+
+void WebMediaPlayerCast::SetMediaPlayerManager(
+    RendererMediaPlayerManagerInterface* media_player_manager) {
+  player_manager_ = media_player_manager;
+  player_id_ = player_manager_->RegisterMediaPlayer(this);
+}
+
+void WebMediaPlayerCast::requestRemotePlayback() {
+  player_manager_->Seek(player_id_, base::TimeDelta::FromSecondsD(
+                                        webmediaplayer_->CurrentTime()));
+  player_manager_->RequestRemotePlayback(player_id_);
+}
+
+void WebMediaPlayerCast::requestRemotePlaybackControl() {
+  player_manager_->RequestRemotePlaybackControl(player_id_);
+}
+
+void WebMediaPlayerCast::requestRemotePlaybackStop() {
+  player_manager_->RequestRemotePlaybackStop(player_id_);
+}
+
+void WebMediaPlayerCast::OnMediaMetadataChanged(base::TimeDelta duration,
+                                                int width,
+                                                int height,
+                                                bool success) {
+  duration_ = duration;
+}
+
+void WebMediaPlayerCast::OnPlaybackComplete() {
+  DVLOG(1) << __func__;
+  webmediaplayer_->OnRemotePlaybackEnded();
+}
+
+void WebMediaPlayerCast::OnBufferingUpdate(int percentage) {
+  DVLOG(1) << __func__;
+}
+
+void WebMediaPlayerCast::OnSeekRequest(base::TimeDelta time_to_seek) {
+  DVLOG(1) << __func__;
+  client_->RequestSeek(time_to_seek.InSecondsF());
+}
+
+void WebMediaPlayerCast::OnSeekComplete(base::TimeDelta current_time) {
+  DVLOG(1) << __func__;
+  remote_time_at_ = base::TimeTicks::Now();
+  remote_time_ = current_time;
+  webmediaplayer_->OnPipelineSeeked(true);
+}
+
+void WebMediaPlayerCast::OnMediaError(int error_type) {
+  DVLOG(1) << __func__;
+}
+
+void WebMediaPlayerCast::OnVideoSizeChanged(int width, int height) {
+  DVLOG(1) << __func__;
+}
+
+void WebMediaPlayerCast::OnTimeUpdate(base::TimeDelta current_timestamp,
+                                      base::TimeTicks current_time_ticks) {
+  DVLOG(1) << __func__ << " " << current_timestamp.InSecondsF();
+  remote_time_at_ = current_time_ticks;
+  remote_time_ = current_timestamp;
+}
+
+void WebMediaPlayerCast::OnPlayerReleased() {
+  DVLOG(1) << __func__;
+}
+
+void WebMediaPlayerCast::OnConnectedToRemoteDevice(
+    const std::string& remote_playback_message) {
+  DVLOG(1) << __func__;
+  remote_time_ = base::TimeDelta::FromSecondsD(webmediaplayer_->CurrentTime());
+  is_remote_ = true;
+  initializing_ = true;
+  paused_ = false;
+  client_->RequestPlay();
+
+  remote_playback_message_ = remote_playback_message;
+  webmediaplayer_->SuspendForRemote();
+  client_->ConnectedToRemoteDevice();
+}
+
+base::TimeDelta WebMediaPlayerCast::currentTime() const {
+  base::TimeDelta ret = remote_time_;
+  if (!paused_ && !initializing_)
+    ret += base::TimeTicks::Now() - remote_time_at_;
+  return ret;
+}
+
+void WebMediaPlayerCast::play() {
+  if (!paused_)
+    return;
+
+  player_manager_->Start(player_id_);
+  remote_time_at_ = base::TimeTicks::Now();
+  paused_ = false;
+}
+
+void WebMediaPlayerCast::pause() {
+  player_manager_->Pause(player_id_, true);
+}
+
+void WebMediaPlayerCast::seek(base::TimeDelta t) {
+  should_notify_time_changed_ = true;
+  player_manager_->Seek(player_id_, t);
+}
+
+void WebMediaPlayerCast::OnDisconnectedFromRemoteDevice() {
+  DVLOG(1) << __func__;
+  if (!paused_)
+    paused_ = true;
+
+  is_remote_ = false;
+  auto t = currentTime();
+  auto d = base::TimeDelta::FromSecondsD(webmediaplayer_->Duration());
+  if (t + base::TimeDelta::FromMilliseconds(media::kTimeUpdateInterval * 2) > d)
+    t = d;
+
+  webmediaplayer_->OnDisconnectedFromRemoteDevice(t.InSecondsF());
+}
+
+void WebMediaPlayerCast::OnCancelledRemotePlaybackRequest() {
+  DVLOG(1) << __func__;
+  client_->CancelledRemotePlaybackRequest();
+}
+
+void WebMediaPlayerCast::OnRemotePlaybackStarted() {
+  client_->RemotePlaybackStarted();
+}
+
+void WebMediaPlayerCast::OnDidExitFullscreen() {
+  DVLOG(1) << __func__;
+}
+
+void WebMediaPlayerCast::OnMediaPlayerPlay() {
+  DVLOG(1) << __func__ << " is_remote_ = " << is_remote_;
+  initializing_ = false;
+  if (is_remote_ && paused_) {
+    paused_ = false;
+    remote_time_at_ = base::TimeTicks::Now();
+    client_->RequestPlay();
+  }
+  // Blink expects a timeChanged() in response to a seek().
+  if (should_notify_time_changed_)
+    client_->TimeChanged();
+}
+
+void WebMediaPlayerCast::OnMediaPlayerPause() {
+  DVLOG(1) << __func__ << " is_remote_ = " << is_remote_;
+  if (is_remote_ && !paused_) {
+    paused_ = true;
+    client_->RequestPause();
+  }
+}
+
+void WebMediaPlayerCast::OnRemoteRouteAvailabilityChanged(
+    blink::WebRemotePlaybackAvailability availability) {
+  DVLOG(1) << __func__;
+  client_->RemoteRouteAvailabilityChanged(availability);
+}
+
+void WebMediaPlayerCast::SuspendAndReleaseResources() {}
+
+void WebMediaPlayerCast::SetDeviceScaleFactor(float scale_factor) {
+  device_scale_factor_ = scale_factor;
+}
+
+scoped_refptr<VideoFrame> WebMediaPlayerCast::GetCastingBanner() {
+  DVLOG(1) << __func__;
+
+  // TODO(johnme): Should redraw this frame if the layer bounds change; but
+  // there seems no easy way to listen for the layer resizing (as opposed to
+  // OnVideoSizeChanged, which is when the frame sizes of the video file
+  // change). Perhaps have to poll (on main thread of course)?
+  gfx::Size video_size_css_px = webmediaplayer_->GetCanvasSize();
+  if (!video_size_css_px.width())
+    return nullptr;
+
+  // canvas_size will be the size in device pixels when pageScaleFactor == 1
+  gfx::Size canvas_size(
+      static_cast<int>(video_size_css_px.width() * device_scale_factor_),
+      static_cast<int>(video_size_css_px.height() * device_scale_factor_));
+
+  if (!canvas_size.width())
+    return nullptr;
+
+  return MakeTextFrameForCast(remote_playback_message_, canvas_size,
+                              webmediaplayer_->NaturalSize(),
+                              base::Bind(&GLCBShim, context_provider_));
+}
+
+void WebMediaPlayerCast::setPoster(const blink::WebURL& poster) {
+  player_manager_->SetPoster(player_id_, poster);
+}
+
+}  // namespace media
--- a/media/blink/webmediaplayer_cast_android.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/blink/webmediaplayer_cast_android.h	2019-05-17 18:53:34.120000000 +0300
@@ -0,0 +1,138 @@
+// Copyright 2016 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+// Delete this file when WMPI_CAST is no longer needed.
+
+#ifndef MEDIA_BLINK_WEBMEDIAPLAYER_CAST_ANDROID_H_
+#define MEDIA_BLINK_WEBMEDIAPLAYER_CAST_ANDROID_H_
+
+#include "base/memory/weak_ptr.h"
+#include "media/blink/media_blink_export.h"
+#include "media/blink/renderer_media_player_interface.h"
+#include "media/blink/webmediaplayer_params.h"
+#include "url/gurl.h"
+
+namespace blink {
+class WebLocalFrame;
+class WebMediaPlayerClient;
+class WebURL;
+}
+
+namespace media {
+
+class VideoFrame;
+class WebMediaPlayerImpl;
+
+// This shim allows WebMediaPlayer to act sufficiently similar to
+// WebMediaPlayerAndroid (by extending RendererMediaPlayerInterface)
+// to implement cast functionality.
+class WebMediaPlayerCast : public RendererMediaPlayerInterface {
+ public:
+  WebMediaPlayerCast(WebMediaPlayerImpl* impl,
+                     blink::WebMediaPlayerClient* client,
+                     scoped_refptr<viz::ContextProvider> context_provider);
+  ~WebMediaPlayerCast();
+
+  void Initialize(const GURL& url,
+                  blink::WebLocalFrame* frame,
+                  int delegate_id);
+
+  void requestRemotePlayback();
+  void requestRemotePlaybackControl();
+  void requestRemotePlaybackStop();
+
+  void SetMediaPlayerManager(
+      RendererMediaPlayerManagerInterface* media_player_manager);
+  bool isRemote() const { return is_remote_; }
+  bool IsPaused() const { return paused_; }
+
+  base::TimeDelta currentTime() const;
+  void play();
+  void pause();
+  void seek(base::TimeDelta t);
+
+  // RendererMediaPlayerInterface implementation
+  void OnMediaMetadataChanged(base::TimeDelta duration,
+                              int width,
+                              int height,
+                              bool success) override;
+  void OnPlaybackComplete() override;
+  void OnBufferingUpdate(int percentage) override;
+  void OnSeekRequest(base::TimeDelta time_to_seek) override;
+  void OnSeekComplete(base::TimeDelta current_time) override;
+  void OnMediaError(int error_type) override;
+  void OnVideoSizeChanged(int width, int height) override;
+
+  // Called to update the current time.
+  void OnTimeUpdate(base::TimeDelta current_timestamp,
+                    base::TimeTicks current_time_ticks) override;
+
+  // void OnWaiting(WaitingReason reason) override;
+  void OnPlayerReleased() override;
+
+  // Functions called when media player status changes.
+  void OnConnectedToRemoteDevice(
+      const std::string& remote_playback_message) override;
+  void OnDisconnectedFromRemoteDevice() override;
+  void OnCancelledRemotePlaybackRequest() override;
+  void OnRemotePlaybackStarted() override;
+  void OnDidExitFullscreen() override;
+  void OnMediaPlayerPlay() override;
+  void OnMediaPlayerPause() override;
+  void OnRemoteRouteAvailabilityChanged(
+      blink::WebRemotePlaybackAvailability availability) override;
+
+  // This function is called by the RendererMediaPlayerManager to pause the
+  // video and release the media player and surface texture when we switch tabs.
+  // However, the actual GlTexture is not released to keep the video screenshot.
+  void SuspendAndReleaseResources() override;
+
+  void SetDeviceScaleFactor(float scale_factor);
+  scoped_refptr<VideoFrame> GetCastingBanner();
+  void setPoster(const blink::WebURL& poster);
+
+ private:
+  WebMediaPlayerImpl* webmediaplayer_;
+  blink::WebMediaPlayerClient* client_;
+  scoped_refptr<viz::ContextProvider> context_provider_;
+
+  // Manages this object and delegates player calls to the browser process.
+  // Owned by RenderFrameImpl.
+  RendererMediaPlayerManagerInterface* player_manager_ = nullptr;
+
+  // Player ID assigned by the |player_manager_|.
+  int player_id_;
+
+  // Whether the browser is currently connected to a remote media player.
+  bool is_remote_ = false;
+
+  bool paused_ = true;
+  bool initializing_ = false;
+  bool should_notify_time_changed_ = false;
+
+  // Last reported playout time.
+  base::TimeDelta remote_time_;
+  base::TimeTicks remote_time_at_;
+  base::TimeDelta duration_;
+
+  // Whether the media player has been initialized.
+  bool is_player_initialized_ = false;
+
+  std::string remote_playback_message_;
+
+  float device_scale_factor_ = 1.0;
+
+  DISALLOW_COPY_AND_ASSIGN(WebMediaPlayerCast);
+};
+
+// Make a texture-backed video of the given size containing the given message.
+MEDIA_BLINK_EXPORT scoped_refptr<VideoFrame> MakeTextFrameForCast(
+    const std::string& remote_playback_message,
+    gfx::Size canvas_size,
+    gfx::Size natural_size,
+    const base::Callback<gpu::gles2::GLES2Interface*()>& context_3d_cb);
+
+}  // namespace media
+
+#endif  // MEDIA_BLINK_WEBMEDIAPLAYER_CAST_ANDROID_H_
--- a/media/capture/video/chromeos/stream_buffer_manager_unittest.cc	1970-01-01 03:00:00.000000000 +0300
+++ b/media/capture/video/chromeos/stream_buffer_manager_unittest.cc	2019-05-17 18:53:34.140000000 +0300
@@ -0,0 +1,600 @@
+// Copyright 2017 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "media/capture/video/chromeos/stream_buffer_manager.h"
+
+#include <map>
+#include <memory>
+#include <utility>
+#include <vector>
+
+#include "base/run_loop.h"
+#include "base/test/scoped_task_environment.h"
+#include "base/threading/thread.h"
+#include "base/threading/thread_task_runner_handle.h"
+#include "media/capture/video/chromeos/camera_buffer_factory.h"
+#include "media/capture/video/chromeos/camera_device_context.h"
+#include "media/capture/video/chromeos/camera_device_delegate.h"
+#include "media/capture/video/chromeos/mock_video_capture_client.h"
+#include "media/capture/video/mock_gpu_memory_buffer_manager.h"
+#include "testing/gmock/include/gmock/gmock.h"
+#include "testing/gtest/include/gtest/gtest.h"
+
+#include "media/capture/video/blob_utils.h"
+using testing::_;
+using testing::A;
+using testing::AtLeast;
+using testing::Invoke;
+using testing::InvokeWithoutArgs;
+using testing::Return;
+
+namespace media {
+
+namespace {
+
+class MockStreamCaptureInterface : public StreamCaptureInterface {
+ public:
+  void RegisterBuffer(uint64_t buffer_id,
+                      cros::mojom::Camera3DeviceOps::BufferType type,
+                      uint32_t drm_format,
+                      cros::mojom::HalPixelFormat hal_pixel_format,
+                      uint32_t width,
+                      uint32_t height,
+                      std::vector<StreamCaptureInterface::Plane> planes,
+                      base::OnceCallback<void(int32_t)> callback) {
+    DoRegisterBuffer(buffer_id, type, drm_format, hal_pixel_format, width,
+                     height, planes, callback);
+  }
+  MOCK_METHOD8(DoRegisterBuffer,
+               void(uint64_t buffer_id,
+                    cros::mojom::Camera3DeviceOps::BufferType type,
+                    uint32_t drm_format,
+                    cros::mojom::HalPixelFormat hal_pixel_format,
+                    uint32_t width,
+                    uint32_t height,
+                    const std::vector<StreamCaptureInterface::Plane>& planes,
+                    base::OnceCallback<void(int32_t)>& callback));
+
+  void ProcessCaptureRequest(cros::mojom::Camera3CaptureRequestPtr request,
+                             base::OnceCallback<void(int32_t)> callback) {
+    DoProcessCaptureRequest(request, callback);
+  }
+  MOCK_METHOD2(DoProcessCaptureRequest,
+               void(cros::mojom::Camera3CaptureRequestPtr& request,
+                    base::OnceCallback<void(int32_t)>& callback));
+
+  void Flush(base::OnceCallback<void(int32_t)> callback) { DoFlush(callback); }
+  MOCK_METHOD1(DoFlush, void(base::OnceCallback<void(int32_t)>& callback));
+};
+
+const VideoCaptureFormat kDefaultCaptureFormat(gfx::Size(1280, 720),
+                                               30.0,
+                                               PIXEL_FORMAT_NV12);
+
+class FakeCameraBufferFactory : public CameraBufferFactory {
+ public:
+  FakeCameraBufferFactory() {
+    gpu_memory_buffer_manager_ =
+        std::make_unique<unittest_internal::MockGpuMemoryBufferManager>();
+  }
+  std::unique_ptr<gfx::GpuMemoryBuffer> CreateGpuMemoryBuffer(
+      const gfx::Size& size,
+      gfx::BufferFormat format) override {
+    return unittest_internal::MockGpuMemoryBufferManager::
+        CreateFakeGpuMemoryBuffer(size, format,
+                                  gfx::BufferUsage::SCANOUT_CAMERA_READ_WRITE,
+                                  gpu::kNullSurfaceHandle);
+  }
+
+  ChromiumPixelFormat ResolveStreamBufferFormat(
+      cros::mojom::HalPixelFormat hal_format) override {
+    return ChromiumPixelFormat{PIXEL_FORMAT_NV12,
+                               gfx::BufferFormat::YUV_420_BIPLANAR};
+  }
+
+ private:
+  std::unique_ptr<unittest_internal::MockGpuMemoryBufferManager>
+      gpu_memory_buffer_manager_;
+};
+
+}  // namespace
+
+class StreamBufferManagerTest : public ::testing::Test {
+ public:
+  void SetUp() override {
+    quit_ = false;
+    cros::mojom::Camera3CallbackOpsRequest callback_ops_request =
+        mojo::MakeRequest(&mock_callback_ops_);
+    device_context_ = std::make_unique<CameraDeviceContext>(
+        std::make_unique<unittest_internal::MockVideoCaptureClient>());
+
+    stream_buffer_manager_ = std::make_unique<StreamBufferManager>(
+        std::move(callback_ops_request),
+        std::make_unique<MockStreamCaptureInterface>(), device_context_.get(),
+        std::make_unique<FakeCameraBufferFactory>(),
+        base::BindRepeating(
+            [](const uint8_t* buffer, const uint32_t bytesused,
+               const VideoCaptureFormat& capture_format,
+               const int rotation) { return mojom::Blob::New(); }),
+        base::ThreadTaskRunnerHandle::Get());
+  }
+
+  void TearDown() override { stream_buffer_manager_.reset(); }
+
+  void DoLoop() {
+    run_loop_.reset(new base::RunLoop());
+    run_loop_->Run();
+  }
+
+  void QuitCaptureLoop() {
+    quit_ = true;
+    if (run_loop_) {
+      run_loop_->Quit();
+    }
+  }
+
+  cros::mojom::CameraMetadataPtr GetFakeStaticMetadata(
+      int32_t partial_result_count) {
+    cros::mojom::CameraMetadataPtr static_metadata =
+        cros::mojom::CameraMetadata::New();
+    static_metadata->entry_count = 2;
+    static_metadata->entry_capacity = 2;
+    static_metadata->entries =
+        std::vector<cros::mojom::CameraMetadataEntryPtr>();
+
+    cros::mojom::CameraMetadataEntryPtr entry =
+        cros::mojom::CameraMetadataEntry::New();
+    entry->index = 0;
+    entry->tag =
+        cros::mojom::CameraMetadataTag::ANDROID_REQUEST_PARTIAL_RESULT_COUNT;
+    entry->type = cros::mojom::EntryType::TYPE_INT32;
+    entry->count = 1;
+    uint8_t* as_int8 = reinterpret_cast<uint8_t*>(&partial_result_count);
+    entry->data.assign(as_int8, as_int8 + entry->count * sizeof(int32_t));
+    static_metadata->entries->push_back(std::move(entry));
+
+    entry = cros::mojom::CameraMetadataEntry::New();
+    entry->index = 1;
+    entry->tag = cros::mojom::CameraMetadataTag::ANDROID_JPEG_MAX_SIZE;
+    entry->type = cros::mojom::EntryType::TYPE_INT32;
+    entry->count = 1;
+    int32_t jpeg_max_size = 65535;
+    as_int8 = reinterpret_cast<uint8_t*>(&jpeg_max_size);
+    entry->data.assign(as_int8, as_int8 + entry->count * sizeof(int32_t));
+    static_metadata->entries->push_back(std::move(entry));
+
+    return static_metadata;
+  }
+
+  void RegisterBuffer(uint64_t buffer_id,
+                      cros::mojom::Camera3DeviceOps::BufferType type,
+                      uint32_t drm_format,
+                      cros::mojom::HalPixelFormat hal_pixel_format,
+                      uint32_t width,
+                      uint32_t height,
+                      const std::vector<StreamCaptureInterface::Plane>& planes,
+                      base::OnceCallback<void(int32_t)>& callback) {
+    if (quit_) {
+      return;
+    }
+    std::move(callback).Run(0);
+  }
+
+  void ProcessCaptureRequest(cros::mojom::Camera3CaptureRequestPtr& request,
+                             base::OnceCallback<void(int32_t)>& callback) {
+    if (quit_) {
+      return;
+    }
+    std::move(callback).Run(0);
+    mock_callback_ops_->Notify(PrepareShutterNotifyMessage(
+        request->frame_number, base::TimeTicks::Now().ToInternalValue()));
+    mock_callback_ops_->ProcessCaptureResult(PrepareCapturedResult(
+        request->frame_number, cros::mojom::CameraMetadata::New(), 1,
+        std::move(request->output_buffers)));
+  }
+
+  MockStreamCaptureInterface* GetMockCaptureInterface() {
+    EXPECT_NE(nullptr, stream_buffer_manager_.get());
+    return reinterpret_cast<MockStreamCaptureInterface*>(
+        stream_buffer_manager_->capture_interface_.get());
+  }
+
+  unittest_internal::MockVideoCaptureClient* GetMockVideoCaptureClient() {
+    EXPECT_NE(nullptr, device_context_);
+    return reinterpret_cast<unittest_internal::MockVideoCaptureClient*>(
+        device_context_->client_.get());
+  }
+
+  std::map<uint32_t, StreamBufferManager::CaptureResult>& GetPendingResults() {
+    EXPECT_NE(nullptr, stream_buffer_manager_.get());
+    return stream_buffer_manager_->pending_results_;
+  }
+
+  std::vector<cros::mojom::Camera3StreamPtr> PrepareCaptureStream(
+      uint32_t max_buffers) {
+    std::vector<cros::mojom::Camera3StreamPtr> streams;
+
+    auto preview_stream = cros::mojom::Camera3Stream::New();
+    preview_stream->id = static_cast<uint64_t>(StreamType::kPreview);
+    preview_stream->stream_type =
+        cros::mojom::Camera3StreamType::CAMERA3_STREAM_OUTPUT;
+    preview_stream->width = kDefaultCaptureFormat.frame_size.width();
+    preview_stream->height = kDefaultCaptureFormat.frame_size.height();
+    preview_stream->format =
+        cros::mojom::HalPixelFormat::HAL_PIXEL_FORMAT_YCbCr_420_888;
+    preview_stream->usage = 0;
+    preview_stream->max_buffers = max_buffers;
+    preview_stream->data_space = 0;
+    preview_stream->rotation =
+        cros::mojom::Camera3StreamRotation::CAMERA3_STREAM_ROTATION_0;
+    streams.push_back(std::move(preview_stream));
+
+    auto still_capture_stream = cros::mojom::Camera3Stream::New();
+    still_capture_stream->id = static_cast<uint64_t>(StreamType::kStillCapture);
+    still_capture_stream->stream_type =
+        cros::mojom::Camera3StreamType::CAMERA3_STREAM_OUTPUT;
+    still_capture_stream->width = kDefaultCaptureFormat.frame_size.width();
+    still_capture_stream->height = kDefaultCaptureFormat.frame_size.height();
+    still_capture_stream->format =
+        cros::mojom::HalPixelFormat::HAL_PIXEL_FORMAT_BLOB;
+    still_capture_stream->usage = 0;
+    still_capture_stream->max_buffers = max_buffers;
+    still_capture_stream->data_space = 0;
+    still_capture_stream->rotation =
+        cros::mojom::Camera3StreamRotation::CAMERA3_STREAM_ROTATION_0;
+    streams.push_back(std::move(still_capture_stream));
+
+    return streams;
+  }
+
+  cros::mojom::Camera3NotifyMsgPtr PrepareErrorNotifyMessage(
+      uint32_t frame_number,
+      cros::mojom::Camera3ErrorMsgCode error_code) {
+    auto error_msg = cros::mojom::Camera3ErrorMsg::New();
+    error_msg->frame_number = frame_number;
+    // There is only the preview stream.
+    error_msg->error_stream_id = static_cast<uint64_t>(StreamType::kPreview);
+    error_msg->error_code = error_code;
+    auto notify_msg = cros::mojom::Camera3NotifyMsg::New();
+    notify_msg->message = cros::mojom::Camera3NotifyMsgMessage::New();
+    notify_msg->type = cros::mojom::Camera3MsgType::CAMERA3_MSG_ERROR;
+    notify_msg->message->set_error(std::move(error_msg));
+    return notify_msg;
+  }
+
+  cros::mojom::Camera3NotifyMsgPtr PrepareShutterNotifyMessage(
+      uint32_t frame_number,
+      uint64_t timestamp) {
+    auto shutter_msg = cros::mojom::Camera3ShutterMsg::New();
+    shutter_msg->frame_number = frame_number;
+    shutter_msg->timestamp = timestamp;
+    auto notify_msg = cros::mojom::Camera3NotifyMsg::New();
+    notify_msg->message = cros::mojom::Camera3NotifyMsgMessage::New();
+    notify_msg->type = cros::mojom::Camera3MsgType::CAMERA3_MSG_SHUTTER;
+    notify_msg->message->set_shutter(std::move(shutter_msg));
+    return notify_msg;
+  }
+
+  cros::mojom::Camera3CaptureResultPtr PrepareCapturedResult(
+      uint32_t frame_number,
+      cros::mojom::CameraMetadataPtr result_metadata,
+      uint32_t partial_result,
+      std::vector<cros::mojom::Camera3StreamBufferPtr> output_buffers) {
+    auto result = cros::mojom::Camera3CaptureResult::New();
+    result->frame_number = frame_number;
+    result->result = std::move(result_metadata);
+    if (output_buffers.size()) {
+      result->output_buffers = std::move(output_buffers);
+    }
+    result->partial_result = partial_result;
+    return result;
+  }
+
+ protected:
+  std::unique_ptr<StreamBufferManager> stream_buffer_manager_;
+  cros::mojom::Camera3CallbackOpsPtr mock_callback_ops_;
+  std::unique_ptr<CameraDeviceContext> device_context_;
+  cros::mojom::Camera3StreamPtr stream;
+
+ private:
+  std::unique_ptr<base::RunLoop> run_loop_;
+  bool quit_;
+  base::test::ScopedTaskEnvironment scoped_test_environment_;
+};
+
+// A basic sanity test to capture one frame with the capture loop.
+TEST_F(StreamBufferManagerTest, SimpleCaptureTest) {
+  GetMockVideoCaptureClient()->SetFrameCb(base::BindOnce(
+      &StreamBufferManagerTest::QuitCaptureLoop, base::Unretained(this)));
+  EXPECT_CALL(
+      *GetMockCaptureInterface(),
+      DoRegisterBuffer(
+          StreamBufferManager::GetBufferIpcId(StreamType::kPreview, 0),
+          cros::mojom::Camera3DeviceOps::BufferType::GRALLOC, _, _, _, _, _, _))
+      .Times(AtLeast(1))
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::RegisterBuffer));
+  EXPECT_CALL(*GetMockCaptureInterface(), DoProcessCaptureRequest(_, _))
+      .Times(1)
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::ProcessCaptureRequest));
+
+  stream_buffer_manager_->SetUpStreamsAndBuffers(
+      kDefaultCaptureFormat,
+      GetFakeStaticMetadata(/* partial_result_count */ 1),
+      PrepareCaptureStream(/* max_buffers */ 1));
+  stream_buffer_manager_->StartPreview(cros::mojom::CameraMetadata::New());
+
+  // Wait until a captured frame is received by MockVideoCaptureClient.
+  DoLoop();
+}
+
+// Test that the StreamBufferManager submits a captured result only after all
+// partial metadata are received.
+TEST_F(StreamBufferManagerTest, PartialResultTest) {
+  GetMockVideoCaptureClient()->SetFrameCb(base::BindOnce(
+      [](StreamBufferManagerTest* test) {
+        EXPECT_EQ(1u, test->GetPendingResults().size());
+        // Make sure all the three partial metadata are received before the
+        // captured result is submitted.
+        EXPECT_EQ(
+            3u, test->GetPendingResults()[0].partial_metadata_received.size());
+        test->QuitCaptureLoop();
+      },
+      base::Unretained(this)));
+  EXPECT_CALL(
+      *GetMockCaptureInterface(),
+      DoRegisterBuffer(
+          StreamBufferManager::GetBufferIpcId(StreamType::kPreview, 0),
+          cros::mojom::Camera3DeviceOps::BufferType::GRALLOC, _, _, _, _, _, _))
+      .Times(AtLeast(1))
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::RegisterBuffer));
+  EXPECT_CALL(*GetMockCaptureInterface(), DoProcessCaptureRequest(_, _))
+      .Times(1)
+      .WillOnce(Invoke([this](cros::mojom::Camera3CaptureRequestPtr& request,
+                              base::OnceCallback<void(int32_t)>& callback) {
+        std::move(callback).Run(0);
+        mock_callback_ops_->Notify(PrepareShutterNotifyMessage(
+            request->frame_number, base::TimeTicks::Now().ToInternalValue()));
+        mock_callback_ops_->ProcessCaptureResult(PrepareCapturedResult(
+            request->frame_number, cros::mojom::CameraMetadata::New(), 1,
+            std::move(request->output_buffers)));
+        mock_callback_ops_->ProcessCaptureResult(PrepareCapturedResult(
+            request->frame_number, cros::mojom::CameraMetadata::New(), 2,
+            std::vector<cros::mojom::Camera3StreamBufferPtr>()));
+        mock_callback_ops_->ProcessCaptureResult(PrepareCapturedResult(
+            request->frame_number, cros::mojom::CameraMetadata::New(), 3,
+            std::vector<cros::mojom::Camera3StreamBufferPtr>()));
+      }));
+
+  stream_buffer_manager_->SetUpStreamsAndBuffers(
+      kDefaultCaptureFormat,
+      GetFakeStaticMetadata(/* partial_result_count */ 3),
+      PrepareCaptureStream(/* max_buffers */ 1));
+  stream_buffer_manager_->StartPreview(cros::mojom::CameraMetadata::New());
+
+  // Wait until a captured frame is received by MockVideoCaptureClient.
+  DoLoop();
+}
+
+// Test that the capture loop is stopped and no frame is submitted when a device
+// error happens.
+TEST_F(StreamBufferManagerTest, DeviceErrorTest) {
+  GetMockVideoCaptureClient()->SetFrameCb(base::BindOnce(
+      [](StreamBufferManagerTest* test) {
+        ADD_FAILURE() << "No frame should be submitted";
+        test->QuitCaptureLoop();
+      },
+      base::Unretained(this)));
+  EXPECT_CALL(*GetMockVideoCaptureClient(), OnError(_, _, _))
+      .Times(1)
+      .WillOnce(
+          InvokeWithoutArgs(this, &StreamBufferManagerTest::QuitCaptureLoop));
+  EXPECT_CALL(
+      *GetMockCaptureInterface(),
+      DoRegisterBuffer(
+          StreamBufferManager::GetBufferIpcId(StreamType::kPreview, 0),
+          cros::mojom::Camera3DeviceOps::BufferType::GRALLOC, _, _, _, _, _, _))
+      .Times(1)
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::RegisterBuffer));
+  EXPECT_CALL(*GetMockCaptureInterface(), DoProcessCaptureRequest(_, _))
+      .Times(1)
+      .WillOnce(Invoke([this](cros::mojom::Camera3CaptureRequestPtr& request,
+                              base::OnceCallback<void(int32_t)>& callback) {
+        std::move(callback).Run(0);
+        mock_callback_ops_->Notify(PrepareErrorNotifyMessage(
+            request->frame_number,
+            cros::mojom::Camera3ErrorMsgCode::CAMERA3_MSG_ERROR_DEVICE));
+      }));
+
+  stream_buffer_manager_->SetUpStreamsAndBuffers(
+      kDefaultCaptureFormat,
+      GetFakeStaticMetadata(/* partial_result_count */ 1),
+      PrepareCaptureStream(/* max_buffers */ 1));
+  stream_buffer_manager_->StartPreview(cros::mojom::CameraMetadata::New());
+
+  // Wait until the MockVideoCaptureClient is deleted.
+  DoLoop();
+}
+
+// Test that upon request error the erroneous frame is dropped, and the capture
+// loop continues.
+TEST_F(StreamBufferManagerTest, RequestErrorTest) {
+  GetMockVideoCaptureClient()->SetFrameCb(base::BindOnce(
+      [](StreamBufferManagerTest* test) {
+        // Frame 0 should be dropped, and the frame callback should be called
+        // with frame 1.
+        EXPECT_EQ(test->GetPendingResults().end(),
+                  test->GetPendingResults().find(0));
+        EXPECT_NE(test->GetPendingResults().end(),
+                  test->GetPendingResults().find(1));
+        test->QuitCaptureLoop();
+      },
+      base::Unretained(this)));
+  EXPECT_CALL(
+      *GetMockCaptureInterface(),
+      DoRegisterBuffer(
+          StreamBufferManager::GetBufferIpcId(StreamType::kPreview, 0),
+          cros::mojom::Camera3DeviceOps::BufferType::GRALLOC, _, _, _, _, _, _))
+      .Times(AtLeast(2))
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::RegisterBuffer))
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::RegisterBuffer));
+  EXPECT_CALL(*GetMockCaptureInterface(), DoProcessCaptureRequest(_, _))
+      .Times(2)
+      .WillOnce(Invoke([this](cros::mojom::Camera3CaptureRequestPtr& request,
+                              base::OnceCallback<void(int32_t)>& callback) {
+        std::move(callback).Run(0);
+        mock_callback_ops_->Notify(PrepareErrorNotifyMessage(
+            request->frame_number,
+            cros::mojom::Camera3ErrorMsgCode::CAMERA3_MSG_ERROR_REQUEST));
+        request->output_buffers[0]->status =
+            cros::mojom::Camera3BufferStatus::CAMERA3_BUFFER_STATUS_ERROR;
+        mock_callback_ops_->ProcessCaptureResult(PrepareCapturedResult(
+            request->frame_number, cros::mojom::CameraMetadata::New(), 1,
+            std::move(request->output_buffers)));
+      }))
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::ProcessCaptureRequest));
+
+  stream_buffer_manager_->SetUpStreamsAndBuffers(
+      kDefaultCaptureFormat,
+      GetFakeStaticMetadata(/* partial_result_count */ 1),
+      PrepareCaptureStream(/* max_buffers */ 1));
+  stream_buffer_manager_->StartPreview(cros::mojom::CameraMetadata::New());
+
+  // Wait until the MockVideoCaptureClient is deleted.
+  DoLoop();
+}
+
+// Test that upon result error the captured buffer is submitted despite of the
+// missing result metadata, and the capture loop continues.
+TEST_F(StreamBufferManagerTest, ResultErrorTest) {
+  GetMockVideoCaptureClient()->SetFrameCb(base::BindOnce(
+      [](StreamBufferManagerTest* test) {
+        // Frame 0 should be submitted.
+        EXPECT_NE(test->GetPendingResults().end(),
+                  test->GetPendingResults().find(0));
+        test->QuitCaptureLoop();
+      },
+      base::Unretained(this)));
+  EXPECT_CALL(
+      *GetMockCaptureInterface(),
+      DoRegisterBuffer(
+          StreamBufferManager::GetBufferIpcId(StreamType::kPreview, 0),
+          cros::mojom::Camera3DeviceOps::BufferType::GRALLOC, _, _, _, _, _, _))
+      .Times(AtLeast(1))
+      .WillRepeatedly(Invoke(this, &StreamBufferManagerTest::RegisterBuffer));
+  EXPECT_CALL(*GetMockCaptureInterface(), DoProcessCaptureRequest(_, _))
+      .Times(AtLeast(1))
+      .WillOnce(Invoke([this](cros::mojom::Camera3CaptureRequestPtr& request,
+                              base::OnceCallback<void(int32_t)>& callback) {
+        std::move(callback).Run(0);
+        mock_callback_ops_->Notify(PrepareShutterNotifyMessage(
+            request->frame_number, base::TimeTicks::Now().ToInternalValue()));
+        mock_callback_ops_->ProcessCaptureResult(PrepareCapturedResult(
+            request->frame_number, cros::mojom::CameraMetadata::New(), 1,
+            std::move(request->output_buffers)));
+        // Send a result error notify without sending the second partial result.
+        // StreamBufferManager should submit the buffer when it receives the
+        // result error.
+        mock_callback_ops_->Notify(PrepareErrorNotifyMessage(
+            request->frame_number,
+            cros::mojom::Camera3ErrorMsgCode::CAMERA3_MSG_ERROR_RESULT));
+      }))
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::ProcessCaptureRequest));
+
+  stream_buffer_manager_->SetUpStreamsAndBuffers(
+      kDefaultCaptureFormat,
+      GetFakeStaticMetadata(/* partial_result_count */ 2),
+      PrepareCaptureStream(/* max_buffers */ 1));
+  stream_buffer_manager_->StartPreview(cros::mojom::CameraMetadata::New());
+
+  // Wait until the MockVideoCaptureClient is deleted.
+  DoLoop();
+}
+
+// Test that upon buffer error the erroneous buffer is dropped, and the capture
+// loop continues.
+TEST_F(StreamBufferManagerTest, BufferErrorTest) {
+  GetMockVideoCaptureClient()->SetFrameCb(base::BindOnce(
+      [](StreamBufferManagerTest* test) {
+        // Frame 0 should be dropped, and the frame callback should be called
+        // with frame 1.
+        EXPECT_EQ(test->GetPendingResults().end(),
+                  test->GetPendingResults().find(0));
+        EXPECT_NE(test->GetPendingResults().end(),
+                  test->GetPendingResults().find(1));
+        test->QuitCaptureLoop();
+      },
+      base::Unretained(this)));
+  EXPECT_CALL(
+      *GetMockCaptureInterface(),
+      DoRegisterBuffer(
+          StreamBufferManager::GetBufferIpcId(StreamType::kPreview, 0),
+          cros::mojom::Camera3DeviceOps::BufferType::GRALLOC, _, _, _, _, _, _))
+      .Times(AtLeast(2))
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::RegisterBuffer))
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::RegisterBuffer));
+  EXPECT_CALL(*GetMockCaptureInterface(), DoProcessCaptureRequest(_, _))
+      .Times(2)
+      .WillOnce(Invoke([this](cros::mojom::Camera3CaptureRequestPtr& request,
+                              base::OnceCallback<void(int32_t)>& callback) {
+        std::move(callback).Run(0);
+        mock_callback_ops_->Notify(PrepareShutterNotifyMessage(
+            request->frame_number, base::TimeTicks::Now().ToInternalValue()));
+        mock_callback_ops_->Notify(PrepareErrorNotifyMessage(
+            request->frame_number,
+            cros::mojom::Camera3ErrorMsgCode::CAMERA3_MSG_ERROR_BUFFER));
+        request->output_buffers[0]->status =
+            cros::mojom::Camera3BufferStatus::CAMERA3_BUFFER_STATUS_ERROR;
+        mock_callback_ops_->ProcessCaptureResult(PrepareCapturedResult(
+            request->frame_number, cros::mojom::CameraMetadata::New(), 1,
+            std::move(request->output_buffers)));
+      }))
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::ProcessCaptureRequest));
+
+  stream_buffer_manager_->SetUpStreamsAndBuffers(
+      kDefaultCaptureFormat,
+      GetFakeStaticMetadata(/* partial_result_count */ 1),
+      PrepareCaptureStream(/* max_buffers */ 1));
+  stream_buffer_manager_->StartPreview(cros::mojom::CameraMetadata::New());
+
+  // Wait until the MockVideoCaptureClient is deleted.
+  DoLoop();
+}
+
+// Test that preview and still capture buffers can be correctly submitted.
+TEST_F(StreamBufferManagerTest, TakePhotoTest) {
+  EXPECT_CALL(
+      *GetMockCaptureInterface(),
+      DoRegisterBuffer(
+          StreamBufferManager::GetBufferIpcId(StreamType::kPreview, 0),
+          cros::mojom::Camera3DeviceOps::BufferType::GRALLOC, _, _, _, _, _, _))
+      .Times(AtLeast(1))
+      .WillRepeatedly(Invoke(this, &StreamBufferManagerTest::RegisterBuffer));
+  EXPECT_CALL(
+      *GetMockCaptureInterface(),
+      DoRegisterBuffer(
+          StreamBufferManager::GetBufferIpcId(StreamType::kStillCapture, 0),
+          cros::mojom::Camera3DeviceOps::BufferType::GRALLOC, _, _, _, _, _, _))
+      .Times(1)
+      .WillOnce(Invoke(this, &StreamBufferManagerTest::RegisterBuffer));
+  EXPECT_CALL(*GetMockCaptureInterface(), DoProcessCaptureRequest(_, _))
+      .Times(AtLeast(1))
+      .WillRepeatedly(
+          Invoke(this, &StreamBufferManagerTest::ProcessCaptureRequest));
+
+  stream_buffer_manager_->SetUpStreamsAndBuffers(
+      kDefaultCaptureFormat,
+      GetFakeStaticMetadata(/* partial_result_count */ 1),
+      PrepareCaptureStream(/* max_buffers */ 1));
+  stream_buffer_manager_->StartPreview(cros::mojom::CameraMetadata::New());
+  stream_buffer_manager_->TakePhoto(
+      GetFakeStaticMetadata(/* partial_result_count */ 1),
+      base::BindOnce([](StreamBufferManagerTest* test,
+                        mojom::BlobPtr blob) { test->QuitCaptureLoop(); },
+                     base::Unretained(this)));
+
+  // Wait until a captured frame is received by MockVideoCaptureClient.
+  DoLoop();
+}
+
+}  // namespace media
--- a/media/gpu/test/video_image_info.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/gpu/test/video_image_info.h	2019-05-17 18:53:34.264000000 +0300
@@ -0,0 +1,60 @@
+
+// Copyright 2019 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef MEDIA_GPU_TEST_VIDEO_IMAGE_INFO_H_
+#define MEDIA_GPU_TEST_VIDEO_IMAGE_INFO_H_
+
+#include <vector>
+
+#include "base/files/file_path.h"
+#include "base/optional.h"
+#include "media/base/video_frame.h"
+#include "media/base/video_frame_layout.h"
+#include "media/base/video_types.h"
+#include "ui/gfx/geometry/size.h"
+
+namespace media {
+namespace test {
+
+// VideoImageInfo is the information about raw video frame in file.
+struct VideoImageInfo {
+  // TODO(crbug.com/917951): Deprecate this constructor once we load these info
+  // from json file.
+  constexpr VideoImageInfo(const base::FilePath::CharType* const file_name,
+                           const char* const md5sum,
+                           VideoPixelFormat pixel_format,
+                           gfx::Size size)
+      : file_name(file_name),
+        md5sum(md5sum),
+        pixel_format(pixel_format),
+        visible_size(size.width(), size.height()) {}
+  VideoImageInfo() = delete;
+  ~VideoImageInfo() = default;
+
+  base::Optional<VideoFrameLayout> VideoFrameLayout() const {
+    return VideoFrameLayout::CreateWithStrides(
+        pixel_format, visible_size,
+        VideoFrame::ComputeStrides(pixel_format, visible_size),
+        std::vector<size_t>(VideoFrame::NumPlanes(pixel_format),
+                            0) /* buffer_sizes */);
+  }
+
+  // |file_name| is a file name to be read(e.g. "bear_320x192.i420.yuv"), not
+  // file path.
+  const base::FilePath::CharType* const file_name;
+  //| md5sum| is the md5sum value of the video frame, whose coded_size is the
+  // same as visible size.
+  const char* const md5sum;
+
+  // |pixel_format| and |visible_size| of the video frame in file.
+  // NOTE: visible_size should be the same as coded_size, i.e., there is no
+  // extra padding in the file.
+  const VideoPixelFormat pixel_format;
+  const gfx::Size visible_size;
+};
+
+}  // namespace test
+}  // namespace media
+#endif  // MEDIA_GPU_TEST_VIDEO_IMAGE_INFO_H_
--- a/media/gpu/vaapi/vaapi_jpeg_decode_accelerator_unittest.cc	1970-01-01 03:00:00.000000000 +0300
+++ b/media/gpu/vaapi/vaapi_jpeg_decode_accelerator_unittest.cc	2019-05-17 18:53:34.272000000 +0300
@@ -0,0 +1,282 @@
+// Copyright 2015 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include <stdint.h>
+#include <string.h>
+
+#include <va/va.h>
+
+// This has to be included first.
+// See http://code.google.com/p/googletest/issues/detail?id=371
+#include "testing/gtest/include/gtest/gtest.h"
+
+#include "base/at_exit.h"
+#include "base/bind.h"
+#include "base/command_line.h"
+#include "base/files/file_util.h"
+#include "base/logging.h"
+#include "base/md5.h"
+#include "base/path_service.h"
+#include "base/strings/string_piece.h"
+#include "base/test/gtest_util.h"
+#include "media/base/test_data_util.h"
+#include "media/base/video_frame.h"
+#include "media/filters/jpeg_parser.h"
+#include "media/gpu/vaapi/vaapi_jpeg_decode_accelerator.h"
+#include "media/gpu/vaapi/vaapi_utils.h"
+#include "media/gpu/vaapi/vaapi_wrapper.h"
+
+namespace media {
+namespace {
+
+constexpr const char* kTestFilename = "pixel-1280x720.jpg";
+constexpr const char* kExpectedMd5SumI420 = "6e9e1716073c9a9a1282e3f0e0dab743";
+constexpr const char* kExpectedMd5SumYUYV = "ff313a6aedbc4e157561e5c2d5c2e079";
+
+constexpr VAImageFormat kImageFormatI420 = {.fourcc = VA_FOURCC_I420,
+                                            .byte_order = VA_LSB_FIRST,
+                                            .bits_per_pixel = 12};
+constexpr VAImageFormat kImageFormatYUYV = {.fourcc = VA_FOURCC_YUYV,
+                                            .byte_order = VA_LSB_FIRST,
+                                            .bits_per_pixel = 16};
+
+void LogOnError() {
+  LOG(FATAL) << "Oh noes! Decoder failed";
+}
+
+uint32_t GetVASurfaceFormat() {
+  if (VaapiWrapper::IsImageFormatSupported(kImageFormatI420))
+    return VA_RT_FORMAT_YUV420;
+  else if (VaapiWrapper::IsImageFormatSupported(kImageFormatYUYV))
+    return VA_RT_FORMAT_YUV422;
+
+  LOG(FATAL) << "Neither I420 nor YUY2 is supported.";
+  return 0;
+}
+
+VAImageFormat GetVAImageFormat() {
+  if (VaapiWrapper::IsImageFormatSupported(kImageFormatI420))
+    return kImageFormatI420;
+  else if (VaapiWrapper::IsImageFormatSupported(kImageFormatYUYV))
+    return kImageFormatYUYV;
+
+  LOG(FATAL) << "Neither I420 nor YUY2 is supported.";
+  return VAImageFormat{};
+}
+
+}  // namespace
+
+class VaapiJpegDecodeAcceleratorTest : public ::testing::Test {
+ protected:
+  VaapiJpegDecodeAcceleratorTest() {
+    const base::CommandLine* cmd_line = base::CommandLine::ForCurrentProcess();
+    if (cmd_line && cmd_line->HasSwitch("test_data_path")) {
+      test_data_path_ = cmd_line->GetSwitchValueASCII("test_data_path");
+    }
+  }
+
+  void SetUp() override {
+    base::RepeatingClosure report_error_cb = base::BindRepeating(&LogOnError);
+    wrapper_ = VaapiWrapper::Create(VaapiWrapper::kDecode,
+                                    VAProfileJPEGBaseline, report_error_cb);
+    ASSERT_TRUE(wrapper_);
+
+    // Load the test data, if not loaded yet.
+    if (jpeg_data_.size() == 0) {
+      base::FilePath input_file = FindTestDataFilePath(kTestFilename);
+      ASSERT_TRUE(base::ReadFileToString(input_file, &jpeg_data_))
+          << "failed to read input data from " << input_file.value();
+    }
+  }
+
+  void TearDown() override { wrapper_ = nullptr; }
+
+  base::FilePath FindTestDataFilePath(const std::string& file_name);
+
+  bool VerifyDecode(const JpegParseResult& parse_result) const;
+  bool Decode(VaapiWrapper* vaapi_wrapper,
+              const JpegParseResult& parse_result,
+              VASurfaceID va_surface) const;
+
+  base::Lock* GetVaapiWrapperLock() const LOCK_RETURNED(wrapper_->va_lock_) {
+    return wrapper_->va_lock_;
+  }
+
+  VADisplay GetVaapiWrapperVaDisplay() const
+      EXCLUSIVE_LOCKS_REQUIRED(wrapper_->va_lock_) {
+    return wrapper_->va_display_;
+  }
+
+ protected:
+  scoped_refptr<VaapiWrapper> wrapper_;
+  std::string jpeg_data_;
+  std::string test_data_path_;
+};
+
+// Find the location of the specified test file. If a file with specified path
+// is not found, treat the file as being relative to the test file directory.
+// This is either a custom test data path provided by --test_data_path, or the
+// default test data path (//media/test/data).
+base::FilePath VaapiJpegDecodeAcceleratorTest::FindTestDataFilePath(
+    const std::string& file_name) {
+  const base::FilePath file_path = base::FilePath(file_name);
+  if (base::PathExists(file_path))
+    return file_path;
+  if (!test_data_path_.empty())
+    return base::FilePath(test_data_path_).Append(file_path);
+  return GetTestDataFilePath(file_name);
+}
+
+bool VaapiJpegDecodeAcceleratorTest::VerifyDecode(
+    const JpegParseResult& parse_result) const {
+  gfx::Size size(parse_result.frame_header.coded_width,
+                 parse_result.frame_header.coded_height);
+
+  uint32_t va_surface_format = GetVASurfaceFormat();
+  VAImageFormat va_image_format = GetVAImageFormat();
+
+  // Depending on the platform, the HW decoder will either convert the image to
+  // the I420 format, or use the JPEG's chroma sub-sampling type.
+  const char* expected_md5sum = nullptr;
+  VideoPixelFormat pixel_format = PIXEL_FORMAT_UNKNOWN;
+  if (VaapiWrapper::IsImageFormatSupported(kImageFormatI420)) {
+    expected_md5sum = kExpectedMd5SumI420;
+    pixel_format = PIXEL_FORMAT_I420;
+  } else if (VaapiWrapper::IsImageFormatSupported(kImageFormatYUYV)) {
+    expected_md5sum = kExpectedMd5SumYUYV;
+    pixel_format = PIXEL_FORMAT_YUY2;
+  } else {
+    LOG(FATAL) << "Neither I420 nor YUY2 is supported.";
+  }
+
+  std::vector<VASurfaceID> va_surfaces;
+  if (!wrapper_->CreateContextAndSurfaces(va_surface_format, size, 1,
+                                          &va_surfaces)) {
+    return false;
+  }
+
+  EXPECT_EQ(va_surfaces.size(), 1u);
+  if (va_surfaces.size() == 0 ||
+      !Decode(wrapper_.get(), parse_result, va_surfaces[0])) {
+    LOG(ERROR) << "Decode failed";
+    return false;
+  }
+
+  auto scoped_image =
+      wrapper_->CreateVaImage(va_surfaces[0], &va_image_format, size);
+  if (!scoped_image) {
+    LOG(ERROR) << "Cannot get VAImage";
+    return false;
+  }
+
+  EXPECT_TRUE(va_image_format.fourcc == scoped_image->image()->format.fourcc);
+  const auto* mem = static_cast<char*>(scoped_image->va_buffer()->data());
+
+  base::StringPiece result(mem, VideoFrame::AllocationSize(pixel_format, size));
+  EXPECT_EQ(expected_md5sum, base::MD5String(result));
+
+  return true;
+}
+
+bool VaapiJpegDecodeAcceleratorTest::Decode(VaapiWrapper* vaapi_wrapper,
+                                            const JpegParseResult& parse_result,
+                                            VASurfaceID va_surface) const {
+  return VaapiJpegDecodeAccelerator::DoDecode(vaapi_wrapper, parse_result,
+                                              va_surface);
+}
+
+TEST_F(VaapiJpegDecodeAcceleratorTest, DecodeSuccess) {
+  JpegParseResult parse_result;
+  ASSERT_TRUE(
+      ParseJpegPicture(reinterpret_cast<const uint8_t*>(jpeg_data_.data()),
+                       jpeg_data_.size(), &parse_result));
+
+  EXPECT_TRUE(VerifyDecode(parse_result));
+}
+
+TEST_F(VaapiJpegDecodeAcceleratorTest, DecodeFail) {
+  JpegParseResult parse_result;
+  ASSERT_TRUE(
+      ParseJpegPicture(reinterpret_cast<const uint8_t*>(jpeg_data_.data()),
+                       jpeg_data_.size(), &parse_result));
+
+  // Not supported by VAAPI.
+  parse_result.frame_header.num_components = 1;
+  parse_result.scan.num_components = 1;
+
+  gfx::Size size(parse_result.frame_header.coded_width,
+                 parse_result.frame_header.coded_height);
+
+  std::vector<VASurfaceID> va_surfaces;
+  ASSERT_TRUE(wrapper_->CreateContextAndSurfaces(GetVASurfaceFormat(), size, 1,
+                                                 &va_surfaces));
+
+  EXPECT_FALSE(Decode(wrapper_.get(), parse_result, va_surfaces[0]));
+}
+
+// This test exercises the usual ScopedVAImage lifetime.
+TEST_F(VaapiJpegDecodeAcceleratorTest, ScopedVAImage) {
+  std::vector<VASurfaceID> va_surfaces;
+  const gfx::Size coded_size(64, 64);
+  ASSERT_TRUE(wrapper_->CreateContextAndSurfaces(VA_RT_FORMAT_YUV420,
+                                                 coded_size, 1, &va_surfaces));
+  ASSERT_EQ(va_surfaces.size(), 1u);
+
+  std::unique_ptr<ScopedVAImage> scoped_image;
+  {
+    // On Stoney-Ridge devices the output image format is dependent on the
+    // surface format. However when DoDecode() is not called the output image
+    // format seems to default to I420. https://crbug.com/828119
+    VAImageFormat va_image_format = kImageFormatI420;
+    base::AutoLock auto_lock(*GetVaapiWrapperLock());
+    scoped_image = std::make_unique<ScopedVAImage>(
+        GetVaapiWrapperLock(), GetVaapiWrapperVaDisplay(), va_surfaces[0],
+        &va_image_format, coded_size);
+
+    EXPECT_TRUE(scoped_image->image());
+    ASSERT_TRUE(scoped_image->IsValid());
+    EXPECT_TRUE(scoped_image->va_buffer()->IsValid());
+    EXPECT_TRUE(scoped_image->va_buffer()->data());
+  }
+}
+
+// This test exercises creation of a ScopedVAImage with a bad VASurfaceID.
+TEST_F(VaapiJpegDecodeAcceleratorTest, BadScopedVAImage) {
+  const std::vector<VASurfaceID> va_surfaces = {VA_INVALID_ID};
+  const gfx::Size coded_size(64, 64);
+
+  std::unique_ptr<ScopedVAImage> scoped_image;
+  {
+    VAImageFormat va_image_format = kImageFormatI420;
+    base::AutoLock auto_lock(*GetVaapiWrapperLock());
+    scoped_image = std::make_unique<ScopedVAImage>(
+        GetVaapiWrapperLock(), GetVaapiWrapperVaDisplay(), va_surfaces[0],
+        &va_image_format, coded_size);
+
+    EXPECT_TRUE(scoped_image->image());
+    EXPECT_FALSE(scoped_image->IsValid());
+#if DCHECK_IS_ON()
+    EXPECT_DCHECK_DEATH(scoped_image->va_buffer());
+#else
+    EXPECT_FALSE(scoped_image->va_buffer());
+#endif
+  }
+}
+
+// This test exercises creation of a ScopedVABufferMapping with bad VABufferIDs.
+TEST_F(VaapiJpegDecodeAcceleratorTest, BadScopedVABufferMapping) {
+  base::AutoLock auto_lock(*GetVaapiWrapperLock());
+
+  // A ScopedVABufferMapping with a VA_INVALID_ID VABufferID is DCHECK()ed.
+  EXPECT_DCHECK_DEATH(std::make_unique<ScopedVABufferMapping>(
+      GetVaapiWrapperLock(), GetVaapiWrapperVaDisplay(), VA_INVALID_ID));
+
+  // This should not hit any DCHECK() but will create an invalid
+  // ScopedVABufferMapping.
+  auto scoped_buffer = std::make_unique<ScopedVABufferMapping>(
+      GetVaapiWrapperLock(), GetVaapiWrapperVaDisplay(), VA_INVALID_ID - 1);
+  EXPECT_FALSE(scoped_buffer->IsValid());
+}
+
+}  // namespace media
--- a/media/learning/impl/learning_task_controller.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/learning/impl/learning_task_controller.h	2019-05-17 18:53:34.288000000 +0300
@@ -0,0 +1,41 @@
+// Copyright 2018 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef MEDIA_LEARNING_IMPL_LEARNING_TASK_CONTROLLER_H_
+#define MEDIA_LEARNING_IMPL_LEARNING_TASK_CONTROLLER_H_
+
+#include "base/callback.h"
+#include "base/component_export.h"
+#include "base/macros.h"
+#include "media/learning/common/labelled_example.h"
+#include "media/learning/common/learning_task.h"
+
+namespace media {
+namespace learning {
+
+// Controller for a single learning task.  Takes training examples, and forwards
+// them to the learner(s).  Responsible for things like:
+//  - Managing underlying learner(s) based on the learning task
+//  - Feature subset selection
+//  - UMA reporting on accuracy / feature importance
+//
+// The idea is that one can create a LearningTask, give it to an LTC, and the
+// LTC will do the work of building / evaluating the model based on training
+// examples that are provided to it.
+class COMPONENT_EXPORT(LEARNING_IMPL) LearningTaskController {
+ public:
+  LearningTaskController() = default;
+  virtual ~LearningTaskController() = default;
+
+  // Receive an example for this task.
+  virtual void AddExample(const LabelledExample& example) = 0;
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(LearningTaskController);
+};
+
+}  // namespace learning
+}  // namespace media
+
+#endif  // MEDIA_LEARNING_IMPL_LEARNING_TASK_CONTROLLER_H_
  a/media/midi/fuzz/corpus/broken_data_3  b/media/midi/fuzz/corpus/broken_data_3 
--- a/media/midi/fuzz/corpus/channel_pressure	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/channel_pressure	2019-05-17 18:53:34.292000000 +0300
@@ -0,0 +1 @@
+
\      
--- a/media/midi/fuzz/corpus/channel_pressure_with_running_status	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/channel_pressure_with_running_status	2019-05-17 18:53:34.292000000 +0300
@@ -0,0 +1 @@
+
\      
  a/media/midi/fuzz/corpus/data_byte_0  b/media/midi/fuzz/corpus/data_byte_0 
--- a/media/midi/fuzz/corpus/gm_on	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/gm_on	2019-05-17 18:53:34.296000000 +0300
@@ -0,0 +1 @@
+~	
\      
--- a/media/midi/fuzz/corpus/gm_on_with_real_time_clock	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/gm_on_with_real_time_clock	2019-05-17 18:53:34.296000000 +0300
@@ -0,0 +1 @@
+~	
\      
  a/media/midi/fuzz/corpus/gs_on  b/media/midi/fuzz/corpus/gs_on 
  a/media/midi/fuzz/corpus/mtc_frame  b/media/midi/fuzz/corpus/mtc_frame 
--- a/media/midi/fuzz/corpus/note_on	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/note_on	2019-05-17 18:53:34.296000000 +0300
@@ -0,0 +1 @@
+<
\      
--- a/media/midi/fuzz/corpus/note_on_with_real_time_clock	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/note_on_with_real_time_clock	2019-05-17 18:53:34.296000000 +0300
@@ -0,0 +1 @@
+<<
\      
--- a/media/midi/fuzz/corpus/note_on_with_running_status	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/note_on_with_running_status	2019-05-17 18:53:34.296000000 +0300
@@ -0,0 +1 @@
+<<<
\      
--- a/media/midi/fuzz/corpus/partial_gm_on_2	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/partial_gm_on_2	2019-05-17 18:53:34.296000000 +0300
@@ -0,0 +1 @@
+~	
\      
--- a/media/midi/fuzz/corpus/partial_note_on_3	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/partial_note_on_3	2019-05-17 18:53:34.296000000 +0300
@@ -0,0 +1 @@
+
\      
--- a/media/midi/fuzz/corpus/reserved_message_1_with_data_bytes	1970-01-01 03:00:00.000000000 +0300
+++ b/media/midi/fuzz/corpus/reserved_message_1_with_data_bytes	2019-05-17 18:53:34.296000000 +0300
@@ -0,0 +1 @@
+
\      
--- a/media/mojo/services/cdm_manifest.json	1970-01-01 03:00:00.000000000 +0300
+++ b/media/mojo/services/cdm_manifest.json	2019-05-17 18:53:34.320000000 +0300
@@ -0,0 +1,15 @@
+{
+  "name": "cdm",
+  "display_name": "Content Decryption Module Service",
+  "sandbox_type": "cdm",
+  "interface_provider_specs": {
+    "service_manager:connector": {
+      "provides": {
+        "media:cdm": [ "media.mojom.CdmService" ]
+      },
+      "requires": {
+        "*": [ "app" ]
+      }
+    }
+  }
+}
--- a/media/mojo/services/media_manifest.json	1970-01-01 03:00:00.000000000 +0300
+++ b/media/mojo/services/media_manifest.json	2019-05-17 18:53:34.320000000 +0300
@@ -0,0 +1,15 @@
+{
+  "name": "media",
+  "display_name": "Media Service",
+  "interface_provider_specs": {
+    "service_manager:connector": {
+      "provides": {
+        "media:media": [ "media.mojom.MediaService" ]
+      },
+      "requires": {
+        "*": [ "app" ],
+        "chromecast": [ "multizone" ]
+      }
+    }
+  }
+}
--- a/media/mojo/services/pipeline_apptest_manifest.json	1970-01-01 03:00:00.000000000 +0300
+++ b/media/mojo/services/pipeline_apptest_manifest.json	2019-05-17 18:53:34.328000000 +0300
@@ -0,0 +1,11 @@
+{
+  "name": "media_pipeline_integration_apptests",
+  "display_name": "Media Pipeline Integration Apptests",
+  "interface_provider_specs": {
+    "service_manager:connector": {
+      "requires": {
+        "*": [ "app" ]
+      }
+    }
+  }
+}
--- a/media/mojo/services/pipeline_test_manifest.json	1970-01-01 03:00:00.000000000 +0300
+++ b/media/mojo/services/pipeline_test_manifest.json	2019-05-17 18:53:34.328000000 +0300
@@ -0,0 +1,11 @@
+{
+  "name": "media_pipeline_integration_unittests",
+  "display_name": "Media Pipeline Integration Unittests",
+  "interface_provider_specs": {
+    "service_manager:connector": {
+      "requires": {
+        "*": [ "app" ]
+      }
+    }
+  }
+}
--- a/media/mojo/services/test_manifest.json	1970-01-01 03:00:00.000000000 +0300
+++ b/media/mojo/services/test_manifest.json	2019-05-17 18:53:34.328000000 +0300
@@ -0,0 +1,12 @@
+{
+  "name": "media_service_unittests",
+  "display_name": "Media Service Unittests",
+  "interface_provider_specs": {
+    "service_manager:connector": {
+      "requires": {
+        "*": [ "app" ],
+        "media": [ "media:media" ]
+      }
+    }
+  }
+}
--- a/media/renderers/flinging_renderer_client_factory.cc	1970-01-01 03:00:00.000000000 +0300
+++ b/media/renderers/flinging_renderer_client_factory.cc	2019-05-17 18:53:34.340000000 +0300
@@ -0,0 +1,41 @@
+// Copyright 2018 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "media/renderers/flinging_renderer_client_factory.h"
+
+#include "base/logging.h"
+#include "media/base/overlay_info.h"
+
+namespace media {
+
+FlingingRendererClientFactory::FlingingRendererClientFactory(
+    std::unique_ptr<RendererFactory> mojo_flinging_factory,
+    std::unique_ptr<RemotePlaybackClientWrapper> remote_playback_client)
+    : mojo_flinging_factory_(std::move(mojo_flinging_factory)),
+      remote_playback_client_(std::move(remote_playback_client)) {}
+
+FlingingRendererClientFactory::~FlingingRendererClientFactory() = default;
+
+std::unique_ptr<Renderer> FlingingRendererClientFactory::CreateRenderer(
+    const scoped_refptr<base::SingleThreadTaskRunner>& media_task_runner,
+    const scoped_refptr<base::TaskRunner>& worker_task_runner,
+    AudioRendererSink* audio_renderer_sink,
+    VideoRendererSink* video_renderer_sink,
+    const RequestOverlayInfoCB& request_overlay_info_cb,
+    const gfx::ColorSpace& target_color_space) {
+  DCHECK(IsFlingingActive());
+  return mojo_flinging_factory_->CreateRenderer(
+      media_task_runner, worker_task_runner, audio_renderer_sink,
+      video_renderer_sink, request_overlay_info_cb, target_color_space);
+}
+
+std::string FlingingRendererClientFactory::GetActivePresentationId() {
+  return remote_playback_client_->GetActivePresentationId();
+}
+
+bool FlingingRendererClientFactory::IsFlingingActive() {
+  return !GetActivePresentationId().empty();
+}
+
+}  // namespace media
--- a/media/renderers/flinging_renderer_client_factory.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/renderers/flinging_renderer_client_factory.h	2019-05-17 18:53:34.340000000 +0300
@@ -0,0 +1,55 @@
+// Copyright 2018 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef MEDIA_RENDERERS_FLINGING_RENDERER_CLIENT_FACTORY_H_
+#define MEDIA_RENDERERS_FLINGING_RENDERER_CLIENT_FACTORY_H_
+
+#include "media/base/media_export.h"
+#include "media/base/renderer_factory.h"
+#include "media/renderers/remote_playback_client_wrapper.h"
+
+namespace media {
+
+// Creates a renderer for media flinging.
+// The FRCF uses a MojoRendererFactory to create a FlingingRenderer in the
+// browser process. The actual renderer returned by the FRCF is a MojoRenderer
+// directly (as opposed to a dedicated FlingingRendererClient), because all the
+// renderer needs to do is forward calls to the FlingingRenderer in the browser.
+class MEDIA_EXPORT FlingingRendererClientFactory : public RendererFactory {
+ public:
+  // |mojo_flinging_factory| should be created using
+  // HostedRendererType::kFlinging, and GetActivePresentationId()
+  // should be given to it through SetGetTypeSpecificIdCB().
+  FlingingRendererClientFactory(
+      std::unique_ptr<RendererFactory> mojo_flinging_factory,
+      std::unique_ptr<RemotePlaybackClientWrapper> remote_playback_client);
+  ~FlingingRendererClientFactory() override;
+
+  std::unique_ptr<Renderer> CreateRenderer(
+      const scoped_refptr<base::SingleThreadTaskRunner>& media_task_runner,
+      const scoped_refptr<base::TaskRunner>& worker_task_runner,
+      AudioRendererSink* audio_renderer_sink,
+      VideoRendererSink* video_renderer_sink,
+      const RequestOverlayInfoCB& request_overlay_info_cb,
+      const gfx::ColorSpace& target_color_space) override;
+
+  // Returns whether media flinging has started, based off of whether the
+  // |remote_playback_client_| has a presentation ID or not. Called by
+  // RendererFactorySelector to determine when to create a FlingingRenderer.
+  bool IsFlingingActive();
+
+  // Used by the |mojo_flinging_factory_| to retrieve the latest presentation ID
+  // when CreateRenderer() is called.
+  std::string GetActivePresentationId();
+
+ private:
+  std::unique_ptr<RendererFactory> mojo_flinging_factory_;
+  std::unique_ptr<RemotePlaybackClientWrapper> remote_playback_client_;
+
+  DISALLOW_COPY_AND_ASSIGN(FlingingRendererClientFactory);
+};
+
+}  // namespace media
+
+#endif  // MEDIA_RENDERERS_FLINGING_RENDERER_CLIENT_FACTORY_H_
--- a/media/test/mock_media_source.cc	1970-01-01 03:00:00.000000000 +0300
+++ b/media/test/mock_media_source.cc	2019-05-17 18:53:35.000000000 +0300
@@ -0,0 +1,285 @@
+// Copyright 2017 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "media/test/mock_media_source.h"
+
+#include "base/bind.h"
+#include "base/bind_helpers.h"
+#include "base/threading/thread_task_runner_handle.h"
+#include "media/base/test_data_util.h"
+#include "media/base/timestamp_constants.h"
+
+namespace {
+
+// Copies parsed type and codecs from |mimetype| into |type| and |codecs|.
+// This code assumes that |mimetype| is one of the following forms:
+// 1. mimetype without codecs (e.g. audio/mpeg)
+// 2. mimetype with codecs (e.g. video/webm; codecs="vorbis,vp8")
+void SplitMime(const std::string& mimetype,
+               std::string* type,
+               std::string* codecs) {
+  DCHECK(type);
+  DCHECK(codecs);
+  size_t semicolon = mimetype.find(";");
+  if (semicolon == std::string::npos) {
+    *type = mimetype;
+    *codecs = "";
+    return;
+  }
+
+  *type = mimetype.substr(0, semicolon);
+  size_t codecs_param_start = mimetype.find("codecs=\"", semicolon);
+  CHECK_NE(codecs_param_start, std::string::npos);
+  codecs_param_start += 8;  // Skip over the codecs=".
+  size_t codecs_param_end = mimetype.find("\"", codecs_param_start);
+  CHECK_NE(codecs_param_end, std::string::npos);
+  *codecs = mimetype.substr(codecs_param_start,
+                            codecs_param_end - codecs_param_start);
+}
+
+}  // namespace
+
+namespace media {
+
+constexpr char kSourceId[] = "SourceId";
+
+MockMediaSource::MockMediaSource(const std::string& filename,
+                                 const std::string& mimetype,
+                                 size_t initial_append_size,
+                                 bool initial_sequence_mode)
+    : current_position_(0),
+      initial_append_size_(initial_append_size),
+      initial_sequence_mode_(initial_sequence_mode),
+      mimetype_(mimetype),
+      chunk_demuxer_(new ChunkDemuxer(
+          base::Bind(&MockMediaSource::DemuxerOpened, base::Unretained(this)),
+          base::DoNothing(),
+          base::BindRepeating(&MockMediaSource::OnEncryptedMediaInitData,
+                              base::Unretained(this)),
+          &media_log_)),
+      owned_chunk_demuxer_(chunk_demuxer_) {
+  file_data_ = ReadTestDataFile(filename);
+
+  if (initial_append_size_ == kAppendWholeFile)
+    initial_append_size_ = file_data_->data_size();
+
+  CHECK_GT(initial_append_size_, 0u);
+  CHECK_LE(initial_append_size_, file_data_->data_size());
+}
+
+MockMediaSource::MockMediaSource(const std::string& filename,
+                                 size_t initial_append_size,
+                                 bool initial_sequence_mode)
+    : MockMediaSource(filename,
+                      GetMimeTypeForFile(filename),
+                      initial_append_size,
+                      initial_sequence_mode) {}
+
+MockMediaSource::MockMediaSource(scoped_refptr<DecoderBuffer> data,
+                                 const std::string& mimetype,
+                                 size_t initial_append_size,
+                                 bool initial_sequence_mode)
+    : file_data_(data),
+      current_position_(0),
+      initial_append_size_(initial_append_size),
+      initial_sequence_mode_(initial_sequence_mode),
+      mimetype_(mimetype),
+      chunk_demuxer_(new ChunkDemuxer(
+          base::Bind(&MockMediaSource::DemuxerOpened, base::Unretained(this)),
+          base::DoNothing(),
+          base::BindRepeating(&MockMediaSource::OnEncryptedMediaInitData,
+                              base::Unretained(this)),
+          &media_log_)),
+      owned_chunk_demuxer_(chunk_demuxer_) {
+  if (initial_append_size_ == kAppendWholeFile)
+    initial_append_size_ = file_data_->data_size();
+
+  CHECK_GT(initial_append_size_, 0u);
+  CHECK_LE(initial_append_size_, file_data_->data_size());
+}
+
+MockMediaSource::~MockMediaSource() = default;
+
+std::unique_ptr<Demuxer> MockMediaSource::GetDemuxer() {
+  return std::move(owned_chunk_demuxer_);
+}
+
+void MockMediaSource::SetAppendWindow(base::TimeDelta timestamp_offset,
+                                      base::TimeDelta append_window_start,
+                                      base::TimeDelta append_window_end) {
+  last_timestamp_offset_ = timestamp_offset;
+  append_window_start_ = append_window_start;
+  append_window_end_ = append_window_end;
+}
+
+void MockMediaSource::Seek(base::TimeDelta seek_time,
+                           size_t new_position,
+                           size_t seek_append_size) {
+  chunk_demuxer_->StartWaitingForSeek(seek_time);
+
+  chunk_demuxer_->ResetParserState(kSourceId, base::TimeDelta(),
+                                   kInfiniteDuration, &last_timestamp_offset_);
+
+  CHECK_LT(new_position, file_data_->data_size());
+  current_position_ = new_position;
+
+  AppendData(seek_append_size);
+}
+
+void MockMediaSource::Seek(base::TimeDelta seek_time) {
+  chunk_demuxer_->StartWaitingForSeek(seek_time);
+}
+
+void MockMediaSource::SetSequenceMode(bool sequence_mode) {
+  CHECK(!chunk_demuxer_->IsParsingMediaSegment(kSourceId));
+  chunk_demuxer_->SetSequenceMode(kSourceId, sequence_mode);
+}
+
+void MockMediaSource::AppendData(size_t size) {
+  CHECK(chunk_demuxer_);
+  CHECK_LT(current_position_, file_data_->data_size());
+  CHECK_LE(current_position_ + size, file_data_->data_size());
+
+  bool success = chunk_demuxer_->AppendData(
+      kSourceId, file_data_->data() + current_position_, size,
+      append_window_start_, append_window_end_, &last_timestamp_offset_);
+  current_position_ += size;
+
+  VerifyExpectedAppendResult(success);
+
+  if (do_eos_after_next_append_) {
+    do_eos_after_next_append_ = false;
+    if (success)
+      EndOfStream();
+  }
+}
+
+bool MockMediaSource::AppendAtTime(base::TimeDelta timestamp_offset,
+                                   const uint8_t* pData,
+                                   int size) {
+  CHECK(!chunk_demuxer_->IsParsingMediaSegment(kSourceId));
+  bool success =
+      chunk_demuxer_->AppendData(kSourceId, pData, size, append_window_start_,
+                                 append_window_end_, &timestamp_offset);
+  last_timestamp_offset_ = timestamp_offset;
+  return success;
+}
+
+void MockMediaSource::AppendAtTimeWithWindow(
+    base::TimeDelta timestamp_offset,
+    base::TimeDelta append_window_start,
+    base::TimeDelta append_window_end,
+    const uint8_t* pData,
+    int size) {
+  CHECK(!chunk_demuxer_->IsParsingMediaSegment(kSourceId));
+  VerifyExpectedAppendResult(
+      chunk_demuxer_->AppendData(kSourceId, pData, size, append_window_start,
+                                 append_window_end, &timestamp_offset));
+  last_timestamp_offset_ = timestamp_offset;
+}
+
+void MockMediaSource::SetMemoryLimits(size_t limit_bytes) {
+  chunk_demuxer_->SetMemoryLimitsForTest(DemuxerStream::AUDIO, limit_bytes);
+  chunk_demuxer_->SetMemoryLimitsForTest(DemuxerStream::VIDEO, limit_bytes);
+}
+
+bool MockMediaSource::EvictCodedFrames(base::TimeDelta currentMediaTime,
+                                       size_t newDataSize) {
+  return chunk_demuxer_->EvictCodedFrames(kSourceId, currentMediaTime,
+                                          newDataSize);
+}
+
+void MockMediaSource::RemoveRange(base::TimeDelta start, base::TimeDelta end) {
+  chunk_demuxer_->Remove(kSourceId, start, end);
+}
+
+void MockMediaSource::EndOfStream() {
+  chunk_demuxer_->MarkEndOfStream(PIPELINE_OK);
+}
+
+void MockMediaSource::UnmarkEndOfStream() {
+  chunk_demuxer_->UnmarkEndOfStream();
+}
+
+void MockMediaSource::Shutdown() {
+  if (!chunk_demuxer_)
+    return;
+  chunk_demuxer_->ResetParserState(kSourceId, base::TimeDelta(),
+                                   kInfiniteDuration, &last_timestamp_offset_);
+  chunk_demuxer_->Shutdown();
+  chunk_demuxer_ = NULL;
+}
+
+void MockMediaSource::DemuxerOpened() {
+  base::ThreadTaskRunnerHandle::Get()->PostTask(
+      FROM_HERE,
+      base::Bind(&MockMediaSource::DemuxerOpenedTask, base::Unretained(this)));
+}
+
+void MockMediaSource::DemuxerOpenedTask() {
+  ChunkDemuxer::Status status = AddId();
+  if (status != ChunkDemuxer::kOk) {
+    CHECK(demuxer_failure_cb_);
+    demuxer_failure_cb_.Run(DEMUXER_ERROR_COULD_NOT_OPEN);
+    return;
+  }
+  chunk_demuxer_->SetTracksWatcher(
+      kSourceId, base::Bind(&MockMediaSource::InitSegmentReceived,
+                            base::Unretained(this)));
+
+  chunk_demuxer_->SetParseWarningCallback(
+      kSourceId,
+      base::Bind(&MockMediaSource::OnParseWarningMock, base::Unretained(this)));
+
+  SetSequenceMode(initial_sequence_mode_);
+  AppendData(initial_append_size_);
+}
+
+ChunkDemuxer::Status MockMediaSource::AddId() {
+  std::string type;
+  std::string codecs;
+  SplitMime(mimetype_, &type, &codecs);
+  return chunk_demuxer_->AddId(kSourceId, type, codecs);
+}
+
+void MockMediaSource::ChangeType(const std::string& mimetype) {
+  chunk_demuxer_->ResetParserState(kSourceId, base::TimeDelta(),
+                                   kInfiniteDuration, &last_timestamp_offset_);
+  std::string type;
+  std::string codecs;
+  SplitMime(mimetype, &type, &codecs);
+  mimetype_ = mimetype;
+  chunk_demuxer_->ChangeType(kSourceId, type, codecs);
+}
+
+void MockMediaSource::OnEncryptedMediaInitData(
+    EmeInitDataType init_data_type,
+    const std::vector<uint8_t>& init_data) {
+  CHECK(!init_data.empty());
+  CHECK(encrypted_media_init_data_cb_);
+  encrypted_media_init_data_cb_.Run(init_data_type, init_data);
+}
+
+void MockMediaSource::InitSegmentReceived(std::unique_ptr<MediaTracks> tracks) {
+  CHECK(tracks.get());
+  EXPECT_GT(tracks->tracks().size(), 0u);
+  CHECK(chunk_demuxer_);
+  // Verify that track ids are unique.
+  std::set<MediaTrack::Id> track_ids;
+  for (const auto& track : tracks->tracks()) {
+    EXPECT_EQ(track_ids.end(), track_ids.find(track->id()));
+    track_ids.insert(track->id());
+  }
+  InitSegmentReceivedMock(tracks);
+}
+
+void MockMediaSource::VerifyExpectedAppendResult(bool append_result) {
+  if (expected_append_result_ == ExpectedAppendResult::kSuccessOrFailure)
+    return;  // |append_result| is ignored in this case.
+
+  ASSERT_EQ(expected_append_result_ == ExpectedAppendResult::kSuccess,
+            append_result);
+}
+
+}  // namespace media
--- a/media/test/mock_media_source.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/test/mock_media_source.h	2019-05-17 18:53:35.000000000 +0300
@@ -0,0 +1,130 @@
+// Copyright 2017 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef MEDIA_TEST_MOCK_MEDIA_SOURCE_H_
+#define MEDIA_TEST_MOCK_MEDIA_SOURCE_H_
+
+#include <limits>
+
+#include "base/time/time.h"
+#include "media/base/demuxer.h"
+#include "media/base/media_util.h"
+#include "media/base/pipeline_status.h"
+#include "media/filters/chunk_demuxer.h"
+#include "testing/gmock/include/gmock/gmock.h"
+
+namespace media {
+
+// Indicates that the whole file should be appended.
+constexpr size_t kAppendWholeFile = std::numeric_limits<size_t>::max();
+
+// Helper class that emulates calls made on the ChunkDemuxer by the
+// Media Source API.
+class MockMediaSource {
+ public:
+  enum class ExpectedAppendResult {
+    kSuccess,
+    kFailure,
+    kSuccessOrFailure,  // e.g., for fuzzing when parse may pass or fail
+  };
+
+  MockMediaSource(const std::string& filename,
+                  const std::string& mimetype,
+                  size_t initial_append_size,
+                  bool initial_sequence_mode = false);
+  // Same as the constructor above, but use GetMimeTypeForFile() to get the mime
+  // type.
+  MockMediaSource(const std::string& filename,
+                  size_t initial_append_size,
+                  bool initial_sequence_mode = false);
+  MockMediaSource(scoped_refptr<DecoderBuffer> data,
+                  const std::string& mimetype,
+                  size_t initial_append_size,
+                  bool initial_sequence_mode = false);
+  ~MockMediaSource();
+
+  std::unique_ptr<Demuxer> GetDemuxer();
+
+  void set_encrypted_media_init_data_cb(
+      const Demuxer::EncryptedMediaInitDataCB& encrypted_media_init_data_cb) {
+    encrypted_media_init_data_cb_ = encrypted_media_init_data_cb;
+  }
+
+  void set_demuxer_failure_cb(const PipelineStatusCB& demuxer_failure_cb) {
+    demuxer_failure_cb_ = demuxer_failure_cb;
+  }
+
+  void set_do_eos_after_next_append(bool flag) {
+    do_eos_after_next_append_ = flag;
+  }
+
+  void SetAppendWindow(base::TimeDelta timestamp_offset,
+                       base::TimeDelta append_window_start,
+                       base::TimeDelta append_window_end);
+
+  void Seek(base::TimeDelta seek_time,
+            size_t new_position,
+            size_t seek_append_size);
+  void Seek(base::TimeDelta seek_time);
+  void SetSequenceMode(bool sequence_mode);
+  void AppendData(size_t size);
+  bool AppendAtTime(base::TimeDelta timestamp_offset,
+                    const uint8_t* pData,
+                    int size);
+  void AppendAtTimeWithWindow(base::TimeDelta timestamp_offset,
+                              base::TimeDelta append_window_start,
+                              base::TimeDelta append_window_end,
+                              const uint8_t* pData,
+                              int size);
+  void SetMemoryLimits(size_t limit_bytes);
+  bool EvictCodedFrames(base::TimeDelta currentMediaTime, size_t newDataSize);
+  void RemoveRange(base::TimeDelta start, base::TimeDelta end);
+  void EndOfStream();
+  void UnmarkEndOfStream();
+  void Shutdown();
+  void DemuxerOpened();
+  void DemuxerOpenedTask();
+  ChunkDemuxer::Status AddId();
+  void ChangeType(const std::string& type);
+  void OnEncryptedMediaInitData(EmeInitDataType init_data_type,
+                                const std::vector<uint8_t>& init_data);
+
+  base::TimeDelta last_timestamp_offset() const {
+    return last_timestamp_offset_;
+  }
+
+  void set_expected_append_result(ExpectedAppendResult expectation) {
+    expected_append_result_ = expectation;
+  }
+
+  void InitSegmentReceived(std::unique_ptr<MediaTracks> tracks);
+  MOCK_METHOD1(InitSegmentReceivedMock, void(std::unique_ptr<MediaTracks>&));
+
+  MOCK_METHOD1(OnParseWarningMock, void(const SourceBufferParseWarning));
+
+ private:
+  void VerifyExpectedAppendResult(bool append_result);
+
+  NullMediaLog media_log_;
+  scoped_refptr<DecoderBuffer> file_data_;
+  size_t current_position_;
+  size_t initial_append_size_;
+  bool initial_sequence_mode_;
+  std::string mimetype_;
+  ChunkDemuxer* chunk_demuxer_;
+  std::unique_ptr<Demuxer> owned_chunk_demuxer_;
+  PipelineStatusCB demuxer_failure_cb_;
+  Demuxer::EncryptedMediaInitDataCB encrypted_media_init_data_cb_;
+  base::TimeDelta last_timestamp_offset_;
+  base::TimeDelta append_window_start_;
+  base::TimeDelta append_window_end_ = kInfiniteDuration;
+  bool do_eos_after_next_append_ = false;
+  ExpectedAppendResult expected_append_result_ = ExpectedAppendResult::kSuccess;
+
+  DISALLOW_COPY_AND_ASSIGN(MockMediaSource);
+};
+
+}  // namespace media
+
+#endif  // MEDIA_TEST_MOCK_MEDIA_SOURCE_H_
--- a/media/webrtc/echo_information.cc	1970-01-01 03:00:00.000000000 +0300
+++ b/media/webrtc/echo_information.cc	2019-05-17 18:53:35.012000000 +0300
@@ -0,0 +1,66 @@
+// Copyright 2018 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "media/webrtc/echo_information.h"
+
+#include "base/metrics/histogram_macros.h"
+#include "third_party/webrtc/modules/audio_processing/include/audio_processing.h"
+
+namespace media {
+
+EchoInformation::EchoInformation()
+    : divergent_filter_stats_time_ms_(0),
+      num_divergent_filter_fraction_(0),
+      num_non_zero_divergent_filter_fraction_(0) {}
+
+EchoInformation::~EchoInformation() {
+  DCHECK_CALLED_ON_VALID_THREAD(thread_checker_);
+  ReportAndResetAecDivergentFilterStats();
+}
+
+void EchoInformation::UpdateAecStats(
+    const webrtc::AudioProcessingStats& audio_processing_stats) {
+  DCHECK_CALLED_ON_VALID_THREAD(thread_checker_);
+
+  if (!audio_processing_stats.divergent_filter_fraction) {
+    return;
+  }
+
+  divergent_filter_stats_time_ms_ += webrtc::AudioProcessing::kChunkSizeMs;
+  if (divergent_filter_stats_time_ms_ <
+      100 * webrtc::AudioProcessing::kChunkSizeMs) {  // 1 second
+    return;
+  }
+
+  double divergent_filter_fraction =
+      *audio_processing_stats.divergent_filter_fraction;
+  // If not yet calculated, |metrics.divergent_filter_fraction| is -1.0. After
+  // being calculated the first time, it is updated periodically.
+  if (divergent_filter_fraction < 0.0f) {
+    DCHECK_EQ(num_divergent_filter_fraction_, 0);
+    return;
+  }
+  if (divergent_filter_fraction > 0.0f) {
+    ++num_non_zero_divergent_filter_fraction_;
+  }
+  ++num_divergent_filter_fraction_;
+  divergent_filter_stats_time_ms_ = 0;
+}
+
+void EchoInformation::ReportAndResetAecDivergentFilterStats() {
+  DCHECK_CALLED_ON_VALID_THREAD(thread_checker_);
+
+  if (num_divergent_filter_fraction_ == 0)
+    return;
+
+  int non_zero_percent = 100 * num_non_zero_divergent_filter_fraction_ /
+                         num_divergent_filter_fraction_;
+  UMA_HISTOGRAM_PERCENTAGE("WebRTC.AecFilterHasDivergence", non_zero_percent);
+
+  divergent_filter_stats_time_ms_ = 0;
+  num_non_zero_divergent_filter_fraction_ = 0;
+  num_divergent_filter_fraction_ = 0;
+}
+
+}  // namespace media
--- a/media/webrtc/echo_information.h	1970-01-01 03:00:00.000000000 +0300
+++ b/media/webrtc/echo_information.h	2019-05-17 18:53:35.012000000 +0300
@@ -0,0 +1,48 @@
+// Copyright 2018 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef MEDIA_WEBRTC_ECHO_INFORMATION_H_
+#define MEDIA_WEBRTC_ECHO_INFORMATION_H_
+
+#include "base/component_export.h"
+#include "base/threading/thread_checker.h"
+#include "third_party/webrtc/modules/audio_processing/include/audio_processing.h"
+
+namespace media {
+
+// A helper class to log echo information in general and AEC2
+// quality in particular.
+class COMPONENT_EXPORT(MEDIA_WEBRTC) EchoInformation {
+ public:
+  EchoInformation();
+  virtual ~EchoInformation();
+
+  // Updates stats, and reports metrics as UMA stats every 5 seconds.
+  // Must be called every time AudioProcessing::ProcessStream() is called.
+  void UpdateAecStats(
+      const webrtc::AudioProcessingStats& audio_processing_stats);
+
+  // Reports AEC divergent filter metrics as UMA and resets the associated data.
+  void ReportAndResetAecDivergentFilterStats();
+
+ private:
+  // Counter to store a new value for the divergent filter fraction metric in
+  // AEC2, once every second.
+  int divergent_filter_stats_time_ms_;
+
+  // Total number of times we queried for the divergent filter fraction metric.
+  int num_divergent_filter_fraction_;
+
+  // Number of non-zero divergent filter fraction metrics.
+  int num_non_zero_divergent_filter_fraction_;
+
+  // Ensures that this class is accessed on the same thread.
+  THREAD_CHECKER(thread_checker_);
+
+  DISALLOW_COPY_AND_ASSIGN(EchoInformation);
+};
+
+}  // namespace media
+
+#endif  // MEDIA_WEBRTC_ECHO_INFORMATION_H_
